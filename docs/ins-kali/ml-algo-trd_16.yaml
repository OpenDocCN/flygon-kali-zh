- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Word Embeddings for Earnings Calls and SEC Filings
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于收益电话和SEC文件的单词嵌入
- en: In the two previous chapters, we converted text data into a numerical format
    using the **bag-of-words model**. The result is sparse, fixed-length vectors that
    represent documents in high-dimensional word space. This allows the similarity
    of documents to be evaluated and creates features to train a model with a view
    to classifying a document's content or rating the sentiment expressed in it. However,
    these vectors ignore the context in which a term is used so that two sentences
    containing the same words in a different order would be encoded by the same vector,
    even if their meaning is quite different.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们使用**词袋模型**将文本数据转换为数字格式。结果是稀疏的、固定长度的向量，代表了高维词空间中的文档。这允许评估文档的相似性，并创建特征来训练模型，以便对文档的内容进行分类或评估其中表达的情感。然而，这些向量忽略了术语的使用上下文，因此两个包含相同单词但顺序不同的句子将被编码为相同的向量，即使它们的含义完全不同。
- en: This chapter introduces an alternative class of algorithms that use **neural
    networks** to learn a vector representation of individual semantic units like
    a word or a paragraph. These vectors are dense rather than sparse, have a few
    hundred real-valued entries, and are called **embeddings** because they assign
    each semantic unit a location in a continuous vector space. They result from training
    a model to **predict tokens from their context** so that similar usage implies
    a similar embedding vector. Moreover, the embeddings encode semantic aspects like
    relationships among words by means of their relative location. As a result, they
    are powerful features for deep learning models for solving tasks that require
    semantic information, such as machine translation, question answering, or maintaining
    a dialogue.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一类使用**神经网络**的替代算法，用于学习单词或段落等个体语义单元的向量表示。这些向量是密集的而不是稀疏的，具有几百个实值条目，并被称为**嵌入**，因为它们为每个语义单元分配了一个连续向量空间中的位置。它们是通过训练模型来**预测上下文中的标记**而产生的，因此相似的使用意味着相似的嵌入向量。此外，嵌入通过它们的相对位置来编码词语之间的关系等语义方面。因此，它们是解决需要语义信息的深度学习模型的强大特征，如机器翻译、问答或对话维护。
- en: To develop a **trading strategy based on text data**, we are usually interested
    in the meaning of documents rather than individual tokens. For example, we might
    want to create a dataset that uses features representing a tweet or a news article
    with sentiment information (refer to *Chapter 14*, *Text Data for Trading – Sentiment
    Analysis*), or an asset's return for a given horizon after publication. Although
    the bag-of-words model loses plenty of information when encoding text data, it
    has the advantage of representing an entire document. However, word embeddings
    have been further developed to represent more than individual tokens. Examples
    include the **doc2vec** extension, which resorts to weighting word embeddings.
    More recently, the **attention** mechanism emerged to produce more context-sensitive
    sentence representations, resulting in **transformer** architectures such as the
    **BERT** family of models that has dramatically improved performance on numerous
    natural language tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发基于文本数据的**交易策略**，我们通常对文档的含义感兴趣，而不是个别标记。例如，我们可能希望创建一个数据集，其中使用特征表示推文或新闻文章，并包含情感信息（参见*第14章*，*用于交易的文本数据-情感分析*），或者在发布后的给定时间段内资产的回报。尽管在编码文本数据时，词袋模型丢失了大量信息，但它具有代表整个文档的优势。然而，单词嵌入已经进一步发展，以表示不仅仅是个别标记。例如，**doc2vec**扩展采用了加权单词嵌入。最近，**注意力**机制出现，以产生更多上下文敏感的句子表示，从而产生了**变压器**架构，如**BERT**系列模型，它在许多自然语言任务上显着提高了性能。
- en: 'More specifically, after working through this chapter and the companion notebooks,
    you will know about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在阅读本章和相关笔记本后，您将了解以下内容：
- en: What word embeddings are, how they work, and why they capture semantic information
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词嵌入是什么，它们是如何工作的，以及为什么它们捕捉了语义信息
- en: How to obtain and use pretrained word vectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何获取和使用预训练的单词向量
- en: Which network architectures are most effective at training word2vec models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪种网络架构最有效地训练word2vec模型
- en: How to train a word2vec model using Keras, Gensim, and TensorFlow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Keras、Gensim和TensorFlow训练word2vec模型
- en: Visualizing and evaluating the quality of word vectors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和评估单词向量的质量
- en: How to train a word2vec model on SEC filings to predict stock price moves
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在SEC文件中训练word2vec模型以预测股价走势
- en: How doc2vec extends word2vec and can be used for sentiment analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: doc2vec如何扩展word2vec并用于情感分析
- en: Why the transformer's attention mechanism had such an impact on natural language
    processing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么变压器的注意力机制对自然语言处理产生了如此大的影响
- en: How to fine-tune pretrained BERT models on financial data and extract high-quality
    embeddings
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在金融数据上微调预训练的BERT模型并提取高质量的嵌入
- en: You can find the code examples and links to additional resources in the GitHub
    directory for this chapter. This chapter uses neural networks and deep learning;
    if unfamiliar, you may want to first read *Chapter 17*, *Deep Learning for Trading,*
    which introduces key concepts and libraries.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章的GitHub目录中找到代码示例和其他资源的链接。本章使用神经网络和深度学习；如果不熟悉，您可能需要先阅读*第17章*，*交易的深度学习*，介绍了关键概念和库。
- en: How word embeddings encode semantics
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词嵌入如何编码语义
- en: The bag-of-words model represents documents as sparse, high-dimensional vectors
    that reflect the tokens they contain. Word embeddings represent tokens as dense,
    lower-dimensional vectors so that the relative location of words reflects how
    they are used in context. They embody the **distributional hypothesis** from linguistics
    that claims words are best defined by the company they keep.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型将文档表示为稀疏的高维向量，反映它们包含的标记。单词嵌入将标记表示为密集的低维向量，以便单词的相对位置反映它们在上下文中的使用方式。它们体现了语言学中的**分布假设**，即单词的最佳定义是通过它们的搭配来确定。
- en: Word vectors are capable of capturing numerous semantic aspects; not only are
    synonyms assigned nearby embeddings, but words can have multiple degrees of similarity.
    For example, the word "driver" could be similar to "motorist" or to "factor."
    Furthermore, embeddings encode relationships among pairs of words like analogies
    (*Tokyo is to Japan* what *Paris is to France*, or *went is to go* what *saw is
    to see*), as we will illustrate later in this section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量能够捕捉许多语义方面；不仅同义词分配在附近的嵌入中，而且单词可以具有多个相似度。例如，单词“driver”可能类似于“motorist”或“factor”。此外，嵌入编码了词对之间的关系，如类比（*东京是日本*，*巴黎是法国*，或*去是去*，*看见是看*），我们将在本节后面进行说明。
- en: Embeddings result from training a neural network to predict words from their
    context or vice versa. In this section, we will introduce how these models work
    and present successful approaches, including word2vec, doc2vec, and the more recent
    transformer family of models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是通过训练神经网络来预测单词及其上下文，或者反之而产生的。在本节中，我们将介绍这些模型的工作原理，并介绍成功的方法，包括word2vec、doc2vec和更近期的transformer模型系列。
- en: How neural language models learn usage in context
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经语言模型如何学习上下文中的用法
- en: Word embeddings result from training a shallow neural network to predict a word
    given its context. Whereas traditional language models define context as the words
    preceding the target, word embedding models use the words contained in a symmetric
    window surrounding the target. In contrast, the bag-of-words model uses the entire
    document as context and relies on (weighted) counts to capture the co-occurrence
    of words.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入是通过训练一个浅层神经网络来预测给定上下文的单词而产生的。而传统的语言模型将上下文定义为目标之前的单词，单词嵌入模型使用包围目标的对称窗口中包含的单词。相比之下，词袋模型使用整个文档作为上下文，并依赖于（加权）计数来捕捉单词的共现。
- en: Earlier neural language models used included nonlinear hidden layers that increased
    the computational complexity. **word2vec**, introduced by Mikolov, Sutskever,
    et al. (2013) and its extensions simplified the architecture to enable training
    on large datasets. The Wikipedia corpus, for example, contains over 2 billion
    tokens. (Refer to *Chapter 17*, *Deep Learning for Trading*, for additional details
    on feedforward networks.)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的神经语言模型包括增加了计算复杂性的非线性隐藏层。**word2vec**由Mikolov, Sutskever, et al.（2013）引入，并简化了架构以便在大型数据集上进行训练。例如，维基百科语料库包含超过20亿个标记。（有关前馈网络的更多细节，请参阅*第17章*，*交易的深度学习*。）
- en: word2vec – scalable word and phrase embeddings
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: word2vec-可扩展的词和短语嵌入
- en: 'A word2vec model is a two-layer neural net that takes a text corpus as input
    and outputs a set of embedding vectors for words in that corpus. There are two
    different architectures, shown in the following diagram, to efficiently learn
    word vectors using shallow neural networks (Mikolov, Chen, et al., 2013):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型是一个两层神经网络，它以文本语料库作为输入，并输出该语料库中单词的一组嵌入向量。有两种不同的架构，如下图所示，可以使用浅层神经网络高效地学习单词向量（Mikolov,
    Chen, et al., 2013）：
- en: The **continuous-bag-of-words** (**CBOW**) model predicts the target word using
    the average of the context word vectors as input so that their order does not
    matter. CBOW trains faster and tends to be slightly more accurate for frequent
    terms, but pays less attention to infrequent words.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续词袋**（**CBOW**）模型使用上下文词向量的平均值作为输入来预测目标词，因此它们的顺序并不重要。CBOW训练速度更快，对频繁词汇的准确性稍微更高，但对不常见的词汇关注较少。'
- en: The **skip-gram** (**SG**) model, in contrast, uses the target word to predict
    words sampled from the context. It works well with small datasets and finds good
    representations even for rare words or phrases.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**skip-gram**（**SG**）模型相反，使用目标单词来预测从上下文中抽样的单词。它在小数据集上表现良好，并且即使对于罕见的单词或短语也能找到良好的表示。'
- en: '![](img/B15439_16_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_01.png)'
- en: 'Figure 16.1: Continuous-bag-of-words versus skip-gram processing logic'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：连续词袋与skip-gram处理逻辑
- en: The model receives an embedding vector as input and computes the dot product
    with another embedding vector. Note that, assuming normed vectors, the dot product
    is maximized (in absolute terms) when vectors are equal, and minimized when they
    are orthogonal.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型接收嵌入向量作为输入，并与另一个嵌入向量计算点积。请注意，假设规范化向量，当向量相等时，点积最大化（绝对值），当它们正交时最小化。
- en: During training, the **backpropagation algorithm** adjusts the embedding weights
    in response to the loss computed by an objective function based on classification
    errors. We will see in the next section how word2vec computes the loss.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，**反向传播算法**根据基于分类错误的目标函数计算的损失来调整嵌入权重。我们将在下一节中看到word2vec如何计算损失。
- en: Training proceeds by sliding the **context window** over the documents, typically
    segmented into sentences. Each complete iteration over the corpus is called an
    **epoch**. Depending on the data, several dozen epochs may be necessary for vector
    quality to converge.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练通过在文档上滑动**上下文窗口**进行，通常将文档分成句子。对语料库的每次完整迭代称为**时代**。根据数据的不同，可能需要几十个时代才能使向量质量收敛。
- en: The skip-gram model implicitly factorizes a word-context matrix that contains
    the pointwise mutual information of the respective word and context pairs (Levy
    and Goldberg, 2014).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram模型隐式地分解了一个包含相应单词和上下文对的点间互信息的矩阵（Levy and Goldberg, 2014）。
- en: Model objective – simplifying the softmax
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型目标-简化softmax
- en: 'Word2vec models aim to predict a single word out of a potentially very large
    vocabulary. Neural networks often use the softmax function as an output unit in
    the final layer to implement the multiclass objective because it maps an arbitrary
    number of real values to an equal number of probabilities. The softmax function
    is defined as follows, where *h* refers to the embedding and *v* to the input
    vectors, and *c* is the context of word *w*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型旨在从一个可能非常庞大的词汇表中预测单个单词。神经网络通常在最后一层使用softmax函数作为输出单元，以实现多类目标，因为它将任意数量的实数映射到相同数量的概率。softmax函数定义如下，其中*h*指的是嵌入，*v*指的是输入向量，*c*是单词*w*的上下文：
- en: '![](img/B15439_16_001.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_001.png)'
- en: 'However, the softmax complexity scales with the number of classes because the
    denominator requires computing the dot product for all words in the vocabulary
    to standardize the probabilities. Word2vec gains efficiency by using a modified
    version of the softmax or sampling-based approximations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于分母需要计算词汇表中所有单词的点积以标准化概率，softmax的复杂度随着类别数量的增加而增加。Word2vec通过使用softmax的修改版本或基于采样的近似来提高效率：
- en: The **hierarchical softmax** organizes the vocabulary as a binary tree with
    words as leaf nodes. The unique path to each node can be used to compute the word
    probability (Morin and Bengio, 2005).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分层softmax**将词汇表组织为具有单词作为叶节点的二叉树。到每个节点的唯一路径可用于计算单词概率（Morin和Bengio，2005）。'
- en: '**Noise contrastive estimation** (**NCE**) samples out-of-context "noise words"
    and approximates the multiclass task by a binary classification problem. The NCE
    derivative approaches the softmax gradient as the number of samples increases,
    but as few as 25 samples can yield convergence similar to the softmax 45 times
    faster (Mnih and Kavukcuoglu, 2013).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声对比估计**（**NCE**）对上下文之外的“噪声词”进行采样，并通过二元分类问题来近似多类任务。随着样本数量的增加，NCE的导数逼近softmax梯度，但只需25个样本就可以实现与softmax相似的收敛速度，快45倍（Mnih和Kavukcuoglu，2013）。'
- en: '**Negative sampling** (**NEG**) omits the noise word samples to approximate
    NCE and directly maximizes the probability of the target word. Hence, NEG optimizes
    the semantic quality of embedding vectors (similar vectors for similar usage)
    rather than the accuracy on a test set. It may, however, produce poorer representations
    for infrequent words than the hierarchical softmax objective (Mikolov et al.,
    2013).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负采样**（**NEG**）省略了噪声词样本以近似NCE，并直接最大化目标词的概率。因此，NEG优化了嵌入向量的语义质量（相似用法的相似向量），而不是测试集上的准确性。然而，对于不经常出现的词，它可能产生比分层softmax目标更差的表示（Mikolov等，2013）。'
- en: Automating phrase detection
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化短语检测
- en: Preprocessing typically involves phrase detection, that is, the identification
    of tokens that are commonly used together and should receive a single vector representation
    (for example, New York City; refer to the discussion of n-grams in *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理通常涉及短语检测，即识别常用在一起的标记，并应该接收单个向量表示（例如，纽约市；参见*第13章*中对n-gram的讨论，*使用无监督学习的数据驱动风险因素和资产配置*）。
- en: 'The original word2vec authors (Mikolov et al., 2013) use a simple lift scoring
    method that identifies two words *w*[i], *w*[j] as a bigram if their joint occurrence
    exceeds a given threshold relative to each word''s individual appearance, corrected
    by a discount factor, *δ*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec的原始作者（Mikolov等，2013）使用一种简单的提升评分方法，如果它们的联合出现超过给定阈值相对于每个词的单独出现，通过一个折扣因子*δ*进行校正，就将两个词*w*[i]，*w*[j]标识为二元组：
- en: '![](img/B15439_16_002.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_002.png)'
- en: The scorer can be applied repeatedly to identify successively longer phrases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 评分器可以重复应用，以识别连续更长的短语。
- en: 'An alternative is the normalized pointwise mutual information score, which
    is more accurate, but also more costly to compute. It uses the relative word frequency
    *P*(*w*) and varies between +1 and -1:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是归一化的点间互信息分数，这更准确，但计算成本更高。它使用相对词频*P*(*w*)，并在+1和-1之间变化：
- en: '![](img/B15439_16_003.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_003.png)'
- en: Evaluating embeddings using semantic arithmetic
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用语义算术评估嵌入
- en: The bag-of-words model creates document vectors that reflect the presence and
    relevance of tokens to the document. As discussed in *Chapter 15*, *Topic Modeling
    – Summarizing Financial News*, **latent semantic analysis** reduces the dimensionality
    of these vectors and identifies what can be interpreted as latent concepts in
    the process. **Latent Dirichlet allocation** represents both documents and terms
    as vectors that contain the weights of latent topics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型创建了反映标记对文档的存在和相关性的文档向量。如*第15章*中所讨论的*主题建模-总结金融新闻*，**潜在语义分析**减少了这些向量的维度，并在过程中确定了可以解释为潜在概念的内容。**潜在狄利克雷分配**将文档和术语都表示为包含潜在主题权重的向量。
- en: The word and phrase vectors produced by word2vec do not have an explicit meaning.
    However, the **embeddings encode similar usage as proximity** in the latent space
    created by the model. The embeddings also capture semantic relationships so that
    analogies can be expressed by adding and subtracting word vectors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec生成的单词和短语向量没有明确的含义。然而，**嵌入在模型创建的潜在空间中的接近性编码了相似的用法**。嵌入还捕获了语义关系，因此可以通过添加和减去单词向量来表示类比。
- en: '*Figure 16.2* shows how the vector that points from "Paris" to "France" (which
    measures the difference between their embedding vectors) reflects the "capital
    of" relationship. The analogous relationship between London and the UK corresponds
    to the same vector: the embedding for the term "UK" is very close to the location
    obtained by adding the "capital of" vector to the embedding for the term "London":'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图16.2*显示了从“巴黎”指向“法国”的向量（衡量它们嵌入向量之间的差异）如何反映“首都”的关系。伦敦和英国之间的类似关系对应于相同的向量：术语“英国”的嵌入非常接近通过将“首都”的向量添加到术语“伦敦”的嵌入而获得的位置：'
- en: '![](img/B15439_16_02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_02.png)'
- en: 'Figure 16.2: Embedding vector arithmetic'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：嵌入向量算术
- en: Just as words can be used in different contexts, they can be related to other
    words in different ways, and these relationships correspond to different directions
    in the latent space. Accordingly, there are several types of analogies that the
    embeddings should reflect if the training data permits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 就像单词可以在不同的上下文中使用一样，它们可以以不同的方式与其他单词相关联，这些关系对应于潜在空间中的不同方向。因此，如果训练数据允许，嵌入应该反映出几种类型的类比。
- en: 'The word2vec authors provide a list of over 25,000 relationships in 14 categories
    spanning aspects of geography, grammar and syntax, and family relationships to
    evaluate the quality of embedding vectors. As illustrated in the preceding diagram,
    the test validates that the target word "UK" is closest to the result of adding
    the vector that represents an analogous relationship "Paris: France" to the target''s
    complement "London".'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec的作者提供了一个包含14个类别的超过25,000个关系的列表，涵盖了地理、语法和句法以及家庭关系的各个方面，以评估嵌入向量的质量。如前图所示，该测试验证了目标词“UK”与表示类比关系“巴黎：法国”的向量添加到目标的补充“伦敦”的结果最接近。
- en: The following table shows the number of samples and illustrates some of the
    analogy categories. The test checks how close the embedding for *d* is to the
    location determined by *c + (b-a)*. Refer to the `evaluating_embeddings` notebook
    for implementation details.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了样本数量，并说明了一些类比类别。该测试检查了*d*的嵌入与*c+(b-a)*确定的位置有多接近。有关实施细节，请参阅`evaluating_embeddings`笔记本。
- en: '| Category | # Samples | a | b | c | d |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | #样本 | a | b | c | d |'
- en: '| Capital-Country | 506 | athens | greece | baghdad | iraq |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 首都-国家 | 506 | 雅典 | 希腊 | 巴格达 | 伊拉克 |'
- en: '| City-State | 4,242 | chicago | illinois | houston | texas |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 城市-州 | 4,242 | 芝加哥 | 伊利诺伊 | 休斯顿 | 德克萨斯 |'
- en: '| Past Tense | 1,560 | dancing | danced | decreasing | decreased |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: 过去式 | 1,560 | 跳舞 | 跳舞 | 减少 | 减少 |
- en: '| Plural | 1,332 | banana | bananas | bird | birds |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 复数 | 1,332 | 香蕉 | 香蕉 | 鸟 | 鸟 |'
- en: '| Comparative | 1,332 | bad | worse | big | bigger |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 比较级 | 1,332 | 差 | 更差 | 大 | 更大 |'
- en: '| Opposite | 812 | acceptable | unacceptable | aware | unaware |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: 相反 | 812 | 可接受 | 不可接受 | 察觉 | 未察觉 |
- en: '| Superlative | 1,122 | bad | worst | big | biggest |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: 超级| 1,122 | 差 | 最差 | 大 | 最大 |
- en: '| Plural (Verbs) | 870 | decrease | decreases | describe | describes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 复数（动词） | 870 | 减少 | 减少 | 描述 | 描述 |'
- en: '| Currency | 866 | algeria | dinar | angola | kwanza |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 货币 | 866 | 阿尔及利亚 | 第纳尔 | 安哥拉 | 宽扎 |'
- en: '| Family | 506 | boy | girl | brother | sister |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: 家庭 | 506 | 男孩 | 女孩 | 兄弟 | 姐妹 |
- en: 'Similar to other unsupervised learning techniques, the goal of learning embedding
    vectors is to generate features for other tasks, such as text classification or
    sentiment analysis. There are a couple of options to obtain embedding vectors
    for a given corpus of documents:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他无监督学习技术类似，学习嵌入向量的目标是为其他任务生成特征，例如文本分类或情感分析。有几种选项可以获得给定文档语料库的嵌入向量：
- en: Use pretrained embeddings learned from a generic large corpus like Wikipedia
    or Google News
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从维基百科或Google新闻等通用大语料库学习的预训练嵌入
- en: Train your own model using documents that reflect a domain of interest
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反映感兴趣领域的文档训练自己的模型。
- en: The less generic and more specialized the content of the subsequent text modeling
    task, the more preferable the second approach. However, quality word embeddings
    are data-hungry and require informative documents containing hundreds of millions
    of words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 后续文本建模任务的内容越专业化，第二种方法就越可取。然而，高质量的词嵌入需要包含数亿词的信息性文档。
- en: We will first look at how you can use pretrained vectors and then demonstrate
    examples of how to build your own word2vec models using financial news and SEC
    filings data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看看如何使用预训练向量，然后演示如何使用金融新闻和SEC申报数据构建自己的word2vec模型的示例。
- en: How to use pretrained word vectors
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用预训练词向量
- en: There are several sources for pretrained word embeddings. Popular options include
    Stanford's GloVE and spaCy's built-in vectors (refer to the `using_pretrained_vectors`
    notebook for details). In this section, we will focus on GloVe.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种预训练词嵌入的来源。流行的选择包括斯坦福的GloVE和spaCy的内置向量（有关详细信息，请参阅`using_pretrained_vectors`笔记本）。在本节中，我们将重点关注GloVe。
- en: GloVe – Global vectors for word representation
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GloVe - 用于词表示的全局向量
- en: 'GloVe (*Global Vectors for Word Representation*, Pennington, Socher, and Manning,
    2014) is an unsupervised algorithm developed at the Stanford NLP lab that learns
    vector representations for words from aggregated global word-word co-occurrence
    statistics (see resources linked on GitHub). Vectors pretrained on the following
    web-scale sources are available:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe（*全局词表示向量*，Pennington，Socher和Manning，2014）是在斯坦福NLP实验室开发的无监督算法，它从聚合的全局词-词共现统计中学习单词的向量表示（请参阅GitHub上链接的资源）。可用于以下网络规模的预训练向量：
- en: '**Common Crawl** with 42 billion or 840 billion tokens and a vocabulary or
    1.9 million or 2.2 million tokens'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Common Crawl**，拥有420亿或840亿标记和190万或220万标记的词汇'
- en: '**Wikipedia** 2014 + Gigaword 5 with 6 billion tokens and a vocabulary of 400,000
    tokens'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维基百科** 2014 + Gigaword 5，拥有60亿标记和40万标记的词汇量'
- en: '**Twitter** using 2 billion tweets, 27 billion tokens, and a vocabulary of
    1.2 million tokens'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Twitter** 使用20亿推文，270亿标记和120万标记的词汇'
- en: 'We can use Gensim to convert the vector text files using `glove2word2vec` and
    then load them into the `KeyedVector` object:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Gensim将向量文本文件转换为`glove2word2vec`，然后将它们加载到`KeyedVector`对象中：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Gensim uses the **word2vec analogy tests** described in the previous section
    using text files made available by the authors to evaluate word vectors. For this
    purpose, the library has the `wv.accuracy` function, which we use to pass the
    path to the analogy file, indicate whether the word vectors are in binary format,
    and whether we want to ignore the case. We can also restrict the vocabulary to
    the most frequent to speed up testing:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim使用了前一节中描述的**word2vec类比测试**，使用作者提供的文本文件来评估词向量。为此，该库具有`wv.accuracy`函数，我们使用它来传递类比文件的路径，指示词向量是否以二进制格式，以及是否要忽略大小写。我们还可以将词汇限制为最常见的词以加快测试速度：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The word vectors trained on the Wikipedia corpus cover all analogies and achieve
    an overall accuracy of 75.44 percent with some variation across categories:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在维基百科语料库上训练的词向量涵盖了所有类比，并在各个类别上实现了75.44%的总体准确性：
- en: '| Category | # Samples | Accuracy |  | Category | # Samples | Accuracy |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | #样本 | 准确性 | | 类别 | #样本 | 准确性 |'
- en: '| Capital-Country | 506 | 94.86% |  | Comparative | 1,332 | 88.21% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 首都-国家 | 506 | 94.86% |  | 比较级 | 1,332 | 88.21% |'
- en: '| Capitals RoW | 8,372 | 96.46% |  | Opposite | 756 | 28.57% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 全球首都 | 8,372 | 96.46% |  | 相反 | 756 | 28.57% |'
- en: '| City-State | 4,242 | 60.00% |  | Superlative | 1,056 | 74.62% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 城邦 | 4,242 | 60.00% |  | 最高级 | 1,056 | 74.62% |'
- en: '| Currency | 752 | 17.42% |  | Present-Participle | 1,056 | 69.98% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 货币 | 752 | 17.42% |  | 现在分词 | 1,056 | 69.98% |'
- en: '| Family | 506 | 88.14% |  | Past Tense | 1,560 | 61.15% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 家庭 | 506 | 88.14% |  | 过去时 | 1,560 | 61.15% |'
- en: '| Nationality | 1,640 | 92.50% |  | Plural | 1,332 | 78.08% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 国籍 | 1,640 | 92.50% |  | 复数 | 1,332 | 78.08% |'
- en: '| Adjective-Adverb | 992 | 22.58% |  | Plural Verbs | 870 | 58.51% |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 形容词-副词 | 992 | 22.58% |  | 复数动词 | 870 | 58.51% |'
- en: '*Figure 16.3* compares the performance for the three GloVe sources for the
    100,000 most common tokens. It shows that Common Crawl vectors, which cover about
    80 percent of the analogies, achieve slightly higher accuracy at 78 percent. The
    Twitter vectors cover only 25 percent, with 56.4 percent accuracy:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图16.3*比较了对最常见的10万个标记的三个GloVe来源的性能。它显示了Common Crawl向量在大约80%的类比上实现了略高于78%的准确率。Twitter向量仅覆盖25%，准确率为56.4%：'
- en: '![](img/B15439_16_03.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_03.png)'
- en: 'Figure 16.3: GloVe accuracy on word2vec analogies'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：GloVe在word2vec类比上的准确性
- en: '*Figure 16.4* projects the 300-dimensional embeddings of the most closely related
    analogies for a word2vec model trained on the Wikipedia corpus with over 2 billion
    tokens into two dimensions using PCA. A test of over 24,400 analogies from the
    following categories achieved an accuracy of over 73.5 percent:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图16.4*将在维基百科语料库上训练的word2vec模型的300维嵌入投影到两个维度上，使用PCA。来自以下类别的超过24400个类比的测试达到了超过73.5%的准确率：'
- en: '![](img/B15439_16_04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_04.png)'
- en: 'Figure 16.4: 2D visualization of selected analogy embeddings'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：所选类比嵌入的二维可视化
- en: Custom embeddings for financial news
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金融新闻的自定义嵌入
- en: Many tasks require embeddings of domain-specific vocabulary that models pretrained
    on a generic corpus may not be able to capture. Standard word2vec models are not
    able to assign vectors to out-of-vocabulary words and instead use a default vector
    that reduces their predictive value.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 许多任务需要领域特定词汇的嵌入，通用语料库上训练的模型可能无法捕捉。标准的word2vec模型无法为词汇外的词汇分配向量，而是使用默认向量，降低了它们的预测价值。
- en: For example, when working with **industry-specific documents**, the vocabulary
    or its usage may change over time as new technologies or products emerge. As a
    result, the embeddings need to evolve as well. In addition, documents like corporate
    earnings releases use nuanced language that GloVe vectors pretrained on Wikipedia
    articles are unlikely to properly reflect.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在处理**行业特定文件**时，词汇或其用法可能随着时间的推移而发生变化，因为新技术或产品出现。因此，嵌入也需要相应地发展。此外，像公司盈利发布这样的文件使用微妙的语言，维基百科文章上预训练的GloVe向量可能无法正确反映。
- en: In this section, we will train and evaluate domain-specific embeddings using
    financial news. We'll first show how to preprocess the data for this task, then
    demonstrate how the skip-gram architecture outlined in the first section works,
    and finally visualize the results. We also will introduce alternative, faster
    training methods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用金融新闻来训练和评估领域特定的嵌入。我们将首先展示如何为此任务预处理数据，然后演示第一节中概述的skip-gram架构的工作原理，并最终可视化结果。我们还将介绍替代的、更快速的训练方法。
- en: Preprocessing – sentence detection and n-grams
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理-句子检测和n-gram
- en: To illustrate the word2vec network architecture, we'll use the financial news
    dataset with over 125,000 relevant articles that we introduced in *Chapter 15*,
    *Topic Modeling – Summarizing Financial News*, on topic modeling. We'll load the
    data as outlined in the `lda_financial_news.ipynb` notebook in that chapter. The
    `financial_news_preprocessing.ipynb` notebook contains the code samples for this
    section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明word2vec网络架构，我们将使用包含超过125,000篇相关文章的金融新闻数据集，该数据集在*第15章*，*主题建模-总结金融新闻*中介绍了主题建模。我们将按照该章节中的`lda_financial_news.ipynb`笔记本中的说明加载数据。`financial_news_preprocessing.ipynb`笔记本包含了本节的代码示例。
- en: 'We use spaCy''s built-in **sentence boundary detection** to split each article
    into sentences, remove less informative items, such as numbers and punctuation,
    and keep the result if it is between 6 and 99 tokens long:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用spaCy内置的**句子边界检测**将每篇文章分割成句子，删除较少信息的项目，如数字和标点，并保留长度在6到99个标记之间的结果：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We end up with 2.43 million sentences that, on average, contain 15 tokens.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了243万个句子，平均包含15个标记。
- en: Next, we create n-grams to capture composite terms. Gensim lets us identify
    n-grams based on the relative frequency of joint versus individual occurrence
    of the components. The `Phrases` module scores the tokens, and the `Phraser` class
    transforms the text data accordingly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建n-gram来捕捉复合术语。Gensim让我们根据组件的联合与单独出现的相对频率来识别n-gram。`Phrases`模块对标记进行评分，`Phraser`类相应地转换文本数据。
- en: 'It transforms our list of sentences into a new dataset that we can write to
    file as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它将我们的句子列表转换为一个新的数据集，我们可以将其写入文件如下：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The notebook illustrates how we can repeat this process using the 2-gram file
    as input to create 3-grams. We end up with some 25,000 2-grams and 15,000 3- or
    4-grams. Inspecting the result shows that the highest-scoring terms are names
    of companies or individuals, suggesting that we might want to tighten our initial
    cleaning criteria. Refer to the notebook for additional details on the dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本演示了如何使用2-gram文件作为输入来创建3-gram。我们最终得到了大约25000个2-gram和15000个3-gram或4-gram。检查结果显示，得分最高的术语是公司或个人的名称，这表明我们可能需要加强我们的初始清洗标准。有关数据集的更多详细信息，请参考笔记本。
- en: The skip-gram architecture in TensorFlow 2
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 2中的skip-gram架构
- en: In this section, we will illustrate how to build a word2vec model using the
    Keras interface of TensorFlow 2 that we will introduce in much more detail in
    the next chapter. The `financial_news_word2vec_tensorflow` notebook contains the
    code samples and additional implementation details.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将说明如何使用TensorFlow 2的Keras接口构建word2vec模型，我们将在下一章中更详细地介绍。`financial_news_word2vec_tensorflow`笔记本包含代码示例和额外的实现细节。
- en: 'We start by tokenizing the documents and assigning a unique ID to each item
    in the vocabulary. First, we sample a subset of the sentences created in the previous
    section to limit the training time:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对文档进行标记化，并为词汇表中的每个项目分配一个唯一的ID。首先，我们对在上一节中创建的句子的子集进行采样，以限制训练时间：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We require at least 10 occurrences in the corpus, keep a vocabulary of 31,300
    tokens, and begin with the following steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在语料库中至少有10次出现，保留31300个标记的词汇表，并从以下步骤开始：
- en: Extract the top *n* most common words to learn embeddings.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取前*n*个最常见的单词来学习嵌入。
- en: Index these *n* words with unique integers.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用唯一整数索引这些*n*个单词。
- en: 'Create an `{index: word}` dictionary.'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建一个`{index: word}`字典。'
- en: 'Replace the *n* words with their index, and a dummy value `''UNK''` elsewhere:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用它们的索引替换*n*个单词，并在其他地方使用虚拟值`'UNK'`：
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We end up with 17.4 million tokens and a vocabulary of close to 60,000 tokens,
    including up to 3-grams. The vocabulary covers around 72.5 percent of the analogies.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了1740万个标记和接近6万个标记的词汇表，包括3-gram。词汇表涵盖了大约72.5%的类比。
- en: Noise-contrastive estimation – creating validation samples
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 噪声对比估计-创建验证样本
- en: 'Keras includes a `make_sampling_table` method that allows us to create a training
    set as pairs of context and noise words with corresponding labels, sampled according
    to their corpus frequencies. A lower factor increases the probability of selecting
    less frequent tokens; a chart in the notebook shows that the value of 0.1 limits
    sampling to the top 10,000 tokens:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Keras包括一个`make_sampling_table`方法，允许我们创建一个训练集，其中上下文和噪声单词成对出现，并根据它们的语料库频率进行采样。较低的因子会增加选择较不频繁标记的概率；笔记本中的图表显示，值为0.1会限制采样到前10000个标记：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Generating target-context word pairs
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成目标-上下文单词对
- en: 'To train our model, we need pairs of tokens where one represents the target
    and the other is selected from the surrounding context window, as shown previously
    in the right panel of *Figure 16.1*. We can use Keras'' `skipgrams()` function
    as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们需要一对代表目标的标记和另一个从周围上下文窗口中选择的标记，如*图16.1*右侧面板中所示。我们可以使用Keras的`skipgrams()`函数如下：
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result is 120.4 million context-target pairs, evenly split between positive
    and negative samples. The negative samples are generated according to the `sampling_table`
    probabilities we created in the previous step. The first five target and context
    word IDs with their matching labels appear as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是1.204亿个上下文-目标对，正负样本均匀分布。负样本是根据我们在上一步中创建的`sampling_table`概率生成的。前五个目标和上下文单词ID及其匹配标签如下所示：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating the word2vec model layers
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建word2vec模型层
- en: 'The word2vec model contains the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型包含以下内容：
- en: An input layer that receives the two scalar values representing the target-context
    pair
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个接收目标-上下文对表示的两个标量值的输入层
- en: A shared embedding layer that computes the dot product of the vector for the
    target and context word
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个共享的嵌入层，计算目标和上下文单词的向量的点积
- en: A sigmoid output layer
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个sigmoid输出层
- en: 'The **input layer** has two components, one for each element of the target-context
    pair:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层**有两个组件，一个用于目标-上下文对的每个元素：'
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The **shared embedding layer** contains one vector for each element of the
    vocabulary that is selected according to the index of the target and context tokens,
    respectively:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**共享的嵌入层**包含词汇表中每个元素的一个向量，该向量根据目标和上下文标记的索引进行选择。'
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The **output layer** measures the similarity of the two embedding vectors by
    their dot product and transforms the result using the `sigmoid` function that
    we encountered when discussing logistic regression in *Chapter 7*, *Linear Models
    – From Risk Factors to Return Forecasts*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出层**通过它们的点积测量两个嵌入向量的相似性，并使用我们在*第7章*中讨论逻辑回归时遇到的`sigmoid`函数进行转换，从而得到验证样本。'
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This skip-gram model contains a 200-dimensional embedding layer that will assume
    different values for each vocabulary item. As a result, we end up with 59,617
    x 200 trainable parameters, plus two for the sigmoid output.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个skip-gram模型包含一个200维的嵌入层，每个词汇项将假定不同的值。因此，我们最终得到59617 x 200个可训练参数，再加上两个用于sigmoid输出的参数。
- en: In each iteration, the model computes the dot product of the context and the
    target embedding vectors, passes the result through the sigmoid to produce a probability,
    and adjusts the embedding based on the gradient of the loss.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，模型计算上下文和目标嵌入向量的点积，通过sigmoid产生概率，并根据损失的梯度调整嵌入。
- en: Visualizing embeddings using TensorBoard
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化嵌入
- en: 'TensorBoard is a visualization tool that permits the projection of the embedding
    vectors into two or three dimensions to explore the word and phrase locations.
    After loading the embedding metadata file we created (refer to the notebook),
    you can also search for specific terms to view and explore its neighbors, projected
    into two or three dimensions using UMAP, t-SNE, or PCA (refer to *Chapter 13,
    Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*). Refer
    to the notebook for a higher-resolution color version of the following screenshot:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一个可视化工具，允许将嵌入向量投影到二维或三维中，以探索单词和短语的位置。加载我们创建的嵌入元数据文件后（参考笔记本），您还可以搜索特定术语，以查看和探索其邻居，使用UMAP、t-SNE或PCA将其投影到二维或三维（参考*第13章，使用无监督学习进行数据驱动的风险因素和资产配置*）。参考笔记本以获取以下截图的更高分辨率彩色版本：
- en: '![](img/B15439_16_05.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_05.png)'
- en: 'Figure 16.5: 3D embeddings and metadata visualization'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：3D嵌入和元数据可视化
- en: How to train embeddings faster with Gensim
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用Gensim更快地训练嵌入
- en: The TensorFlow implementation is very transparent in terms of its architecture,
    but it is not particularly fast. The **natural language processing** (**NLP**)
    library Gensim, which we also used for topic modeling in the last chapter, offers
    better performance and more closely resembles the C-based word2vec implementation
    provided by the original authors.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow实现在架构方面非常透明，但速度不是特别快。我们在上一章中用于主题建模的**自然语言处理**（**NLP**）库Gensim性能更好，更接近原始作者提供的基于C的word2vec实现。
- en: 'Usage is very straightforward. We first create a sentence generator that just
    takes the name of the file we produced in the preprocessing step as input (we''ll
    work with 3-grams again):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非常简单。我们首先创建一个句子生成器，它只需要我们在预处理步骤中生成的文件名作为输入（我们将再次使用3-gram）：
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In a second step, we configure the word2vec model with the familiar parameters
    concerning the sizes of the embedding vector and the context window, the minimum
    token frequency, and the number of negative samples, among others:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们使用熟悉的参数配置word2vec模型，包括嵌入向量和上下文窗口的大小，最小标记频率以及负样本数量等：
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One epoch of training takes a bit over 2 minutes on a modern 4-core i7 processor.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代4核i7处理器上，一次训练需要2分钟多一点。
- en: 'We can persist both the model and the word vectors, or just the word vectors,
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以保存模型和单词向量，或者只保存单词向量，如下所示：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can validate model performance and continue training until we are satisfied
    with the results like so:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证模型性能，并继续训练，直到我们对结果满意为止：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this case, training for six additional epochs yields the best results with
    an accuracy of 41.75 percent across all analogies covered by the vocabulary. The
    left panel of *Figure 16.6* shows the correct/incorrect predictions and accuracy
    breakdown per category.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，额外训练六个时期可以获得最佳结果，词汇表中所有类比的准确率为41.75％。*图16.6*的左面板显示了每个类别的正确/错误预测和准确度。
- en: 'Gensim also allows us to evaluate custom semantic algebra. We can check the
    popular `"woman"+"king"-"man" ~ "queen"` example as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还允许我们评估自定义语义代数。我们可以按照以下示例检查流行的““女人”+“国王”-“男人”~“女王””：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The right panel of the figure shows that "queen" is the third token, right
    after "monarch" and the less obvious "lewis", followed by several royalties:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图的右面板显示，“女王”是第三个标记，紧随“君主”和不太明显的“刘易斯”，然后是几位皇室成员：
- en: '![](img/B15439_16_06.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_06.png)'
- en: 'Figure 16.6: Analogy accuracy by category and for a specific example'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6：类别和特定示例的类比准确性
- en: 'We can also evaluate the tokens most similar to a given target to gain a better
    understanding of the embedding characteristics. We randomly select based on log
    corpus frequency:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以评估与给定目标最相似的标记，以更好地了解嵌入特征。我们根据对数语料库频率随机选择：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following table exemplifies the results that include several n-grams:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格举例说明了包括几个n-gram的结果：
- en: '| Target | Closest Match |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 最接近的匹配 |'
- en: '| 0 | 1 | 2 | 3 | 4 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: 0 1 2 3 4
- en: '| profiles | profile | users | political_consultancy_cambridge_analytica |
    sophisticated | facebook |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: 概要 概要 用户 政治咨询公司剑桥分析 高级 facebook
- en: '| divestments | divestitures | acquisitions | takeovers | bayer | consolidation
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: 剥离 收购 收购 拆分 拜耳 合并
- en: '| readiness | training | military | command | air_force | preparations |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: 准备 训练 军事 指挥 空军 准备
- en: '| arsenal | nuclear_weapons | russia | ballistic_missile | weapons | hezbollah
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: 军火库 核武器 俄罗斯 弹道导弹 武器 希巴拉
- en: '| supply_disruptions | disruptions | raw_material | disruption | prices | downturn
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: 供应中断 中断 原材料 中断 价格 下降
- en: We will now proceed to develop an application more closely related to real-life
    trading using SEC filings.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将继续开发一个与SEC提交文件更密切相关的应用程序。
- en: word2vec for trading with SEC filings
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于使用SEC提交文件进行交易的word2vec
- en: In this section, we will learn word and phrase vectors from annual SEC filings
    using Gensim to illustrate the potential value of word embeddings for algorithmic
    trading. In the following sections, we will combine these vectors as features
    with price returns to train neural networks to predict equity prices from the
    content of security filings.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Gensim从年度SEC提交文件中学习单词和短语向量，以说明单词嵌入对算法交易的潜在价值。在接下来的几节中，我们将将这些向量与价格回报结合为特征，训练神经网络从安全提交的内容中预测股票价格。
- en: In particular, we will use a dataset containing over **22,000 10-K annual reports**
    from the period **2013-2016** that are filed by over 6,500 listed companies and
    contain both financial information and management commentary (see *Chapter 2*,
    *Market and Fundamental Data – Sources and Techniques*).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将使用包含来自2013-2016年期间由6500多家上市公司提交的**22,000份10-K年度报告**的数据集，其中包含财务信息和管理评论（请参见*第2章*，*市场和基本数据-来源和技术*）。
- en: For about 3,000 companies corresponding to 11,000 filings, we have stock prices
    to label the data for predictive modeling. (See data source details and download
    instructions and preprocessing code samples in the `sec_preprocessing` notebook
    in the `sec-filings` folder.)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大约3000家公司对应的11000份提交文件，我们有股价来标记用于预测建模的数据。（有关数据来源详细信息和下载说明以及在`sec_preprocessing`文件夹中的`sec-filings`文件夹中的预处理代码示例，请参见`sec_preprocessing`笔记本。）
- en: Preprocessing – sentence detection and n-grams
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理-句子检测和n-gram
- en: 'Each filing is a separate text file, and a master index contains filing metadata.
    We extract the most informative sections, namely:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提交文件都是一个单独的文本文件，主索引包含提交文件的元数据。我们提取最具信息量的部分，即：
- en: 'Item 1 and 1A: Business and Risk Factors'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目1和1A：业务和风险因素
- en: 'Item 7: Management''s Discussion'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目7：管理讨论
- en: 'Item 7a: Disclosures about Market Risks'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目7a：关于市场风险的披露
- en: The `sec_preprocessing` notebook shows how to parse and tokenize the text using
    spaCy, similar to the approach in *Chapter 14.* We do not lemmatize the tokens
    to preserve nuances of word usage.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`sec_preprocessing`笔记本展示了如何使用spaCy解析和标记文本，类似于*第14章*中的方法。我们不对标记进行词形还原，以保留单词用法的细微差别。'
- en: Automatic phrase detection
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动短语检测
- en: As in the previous section, we use Gensim to detect phrases that consist of
    multiple tokens, or n-grams. The notebook shows that the most frequent bigrams
    include `common_stock`, `united_states`, `cash_flows`, `real_estate`, and `interest_rates`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节类似，我们使用Gensim来检测由多个标记或n-gram组成的短语。笔记本显示，最常见的二元组包括`common_stock`、`united_states`、`cash_flows`、`real_estate`和`interest_rates`。
- en: We end up with a vocabulary of slightly over 201,000 tokens with a median frequency
    of 7, suggesting substantial noise that we can remove by increasing the minimum
    frequency when training our word2vec model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了稍多于201,000个标记的词汇表，其中频率中位数为7，这表明我们可以通过增加训练word2vec模型时的最小频率来消除相当大的噪音。
- en: Labeling filings with returns to predict earnings surprises
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将文件标记与回报一起用于预测盈利惊喜
- en: The dataset comes with a list of tickers and filing dates associated with the
    10,000 documents. We can use this information to select stock prices for a certain
    period surrounding the filing publication. The goal would be to train a model
    that uses word vectors for a given filing as input to predict post-filing returns.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集附带了与1万份文件相关的股票代码和申报日期的列表。我们可以利用这些信息来选择与申报发布期间相关的一定时期内的股价。目标是训练一个模型，该模型使用给定申报的单词向量作为输入来预测申报后的回报。
- en: 'The following code example shows how to label individual filings with the 1-month
    return for the period after filing:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例显示了如何使用1个月的回报来标记单个申报文件：
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We will come back to this when we work with deep learning architectures in the
    following chapters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在接下来的章节中使用深度学习架构时，我们会回到这个问题。
- en: Model training
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: The `gensim.models.word2vec` class implements the skip-gram and CBOW architectures
    introduced previously. The notebook `word2vec` contains additional implementation
    details.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.word2vec`类实现了先前介绍的skip-gram和CBOW架构。笔记本`word2vec`包含了额外的实现细节。'
- en: 'To facilitate memory-efficient text ingestion, the `LineSentence` class creates
    a generator from individual sentences contained in the text file provided:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于内存高效地摄取文本，`LineSentence`类从提供的文本文件中的单个句子创建一个生成器：
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `Word2Vec` class offers the configuration options introduced earlier in
    this chapter:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2Vec`类提供了本章前面介绍的配置选项：'
- en: '[PRE20]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The notebook shows how to persist and reload models to continue training, or
    how to store the embedding vectors separately, for example, for use in machine
    learning models.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本展示了如何持久化和重新加载模型以继续训练，或者如何单独存储嵌入向量，例如用于机器学习模型。
- en: Model evaluation
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Basic functionality includes identifying similar words:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 基本功能包括识别相似的单词：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also validate individual analogies using positive and negative contributions
    accordingly:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用正负贡献来验证单个类比：
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Performance impact of parameter settings
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数设置的性能影响
- en: 'We can use the analogies to evaluate the impact of different parameter settings.
    The following results stand out (refer to the detailed results in the `models`
    folder):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类比来评估不同参数设置的影响。以下结果突出显示（参考`models`文件夹中的详细结果）：
- en: Negative sampling outperforms the hierarchical softmax, while also training
    faster.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负采样优于分层softmax，同时训练速度更快。
- en: The skip-gram architecture outperforms CBOW.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: skip-gram架构优于CBOW。
- en: Different `min_count` settings have a smaller impact; the midpoint of 50 performs
    best.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的`min_count`设置影响较小，50的中点效果最好。
- en: 'Further experiments with the best-performing skip-gram model using negative
    sampling and a `min_count` of 50 show the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用负采样和`min_count`为50的最佳skip-gram模型进行进一步实验得到以下结果：
- en: Context windows smaller than 5 reduce performance.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小于5的上下文窗口会降低性能。
- en: A higher negative sampling rate improves performance at the expense of slower training.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的负采样率会提高性能，但训练速度会变慢。
- en: Larger vectors improve performance, with a `size` of 600 yielding the best accuracy
    at 38.5 percent.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较大的向量会提高性能，`size`为600时的准确率最高，为38.5%。
- en: Sentiment analysis using doc2vec embeddings
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用doc2vec嵌入进行情感分析
- en: Text classification requires combining multiple word embeddings. A common approach
    is to average the embedding vectors for each word in the document. This uses information
    from all embeddings and effectively uses vector addition to arrive at a different
    location point in the embedding space. However, relevant information about the
    order of words is lost.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类需要结合多个单词嵌入。一种常见的方法是对文档中每个单词的嵌入向量进行平均。这使用了所有嵌入的信息，并有效地使用向量加法来到达嵌入空间中的不同位置。然而，有关单词顺序的相关信息会丢失。
- en: 'In contrast, the document embedding model, doc2vec, developed by the word2vec
    authors shortly after publishing their original contribution, produces embeddings
    for pieces of text like a paragraph or a product review directly. Similar to word2vec,
    there are also two flavors of doc2vec:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，文档嵌入模型doc2vec是由word2vec的作者在发布原始贡献后不久开发的，它直接为段落或产品评论等文本片段生成嵌入。与word2vec类似，doc2vec也有两种风格：
- en: The **distributed bag of words** (**DBOW**) model corresponds to the word2vec
    CBOW model. The document vectors result from training a network on the synthetic
    task of predicting a target word based on both the context word vectors and the
    document's doc vector.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式词袋**（**DBOW**）模型对应于word2vec的CBOW模型。文档向量是通过训练网络来预测基于上下文单词向量和文档的doc向量的目标单词而产生的。'
- en: The **distributed memory** (**DM**) model corresponds to the word2wec skip-gram
    architecture. The doc vectors result from training a neural net to predict a target
    word using the full document's doc vector.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式记忆**（**DM**）模型对应于word2wec skip-gram架构。文档向量是通过训练神经网络来预测目标词而产生的。'
- en: Gensim's `Doc2Vec` class implements this algorithm. We'll illustrate the use
    of doc2vec by applying it to the Yelp sentiment dataset that we introduced in
    *Chapter 14*. To speed up training, we limit the data to a stratified random sample
    of 0.5 million Yelp reviews with their associated star ratings. The `doc2vec_yelp_sentiment`
    notebook contains the code examples for this section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim的`Doc2Vec`类实现了这个算法。我们将通过将其应用于我们在*第14章*中介绍的Yelp情感数据集来说明doc2vec的用法。为了加快训练速度，我们将数据限制为50万条Yelp评论的分层随机样本及其相关的星级评分。`doc2vec_yelp_sentiment`笔记本包含了本节的代码示例。
- en: Creating doc2vec input from Yelp sentiment data
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Yelp情感数据创建doc2vec输入
- en: 'We load the combined Yelp dataset containing 6 million reviews, as created
    in *Chapter 14*, *Text Data for Trading – Sentiment Analysis*, and sample 100,000
    reviews for each star rating:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了包含600万条评论的综合Yelp数据集，如*第14章*，*用于交易的文本数据-情感分析*中所创建的，并为每个星级评分样本了10万条评论：
- en: '[PRE23]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We use nltk''s `RegexpTokenizer` for simple and quick text cleaning:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用nltk的`RegexpTokenizer`进行简单快速的文本清洗：
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After we filter out reviews shorter than 10 tokens, we are left with 485,825
    samples. The left panel of *Figure 16.6* shows the distribution of the number
    of tokens per review.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤掉少于10个标记的评论后，我们剩下485,825个样本。*图16.6*的左面板显示了每条评论的标记数的分布。
- en: 'The `gensim.models.Doc2Vec` class processes documents in the `TaggedDocument`
    format that contains the tokenized documents alongside a unique tag that permits
    the document vectors to be accessed after training:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim.models.Doc2Vec`类以`TaggedDocument`格式处理文档，其中包含标记化的文档以及一个唯一标记，允许在训练后访问文档向量：'
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Training a doc2vec model
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个doc2vec模型
- en: 'The training interface works in a similar fashion to word2vec and also allows
    continued training and persistence:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 训练界面的工作方式与word2vec类似，并且还允许持续训练和持久性：
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can query the *n* terms most similar to a given token as a quick way to
    evaluate the resulting word vectors as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查询与给定标记最相似的*n*个术语，以快速评估生成的词向量，如下所示：
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The right panel of *Figure 16.7* displays the returned tokens and their similarity:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*图16.7*的右面板显示了返回的标记及其相似性：'
- en: '![](img/B15439_16_07.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_07.png)'
- en: 'Figure 16.7: Histogram of the number of tokens per review (left) and terms
    most similar to the token ''good'''
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7：每条评论的标记数的直方图（左）和与标记'good'最相似的术语
- en: Training a classifier with document vectors
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用文档向量训练分类器
- en: 'Now, we can access the document vectors to create features for a sentiment
    classifier:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以访问文档向量，为情感分类器创建特征：
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We create training and test sets as usual:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样创建训练集和测试集：
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we proceed to train a `RandomForestClassifier`, a LightGBM gradient boosting
    model, and a multinomial logistic regression. We use 500 trees for the random
    forest:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续训练`RandomForestClassifier`，LightGBM梯度提升模型和多项式逻辑回归。我们为随机森林使用了500棵树：
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We use early stopping with the LightGBM classifier, but it runs for the full
    5,000 rounds because it continues to improve its validation performance:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用LightGBM分类器进行早停，但它运行了完整的5,000轮，因为它继续改善其验证性能：
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we build a multinomial logistic regression model as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们按以下方式构建多项式逻辑回归模型：
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When we compute the accuracy for each model on the validation set, gradient
    boosting performs significantly better at 62.24 percent. *Figure 16.8* shows the
    confusion matrix and accuracy for each model:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在验证集上计算每个模型的准确性时，梯度提升表现显著更好，为62.24%。*图16.8*显示了每个模型的混淆矩阵和准确性：
- en: '![](img/B15439_16_08.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_16_08.png)'
- en: 'Figure 16.8: Confusion matrix and test accuracy for alternative models'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8：备选模型的混淆矩阵和测试准确性
- en: The sentiment classification result in *Chapter 14*, *Text Data for Trading
    – Sentiment Analysis*, produced better accuracy for LightGBM (73.6 percent), but
    we used the full dataset and included additional features. You may want to test
    whether increasing the sample size or tuning the model parameters makes doc2vec
    perform equally well.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*第14章*，*用于交易的文本数据-情感分析*中的情感分类结果对LightGBM（73.6%）产生了更好的准确性，但我们使用了完整数据集并包含了额外的特征。您可能希望测试增加样本量或调整模型参数是否使doc2vec表现同样良好。'
- en: Lessons learned and next steps
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学到的教训和下一步
- en: This example applied sentiment analysis using doc2vec to **product reviews rather
    than financial documents**. We selected product reviews because it is very difficult
    to find financial text data that is large enough for training word embeddings
    from scratch and also has useful sentiment labels or sufficient information for
    us to assign them labels, such as asset returns, ourselves.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子应用了情感分析，使用了doc2vec而不是金融文件。我们选择了产品评论，因为很难找到足够大的金融文本数据，以便从头开始训练词嵌入，并且还具有有用的情感标签或足够的信息让我们分配它们的标签，比如资产回报。
- en: 'While product reviews allow us to demonstrate the workflow, we need to keep
    in mind **important structural differences**: product reviews are often short,
    informal, and specific to one particular object. Many financial documents, in
    contrast, are longer, more formal, and the target object may or may not be clearly
    identified. Financial news articles could concern multiple targets, and while
    corporate disclosures may have a clear source, they may also discuss competitors.
    An analyst report, for instance, may also discuss both positive and negative aspects
    of the same object or topic.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管产品评论允许我们展示工作流程，但我们需要牢记**重要的结构差异**：产品评论通常很短，非正式，并且特定于一个特定对象。相比之下，许多金融文件更长，更正式，目标对象可能明确标识，也可能没有。金融新闻文章可能涉及多个目标，而公司披露可能有明确的来源，也可能讨论竞争对手。例如，分析师报告可能讨论同一对象或主题的积极和消极方面。
- en: In short, the interpretation of sentiment expressed in financial documents often
    requires a more sophisticated, nuanced, and granular approach that builds up an
    understanding of the content's meaning from different aspects. Decision makers
    also often care to understand how a model arrives at its conclusion.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，对金融文件中表达的情绪的解释通常需要更复杂、更细微和更细粒度的方法，从不同的角度建立对内容意义的理解。决策者也经常关心模型是如何得出结论的。
- en: These challenges have not yet been solved and remain an area of very active
    research, complicated not least by the scarcity of suitable data sources. However,
    recent breakthroughs that significantly boosted performance on various NLP tasks
    since 2018 suggest that financial sentiment analysis may also become more robust
    in the coming years. We will turn to these innovations next.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战尚未得到解决，仍然是非常活跃的研究领域，尤其是由于适合的数据来源稀缺。然而，自2018年以来显著提高了各种自然语言处理任务性能的最新突破表明，金融情绪分析在未来几年可能也会变得更加稳健。我们将在接下来转向这些创新。
- en: New frontiers – pretrained transformer models
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新的前沿-预训练变压器模型
- en: 'Word2vec and GloVe embeddings capture more semantic information than the bag-of-words
    approach. However, they allow only a single fixed-length representation of each
    token that does not differentiate between context-specific usages. To address
    unsolved problems such as multiple meanings for the same word, called **polysemy**,
    several new models have emerged that build on the **attention mechanism** designed
    to learn more contextualized word embeddings (Vaswani et al., 2017). The key characteristics
    of these models are as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec和GloVe嵌入捕捉的语义信息比词袋方法更多。然而，它们只允许每个标记的单一固定长度表示，不能区分上下文特定的用法。为了解决诸如同一个词的多个含义（称为**多义性**）等未解决的问题，出现了几种新模型，它们建立在旨在学习更加情境化的单词嵌入的**注意机制**上（Vaswani等人，2017）。这些模型的关键特征如下：
- en: The use of **bidirectional language models** that process text both left-to-right
    and right-to-left for a richer context representation
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向语言模型的使用，可以处理文本的左到右和右到左，以获得更丰富的上下文表示
- en: The use of **semi-supervised pretraining** on a large generic corpus to learn
    universal language aspects in the form of embeddings and network weights that
    can be used and fine-tuned for specific tasks (a form of **transfer learning**
    that we will discuss in more detail in *Chapter 18*, *CNNs for Financial Time
    Series and Satellite Images*)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用大型通用语料库进行**半监督预训练**，以学习嵌入和网络权重的通用语言方面，这些可以用于特定任务的微调（一种我们将在*第18章*《金融时间序列和卫星图像的CNNs》中更详细讨论的**迁移学习**形式）
- en: In this section, we briefly describe the attention mechanism, outline how the
    recent transformer models—starting with **Bidirectional Encoder Representation
    from Transformers** (**BERT**)—use it to improve performance on key NLP tasks,
    reference several sources for pretrained language models, and explain how to use
    them for financial sentiment analysis.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要描述了注意机制，概述了最近的变压器模型（从**BERT**开始），它们如何利用它来提高关键自然语言处理任务的性能，引用了几个预训练语言模型的来源，并解释了如何将它们用于金融情绪分析。
- en: Attention is all you need
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力就是一切
- en: The **attention mechanism** explicitly models the relationships between words
    in a sentence to better incorporate the context. It was first applied to machine
    translation (Bahdanau, Cho, and Bengio, 2016), but has since become integral to
    neural language models for a wide variety of tasks.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意机制**明确地模拟了句子中单词之间的关系，以更好地融入上下文。它首先应用于机器翻译（Bahdanau, Cho, and Bengio, 2016），但自那时起已成为神经语言模型在各种任务中的核心。'
- en: Until 2017, **recurrent neural networks** (**RNNs**), which sequentially process
    text left-to-right or right-to-left, represented the state of the art for NLP
    tasks like translation. Google, for example, has employed such a model in production
    since late 2016\. Sequential processing implies several steps to semantically
    connect words at distant locations and precludes parallel processing, which greatly
    speeds up computation on modern, specialized hardware like GPUs. (For more information
    on RNNs, refer to *Chapter 19*, *RNNs for Multivariate Time Series and Sentiment
    Analysis*.)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2017年，**循环神经网络**（**RNNs**），它们按顺序处理文本，从左到右或从右到左，代表了自然语言处理任务（如翻译）的最新技术。例如，谷歌自2016年末以来一直在生产中使用这样的模型。顺序处理意味着需要多个步骤来语义连接远距离位置的单词，并且排除了并行处理，这在现代专用硬件（如GPU）上大大加快了计算速度。（有关RNN的更多信息，请参阅*第19章*《多变量时间序列和情感分析的RNNs》。）
- en: In contrast, the **Transformer** model, introduced in the seminal paper *Attention
    is all you need* (Vaswani et al., 2017), requires only a constant number of steps
    to identify semantically related words. It relies on a self-attention mechanism
    that captures links between all words in a sentence, regardless of their relative
    position. The model learns the representation of a word by assigning an attention
    score to every other word in the sentence that determines how much each of the
    other words should contribute to the representation. These scores then inform
    a weighted average of all words' representations, which is fed into a fully connected
    network to generate a new representation for the target word.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**变压器**模型，引入了具有里程碑意义的论文*注意力就是一切*（Vaswani等人，2017），只需要恒定数量的步骤来识别语义相关的单词。它依赖于一种自注意机制，可以捕捉句子中所有单词之间的联系，而不考虑它们的相对位置。该模型通过为句子中的每个其他单词分配一个注意力分数来学习单词的表示，该分数决定了每个其他单词应该对表示贡献多少。然后，这些分数会影响所有单词表示的加权平均值，然后输入到全连接网络中，以生成目标单词的新表示。
- en: The Transformer model uses an encoder-decoder architecture with several layers,
    each of which uses several attention mechanisms (called **heads**) in parallel.
    It yielded large performance improvements on various translation tasks and, more
    importantly, inspired a wave of new research into neural language models addressing
    a broader range of tasks. The resources linked on GitHub contain various excellent
    visual explanations of how the attention mechanism works, so we won't go into
    more detail here.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型使用编码器-解码器架构，每个架构使用多个并行的注意力机制（称为**头**）。它在各种翻译任务上取得了巨大的性能改进，并且更重要的是，激发了对解决更广泛任务的神经语言模型的新研究浪潮。GitHub上的资源包含了关于注意力机制如何工作的各种出色的视觉解释，因此我们在这里不会详细介绍。
- en: BERT – towards a more universal language model
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT - 迈向更普遍的语言模型
- en: In 2018, Google released the **BERT** model, which stands for **Bidirectional
    Encoder Representations from Transformers** (Devlin et al., 2019). In a major
    breakthrough for NLP research, it achieved groundbreaking results on eleven natural
    language understanding tasks, ranging from question answering and named entity
    recognition to paraphrasing and sentiment analysis, as measured by the **General
    Language Understanding Evaluation** (**GLUE**) benchmark (see GitHub for links
    to task descriptions and a leaderboard).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，谷歌发布了BERT模型，全称为来自变压器的双向编码器表示（Devlin等人，2019）。在NLP研究的重大突破中，它在十一个自然语言理解任务上取得了突破性的成果，从问答和命名实体识别到释义和情感分析，都是通过**通用语言理解评估**（**GLUE**）基准来衡量的（有关任务描述和排行榜的链接，请参阅GitHub）。
- en: The new ideas introduced by BERT unleashed a flurry of new research that produced
    dozens of improvements that soon surpassed non-expert humans on the GLUE tasks
    and led to the more challenging **SuperGLUE** benchmark designed by DeepMind (Wang
    et al., 2019). As a result, 2018 is now considered a turning point for NLP research;
    both Google Search and Microsoft's Bing are now using variations of BERT to interpret
    user queries and provide more accurate results.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: BERT引入的新思想引发了一系列新研究，产生了数十项改进，很快就超过了GLUE任务上的非专家人类，并导致了由DeepMind设计的更具挑战性的**SuperGLUE**基准（Wang等人，2019）。因此，2018年现在被认为是NLP研究的一个转折点；谷歌搜索和微软的必应现在都使用BERT的变体来解释用户查询并提供更准确的结果。
- en: We will briefly outline BERT's key innovations and provide indications on how
    to get started using it and its subsequent enhancements with one of several open
    source libraries providing pretrained models.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要概述BERT的关键创新，并提供如何开始使用它及其后续增强的指示，其中包括使用几种提供预训练模型的开源库之一。
- en: Key innovations – deeper attention and pretraining
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键创新 - 更深入的注意力和预训练
- en: 'The BERT model builds on **two key ideas**, namely, the **transformer architecture**
    described in the previous section and **unsupervised pretraining** so that it
    doesn''t need to be trained from scratch for each new task; rather, its weights
    are fine-tuned:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型建立在**两个关键思想**上，即在前一节中描述的**变压器架构**和**无监督预训练**，因此它不需要为每个新任务从头开始训练；相反，它的权重被微调：
- en: BERT takes the **attention mechanism** to a new (deeper) level by using 12 or
    24 layers, depending on the architecture, each with 12 or 16 attention heads.
    This results in up to 24 × 16 = 384 attention mechanisms to learn context-specific
    embeddings.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT通过使用12或24层（取决于架构）每层有12或16个注意力头，将**注意力机制**提升到一个新的（更深入的）层次。这导致最多24 x 16 =
    384个注意力机制来学习特定上下文的嵌入。
- en: 'BERT uses **unsupervised, bidirectional pretraining** to learn its weights
    in advance on two tasks: **masked language modeling** (predicting a missing word
    given the left and right context) and **next sentence prediction** (predicting
    whether one sentence follows another).'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT使用**无监督的双向预训练**来提前学习其权重，用于两个任务：**掩码语言建模**（根据左右上下文预测缺失的单词）和**下一个句子预测**（预测一个句子是否跟随另一个句子）。
- en: '**Context-free** models such as word2vec or GloVe generate a single embedding
    for each word in the vocabulary: the word "bank" would have the same context-free
    representation in "bank account" and "bank of the river." In contrast, BERT learns
    to represent each word based on the other words in the sentence. As a **bidirectional
    model**, BERT is able to represent the word "bank" in the sentence "I accessed
    the bank account," not only based on "I accessed the" as a unidirectional contextual
    model, but also based on "account."'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**无上下文**模型，如word2vec或GloVe，为词汇表中的每个单词生成单个嵌入：例如，单词"bank"在"bank account"和"bank
    of the river"中具有相同的无上下文表示。相反，BERT学习根据句子中的其他单词来表示每个单词。作为**双向模型**，BERT能够表示句子"I accessed
    the bank account"中的单词"bank"，不仅基于"我访问了"作为单向上下文模型，还基于"账户"。'
- en: BERT and its successors can be **pretrained on a generic corpus** like Wikipedia
    before adapting its final layers to a specific task and **fine-tuning its weights**.
    As a result, you can use large-scale, state-of-the-art models with billions of
    parameters, while only incurring a few hours rather than days or weeks of training
    costs. Several libraries offer such pretrained models that you can build on to
    develop a custom sentiment classifier for your dataset of choice.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: BERT及其后继者可以在类似维基百科的通用语料库上**预训练**，然后调整其最终层以适应特定任务，并**微调其权重**。因此，您可以使用拥有数十亿参数的大规模最先进模型，而只需几个小时而不是几天或几周的训练成本。几个库提供了这样的预训练模型，您可以在此基础上构建自定义情感分类器，以适应您选择的数据集。
- en: Using pretrained state-of-the-art models
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的最先进模型
- en: The recent NLP breakthroughs described in this section have shown how to acquire
    linguistic knowledge from unlabeled text with networks large enough to represent
    the long tail of rare usage phenomena. The resulting Transformer architectures
    make fewer assumptions about word order and context; instead, they learn a much
    more subtle understanding of language from very large amounts of data, using hundreds
    of millions or even billions of parameters.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述的最新NLP突破展示了如何从未标记的文本中获取语言知识，使用足够大的网络来表示罕见用法现象的长尾部分。由此产生的Transformer架构对单词顺序和上下文的假设更少；相反，它们从大量数据中学习了对语言的更微妙的理解，使用了数亿甚至数十亿的参数。
- en: We will highlight several libraries that make pretrained networks, as well as
    excellent Python tutorials available.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍几个提供预训练网络和优秀Python教程的库。
- en: The Hugging Face Transformers library
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hugging Face Transformers库
- en: Hugging Face is a US start-up developing chatbot applications designed to offer
    personalized AI-powered communication. It raised $15 million in late 2019 to further
    develop its very successful open source NLP library, Transformers.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face是一家美国初创公司，开发旨在提供个性化AI驱动通信的聊天机器人应用。它在2019年底筹集了1500万美元，以进一步开发其非常成功的开源NLP库Transformers。
- en: The library provides general-purpose architectures for natural language understanding
    and generation with more than 32 pretrained models in more than 100 languages
    and deep interoperability between TensorFlow 2 and PyTorch. It has excellent documentation.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 该库提供了通用的自然语言理解和生成架构，拥有32多个预训练模型，支持100多种语言，并且在TensorFlow 2和PyTorch之间具有深度的互操作性。它有很好的文档。
- en: The spacy-transformers library includes wrappers to facilitate the inclusion
    of the pretrained transformer models in a spaCy pipeline. Refer to the reference
    links on GitHub for more information.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: spacy-transformers库包括包装器，以便在spaCy管道中方便地包含预训练的transformer模型。有关更多信息，请参考GitHub上的参考链接。
- en: AllenNLP
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AllenNLP
- en: AllenNLP is built and maintained by the Allen Institute for AI, started by Microsoft
    cofounder Paul Allen, in close collaboration with researchers at the University
    of Washington. It has been designed as a research library for developing state-of-the-art
    deep learning models on a wide variety of linguistic tasks, built on PyTorch.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP由由微软联合创始人保罗·艾伦创立的艾伦人工智能研究所建立和维护，与华盛顿大学的研究人员密切合作。它被设计为一个研究库，用于在各种语言任务上开发最先进的深度学习模型，基于PyTorch。
- en: It offers solutions for key tasks from question answering to sentence annotation,
    including reading comprehension, named entity recognition, and sentiment analysis.
    A pretrained **RoBERTa** model (a more robust version of BERT; Liu et al., 2019)
    achieves over 95 percent accuracy on the Stanford sentiment treebank and can be
    used with just a few lines of code (see links to the documentation on GitHub).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了从问答到句子注释的关键任务的解决方案，包括阅读理解、命名实体识别和情感分析。一个预训练的**RoBERTa**模型（BERT的更强大版本；Liu等，2019）在斯坦福情感树库上实现了95%以上的准确率，并且可以只用几行代码就能使用（参见GitHub上的文档链接）。
- en: Trading on text data – lessons learned and next steps
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本数据交易-经验教训和下一步
- en: As highlighted at the end of the section *Sentiment analysis using doc2vec embeddings*,
    there are important structural characteristics of financial documents that often
    complicate their interpretation and undermine simple dictionary-based methods.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用doc2vec嵌入进行情感分析*部分的结尾处，强调了金融文件的重要结构特征，这些特征经常使它们的解释变得复杂，并削弱了简单的基于词典的方法。
- en: In a recent survey of financial sentiment analysis, Man, Luo, and Lin (2019)
    found that most existing approaches only identify high-level polarities, such
    as positive, negative, or neutral. However, practical applications that lead to
    real decisions typically require a more nuanced and transparent analysis. In addition,
    the lack of large financial text datasets with relevant labels limits the potential
    for using traditional machine learning methods or neural networks for sentiment
    analysis.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近一项金融情感分析调查中，Man、Luo和Lin（2019）发现，大多数现有方法只能识别高级极性，如积极、消极或中性。然而，通常需要更细致和透明的分析才能导致真正的决策。此外，缺乏具有相关标签的大型金融文本数据集限制了使用传统机器学习方法或神经网络进行情感分析的潜力。
- en: The pretraining approach just described, which, in principle, yields a deeper
    understanding of textual information, thus offers substantial promise. However,
    most applied research using transformers has focused on NLP tasks such as translation,
    question answering, logic, or dialog systems. Applications in relation to financial
    data are still in their infancy (see, for example, Araci 2019). This is likely
    to change soon given the availability of pretrained models and their potential
    to extract more valuable information from financial text data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚描述的预训练方法原则上可以更深入地理解文本信息，因此具有很大的潜力。然而，大多数使用transformers的应用研究都集中在NLP任务，如翻译、问答、逻辑或对话系统。与金融数据相关的应用仍处于起步阶段（例如，参见Araci
    2019）。鉴于预训练模型的可用性及其从金融文本数据中提取更有价值信息的潜力，这很可能很快会发生改变。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed a new way of generating text features that use
    shallow neural networks for unsupervised machine learning. We saw how the resulting
    word embeddings capture interesting semantic aspects beyond the meaning of individual
    tokens by capturing some of the context in which they are used. We also covered
    how to evaluate the quality of word vectors using analogies and linear algebra.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一种使用浅层神经网络进行无监督机器学习的新的生成文本特征的方法。我们看到了由此产生的词嵌入如何捕捉超出单个标记含义的有趣语义方面，以及如何使用类比和线性代数来评估词向量的质量。
- en: We used Keras to build the network architecture that produces these features
    and applied the more performant Gensim implementation to financial news and SEC
    filings. Despite the relatively small datasets, the word2vec embeddings did capture
    meaningful relationships. We also demonstrated how appropriate labeling with stock
    price data can form the basis for supervised learning.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Keras构建了网络架构，生成了这些特征，并应用了更高性能的Gensim实现来处理财经新闻和SEC提交文件。尽管数据集相对较小，word2vec嵌入确实捕捉到了有意义的关系。我们还展示了如何通过股价数据进行适当标记，从而形成监督学习的基础。
- en: We applied the doc2vec algorithm, which produces a document rather than token
    vectors, to build a sentiment classifier based on Yelp business reviews. While
    this is unlikely to yield tradeable signals, it illustrates the process of how
    to extract features from relevant text data and train a model to predict an outcome
    that may be informative for a trading strategy.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了doc2vec算法，它生成的是文档而不是标记向量，用于构建基于Yelp商业评论的情感分类器。虽然这不太可能产生可交易的信号，但它说明了如何从相关文本数据中提取特征并训练模型来预测可能对交易策略有信息价值的结果。
- en: Finally, we outlined recent research breakthroughs that promise to yield more
    powerful natural language models due to the availability of pretrained architectures
    that only require fine-tuning. Applications to financial data, however, are still
    at the research frontier.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们概述了最近的研究突破，承诺通过预训练架构的可用性，将产生更强大的自然语言模型，这些模型只需要进行微调。然而，对于金融数据的应用仍处于研究前沿。
- en: In the next chapter, we will dive into the final part of this book, which covers
    how various deep learning architectures can be useful for algorithmic trading.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨本书的最后部分，介绍各种深度学习架构如何对算法交易有用。
