- en: What&#x27;s Next
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么
- en: 'So far we have gone through topics around carrying out DevOps'' tasks on Kubernetes
    across the board. Nevertheless, it''s always challenging to implement knowledge
    under real-world circumstances, hence you may wonder whether Kubernetes is able
    to solve particular problems that you are currently facing. In this chapter, we''ll
    learn the following topics to work out with challenges:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了在Kubernetes上进行DevOps任务的各种主题。然而，在实际情况下实施知识总是具有挑战性，因此您可能会想知道Kubernetes是否能够解决您目前面临的特定问题。在本章中，我们将学习以下主题来解决挑战：
- en: Advanced Kubernetes features
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级Kubernetes特性
- en: Kubernetes communities
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes社区
- en: Other container orchestrator frameworks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他容器编排框架
- en: Exploring the possibilities of Kubernetes
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Kubernetes的可能性
- en: Kubernetes is evolving day by day, and it's at a pace where it is publishing
    one major version quarterly. Aside from the built-in functions that come with
    every new Kubernetes distribution, contributions from the community also play
    an important role in the ecosystem, and we'll have a tour around them in this
    section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes每天都在不断发展，以每季度发布一个主要版本的速度发展。除了每个新Kubernetes发行版都带有的内置功能之外，社区的贡献也在生态系统中发挥着重要作用，我们将在本节中对它们进行一番探讨。
- en: Mastering Kubernetes
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精通Kubernetes
- en: Kubernetes' objects and resources are categorized into three API tracks, namely,
    alpha, beta, and stable to denote their maturity. The `apiVersion` field at the
    head of every resource indicates their level. If a feature has a versioning such
    as v1alpha1, it belongs to alpha-level API, and beta API is named in the same
    way. An alpha-level API is disabled by default and is subject to change without
    notice.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的对象和资源分为三个API跟踪，即alpha、beta和stable，以表示它们的成熟度。每个资源开头的`apiVersion`字段表示它们的级别。如果一个功能有类似v1alpha1的版本，它属于alpha级API，beta
    API以相同的方式命名。alpha级API默认禁用，并且可能会在不通知的情况下发生变化。
- en: The beta-level API is enabled by default; it's well tested and considered to
    be stable, but the schema or object semantics could be changed as well. The rest
    of the parts are the stable, generally available ones. Once an API enters a stable
    stage, it's unlikely to be changed anymore.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: beta级别的API默认启用；经过充分测试，被认为是稳定的，但模式或对象语义也可能会发生变化。其余部分是稳定的，通常可用的部分。一旦API进入稳定阶段，就不太可能再发生变化。
- en: Even though we've discussed concepts and practices about Kubernetes extensively,
    there are still considerable features that we haven't mentioned, that deal with
    a variety of workload as well as scenarios, and make Kubernetes extremely flexible.
    They may or may not apply to everyone's needs and are not stable enough in particular
    cases. Let's take a brief look at the popular ones.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经广泛讨论了Kubernetes的概念和实践，但仍然有许多重要特性尚未提及，涉及各种工作负载和场景，使Kubernetes变得非常灵活。它们可能适用于某些人的需求，但在特定情况下并不稳定。让我们简要地看一下流行的特性。
- en: Job and CronJob
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作业和定时作业
- en: 'They are also high-level pod controllers, that allow us to run containers that
    will eventually terminate. A job ensures a certain number of pods run to completion
    with success; a CronJob ensures that a job is invoked at given times. If we have
    the need to run batch workloads or scheduled tasks, we''d know that there are
    built-in controllers that come into play. Related information can be found at:
    [https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还是高级的Pod控制器，允许我们运行最终会终止的容器。作业确保一定数量的Pod成功完成运行；CronJob确保在指定时间调用作业。如果我们需要运行批量工作负载或定时任务，我们会知道内置控制器会发挥作用。相关信息可以在以下网址找到：[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/)。
- en: Affinity and anti-affinity between pods and nodes
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod和节点之间的亲和性和反亲和性
- en: 'We know a pod can be manually assigned to some nodes with the node selector,
    and a node can reject pods with taints. However, when it comes to more flexible
    circumstances, say, maybe we want some pods to be co-located, or we want pods
    to be distributed equally across availability zones, arranging our pods either
    by node selectors or by node taints may take a great effort. Thus, the affinity
    is designed to solve the case: [https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道可以使用节点选择器手动将Pod分配给某些节点，并且节点可以拒绝带有污点的Pod。然而，在更灵活的情况下，也许我们希望一些Pod共存，或者我们希望Pod在可用性区域之间均匀分布，通过节点选择器或节点污点来安排我们的Pod可能需要很大的努力。因此，亲和性旨在解决这种情况：[https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity)。
- en: Auto-scaling of pods
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod的自动扩展
- en: Almost all modern infrastructure supports auto-scaling an instance group that
    runs the application, so does Kubernetes. The pod horizontal scaler (`PodHorizontalScaler`)
    is able to scale pod replicas with CPU/memory metrics in a controller such as
    Deployment. Starting from Kubernetes 1.6, the scaler formally supports scaling
    based on custom metrics, say transactions-per-second. More information can be
    found at [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现代基础设施都支持自动扩展运行应用程序的实例组，Kubernetes也是如此。Pod水平扩展器（`PodHorizontalScaler`）能够根据CPU/内存指标在诸如Deployment的控制器中扩展Pod副本。从Kubernetes
    1.6开始，该扩展器正式支持基于自定义指标（例如每秒事务数）的扩展。更多信息可以在[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)找到。
- en: Prevention and mitigation of pod disruptions
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预防和减轻Pod中断
- en: We know pods are volatile, and they'd be terminated and relaunched across nodes
    as the cluster scales in and out. If too many pods of an application are destroyed
    simultaneously, it could result in lowering the service level or even the application
    fails. Especially when the application is stateful or quorum-based, it might barely
    tolerate, pod disruptions. To mitigate the disruption, we could leverage `PodDisruptionBudget`
    to inform Kubernetes of how many unavailable pods at any given time our application
    can tolerate so that Kubernetes is able to take proper actions with the knowledge
    of the applications on top of it. For more information, refer to [https://kubernetes.io/docs/concepts/workloads/pods/disruptions/](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道Pod是不稳定的，它们会在集群的扩展和收缩过程中在节点之间被终止和重新启动。如果一个应用程序的太多Pod同时被销毁，可能会导致服务水平下降甚至应用程序失败。特别是当应用程序是有状态的或基于法定人数的时候，它可能几乎无法容忍Pod的中断。为了减轻中断，我们可以利用`PodDisruptionBudget`来告知Kubernetes我们的应用程序在任何给定时间内可以容忍多少个不可用的Pod，以便Kubernetes能够在了解其上的应用程序的情况下采取适当的行动。有关更多信息，请参阅[https://kubernetes.io/docs/concepts/workloads/pods/disruptions/](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)。
- en: 'On the other hand, since `PodDisruptionBudget` is a managed object, it still
    cannot preclude disruptions caused by factors outside Kubernetes, such as hardware
    failures of a node, or node components being killed by the system due to insufficient
    memory. As such, we can incorporate tools such as node-problem-detector into our
    monitoring stack and properly configure the threshold on the resources of a node,
    to notify Kubernetes which begins to drain the node or evict excessive pods to
    prevent situations getting worse. For more detailed guides on node-problem-detector
    and resource thresholds, refer to the following topics:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于`PodDisruptionBudget`是一个受管理的对象，它仍然无法排除Kubernetes之外的因素引起的中断，例如节点的硬件故障，或者由于内存不足而被系统杀死的节点组件。因此，我们可以将诸如node-problem-detector之类的工具纳入我们的监控堆栈，并适当配置节点资源的阈值，以通知Kubernetes开始排空节点或驱逐过多的Pod，以防止情况恶化。有关node-problem-detector和资源阈值的更详细指南，请参阅以下主题：
- en: '[https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/](https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/](https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/)'
- en: '[https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/](https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/)'
- en: Kubernetes federation
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes联邦
- en: A federation is a cluster of clusters. In other words, it's formed by multiple
    Kubernetes clusters and is accessible from a single control plane. The resources
    that are created on a federation will be synchronized across all connected clusters.
    As of Kubernetes 1.7, resources that can be federated include Namespace, ConfigMap,
    Secret, Deployment, DaemonSet, Service, and Ingress.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦是一组集群。换句话说，它由多个Kubernetes集群组成，并且可以从单个控制平面访问。在联邦上创建的资源将在所有连接的集群中同步。截至Kubernetes
    1.7，可以联邦的资源包括Namespace、ConfigMap、Secret、Deployment、DaemonSet、Service和Ingress。
- en: Capabilities of the federation to build a hybrid platform bring us another level
    of flexibility when architecting our software. For instance, we can federate clusters
    deployed in on-premise data centers and various public clouds together, to distribute
    workloads by cost, and utilize platform-specific features while keeping the elasticity
    to move around. Another typical use case is federating clusters scattered in different
    geographical locations to lower the edge latency to customers across the globe.
    Moreover, a single Kubernetes cluster backed by etcd3 supports 5,000 nodes while
    keeping the p99 of API response time less than 1 second (on version 1.6). If there
    is a need to have a cluster with thousands of nodes or beyond, we can surely federate
    clusters to get there.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦的能力为我们构建混合平台带来了另一个灵活性水平，例如，我们可以将部署在本地数据中心和各种公共云中的集群联合起来，通过成本分配工作负载，并利用特定于平台的功能，同时保持灵活性以便移动。另一个典型的用例是将分散在不同地理位置的集群联合起来，以降低全球客户的边缘延迟。此外，支持5,000个节点的单个Kubernetes集群（在版本1.6上）可以保持API响应时间的p99小于1秒。如果需要具有数千个节点或更多的集群，我们可以通过联合集群来实现。
- en: 'The guide for a federation can be found at the following link: [https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/](https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦指南可以在以下链接找到：[https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/](https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/)。
- en: Cluster add-ons
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群附加组件
- en: Cluster add-ons are programs, that are designed or configured to enhance a Kubernetes
    cluster, and they are considered to be inherent parts of Kubernetes. For instance,
    Heapster, which we used in [Chapter 6](part0138.html#43JDK0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Monitoring and Logging*, is one of the add-on components, and so is the node-problem-detector
    we mentioned earlier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 集群附加组件是旨在增强Kubernetes集群的程序，被认为是Kubernetes的固有部分。例如，我们在[第6章](part0138.html#43JDK0-6c8359cae3d4492eb9973d94ec3e4f1e)中使用的Heapster，*监控和日志*，是其中一个附加组件，我们之前提到的node-problem-detector也是如此。
- en: 'As cluster add-ons may be used in some critical functions, some hosted Kubernetes
    services such as GKE deploy the add-on manager to safeguard the state of the add-ons
    from being modified or deleted. Managed add-ons will be deployed with a label,
    `addonmanager.kubernetes.io/mode`, on the pod controller. If the mode is `Reconcile`,
    any modification to the specification will be rolled back to its initial state;
    the `EnsureExists` mode only checks whether the controller exists, but doesn''t
    check whether its specification is modified. For instance, the following Deployments
    are deployed on a 1.7.3 GKE cluster by default, and all of them are protected
    in the `Reconcile` mode:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集群附加组件可能用于一些关键功能，一些托管的Kubernetes服务（如GKE）部署了附加组件管理器，以保护附加组件的状态不被修改或删除。托管的附加组件将在pod控制器上部署一个标签`addonmanager.kubernetes.io/mode`。如果模式是`Reconcile`，对规范的任何修改都将回滚到其初始状态；`EnsureExists`模式只检查控制器是否存在，但不检查其规范是否被修改。例如，默认情况下，在1.7.3
    GKE集群上部署以下部署，并且它们都受到`Reconcile`模式的保护：
- en: '![](../images/00147.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00147.jpeg)'
- en: 'If you''d like to deploy add-ons in your own cluster, they can be found at:
    [https://github.com/kubernetes/kubernetes/tree/master/cluster/addons](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在自己的集群中部署附加组件，可以在以下链接找到它们：[https://github.com/kubernetes/kubernetes/tree/master/cluster/addons](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)。
- en: Kubernetes and communities
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes和社区
- en: When choosing an open source tool to use, we definitely wonder how supportiveness
    it is after we begin to use it. The supportiveness includes factors such as who
    is leading the project, whether the project is opinionated, how is the project's
    popularity, and so on.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择要使用的开源工具时，我们肯定会想知道在开始使用后它的支持性如何。支持性包括项目的领导者是谁，项目是否持有意见，项目的受欢迎程度等因素。
- en: Kubernetes originated from Google, and it's now backed by the **Cloud Native
    Computing Foundation** (**CNCF**, [https://www.cncf.io](https://www.cncf.io)).
    At the time when Kubernetes 1.0 was released, Google partnered with the Linux
    Foundation to form the CNCF, and donated Kubernetes as the seed project. The CNCF
    is meant to promote the development of containerized, dynamic orchestrated, and
    microservices-oriented applications.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes起源于Google，现在由**Cloud Native Computing Foundation**（**CNCF**，[https://www.cncf.io](https://www.cncf.io)）支持。在Kubernetes
    1.0发布时，Google与Linux基金会合作成立了CNCF，并捐赠了Kubernetes作为种子项目。CNCF旨在推动容器化、动态编排和面向微服务的应用程序的发展。
- en: Since all projects under the CNCF is container-based, they certainly could work
    fluently with Kubernetes. Prometheus, Fluentd, and OpenTracing, which we demonstrated
    and mentioned in [Chapter 6](part0138.html#43JDK0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Monitoring and Logging*, are all member projects of the CNCF.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CNCF下的所有项目都是基于容器的，它们肯定可以与Kubernetes无缝配合。Prometheus、Fluentd和OpenTracing，我们在第6章中展示和提到的*监控和日志*，都是CNCF的成员项目。
- en: Kubernetes incubator
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes孵化器
- en: 'Kubernetes incubator is a process to support projects for Kubernetes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes孵化器是支持Kubernetes项目的过程：
- en: '[https://github.com/kubernetes/community/blob/master/incubator.md](https://github.com/kubernetes/community/blob/master/incubator.md).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes/community/blob/master/incubator.md](https://github.com/kubernetes/community/blob/master/incubator.md).'
- en: Graduated projects might become a core function of Kubernetes, a cluster add-on,
    or an independent tool for Kubernetes. Throughout the book, we have already seen
    and used many of them, including the Heapster, cAdvisor, dashboard, minikube,
    kops, kube-state-metrics, and kube-problem-detector, whatever makes Kubernetes
    better and better. You can explore these projects under Kubernetes ([https://github.com/kubernetes](https://github.com/kubernetes)),
    or the Incubator ([https://github.com/kubernetes-incubator](https://github.com/kubernetes-incubator)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 毕业项目可能成为Kubernetes的核心功能、集群附加组件，或者是Kubernetes的独立工具。在整本书中，我们已经看到并使用了许多这样的项目，包括Heapster、cAdvisor、仪表板、minikube、kops、kube-state-metrics和kube-problem-detector，所有这些都让Kubernetes变得更好。您可以在Kubernetes（[https://github.com/kubernetes](https://github.com/kubernetes)）或孵化器（[https://github.com/kubernetes-incubator](https://github.com/kubernetes-incubator)）下探索这些项目。
- en: Helm and charts
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Helm和图表
- en: Helm ([https://github.com/kubernetes/helm](https://github.com/kubernetes/helm))
    is a package manager, that simplifies the day-0 through day-n operations of running
    software on Kubernetes. It's also a graduated project from the incubator.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Helm（[https://github.com/kubernetes/helm](https://github.com/kubernetes/helm)）是一个软件包管理器，简化了在Kubernetes上运行软件的day-0到day-n操作。它也是孵化器中的一个毕业项目。
- en: As what we've learned in [Chapter 7](part0163.html#4REBM0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Continuous Delivery*, deploying a containerized software to Kubernetes is basically
    writing manifests. Nonetheless, an application may be built with dozens of Kubernetes
    resources. If we're going to deploy such an application many times, the task to
    rename the conflict parts could be cumbersome. If we introduce the idea of template
    engines to solve the renaming hell, we will soon realize that we should have a
    place to store the templates as well as the rendered manifests. Hence, the Helm
    is meant to solve such annoying chores.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第7章](part0163.html#4REBM0-6c8359cae3d4492eb9973d94ec3e4f1e)中学到的，*持续交付*，将容器化软件部署到Kubernetes基本上就是编写清单。尽管如此，一个应用可能由数十个Kubernetes资源构建而成。如果我们要多次部署这样的应用程序，重命名冲突部分的任务可能会很繁琐。如果我们引入模板引擎的概念来解决重命名的困扰，我们很快就会意识到我们应该有一个地方来存储模板以及渲染后的清单。因此，Helm旨在解决这些烦人的琐事。
- en: 'A package in Helm is called a chart, and it''s a collection of configurations,
    definitions, and manifests to run an application. Charts contributed by the communities
    are published here: [https://github.com/kubernetes/charts](https://github.com/kubernetes/charts).
    Even if we are not going to use it, we can still find verified manifests for a
    certain package there.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Helm中的一个包被称为chart，它是运行应用程序的配置、定义和清单的集合。社区贡献的图表发布在这里：[https://github.com/kubernetes/charts](https://github.com/kubernetes/charts)。即使我们不打算使用它，我们仍然可以在那里找到特定包的经过验证的清单。
- en: 'Using Helm is quite simple. First get the Helm by running the official installation
    script here: [https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get](https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Helm非常简单。首先通过运行官方安装脚本获取Helm：[https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get](https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get)。
- en: 'After getting the Helm binary working, it fetches our kubectl configurations
    to connect to the cluster. We''d need to have a manager `Tiller` inside our Kubernetes
    cluster to manage every deployment task from Helm:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取Helm二进制文件后，它会获取我们的kubectl配置以连接到集群。我们需要在Kubernetes集群中有一个名为`Tiller`的管理器来管理Helm的每个部署任务：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If we'd like to initialize the Helm client without installing the Tiller to
    our Kubernetes cluster, we can add the `--client-only` flag to `helm init`. Furthermore,
    using the `--skip-refresh` flag together allows us to initialize the client offline.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要在不将Tiller安装到我们的Kubernetes集群中的情况下初始化Helm客户端，我们可以在`helm init`中添加`--client-only`标志。此外，一起使用`--skip-refresh`标志可以让我们离线初始化客户端。
- en: 'The Helm client is able to search the available charts from the command line:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Helm客户端能够从命令行搜索可用的图表：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s install a chart from the repository, say the last one, `wordpress`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从存储库安装一个图表，比如最后一个`wordpress`：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The deployed chart in Helm is a release. Here, we have a release, `plinking-billygoat`,
    installed. Once the pods and the services are ready, we can connect to our site
    and check the result:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在Helm中部署的图表被称为发布。在这里，我们有一个名为`plinking-billygoat`的发布已安装。一旦pod和服务准备就绪，我们就可以连接到我们的站点并检查结果。
- en: '![](../images/00148.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00148.jpeg)'
- en: 'The teardown of a release also takes only one line of command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 释放发布也只需要一行命令：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Helm leverages ConfigMap to store the metadata of a release, but deleting a
    release with `helm delete` won't delete its metadata. To wholly clear these metadata,
    we could either manually delete these ConfigMaps or add the `--purge` flag when
    executing `helm delete`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Helm利用ConfigMap来存储发布的元数据，但使用`helm delete`删除发布不会删除其元数据。要完全清除这些元数据，我们可以手动删除这些ConfigMaps，或者在执行`helm
    delete`时添加`--purge`标志。
- en: In addition to managing packages in our cluster, another value brought by Helm
    is it is established as a standard to share packages and so it allows us to install
    popular software directly, such as the Wordpress we installed, rather than rewriting
    manifests for every software we used.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在我们的集群中管理软件包，Helm带来的另一个价值是它被确立为共享软件包的标准，因此它允许我们直接安装流行的软件，比如我们安装的Wordpress，而不是为我们使用的每个软件重写清单。
- en: Gravitating towards a future infrastructure
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝着未来基础设施的发展趋势。
- en: It's always hard to tell whether a tool is a right fit or not, especially on
    opting for a cluster management software to underpin business missions, because
    the difficulties and challenges with which everyone is confronted varies. Apart
    from objective concerns such as performance, stability, availability, scalability,
    and usability, real circumstances also account for a significant portion of the
    decision. For instance, perspective on choosing a stack for developing greenfield
    projects and for building additional layers on top of bulky legacy systems could
    be diverse. Likewise, operating services by a highly cohesive DevOps team and
    by an organization working in the old day styles could also lead to distinct choices.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 很难判断一个工具是否合适，特别是在选择一个支撑业务任务的集群管理软件时，因为每个人面临的困难和挑战都不同。除了性能、稳定性、可用性、可扩展性和可用性等客观问题外，实际情况也占据了决策的重要部分。例如，选择一个堆栈来开发全新项目和在庞大的传统系统之上构建额外层次的观点可能是不同的。同样，由高度内聚的DevOps团队和以老式方式工作的组织来操作服务也可能导致不同的选择。
- en: In addition to Kubernetes, there are still other platforms, which also feature
    orchestrating containers, and they all provide some easy ways to getting started.
    Let's step back and take an overview over them to find out the best fit.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Kubernetes，还有其他平台也具有容器编排功能，并且它们都提供了一些简单的入门方式。让我们退一步，对它们进行概述，找出最合适的选择。
- en: Docker swarm mode
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker swarm模式
- en: Swarm mode ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/))
    is Docker's native orchestrator integrated in the Docker engine since version
    1.12\. As such, it shares the same API and user interface with Docker itself,
    including the use of Docker Compose files. Such degrees of integration are considered
    to be advantages as well as disadvantages depending on if one is comfortable with
    working on a stack, where all the components are from the same vendor.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm模式（[https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)）是自Docker
    1.12版本以来集成在Docker引擎中的本地编排器。因此，它与Docker本身共享相同的API和用户界面，包括使用Docker Compose文件。这种程度的集成被认为是优势，也被认为是劣势，取决于一个人是否习惯于在一个堆栈上工作，其中所有组件都来自同一个供应商。
- en: 'A swarm cluster consists of managers and workers, where the managers are part
    of a consensus group to maintain the state of a cluster while keeping high availability.
    Enabling the swarm mode is quite easy. Roughly speaking, it''s only two steps
    here: creating a cluster with `docker swarm init` and joining other managers and
    workers with `docker swarm join`. Additionally, Docker Cloud ([https://cloud.docker.com/swarm](https://cloud.docker.com/swarm))
    provided by Docker helps us bootstrap a swam cluster on various cloud providers.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个swarm集群由管理者和工作者组成，其中管理者是共识组的一部分，用于维护集群的状态并保持高可用性。启用swarm模式非常容易。大致上，这里只有两个步骤：使用`docker
    swarm init`创建一个集群，然后使用`docker swarm join`加入其他管理者和工作者。此外，Docker提供的Docker Cloud（[https://cloud.docker.com/swarm](https://cloud.docker.com/swarm)）帮助我们在各种云提供商上引导一个swam集群。
- en: Features that come with the swarm mode are the ones we'd expect to have in a
    container platform, that is to say, container lifecycle managements, two scheduling
    strategies (replicated and global, which resemble to Deployment and DaemonSet
    in Kubernetes respectively), service discovery, secret managements, and so on.
    There is also an ingress network that works like the NodePort type service in
    Kubernetes, but we'll have to bring up something such as nginx or Traefik if we
    need a L7 layer LoadBalancer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm模式带来的功能是我们在容器平台中所期望的，也就是说，容器生命周期管理，两种调度策略（复制和全局，分别类似于Kubernetes中的部署和DaemonSet），服务发现，秘密管理等。还有一个类似于Kubernetes中NodePort类型服务的入口网络，但如果我们需要L7层负载均衡器，我们将不得不启动类似nginx或Traefik的东西。
- en: All in all, the swarm mode proffers an option to orchestrate containerized applications
    that works out of the box once one begins to use Docker. Meanwhile, as it speaks
    the same language with Docker and simple architecture, it's also considered to
    be the easiest platform among all choices. Therefore, it's indeed reasonable to
    choose the swarm mode to get something done quickly. However, its simplicity sometimes
    leads to lack of flexibility. For example, in Kubernetes we are able to employ
    Blue/Green deployment strategy by merely manipulating selector and labels, but
    there is no easy way to do so in the swarm mode. Since the swarm mode is still
    under active development, such as the function to store configuration data, which
    is analogous to ConfigMap in Kubernetes is introduced in version 17.06, we definitely
    could look forward to the swarm mode becoming more powerful in the future while
    retaining its simplicity.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Swarm模式提供了一个选项，可以在开始使用Docker后立即使用的编排容器化应用程序。与此同时，由于它与Docker使用相同的语言和简单的架构，它也被认为是所有选择中最简单的平台。因此，选择Swarm模式来快速完成某些工作是合理的。然而，它的简单性有时会导致缺乏灵活性。例如，在Kubernetes中，我们可以通过简单地操作选择器和标签来使用蓝/绿部署策略，但在Swarm模式中没有简单的方法来实现这一点。由于Swarm模式仍在积极开发中，例如在17.06版本中引入了存储配置数据的功能，这类似于Kubernetes中的ConfigMap，我们确实可以期待Swarm模式在保持简单性的同时在未来变得更加强大。
- en: Amazon EC2 container service
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊EC2容器服务
- en: EC2 container service (ECS, [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/))
    is AWS' answer to the Docker upsurge. Unlike Google Cloud Platform and Microsoft
    Azure, which provides open source cluster managers such as Kubernetes, Docker
    Swarm, and DC/OS, AWS sticks to its own way in response to the need of container
    services.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: EC2容器服务（ECS，[https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)）是AWS对Docker激增的回应。与谷歌云平台和微软Azure提供的开源集群管理器（如Kubernetes，Docker
    Swarm和DC/OS）不同，AWS坚持采用自己的方式来满足容器服务的需求。
- en: ECS takes its Docker as its container runtime, and it also accepts Docker Compose
    files in syntax version 2\. Moreover, terminologies of ECS and Docker Swarm mode
    are pretty much the same thing, such as the idea of task and service. Yet the
    similarities stop here. Although the core functions of ECS is simple and even
    rudimentary, as a part of AWS, ECS fully utilizes other AWS products to enhance
    itself such as VPC for container networking, CloudWatch, and CloudWatch Logs for
    monitoring and logging, Application LoadBalancer and Network LoadBalancer with
    Target Groups for service discovering, Lambda with Route 53 for DNS-based service
    discovering, CloudWatch Events for CronJob, EBS and EFS for data persistence,
    ECR for docker registry, Parameter Store and KMS for storing configuration files
    and secrets, CodePipeline for CI/CD, and so forth. There is another AWS product,
    AWS Batch ([https://aws.amazon.com/batch/](https://aws.amazon.com/batch/)) that
    is built on top of ECS for processing batch workloads. Furthermore, an open source
    tool from AWS ECS team, Blox ([https://blox.github.io](https://blox.github.io)),
    augments the capabilities to customize the scheduling that are not shipped with
    ECS, such as the DaemonSet-like strategy, by wiring couples of AWS products up.
    From another perspective, if we take AWS as an integral whole to evaluate ECS,
    it's truly mighty.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ECS将Docker作为其容器运行时，并且还接受语法版本2的Docker Compose文件。此外，ECS和Docker Swarm模式的术语基本相同，比如任务和服务的概念。然而，相似之处止步于此。尽管ECS的核心功能简单甚至基本，作为AWS的一部分，ECS充分利用其他AWS产品来增强自身，比如用于容器网络的VPC，用于监控和日志记录的CloudWatch和CloudWatch
    Logs，用于服务发现的Application LoadBalancer和Network LoadBalancer与Target Groups，用于基于DNS的服务发现的Lambda与Route
    53，用于CronJob的CloudWatch Events，用于数据持久性的EBS和EFS，用于Docker注册表的ECR，用于存储配置文件和秘密的Parameter
    Store和KMS，用于CI/CD的CodePipeline等等。还有另一个AWS产品，AWS Batch（[https://aws.amazon.com/batch/](https://aws.amazon.com/batch/)），它是建立在ECS之上用于处理批处理工作负载的。此外，AWS
    ECS团队的开源工具Blox（[https://blox.github.io](https://blox.github.io)）增强了定制调度的能力，这些能力在ECS中没有提供，比如类似DaemonSet的策略，通过将AWS产品进行耦合。从另一个角度来看，如果我们将AWS作为一个整体来评估ECS，它确实非常强大。
- en: 'Setting up an ECS cluster is easy: create an ECS cluster via the AWS console
    or API and join EC2 nodes with the ECS agent to the cluster. The good thing is
    that the master side is managed by AWS so that we are free from keeping wary eye
    on the master.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 建立ECS集群很容易：通过AWS控制台或API创建ECS集群，并使用ECS代理将EC2节点加入集群。好处在于，主控端由AWS管理，因此我们无需时刻警惕主控端。
- en: Overall, ECS is easy to getting started, especially for people who are familiar
    with Docker as well as AWS products. On the other hand, if we aren't satisfied
    with the primitives currently provided, we have to do some handworks either with
    other AWS services mentioned earlier or third-party solutions to get things done,
    and this could result in undesired costs on those services and efforts on configurations
    and maintenances to make sure every component works together nicely. Besides,
    ECS is only available on AWS, which could also be one concern that people would
    take it seriously.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，ECS很容易上手，特别是对于熟悉Docker和AWS产品的人来说。另一方面，如果我们对目前提供的基本功能不满意，我们必须通过其他AWS服务或第三方解决方案来完成一些手工工作，这可能会导致在这些服务上产生不必要的成本，并且需要配置和维护来确保每个组件能够良好地协同工作。此外，ECS仅在AWS上可用，这也可能是人们认真对待它的一个关注点。
- en: Apache Mesos
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mesos
- en: '*Mesos* ([http://mesos.apache.org/)](http://mesos.apache.org/)) had been created
    long before Docker set off the trend of containers, and its goal is to solve the
    difficulties regarding management of resources in a cluster comprising general
    hardware while supporting diverse workloads. To build such a general platform,
    Mesos makes use of a two-tier architecture to divide the resource allocation and
    the task execution. As such, the execution part can theoretically extend to any
    kind of task, including orchestrating Docker containers.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos（[http://mesos.apache.org/)](http://mesos.apache.org/)）在Docker引发容器潮流之前就已经存在，其目标是解决集群中资源管理的困难，同时支持各种工作负载。为了构建这样一个通用平台，Mesos利用两层架构来划分资源分配和任务执行。因此，执行部分理论上可以扩展到任何类型的任务，包括编排Docker容器。
- en: Even though we talked about only the name Mesos here, it is basically in charge
    of one tier of jobs as a matter of fact, and the execution part is done by other
    components called Mesos frameworks. For example, Marathon ([https://mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/))
    and Chronos ([https://mesos.github.io/chronos/](https://mesos.github.io/chronos/))
    were two popular frameworks to deploy long-running and batch-job tasks respectively,
    and both of them support the Docker container. In this way, when it comes to the
    term Mesos, it's referring to a stack such as Mesos/Marathon/Chronos or Mesos/Aurora.
    In fact, under Mesos' two-tier architecture, it's viable to run Kubernetes as
    a Mesos framework as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这里只谈到了Mesos这个名字，但实际上它基本上负责一层工作，执行部分是由其他组件称为Mesos框架来完成的。例如，Marathon（[https://mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/)）和Chronos（[https://mesos.github.io/chronos/](https://mesos.github.io/chronos/)）是两个流行的框架，分别用于部署长时间运行和批处理任务，并且两者都支持Docker容器。因此，当提到Mesos这个术语时，它指的是一个堆栈，比如Mesos/Marathon/Chronos或Mesos/Aurora。事实上，在Mesos的两层架构下，也可以将Kubernetes作为Mesos框架来运行。
- en: Frankly speaking, a properly organized Mesos stack and Kubernetes are pretty
    much the same in terms of capabilities except that Kubernetes requires that everything
    that is run on it should be containerized regardless of Docker, rkt, or a hypervisor
    container. On the other hand, as Mesos focuses on its generic scheduling and tends
    to keep its core small, some essential functions should be installed, tested,
    and operated separately, which could bring about extra efforts.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 坦率地说，一个组织良好的Mesos堆栈和Kubernetes在能力方面基本上是一样的，只是Kubernetes要求在其上运行的一切都应该是容器化的，无论是Docker、rkt还是虚拟化容器。另一方面，由于Mesos专注于其通用调度，并倾向于保持其核心精简，一些基本功能需要单独安装、测试和操作，这可能需要额外的努力。
- en: DC/OS ([https://dcos.io/](https://dcos.io/)) published by Mesosphere takes advantages
    of Mesos to build a full-stack cluster management platform, which is more comparable
    to Kubernetes with respect to capabilities. As a one-stop-shop for every solution
    built atop Mesos, it bundles couples of components to drive the whole system,
    Marathon for common workloads, Metronome for scheduled jobs, Mesos-DNS for service
    discovery, and so forth. Though these building blocks seem to be complicated,
    DC/OS greatly simplified the works on installations and configurations by CloudFormation/Terraform
    templates, and its package management system, Mesosphere Universe. Since DC/OS
    1.10, Kubernetes is officially integrated into DC/OS, and it can be installed
    via the Universe. Hosted DC/OS is also available on some cloud providers such
    as Microsoft Azure.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Mesosphere发布的DC/OS（[https://dcos.io/](https://dcos.io/)）利用Mesos构建了一个全栈集群管理平台，这在功能上更类似于Kubernetes。作为建立在Mesos之上的每个解决方案的一站式商店，它捆绑了一些组件来驱动整个系统，Marathon用于常见工作负载，Metronome用于定期作业，Mesos-DNS用于服务发现等等。尽管这些构建块似乎很复杂，但DC/OS通过CloudFormation/Terraform模板和其软件包管理系统Mesosphere
    Universe大大简化了安装和配置工作。自DC/OS 1.10以来，Kubernetes已正式集成到DC/OS中，并可以通过Universe安装。托管的DC/OS也可在一些云提供商上使用，如Microsoft
    Azure。
- en: 'The following screenshot is the web console interface of DC/OS, which aggregates
    information from every component:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是DC/OS的Web控制台界面，汇总了来自每个组件的信息：
- en: '![](../images/00149.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00149.jpeg)'
- en: So far we have discussed the community version of DC/OS, but some features are
    only available in the enterprise edition. They are mostly on security and compliance,
    and the list can be found at [https://mesosphere.com/pricing/](https://mesosphere.com/pricing/).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了DC/OS的社区版本，但一些功能仅在企业版中可用。它们主要涉及安全性和合规性，列表可以在[https://mesosphere.com/pricing/](https://mesosphere.com/pricing/)找到。
- en: Summary
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have briefly discussed Kubernetes features that applies
    to certain more specific use cases, and guided where and how to leverage the strong
    communities, including the Kubernetes incubator and the package manager Helm.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要讨论了适用于特定更具体用例的Kubernetes功能，并指导了如何利用强大的社区，包括Kubernetes孵化器和软件包管理器Helm。
- en: 'In the end, we went back to the start and gave overview to three other popular
    alternatives for the same goal: orchestrating containers, so as to leave the conclusion
    in your mind for choosing your next generation infrastructure.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回到起点，概述了实现相同目标的其他三种流行替代方案：编排容器，以便让您选择下一代基础架构的结论留在您的脑海中。
