- en: Introduction to Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes简介
- en: In the previous chapter, we learned how SwarmKit uses rolling updates to achieve
    zero downtime deployments. We were also introduced to Docker configs, which are
    used to store nonsensitive data in clusters and use this to configure application
    services, as well as Docker secrets, which are used to share confidential data
    with an application service running in a Docker Swarm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了SwarmKit如何使用滚动更新来实现零停机部署。我们还介绍了Docker配置文件，用于在集群中存储非敏感数据并用于配置应用程序服务，以及Docker秘密，用于与在Docker
    Swarm中运行的应用程序服务共享机密数据。
- en: In this chapter, we're going to introduce Kubernetes. Kubernetes is currently
    the clear leader in the container orchestration space. We will start with a high-level
    overview of the architecture of a Kubernetes cluster and then discuss the main
    objects used in Kubernetes to define and run containerized applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Kubernetes。Kubernetes目前是容器编排领域的明显领导者。我们将从高层次概述Kubernetes集群的架构开始，然后讨论Kubernetes中用于定义和运行容器化应用程序的主要对象。
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Kubernetes architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: Kubernetes master nodes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes主节点
- en: Cluster nodes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群节点
- en: Introduction to MiniKube
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MiniKube简介
- en: Kubernetes support in Docker for Desktop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker for Desktop中的Kubernetes支持
- en: Introduction to pods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod简介
- en: Kubernetes ReplicaSet
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes ReplicaSet
- en: Kubernetes deployment
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes部署
- en: Kubernetes service
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes服务
- en: Context-based routing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于上下文的路由
- en: Comparing SwarmKit with Kubernetes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较SwarmKit和Kubernetes
- en: 'After finishing this chapter, you will be able to do the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够做到以下事项：
- en: Draft the high-level architecture of a Kubernetes cluster on a napkin
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在餐巾纸上起草Kubernetes集群的高层架构
- en: Explain three to four main characteristics of a Kubernetes pod
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Kubernetes pod的三到四个主要特征
- en: Describe the role of Kubernetes ReplicaSets in two to three short sentences
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用两三句话描述Kubernetes ReplicaSets的作用
- en: Explain two or three main responsibilities of a Kubernetes service
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Kubernetes服务的两三个主要职责
- en: Create a pod in Minikube
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Minikube中创建一个pod
- en: Configure Docker for Desktop in order to use Kubernetes as an orchestrator
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Docker for Desktop以使用Kubernetes作为编排器
- en: Create a deployment in Docker for Desktop
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Docker for Desktop中创建一个部署
- en: Create a Kubernetes service to expose an application service internally (or
    externally) to the cluster
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个Kubernetes服务，将应用程序服务在集群内（或外部）暴露出来
- en: Technical requirements
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code files for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition).
    Alternatively, if you cloned the GitHub repository that accompanies this book
    to your computer, as described in [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml),
    *Setting Up a Working Environment*, then you can find the code at `~/fod-solution/ch15`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：[https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition)。或者，如果您在计算机上克隆了伴随本书的GitHub存储库，如[第2章](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml)中所述，*设置工作环境*，那么您可以在`~/fod-solution/ch15`找到代码。
- en: Kubernetes architecture
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: A Kubernetes cluster consists of a set of servers. These servers can be VMs
    or physical servers. The latter are also called *bare metal*. Each member of the
    cluster can have one of two roles. It is either a Kubernetes master or a (worker)
    node. The former is used to manage the cluster, while the latter will run an application
    workload. I have put the worker in parentheses since, in Kubernetes parlance,
    you only talk about a node when you're talking about a server that runs application
    workloads. But in Docker parlance and in Swarm, the equivalent is a *worker node*.
    I think that the notion of a worker node better describes the role of the server
    than a simple *node*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群由一组服务器组成。这些服务器可以是虚拟机或物理服务器。后者也被称为**裸金属**。集群的每个成员可以扮演两种角色中的一种。它要么是Kubernetes主节点，要么是（工作）节点。前者用于管理集群，而后者将运行应用程序工作负载。我在工作节点中加了括号，因为在Kubernetes术语中，只有在谈论运行应用程序工作负载的服务器时才会谈论节点。但在Docker术语和Swarm中，相当于的是*工作节点*。我认为工作节点这个概念更好地描述了服务器的角色，而不仅仅是一个*节点*。
- en: In a cluster, you have a small and odd number of masters and as many worker
    nodes as needed. Small clusters might only have a few worker nodes, while more
    realistic clusters might have dozens or even hundreds of worker nodes. Technically,
    there is no limit to how many worker nodes a cluster can have; in reality, though,
    you might experience a significant slowdown in some management operations when
    dealing with thousands of nodes. All members of the cluster need to be connected
    by a physical network, the so-called **underlay network**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个集群中，你会有少量奇数个的主节点和所需数量的工作节点。小集群可能只有几个工作节点，而更现实的集群可能有数十甚至数百个工作节点。从技术上讲，集群可以拥有无限数量的工作节点；但实际上，当处理数千个节点时，你可能会在一些管理操作中遇到显著的减速。集群的所有成员都需要通过一个物理网络连接，即所谓的**底层网络**。
- en: Kubernetes defines one flat network for the whole cluster. Kubernetes does not
    provide any networking implementation out of the box; instead, it relies on plugins
    from third parties. Kubernetes just defines the **Container Network Interface** (**CNI**)
    and leaves the implementation to others. The CNI is pretty simple. It basically
    states that each pod running in the cluster must be able to reach any other pod
    also running in the cluster without any **Network** **Address** **Translation**
    (**NAT**) happening in-between. The same must be true between cluster nodes and
    pods, that is, applications or daemons running directly on a cluster node must
    be able to reach each pod in the cluster and vice versa.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为整个集群定义了一个扁平网络。Kubernetes不会提供任何开箱即用的网络实现；相反，它依赖于第三方的插件。Kubernetes只是定义了**容器网络接口**（CNI），并将实现留给其他人。CNI非常简单。它基本上规定了集群中运行的每个pod必须能够在不经过任何**网络地址转换**（NAT）的情况下到达集群中运行的任何其他pod。集群节点和pod之间也必须是如此，也就是说，直接在集群节点上运行的应用程序或守护程序必须能够到达集群中的每个pod，反之亦然。
- en: 'The following diagram illustrates the high-level architecture of a Kubernetes
    cluster:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了Kubernetes集群的高级架构：
- en: '![](assets/8f31f291-f723-4876-8ff9-349f2c42d114.jpg)High-level architecture
    diagram of Kubernetes'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/8f31f291-f723-4876-8ff9-349f2c42d114.jpg)Kubernetes的高级架构图'
- en: 'The preceding diagram is explained as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图解释如下：
- en: On the top, in the middle, we have a cluster of **etcd** nodes. **etcd** is
    a distributed key-value store that, in a Kubernetes cluster, is used to store
    all the state of the cluster. The number of **etcd** nodes has to be odd, as mandated
    by the Raft consensus protocol, which states which nodes are used to coordinate
    among themselves. When we talk about the **Cluster State**, we do not include
    data that is produced or consumed by applications running in the cluster; instead,
    we're talking about all the information on the topology of the cluster, what services
    are running, network settings, secrets used, and more. That said, this **etcd**
    cluster is really mission-critical to the overall cluster and thus, we should
    never run only a single **etcd** server in a production environment or any environment
    that needs to be highly available.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在顶部中间，我们有一组**etcd**节点。**etcd**是一个分布式键值存储，在Kubernetes集群中用于存储集群的所有状态。**etcd**节点的数量必须是奇数，根据Raft共识协议的规定，该协议规定了用于彼此协调的节点。当我们谈论**集群状态**时，我们不包括集群中运行的应用程序产生或消耗的数据；相反，我们谈论的是集群拓扑的所有信息，正在运行的服务，网络设置，使用的密钥等。也就是说，这个**etcd**集群对整个集群非常关键，因此，在生产环境或需要高可用性的任何环境中，我们永远不应该只运行单个**etcd**服务器。
- en: Then, we have a cluster of Kubernetes **master** nodes, which also form a **Consensus**
    **Group** among themselves, similar to the **etcd** nodes. The number of master
    nodes also has to be odd. We can run cluster with a single master but we should
    never do that in a production or mission-critical system. There, we should always
    have at least three master nodes. Since the master nodes are used to manage the
    whole cluster, we are also talking about the management plane. Master nodes use
    the **etcd** cluster as their backing store. It is good practice to put a **load**
    **balancer** (**LB**) in front of master nodes with a well-known **Fully Qualified
    Domain Name** (**FQDN**), such as `https://admin.example.com`. All tools that
    are used to manage the Kubernetes cluster should access it through this LB rather
    than using the public IP address of one of the master nodes. This is shown on
    the left upper side of the preceding diagram.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们有一组Kubernetes **master**节点，它们也形成一个**共识组**，类似于**etcd**节点。主节点的数量也必须是奇数。我们可以使用单个主节点运行集群，但在生产或关键系统中绝不能这样做。在那里，我们应该始终至少有三个主节点。由于主节点用于管理整个集群，我们也在谈论管理平面。主节点使用**etcd**集群作为其后备存储。在主节点前面放置一个**负载均衡器**（**LB**）是一个良好的做法，具有一个众所周知的**完全合格的域名**（**FQDN**），例如`https://admin.example.com`。用于管理Kubernetes集群的所有工具都应该通过这个LB访问，而不是使用其中一个主节点的公共IP地址。这在上图的左上方显示。
- en: Toward the bottom of the diagram, we have a cluster of **worker** nodes. The
    number of nodes can be as low as one and does not have an upper limit. Kubernetes
    master and worker nodes communicate with each other. It is a bidirectional form
    of communication that is different from the one we know from Docker Swarm. In
    Docker Swarm, only manager nodes communicate with worker nodes and never the other
    way around. All ingress traffic accessing applications running in the cluster
    should go through another **load balancer**. This is the application **load**
    **balancer** or reverse proxy. We never want external traffic to directly access
    any of the worker nodes.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图表底部，我们有一组**worker**节点。节点数量可以低至一个，没有上限。Kubernetes的主节点和工作节点之间进行通信。这是一种双向通信，与我们从Docker
    Swarm中所知的通信方式不同。在Docker Swarm中，只有管理节点与工作节点通信，而不是相反。访问集群中运行的应用程序的所有入口流量都应该通过另一个**负载均衡器**。这是应用程序**负载均衡器**或反向代理。我们永远不希望外部流量直接访问任何工作节点。
- en: Now that we have an idea about the high-level architecture of a Kubernetes cluster,
    let's delve a bit more deeply and look at the Kubernetes master and worker nodes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Kubernetes集群的高级架构有了一个概念，让我们深入一点，看看Kubernetes的主节点和工作节点。
- en: Kubernetes master nodes
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes主节点
- en: 'Kubernetes master nodes are used to manage a Kubernetes cluster. The following
    is a high-level diagram of such a master:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes主节点用于管理Kubernetes集群。以下是这样一个主节点的高级图表：
- en: '![](assets/36896fcb-357f-497a-822d-6d08c3a72c1e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/36896fcb-357f-497a-822d-6d08c3a72c1e.png)'
- en: Kubernetes master
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes主节点
- en: 'At the bottom of the preceding diagram, we have the **Infrastructure**, which
    can be a VM on-premise or in the cloud or a server (often called bare metal) on-premise
    or in the cloud. Currently, Kubernetes masters only run on **Linux**. The most
    popular Linux distributions, such as RHEL, CentOS, and Ubuntu, are supported.
    On this Linux machine, we have at least the following four Kubernetes services
    running:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图的底部，我们有**基础设施**，它可以是本地或云端的虚拟机，也可以是本地或云端的服务器（通常称为裸金属）。目前，Kubernetes主节点只能在**Linux**上运行。支持最流行的Linux发行版，如RHEL、CentOS和Ubuntu。在这台Linux机器上，我们至少运行以下四个Kubernetes服务：
- en: '**API server**: This is the gateway to Kubernetes. All requests to list, create,
    modify, or delete any resources in the cluster must go through this service. It
    exposes a REST interface that tools such as `kubectl` use to manage the cluster
    and applications in the cluster.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API服务器**：这是Kubernetes的网关。所有对集群中任何资源进行列出、创建、修改或删除的请求都必须通过这个服务。它暴露了一个REST接口，像`kubectl`这样的工具用来管理集群和集群中的应用程序。'
- en: '**Controller**: The controller, or more precisely the controller manager, is a
    control loop that observes the state of the cluster through the API server and
    makes changes, attempting to move the current or effective state toward the desired
    state if they differ.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器**：控制器，或者更准确地说是控制器管理器，是一个控制循环，通过API服务器观察集群的状态并进行更改，试图将当前状态或有效状态移向期望的状态，如果它们不同。'
- en: '**Scheduler**: The scheduler is a service that tries its best to schedule pods
    on worker nodes while considering various boundary conditions, such as resource
    requirements, policies, quality of service requirements, and more.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**：调度器是一个服务，它尽力在考虑各种边界条件时将pod调度到工作节点上，例如资源需求、策略、服务质量需求等。'
- en: '**Cluster Store**: This is an instance of etcd that is used to store all information
    about the state of the cluster.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群存储**：这是一个etcd的实例，用于存储集群状态的所有信息。'
- en: To be more precise, etcd, which is used as a cluster store, does not necessarily
    have to be installed on the same node as the other Kubernetes services. Sometimes,
    Kubernetes clusters are configured to use standalone clusters of etcd servers,
    as shown in the architecture diagram in the previous section. But which variant
    to use is an advanced management decision and is outside the scope of this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，作为集群存储使用的etcd不一定要安装在与其他Kubernetes服务相同的节点上。有时，Kubernetes集群配置为使用独立的etcd服务器集群，就像在前一节的架构图中所示。但使用哪种变体是一个高级管理决策，超出了本书的范围。
- en: We need at least one master, but to achieve high availability, we need three
    or more master nodes. This is very similar to what we have learned about the manager
    nodes of a Docker Swarm. In this regard, a Kubernetes master is equivalent to
    a Swarm manager node.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们至少需要一个主节点，但为了实现高可用性，我们需要三个或更多的主节点。这与我们所学习的Docker Swarm的管理节点非常相似。在这方面，Kubernetes的主节点相当于Swarm的管理节点。
- en: Kubernetes masters never run application workloads. Their sole purpose is to
    manage the cluster. Kubernetes masters build a Raft consensus group. The Raft
    protocol is a standard protocol used in situations where a group of members needs
    to make decisions. It is used in many well-known software products such as MongoDB,
    Docker SwarmKit, and Kubernetes. For a more thorough discussion of the Raft protocol,
    see the link in the *Further reading* section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes主节点从不运行应用负载。它们的唯一目的是管理集群。Kubernetes主节点构建Raft一致性组。Raft协议是一种标准协议，用于需要做出决策的成员组的情况。它被用于许多知名软件产品，如MongoDB、Docker
    SwarmKit和Kubernetes。有关Raft协议的更详细讨论，请参见*进一步阅读*部分中的链接。
- en: As we mentioned in the previous section, the state of the Kubernetes cluster
    is stored in etcd. If the Kubernetes cluster is supposed to be highly available,
    then etcd must also be configured in HA mode, which normally means that we have
    at least three etcd instances running on different nodes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中提到的，Kubernetes集群的状态存储在etcd中。如果Kubernetes集群应该是高可用的，那么etcd也必须配置为HA模式，这通常意味着我们至少有三个运行在不同节点上的etcd实例。
- en: Let's state once again that the whole cluster state is stored in etcd. This
    includes all the information about all the cluster nodes, all the replica sets,
    deployments, secrets, network policies, routing information, and so on. It is,
    therefore, crucial that we have a robust backup strategy in place for this key-value
    store.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次声明，整个集群状态存储在etcd中。这包括所有集群节点的所有信息，所有副本集、部署、秘密、网络策略、路由信息等等。因此，对于这个键值存储，我们必须有一个强大的备份策略。
- en: Now, let's look at the nodes that will be running the actual workload of the
    cluster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看将运行集群实际工作负载的节点。
- en: Cluster nodes
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群节点
- en: 'Cluster nodes are the nodes with which Kubernetes schedules application workloads.
    They are the workhorses of the cluster. A Kubernetes cluster can have a few, dozens,
    hundreds, or even thousands of cluster nodes. Kubernetes has been built from the
    ground up for high scalability. Don''t forget that Kubernetes was modeled after
    Google Borg, which has been running tens of thousands of containers for years:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 集群节点是Kubernetes调度应用负载的节点。它们是集群的工作马。Kubernetes集群可以有少数、几十个、上百个，甚至上千个集群节点。Kubernetes是从头开始构建的，具有高可扩展性。不要忘记，Kubernetes是模仿Google
    Borg而建立的，Google Borg多年来一直在运行数万个容器：
- en: '![](assets/995b10fd-c87a-4b18-b291-104acf1bd1e8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/995b10fd-c87a-4b18-b291-104acf1bd1e8.png)'
- en: Kubernetes worker node
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes工作节点
- en: A worker node can run on a VM, bare metal, on-premise, or in the cloud. Originally,
    worker nodes could only be configured on Linux. But since version 1.10 of Kubernetes,
    worker nodes can also run on Windows Server. It is perfectly fine to have a mixed
    cluster with Linux and Windows worker nodes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点可以在虚拟机、裸机、本地或云上运行。最初，工作节点只能在Linux上配置。但自Kubernetes 1.10版本以来，工作节点也可以在Windows
    Server上运行。在混合集群中拥有Linux和Windows工作节点是完全可以的。
- en: 'On each node, we have three services that need to run, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点上，我们需要运行三个服务，如下：
- en: '**Kubelet**: This is the first and foremost service. Kubelet is the primary
    node agent. The kubelet service uses pod specifications to make sure all of the
    containers of the corresponding pods are running and healthy. Pod specifications
    are files written in YAML or JSON format and they declaratively describe a pod.
    We will get to know what pods are in the next section. PodSpecs are provided to
    kubelet primarily through the API server.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubelet**：这是第一个，也是最重要的服务。Kubelet是主要的节点代理。kubelet服务使用pod规范来确保相应pod的所有容器都在运行并且健康。Pod规范是以YAML或JSON格式编写的文件，它们以声明方式描述一个pod。我们将在下一节了解什么是pod。Pod规范主要通过API服务器提供给kubelet。'
- en: '**Container runtime**: The second service that needs to be present on each
    worker node is a container runtime. Kubernetes, by default, has used `containerd` since
    version 1.9 as its container runtime. Prior to that, it used the Docker daemon.
    Other container runtimes, such as rkt or CRI-O, can be used. The container runtime
    is responsible for managing and running the individual containers of a pod.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：每个工作节点上需要存在的第二个服务是容器运行时。Kubernetes默认从1.9版本开始使用`containerd`作为其容器运行时。在那之前，它使用Docker守护程序。其他容器运行时，如rkt或CRI-O，也可以使用。容器运行时负责管理和运行pod中的各个容器。'
- en: '**kube-proxy**: Finally, there is the kube-proxy. It runs as a daemon and is
    a simple network proxy and load balancer for all application services running
    on that particular node.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-proxy**：最后，还有kube-proxy。它作为一个守护进程运行，是一个简单的网络代理和负载均衡器，用于运行在该特定节点上的所有应用服务。'
- en: Now that we have learned about the architecture of Kubernetes and the master
    and worker nodes, it is time to introduce the tooling that we can use to develop
    applications targeted at Kubernetes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Kubernetes的架构、主节点和工作节点，是时候介绍一下我们可以用来开发针对Kubernetes的应用程序的工具了。
- en: Introduction to Minikube
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Minikube简介
- en: Minikube is a tool that creates a single-node Kubernetes cluster in VirtualBox
    or Hyper-V (other hypervisors are supported too) ready to be used during the development
    of a containerized application. In [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml), *Setting
    Up a Working Environment,* we learned how Minikube and `kubectl` can be installed
    on our macOS or Windows laptop. As stated there, Minikube is a single-node Kubernetes
    cluster and thus the node is, at the same time, a Kubernetes master as well as
    a worker node.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube是一个工具，它在VirtualBox或Hyper-V中创建一个单节点Kubernetes集群（其他虚拟化程序也支持），可以在开发容器化应用程序期间使用。在第2章《设置工作环境》中，我们了解了如何在我们的macOS或Windows笔记本电脑上安装Minikube和`kubectl`。正如在那里所述，Minikube是一个单节点Kubernetes集群，因此该节点同时也是Kubernetes主节点和工作节点。
- en: 'Let''s make sure that Minikube is running with the following command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确保Minikube正在运行，使用以下命令：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once Minikube is ready, we can access its single node cluster using `kubectl`.
    We should see something similar to the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Minikube准备就绪，我们可以使用`kubectl`访问它的单节点集群。我们应该会看到类似以下的内容：
- en: '![](assets/ee27b902-c2d7-4d1f-8c7d-8412d06b788b.png)Listing all nodes in Minikube'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ee27b902-c2d7-4d1f-8c7d-8412d06b788b.png)列出Minikube中的所有节点'
- en: As we mentioned previously, we have a single-node cluster with a node called `minikube`.
    The version of Kubernetes that Minikube is using is `v1.16.2` in my case.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们有一个名为`minikube`的单节点集群。Minikube使用的Kubernetes版本是`v1.16.2`（在我的情况下）。
- en: Now, let's try to deploy a pod to this cluster. Don't worry about what a pod
    is for now; we will delve into all the details about it later in this chapter.
    For the moment, just take it as-is.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将一个pod部署到这个集群中。现在不要担心pod是什么；我们将在本章后面深入了解所有细节。暂时就按原样进行。
- en: 'We can use the `sample-pod.yaml` file in the `ch15` subfolder of our `labs` folder
    to create such a pod. It has the following content:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`labs`文件夹中`ch15`子文件夹中的`sample-pod.yaml`文件来创建这样一个pod。它的内容如下：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the following steps to run the pod:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下步骤运行pod：
- en: 'First, navigate to the correct folder:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到正确的文件夹：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s use the Kubernetes CLI called `kubectl` to deploy this pod:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用名为`kubectl`的Kubernetes CLI来部署这个pod：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we now list all of the pods, we should see the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在列出所有的pod，我们应该会看到以下内容：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To be able to access this pod, we need to create a service. Let''s use the `sample-service.yaml` file,
    which has the following content:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了能够访问这个pod，我们需要创建一个服务。让我们使用名为`sample-service.yaml`的文件，它的内容如下：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, don''t worry about what exactly a service is at this time. We''ll explain
    this later. Let''s just create this service:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次强调，现在不用担心服务是什么。我们稍后会解释这个。让我们创建这个服务：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can use `curl` to access the service:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`curl`来访问服务：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We should receive the Nginx welcome page as an answer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该收到Nginx欢迎页面作为答案。
- en: 'Before you continue, please remove the two objects you just created:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，请删除刚刚创建的两个对象：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Kubernetes support in Docker for Desktop
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker for Desktop中的Kubernetes支持
- en: 'Starting from version 18.01-ce, Docker for macOS and Docker for Windows have started
    to support Kubernetes out of the box. Developers who want to deploy their containerized
    applications to Kubernetes can use this orchestrator instead of SwarmKit. Kubernetes
    support is turned off by default and has to be enabled in the settings. The first
    time Kubernetes is enabled, Docker for macOS or Windows will need a moment to
    download all the components that are needed to create a single-node Kubernetes
    cluster. Contrary to Minikube, which is also a single-node cluster, the version
    provided by the Docker tools uses containerized versions of all Kubernetes components:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本18.01-ce开始，Docker for macOS和Docker for Windows已经开始默认支持Kubernetes。想要将其容器化应用程序部署到Kubernetes的开发人员可以使用这个编排器，而不是SwarmKit。Kubernetes支持默认关闭，必须在设置中启用。第一次启用Kubernetes时，Docker
    for macOS或Windows需要一些时间来下载创建单节点Kubernetes集群所需的所有组件。与Minikube相反，后者也是单节点集群，Docker工具提供的版本使用所有Kubernetes组件的容器化版本：
- en: '![](assets/bf2e24ff-9110-43ac-8e4a-b30f0c5d2df5.png)Kubernetes support in Docker
    for macOS and Windows'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/bf2e24ff-9110-43ac-8e4a-b30f0c5d2df5.png)Docker for macOS和Windows中的Kubernetes支持'
- en: 'The preceding diagram gives us a rough overview of how Kubernetes support has
    been added to Docker for macOS and Windows. Docker for macOS uses hyperkit to
    run a LinuxKit-based VM. Docker for Windows uses Hyper-V to achieve the result.
    Inside the VM, the Docker engine is installed. Part of the engine is SwarmKit,
    which enables **Swarm-Mode**. Docker for macOS or Windows uses the **kubeadm** tool
    to set up and configure Kubernetes in that VM. The following three facts are worth
    mentioning: Kubernetes stores its cluster state in **etcd**, thus we have **etcd**
    running on this VM. Then, we have all the services that make up Kubernetes and,
    finally, some services that support the deployment of Docker stacks from the **Docker
    CLI** into Kubernetes. This service is not part of the official Kubernetes distribution,
    but it is Docker-specific.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 上图大致概述了Kubernetes支持是如何添加到Docker for macOS和Windows中的。Docker for macOS使用hyperkit来运行基于LinuxKit的VM。Docker
    for Windows使用Hyper-V来实现结果。在VM内部，安装了Docker引擎。引擎的一部分是SwarmKit，它启用了**Swarm-Mode**。Docker
    for macOS或Windows使用**kubeadm**工具在VM中设置和配置Kubernetes。以下三个事实值得一提：Kubernetes将其集群状态存储在**etcd**中，因此我们在此VM上运行**etcd**。然后，我们有组成Kubernetes的所有服务，最后，一些支持从**Docker
    CLI**部署Docker堆栈到Kubernetes的服务。这项服务不是官方Kubernetes发行版的一部分，但它是特定于Docker的。
- en: All Kubernetes components run in containers in the **LinuxKit VM**. These containers
    can be hidden through a setting in Docker for macOS or Windows. Later in this
    section, we'll provide a complete list of Kubernetes system containers that will
    be running on your laptop, if you have Kubernetes support enabled. To avoid repetition,
    from now on, I will only talk about Docker for Desktop instead of Docker for macOS
    and Docker for Windows. Everything that I will be saying equally applies to both
    editions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Kubernetes组件都在**LinuxKit VM**中以容器形式运行。这些容器可以通过Docker for macOS或Windows中的设置进行隐藏。在本节的后面，我们将提供在您的笔记本电脑上运行的所有Kubernetes系统容器的完整列表，如果您启用了Kubernetes支持。为避免重复，从现在开始，我将只谈论Docker
    for Desktop而不是Docker for macOS和Docker for Windows。我将要说的一切同样适用于两个版本。
- en: One big advantage of Docker for Desktop with Kubernetes enabled over Minikube
    is that the former allows developers to use a single tool to build, test, and
    run a containerized application targeted at Kubernetes. It is even possible to
    deploy a multi-service application into Kubernetes using a Docker Compose file.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 启用Docker Desktop的Kubernetes的一个很大优势是，它允许开发人员使用单个工具构建、测试和运行针对Kubernetes的容器化应用程序。甚至可以使用Docker
    Compose文件将多服务应用程序部署到Kubernetes。
- en: 'Now, let''s get our hands dirty:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们动手：
- en: 'First, we have to enable Kubernetes. On macOS, click on the Docker icon in
    the menu bar; or, on Windows, go to the command tray and select Preferences. In
    the dialog box that opens, select Kubernetes, as shown in the following screenshot:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须启用Kubernetes。在macOS上，点击菜单栏中的Docker图标；或者在Windows上，转到命令托盘并选择“首选项”。在打开的对话框中，选择Kubernetes，如下面的屏幕截图所示：
- en: '![](assets/53d9153e-43f2-4b1c-ab52-bf08d8889001.png)Enabling Kubernetes in
    Docker for Desktop'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/53d9153e-43f2-4b1c-ab52-bf08d8889001.png)在Docker Desktop中启用Kubernetes'
- en: Then, tick the Enable Kubernetes checkbox. Also, tick the Deploy Docker Stacks
    to Kubernetes by default and Show system containers (advanced) checkboxes. Then,
    click the Apply & Restart button. Installing and configuring of Kubernetes takes
    a few minutes. Now, it's time to take a break and enjoy a nice cup of tea.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选中“启用Kubernetes”复选框。还要选中“默认情况下将Docker堆叠部署到Kubernetes”和“显示系统容器（高级）”复选框。然后，点击“应用并重启”按钮。安装和配置Kubernetes需要几分钟。现在，是时候休息一下，享受一杯好茶了。
- en: Once the installation is finished (which Docker notifies us of by showing a
    green status icon in the Settings dialog), we can test it. Since we now have two
    Kubernetes clusters running on our laptop, that is, Minikube and Docker for Desktop,
    we need to configure `kubectl` to access the latter.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后（Docker通过在设置对话框中显示绿色状态图标来通知我们），我们可以进行测试。由于我们现在在笔记本电脑上运行了两个Kubernetes集群，即Minikube和Docker
    Desktop，我们需要配置`kubectl`以访问后者。
- en: 'First, let''s list all the contexts that we have:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们列出所有我们拥有的上下文：
- en: '![](assets/ca14801c-00eb-46cc-8edc-0b3edaba44f4.png)List of contexts for kubectl'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ca14801c-00eb-46cc-8edc-0b3edaba44f4.png)kubectl的上下文列表'
- en: 'Here, we can see that, on my laptop, I have the two contexts we mentioned previously.
    Currently, the Minikube context is still active, flagged by the asterisk in the
    `CURRENT` column. We can switch to the `docker-for-desktop` context using the
    following command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，在我的笔记本电脑上，我有之前提到的两个上下文。当前，Minikube上下文仍然处于活动状态，在“CURRENT”列中标有星号。我们可以使用以下命令切换到`docker-for-desktop`上下文：
- en: '![](assets/903685b8-ecdb-455e-85a1-03188389f5c1.png)Changing the context for
    the Kubernetes CLI'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/903685b8-ecdb-455e-85a1-03188389f5c1.png)更改Kubernetes CLI的上下文'
- en: 'Now, we can use `kubectl` to access the cluster that Docker for Desktop just
    created. We should see the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`kubectl`来访问Docker Desktop刚刚创建的集群。我们应该看到以下内容：
- en: '![](assets/d3b65b94-4772-4584-9d06-3fa88a05a0eb.png)The single-node Kubernetes
    cluster created by Docker for Desktop'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d3b65b94-4772-4584-9d06-3fa88a05a0eb.png)Docker Desktop创建的单节点Kubernetes集群'
- en: OK, this looks very familiar. It is pretty much the same as what we saw when
    working with Minikube. The version of Kubernetes that my Docker for Desktop is
    using is `1.15.5`. We can also see that the node is a master node.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这看起来非常熟悉。这几乎与我们在使用Minikube时看到的一样。我的Docker Desktop使用的Kubernetes版本是`1.15.5`。我们还可以看到节点是主节点。
- en: 'If we list all the containers that are currently running on our Docker for
    Desktop, we get the list shown in the following screenshot (note that I use the `--format` argument
    to output the `Container ID` and `Names` of the containers):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们列出当前在Docker Desktop上运行的所有容器，我们将得到下面截图中显示的列表（请注意，我使用`--format`参数来输出容器的`Container
    ID`和`Names`）：
- en: '![](assets/6837f756-ac8a-41a7-b8af-6357e140ea1c.png)Kubernetes system containers'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/6837f756-ac8a-41a7-b8af-6357e140ea1c.png)Kubernetes系统容器'
- en: 'In the preceding list, we can identify all the now-familiar components that
    make up Kubernetes, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的列表中，我们可以识别出组成Kubernetes的所有熟悉组件，如下所示：
- en: API server
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API服务器
- en: etcd
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd
- en: Kube proxy
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube代理
- en: DNS service
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS服务
- en: Kube controller
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube控制器
- en: Kube scheduler
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube调度程序
- en: There are also containers that have the word `compose` in them. These are Docker-specific
    services and allow us to deploy Docker Compose applications onto Kubernetes. Docker
    translates the Docker Compose syntax and implicitly creates the necessary Kubernetes
    objects, such as deployments, pods, and services.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些容器中带有`compose`一词。这些是特定于Docker的服务，允许我们将Docker Compose应用程序部署到Kubernetes上。Docker将Docker
    Compose语法进行转换，并隐式创建必要的Kubernetes对象，如部署、Pod和服务。
- en: Normally, we don't want to clutter our list of containers with these system
    containers. Therefore, we can uncheck the Show system containers (advanced) checkbox
    in the settings for Kubernetes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不希望在容器列表中混杂这些系统容器。因此，我们可以在Kubernetes的设置中取消选中“显示系统容器（高级）”复选框。
- en: 'Now, let''s try to deploy a Docker Compose application to Kubernetes. Navigate
    to the `ch15` subfolder of our `~/fod` folder. We deploy the app as a stack using
    the `docker-compose.yml` file:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将Docker Compose应用程序部署到Kubernetes。转到`~/fod`文件夹的`ch15`子文件夹。我们使用`docker-compose.yml`文件将应用程序部署为堆栈：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We should see the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下内容：
- en: '![](assets/c712f4fd-1771-4475-a125-e9286974bdb6.png)Deploying the stack to
    Kubernetes'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c712f4fd-1771-4475-a125-e9286974bdb6.png)将堆栈部署到Kubernetes'
- en: 'We can test the application, for example, using `curl`, and we will see that
    it is running as expected:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`curl`来测试应用程序，并且会发现它按预期运行：
- en: '![](assets/86a7e757-6c9f-4706-b38d-1d13096cb908.png)Pets application running
    in Kubernetes on Docker for Desktop'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/86a7e757-6c9f-4706-b38d-1d13096cb908.png)在Docker桌面上的Kubernetes中运行的宠物应用程序'
- en: 'Now, let''s see exactly what Docker did when we executed the `docker stack
    deploy` command. We can use `kubectl` to find out:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在执行`docker stack deploy`命令时Docker到底做了什么。我们可以使用`kubectl`来找出：
- en: '![](assets/d4a81014-a75b-40c2-8c45-56412e93455d.png)Listing all Kubernetes
    objects created by docker stack deploy'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d4a81014-a75b-40c2-8c45-56412e93455d.png)列出由docker stack deploy创建的所有Kubernetes对象'
- en: Docker created a deployment for the `web` service and a stateful set for the `db` service.
    It also automatically created Kubernetes services for `web` and `db` so that they
    can be accessed inside the cluster. It also created the Kubernetes `svc/web-published` service, which
    is used for external access.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Docker为`web`服务创建了一个部署，为`db`服务创建了一个有状态集。它还自动为`web`和`db`创建了Kubernetes服务，以便它们可以在集群内部访问。它还创建了Kubernetes
    `svc/web-published`服务，用于外部访问。
- en: This is pretty cool, to say the least, and tremendously decreases friction in
    the development process for teams targeting Kubernetes as their orchestration
    platform
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当酷，至少可以说，极大地减少了团队在开发过程中针对Kubernetes作为编排平台时的摩擦
- en: 'Before you continue, please remove the stack from the cluster:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请从集群中删除堆栈：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Also, make sure you reset the context for `kubectl` back to Minikube, as we
    will be using Minikube for all our samples in this chapter:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 还要确保将`kubectl`的上下文重置回Minikube，因为我们将在本章中使用Minikube进行所有示例：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have had an introduction to the tools we can use to develop applications
    that will eventually run in a Kubernetes cluster, it is time to learn about all
    the important Kubernetes objects that are used to define and manage such an application.
    We will start with pods.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经介绍了用于开发最终将在Kubernetes集群中运行的应用程序的工具，是时候了解用于定义和管理这样的应用程序的所有重要Kubernetes对象了。我们将从Pod开始。
- en: Introduction to pods
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod简介
- en: 'Contrary to what is possible in Docker Swarm, you cannot run containers directly
    in a Kubernetes cluster. In a Kubernetes cluster, you can only run pods. Pods
    are the atomic units of deployment in Kubernetes. A pod is an abstraction of one
    or many co-located containers that share the same Kernel namespaces, such as the
    network namespace. No equivalent exists in Docker SwarmKit. The fact that more
    than one container can be co-located and share the same network namespace is a
    very powerful concept. The following diagram illustrates two pods:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与Docker Swarm中可能的情况相反，在Kubernetes集群中不能直接运行容器。在Kubernetes集群中，您只能运行Pod。Pod是Kubernetes中部署的原子单位。Pod是一个或多个共同定位的容器的抽象，它们共享相同的内核命名空间，如网络命名空间。在Docker
    SwarmKit中不存在等价物。多个容器可以共同定位并共享相同的网络命名空间的事实是一个非常强大的概念。下图说明了两个Pod：
- en: '![](assets/81cd2bb8-23c9-4034-9ad3-eb03b5d6a1ba.png)Kubernetes pods'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/81cd2bb8-23c9-4034-9ad3-eb03b5d6a1ba.png)Kubernetes pods'
- en: In the preceding diagram, we have two pods, **Pod 1** and **Pod 2**. The first
    pod contains two containers, while the second one only contains a single container.
    Each pod gets an IP address assigned by Kubernetes that is unique in the whole
    Kubernetes cluster. In our case, these are the following IP addresses: `10.0.12.3` and `10.0.12.5`.
    Both are part of a private subnet managed by the Kubernetes network driver.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们有两个Pod，**Pod 1**和**Pod 2**。第一个Pod包含两个容器，而第二个Pod只包含一个容器。每个Pod都由Kubernetes分配一个IP地址，在整个Kubernetes集群中是唯一的。在我们的情况下，它们的IP地址分别是：`10.0.12.3`和`10.0.12.5`。它们都是由Kubernetes网络驱动程序管理的私有子网的一部分。
- en: A pod can contain one to many containers. All those containers share the same
    Linux kernel namespaces, and in particular, they share the network namespace.
    This is indicated by the dashed rectangle surrounding the containers. Since all
    containers running in the same pod share the network namespace, each container
    needs to make sure to use their own port since duplicate ports are not allowed
    in a single network namespace. In this case, in **Pod 1**, the **main container**
    is using port `80` while the **supporting container** is using port `3000`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Pod可以包含一个到多个容器。所有这些容器共享相同的Linux内核命名空间，特别是它们共享网络命名空间。这是由包围容器的虚线矩形表示的。由于在同一个Pod中运行的所有容器共享网络命名空间，因此每个容器都需要确保使用自己的端口，因为在单个网络命名空间中不允许重复端口。在这种情况下，在**Pod
    1**中，**主容器**使用端口`80`，而**支持容器**使用端口`3000`。
- en: Requests from other pods or nodes can use the pod's IP address combined with
    the corresponding port number to access the individual containers. For example,
    you could access the application running in the main container of **Pod 1** through `10.0.12.3:80`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 来自其他Pod或节点的请求可以使用Pod的IP地址和相应的端口号来访问各个容器。例如，您可以通过`10.0.12.3:80`访问**Pod 1**中主容器中运行的应用程序。
- en: Comparing Docker container and Kubernetes pod networking
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较Docker容器和Kubernetes Pod网络
- en: 'Now, let''s compare Docker''s container networking and Kubernetes pod networking.
    In the following diagram, we have the former on the left-hand side and the latter
    on the right-hand side:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们比较一下Docker的容器网络和Kubernetes的Pod网络。在下图中，我们将前者放在左侧，后者放在右侧：
- en: '![](assets/23036596-986c-43b4-83db-2732000cfad9.png)Containers in a pod sharing
    the same network namespace'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/23036596-986c-43b4-83db-2732000cfad9.png)Pod中的容器共享相同的网络命名空间'
- en: When a Docker container is created and no specific network is specified, then
    the Docker engine creates a **virtual ethernet** (**veth**) endpoint. The first
    container gets **veth0**, the next one gets **veth1**, and so on. These virtual
    ethernet endpoints are connected to the Linux bridge, **docker0**, that Docker
    automatically creates upon installation. Traffic is routed from the **docker0** bridge
    to every connected **veth** endpoint. Every container has its own network namespace.
    No two containers use the same namespace. This is on purpose, to isolate applications
    running inside the containers from each other.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个Docker容器并且没有指定特定的网络时，Docker引擎会创建一个**虚拟以太网**（veth）端点。第一个容器得到**veth0**，下一个得到**veth1**，以此类推。这些虚拟以太网端点连接到Linux桥**docker0**，Docker在安装时自动创建。流量从**docker0**桥路由到每个连接的**veth**端点。每个容器都有自己的网络命名空间。没有两个容器使用相同的命名空间。这是有意为之，目的是隔离容器内运行的应用程序。
- en: 'For a Kubernetes pod, the situation is different. When creating a new pod,
    Kubernetes first creates a so-called **pause **container whose only purpose is
    to create and manage the namespaces that the pod will share with all containers.
    Other than that, it does nothing useful; it is just sleeping. The **pause **container
    is connected to the **docker0 **bridge through **veth0**. Any subsequent container
    that will be part of the pod uses a special feature of the Docker engine that
    allows it to reuse an existing network namespace. The syntax to do so looks like
    this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kubernetes pod，情况是不同的。在创建一个新的pod时，Kubernetes首先创建一个所谓的**pause**容器，其唯一目的是创建和管理pod将与所有容器共享的命名空间。除此之外，它没有任何有用的功能；它只是在睡觉。**pause**容器通过**veth0**连接到**docker0**桥。任何随后成为pod一部分的容器都使用Docker引擎的一个特殊功能，允许它重用现有的网络命名空间。这样做的语法看起来像这样：
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The important part is the `--net` argument, which uses `container:<container
    name>`as a value. If we create a new container this way, then Docker does not
    create a new veth endpoint; the container uses the same one as the `pause` container.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的部分是`--net`参数，它使用`container:<container name>`作为值。如果我们以这种方式创建一个新容器，那么Docker不会创建一个新的veth端点；容器使用与`pause`容器相同的端点。
- en: 'Another important consequence of multiple containers sharing the same network
    namespace is the way they communicate with each other. Let''s consider the following
    situation: a pod containing two containers, one listening at port `80` and the
    other at port `3000`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 多个容器共享相同的网络命名空间的另一个重要后果是它们相互通信的方式。让我们考虑以下情况：一个包含两个容器的pod，一个在端口`80`上监听，另一个在端口`3000`上监听。
- en: '![](assets/55170356-9c32-42bb-bbcd-a9f51f59017e.png)Containers in pods communicating
    via localhost'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/55170356-9c32-42bb-bbcd-a9f51f59017e.png)Pod中的容器通过localhost通信'
- en: When two containers use the same Linux kernel network namespace, they can communicate
    with each other through localhost, similarly to how, when two processes are running
    on the same host, they can communicate with each other through localhost too.
    This is illustrated in the preceding diagram. From the main container, the containerized
    application inside it can reach out to the service running inside the supporting
    container through `http://localhost:3000`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个容器使用相同的Linux内核网络命名空间时，它们可以通过localhost相互通信，类似于当两个进程在同一主机上运行时，它们也可以通过localhost相互通信。这在前面的图表中有所说明。从主容器中，其中的容器化应用程序可以通过`http://localhost:3000`访问支持容器内运行的服务。
- en: Sharing the network namespace
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享网络命名空间
- en: 'After all this theory, you might be wondering how a pod is actually created by
    Kubernetes. Kubernetes only uses what Docker provides. So, *how does this network
    namespace share work?* First, Kubernetes creates the so-called `pause` container,
    as mentioned previously. This container has no other function than to reserve
    the kernel namespaces for that pod and keep them alive, even if no other container
    inside the pod is running. Let''s simulate the creation of a pod, then. We start
    by creating the `pause` container and use Nginx for this purpose:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些理论之后，你可能会想知道Kubernetes是如何实际创建一个Pod的。Kubernetes只使用Docker提供的内容。那么，*这个网络命名空间共享是如何工作的呢？*首先，Kubernetes创建所谓的`pause`容器，如前所述。这个容器除了保留内核命名空间给该Pod并保持它们的活动状态外，没有其他功能，即使Pod内没有其他容器在运行。然后，我们模拟创建一个Pod。我们首先创建`pause`容器，并使用Nginx来实现这个目的：
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we add a second container called `main`, attaching it to the same network
    namespace as the `pause` container:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们添加一个名为`main`的第二个容器，将其附加到与`pause`容器相同的网络命名空间：
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Since `pause` and the sample container are both parts of the same network namespace,
    they can reach each other through `localhost`. To show this, we have to `exec` into
    the main container:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`pause`和示例容器都是同一个网络命名空间的一部分，它们可以通过`localhost`相互访问。为了证明这一点，我们必须`exec`进入主容器：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can test the connection to Nginx running in the `pause` container and
    listening on port `80`. The following what we get if we use the `wget` utility
    to do so:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以测试连接到运行在`pause`容器中并监听端口`80`的Nginx。如果我们使用`wget`工具来做到这一点，我们会得到以下结果：
- en: '![](assets/ccfdb453-ac21-47bd-a2af-716d7c2bcb1e.png)Two containers sharing
    the same network namespace'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ccfdb453-ac21-47bd-a2af-716d7c2bcb1e.png)两个共享相同网络命名空间的容器'
- en: 'The output shows that we can indeed access Nginx on `localhost`. This is proof
    that the two containers share the same namespace. If that is not enough, we can
    use the `ip` tool to show `eth0` inside both containers and we will get the exact
    same result, specifically, the same IP address, which is one of the characteristics
    of a pod where all its containers share the same IP address:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们确实可以在`localhost`上访问Nginx。这证明了这两个容器共享相同的命名空间。如果这还不够，我们可以使用`ip`工具来显示两个容器内部的`eth0`，我们将得到完全相同的结果，具体来说，相同的IP地址，这是Pod的特征之一，所有容器共享相同的IP地址：
- en: '![](assets/ad510dd1-a0ea-4767-ac14-94eecd16c1a3.png)Displaying the properties
    of eth0 with the ip tool'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ad510dd1-a0ea-4767-ac14-94eecd16c1a3.png)使用`ip`工具显示`eth0`的属性'
- en: 'If we inspect the `bridge` network, we can see that only the `pause` container
    is listed. The other container didn''t get an entry in the `Containers` list since
    it is reusing the `pause` container''s endpoint:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查`bridge`网络，我们会看到只有`pause`容器被列出。另一个容器没有在`Containers`列表中得到条目，因为它正在重用`pause`容器的端点：
- en: '![](assets/e553023b-c1e7-4abf-893c-b8cb75208b13.png)Inspecting the Docker default
    bridge network'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/e553023b-c1e7-4abf-893c-b8cb75208b13.png)检查Docker默认桥接网络'
- en: Next, we will be looking at the pod life cycle.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将研究Pod的生命周期。
- en: Pod life cycle
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod的生命周期
- en: Earlier in this book, we learned that containers have a life cycle. A container
    is initialized, run, and ultimately exited. When a container exits, it can do
    this gracefully with an exit code zero or it can terminate with an error, which
    is equivalent to a nonzero exit code.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前面，我们学到了容器有一个生命周期。容器被初始化，运行，最终退出。当一个容器退出时，它可以以退出码零的方式优雅地退出，也可以以错误终止，这相当于非零的退出码。
- en: 'Similarly, a pod has a life cycle. Due to the fact that a pod can contain more
    than one container, this life cycle is slightly more complicated than that of
    a single container. The life cycle of a pod can be seen in the following diagram:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，一个Pod也有一个生命周期。由于一个Pod可以包含多个容器，因此其生命周期比单个容器的生命周期稍微复杂一些。Pod的生命周期可以在下图中看到：
- en: '![](assets/7f161a33-21c8-4c48-888d-881ef4fbe6a7.png)The life cycle of Kubernetes
    pods'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Pod的生命周期
- en: When a **Pod** is created on a cluster node, it first enters into the **pending** status.
    Once all the containers of the pod are up and running, the pod enters into the **running** status.
    The pod only enters into this state if all its containers run successfully. If
    the pod is asked to terminate, it will request all its containers to terminate.
    If all containers terminate with exit code zero, then the pod enters into the **succeeded** status.
    This is the happy path.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当在集群节点上创建一个**Pod**时，它首先进入**pending**状态。一旦所有的Pod容器都启动并运行，Pod就会进入**running**状态。只有当所有容器成功运行时，Pod才会进入这个状态。如果要求Pod终止，它将请求所有容器终止。如果所有容器以退出码零终止，那么Pod就会进入**succeeded**状态。这是一条顺利的路径。
- en: 'Now, let''s look at some scenarios that lead to the pod being in the **failed** state.
    There are three possible scenarios:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一些导致Pod处于failed状态的情景。有三种可能的情景：
- en: If, during the startup of the pod, at least one container is not able to run
    and fails (that is, it exits with a nonzero exit code), the pod goes from the **pending** state
    into the **failed** state.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在Pod启动过程中，至少有一个容器无法运行并失败（即以非零退出码退出），Pod将从**pending**状态转换为**failed**状态。
- en: If the pod is in the running status and one of the containers suddenly crashes
    or exits with a nonzero exit code, then the pod transitions from the **running** state
    into the **failed** state.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Pod处于running状态，而其中一个容器突然崩溃或以非零退出码退出，那么Pod将从running状态转换为failed状态。
- en: If the pod is asked to terminate and, during the shutdown at least one of the
    containers, exits with a nonzero exit code, then the pod also enters into the **failed** state.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果要求Pod终止，并且在关闭过程中至少有一个容器以非零退出码退出，那么Pod也会进入failed状态。
- en: Now, let's look at the specifications for a pod.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下Pod的规范。
- en: Pod specifications
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod规范
- en: When creating a pod in a Kubernetes cluster, we can use either an imperative
    or a declarative approach. We discussed the difference between the two approaches
    earlier in this book but, to rephrase the most important aspect, using a declarative
    approach signifies that we write a manifest that describes the end state we want
    to achieve. We'll leave out the details of the orchestrator. The end state that
    we want to achieve is also called the **desired state**. In general, the declarative
    approach is strongly preferred in all established orchestrators, and Kubernetes
    is no exception.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中创建一个Pod时，我们可以使用命令式或声明式方法。我们之前在本书中讨论过这两种方法的区别，但是，重申最重要的一点，使用声明式方法意味着我们编写一个描述我们想要实现的最终状态的清单。我们将略去编排器的细节。我们想要实现的最终状态也被称为**desired
    state**。一般来说，在所有已建立的编排器中，声明式方法都是强烈推荐的，Kubernetes也不例外。
- en: 'Thus, in this chapter, we will exclusively concentrate on the declarative approach.
    Manifests or specifications for a pod can be written using either the YAML or
    JSON formats. In this chapter, we will concentrate on YAML since it is easier
    to read for us humans. Let''s look at a sample specification. Here is the content
    of the `pod.yaml` file, which can be found in the `ch12` subfolder of our `labs` folder:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将专注于声明式方法。Pod的清单或规范可以使用YAML或JSON格式编写。在本章中，我们将专注于YAML，因为它对我们人类来说更容易阅读。让我们看一个样本规范。这是`pod.yaml`文件的内容，可以在我们的`labs`文件夹的`ch12`子文件夹中找到：
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Each specification in Kubernetes starts with the version information. Pods have
    been around for quite some time and thus the API version is `v1`. The second line
    specifies the type of Kubernetes object or resource we want to define. Obviously,
    in this case, we want to specify a `Pod`. Next follows a block containing metadata.
    At a bare minimum, we need to give the pod a name. Here, we call it `web-pod`.
    The next block that follows is the `spec` block, which contains the specification
    of the pod. The most important part (and the only one in this simple sample) is
    a list of all containers that are part of this pod. We only have one container
    here, but multiple containers are possible. The name we choose for our container
    is `web` and the container image is `nginx:alpine`. Finally, we define a list
    of ports the container is exposing.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的每个规范都以版本信息开头。Pods已经存在了相当长的时间，因此API版本是`v1`。第二行指定了我们要定义的Kubernetes对象或资源的类型。显然，在这种情况下，我们要指定一个`Pod`。接下来是包含元数据的块。至少，我们需要给pod一个名称。在这里，我们称其为`web-pod`。接下来跟随的是`spec`块，其中包含pod的规范。最重要的部分（也是这个简单示例中唯一的部分）是这个pod中所有容器的列表。我们这里只有一个容器，但是多个容器是可能的。我们为容器选择的名称是`web`，容器镜像是`nginx:alpine`。最后，我们定义了容器正在暴露的端口列表。
- en: 'Once we have authored such a specification, we can apply it to the cluster
    using the Kubernetes CLI, `kubectl`. In a Terminal, navigate to the `ch15` subfolder
    and execute the following command:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们编写了这样的规范，我们就可以使用Kubernetes CLI `kubectl`将其应用到集群中。在终端中，导航到`ch15`子文件夹，并执行以下命令：
- en: '[PRE17]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will respond with `pod "web-pod" created`. We can then list all the pods
    in the cluster with `kubectl get pods`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将回应`pod "web-pod" created`。然后我们可以使用`kubectl get pods`列出集群中的所有pod：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As expected, we have one of one pods in the running status. The pod is called `web-pod`,
    as defined. We can get more detailed information about the running pod by using the
    `describe` command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们在运行状态中有一个pod。该pod被称为`web-pod`，如所定义。我们可以使用`describe`命令获取有关运行中pod的更详细信息：
- en: '![](assets/99fb4f13-8084-40a5-a7c5-773f47e056e2.png)Describing a pod running
    in the cluster'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/99fb4f13-8084-40a5-a7c5-773f47e056e2.png)描述运行在集群中的pod'
- en: Please note the `pod/web-pod` notation in the previous `describe` command. Other
    variants are possible; for example, `pods/web-pod`, `po/web-pod`. `pod` and `po` are
    aliases of `pods`. The `kubectl` tool defines many aliases to make our lives a
    bit easier.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在前面的`describe`命令中的`pod/web-pod`表示法。其他变体也是可能的；例如，`pods/web-pod`，`po/web-pod`。`pod`和`po`是`pods`的别名。`kubectl`工具定义了许多别名，使我们的生活变得更加轻松。
- en: The `describe` command gives us a plethora of valuable information about the
    pod, not the least of which is a list of events that happened and affected this
    pod. The list is shown at the end of the output.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe`命令为我们提供了关于pod的大量有价值的信息，其中包括发生的事件列表，以及影响了这个pod的事件。列表显示在输出的末尾。'
- en: The information in the `Containers` section is very similar to what we find
    in a `docker container inspect` output.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`Containers`部分中的信息与`docker container inspect`输出中的信息非常相似。'
- en: We can also see a `Volumes` section with an entry of the `Secret` type. We will
    discuss Kubernetes secrets in the next chapter. Volumes, on the other hand, will
    be discussed next.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到`Volumes`部分中有一个`Secret`类型的条目。我们将在下一章讨论Kubernetes secrets。另一方面，卷将在下一章讨论。
- en: Pods and volumes
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pods和volumes
- en: 'In [Chapter 5](f3a48b12-d541-467b-aeb3-df014e60da6b.xhtml), *Data Volumes and
    Configuration*, we learned about volumes and their purpose: accessing and storing
    persistent data. Since containers can mount volumes, pods can do so as well. In
    reality, it is really the containers inside the pod that mount the volumes, but
    that is just a semantic detail. First, let''s see how we can define a volume in
    Kubernetes. Kubernetes supports a plethora of volume types, so we won''t delve
    into too much detail about this. Let''s just create a local volume implicitly
    by defining a `PersistentVolumeClaim` called `my-data-claim`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](f3a48b12-d541-467b-aeb3-df014e60da6b.xhtml)中，*数据卷和配置*，我们学习了卷及其目的：访问和存储持久数据。由于容器可以挂载卷，Pod也可以这样做。实际上，实际上是Pod内的容器挂载卷，但这只是一个语义细节。首先，让我们看看如何在Kubernetes中定义卷。Kubernetes支持大量的卷类型，所以我们不会深入讨论这个问题。让我们通过隐式定义一个名为`my-data-claim`的`PersistentVolumeClaim`来创建一个本地卷：
- en: '[PRE19]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We have defined a claim that requests 2 GB of data. Let''s create this claim:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了一个请求2GB数据的声明。让我们创建这个声明：
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can list the claim using `kubectl` (`pvc` is a shortcut for `PersistentVolumeClaim`):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`kubectl`列出声明（`pvc`是`PersistentVolumeClaim`的快捷方式）：
- en: '![](assets/e8b60fdf-3b0e-456c-bf0e-5a7a5948ab5c.png)List of PersistentStorageClaim
    objects in the cluster'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中列出持久存储声明对象
- en: 'In the output, we can see that the claim has implicitly created a volume called `pvc-<ID>`.
    We are now ready to use the volume created by the claim in a pod. Let''s use a
    modified version of the pod specification that we used previously. We can find
    this updated specification in the `pod-with-vol.yaml` file in the `ch12` folder.
    Let''s look at this specification in detail:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们可以看到声明已经隐式创建了一个名为`pvc-<ID>`的卷。我们现在准备在Pod中使用声明创建的卷。让我们使用之前使用的Pod规范的修改版本。我们可以在`ch12`文件夹中的`pod-with-vol.yaml`文件中找到这个更新的规范。让我们详细看一下这个规范：
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the last four lines, in the `volumes` block, we define a list of volumes
    we want to use for this pod. The volumes that we list here can be used by any
    of the containers of the pod. In our particular case, we only have one volume.
    We specify that we have a volume called `my-data`, which is a persistent volume
    claim whose claim name is the one we just created. Then, in the container specification,
    we have the `volumeMounts` block, which is where we define the volume we want
    to use and the (absolute) path inside the container where the volume will be mounted.
    In our case, we mount the volume to the `/data` folder of the container filesystem.
    Let''s create this pod:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后四行中，在`volumes`块中，我们定义了我们想要为这个Pod使用的卷的列表。我们在这里列出的卷可以被Pod的任何一个容器使用。在我们的特定情况下，我们只有一个卷。我们指定我们有一个名为`my-data`的卷，这是一个持久卷声明，其声明名称就是我们刚刚创建的。然后，在容器规范中，我们有`volumeMounts`块，这是我们定义我们想要使用的卷以及容器内部的（绝对）路径的地方，卷将被挂载到容器文件系统的`/data`文件夹。让我们创建这个Pod：
- en: '[PRE22]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we can `exec` into the container to double-check that the volume has
    mounted by navigating to the `/data` folder, creating a file there, and exiting
    the container:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过`exec`进入容器，通过导航到`/data`文件夹，创建一个文件，并退出容器来再次检查卷是否已挂载：
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we are right, then the data in this container must persist beyond the life
    cycle of the pod. Thus, let''s delete the pod and then recreate it and exec into
    it to make sure the data is still there. This is the result:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们是正确的，那么这个容器中的数据必须在Pod的生命周期之外持续存在。因此，让我们删除Pod，然后重新创建它并进入其中，以确保数据仍然存在。这是结果：
- en: '![](assets/b789efc4-6ea6-4338-a236-22aa885902f9.png)Data stored in volume survives
    pod recreation'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在卷中的数据在Pod重新创建时仍然存在
- en: Now that we have a good understanding of pods, let's look into how those pods
    are managed with the help of ReplicaSets.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对pod有了很好的理解，让我们来看看如何借助ReplicaSets来管理这些pod。
- en: Kubernetes ReplicaSet
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes ReplicaSet
- en: A single pod in an environment with high availability requirements is insufficient. *What
    if the pod crashes?* *What if we need to update the application running inside
    the pod but cannot afford any service interruption?* These questions and more
    indicate that pods alone are not enough and we need a higher-level concept that
    can manage multiple instances of the same pod. In Kubernetes, the **ReplicaSet **is
    used to define and manage such a collection of identical pods that are running
    on different cluster nodes. Among other things, a ReplicaSet defines which container
    images are used by the containers running inside a pod and how many instances
    of the pod will run in the cluster. These properties and many others are called
    the desired state.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有高可用性要求的环境中，单个pod是不够的。如果pod崩溃了怎么办？如果我们需要更新pod内运行的应用程序，但又不能承受任何服务中断怎么办？这些问题等等表明单独的pod是不够的，我们需要一个可以管理多个相同pod实例的更高级概念。在Kubernetes中，ReplicaSet用于定义和管理在不同集群节点上运行的相同pod的集合。除其他事项外，ReplicaSet定义了在pod内运行的容器使用哪些容器镜像，以及集群中将运行多少个pod实例。这些属性和许多其他属性被称为所需状态。
- en: 'The ReplicaSet is responsible for reconciling the desired state at all times,
    if the actual state ever deviates from it. Here is a Kubernetes ReplicaSet:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet负责始终协调所需的状态，如果实际状态偏离所需状态。这是一个Kubernetes ReplicaSet：
- en: '![](assets/7f6cc3c9-2f46-4517-8b87-372a24c0c8bc.png)Kubernetes ReplicaSet'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/7f6cc3c9-2f46-4517-8b87-372a24c0c8bc.png)Kubernetes ReplicaSet'
- en: In the preceding diagram, we can see a **ReplicaSet** called **rs-api,** which
    governs a number of pods. The pods are called **pod-api**. The **ReplicaSet**
    is responsible for making sure that, at any given time, there are always the desired
    number of pods running. If one of the pods crashes for whatever reason, the **ReplicaSet**
    schedules a new pod on a node with free resources instead. If there are more pods
    than the desired number, then the **ReplicaSet** kills superfluous pods. With
    this, we can say that the **ReplicaSet** guarantees a self-healing and scalable
    set of pods. There is no limit to how many pods a **ReplicaSet** can hold.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到一个名为rs-api的ReplicaSet，它管理着一些pod。这些pod被称为pod-api。ReplicaSet负责确保在任何给定时间，始终有所需数量的pod在运行。如果其中一个pod因任何原因崩溃，ReplicaSet会在具有空闲资源的节点上安排一个新的pod。如果pod的数量超过所需数量，那么ReplicaSet会终止多余的pod。通过这种方式，我们可以说ReplicaSet保证了一组pod的自愈和可伸缩性。ReplicaSet可以容纳多少个pod没有限制。
- en: ReplicaSet specification
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReplicaSet规范
- en: 'Similar to what we have learned about pods, Kubernetes also allows us to either
    imperatively or declaratively define and create a `ReplicaSet`. Since the declarative
    approach is by far the most recommended one in most cases, we''re going to concentrate
    on this approach. Here is a sample specification for a Kubernetes `ReplicaSet`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们对pod的学习类似，Kubernetes也允许我们以命令式或声明式方式定义和创建ReplicaSet。由于在大多数情况下，声明式方法是最推荐的方法，我们将集中讨论这种方法。以下是一个Kubernetes
    ReplicaSet的样本规范：
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This looks an awful lot like the pod specification we introduced earlier. Let's
    concentrate on the differences, then. First, on line 2, we have the `kind`, which
    was `Pod `and is now `ReplicaSet`. Then, on lines 6–8, we have a selector, which
    determines the pods that will be part of the `ReplicaSet`. In this case, it is
    all the pods that have `app` as a label with the value `web`. Then, on line 9,
    we define how many replicas of the pod we want to run; three, in this case. Finally,
    we have the `template` section, which first defines the `metadata` and then the
    `spec`, which defines the containers that run inside the pod. In our case, we
    have a single container using the `nginx:alpine` image and exporting port `80`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常像我们之前介绍的Pod规范。让我们集中精力关注不同之处。首先，在第2行，我们有`kind`，它曾经是`Pod`，现在是`ReplicaSet`。然后，在第6-8行，我们有一个选择器，它确定将成为`ReplicaSet`一部分的Pods。在这种情况下，它是所有具有`app`标签值为`web`的Pods。然后，在第9行，我们定义了我们想要运行的Pod的副本数量；在这种情况下是三个。最后，我们有`template`部分，首先定义了`metadata`，然后定义了`spec`，它定义了在Pod内运行的容器。在我们的情况下，我们有一个使用`nginx:alpine`镜像并导出端口`80`的单个容器。
- en: The really important elements are the number of replicas and the selector, which
    specifies the set of pods governed by the `ReplicaSet`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 真正重要的元素是副本的数量和选择器，它指定了由`ReplicaSet`管理的Pod集合。
- en: 'In our `ch15` folder, we have a file called `replicaset.yaml` that contains
    the preceding specification. Let''s use this file to create the `ReplicaSet`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`ch15`文件夹中，有一个名为`replicaset.yaml`的文件，其中包含了前面的规范。让我们使用这个文件来创建`ReplicaSet`：
- en: '[PRE25]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If we list all the ReplicaSets in the cluster, we get the following (`rs` is
    a shortcut for `replicaset`):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们列出集群中的所有ReplicaSets，我们会得到以下结果（`rs`是`replicaset`的缩写）：
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding output, we can see that we have a single ReplicaSet called `rs-web` whose
    desired state is three (pods). The current state also shows three pods and tell
    us that all three pods are ready. We can also list all the pods in the system.
    This results in the following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的输出中，我们可以看到我们有一个名为`rs-web`的单个ReplicaSet，其期望状态为三（个Pods）。当前状态也显示了三个Pods，并告诉我们所有三个Pods都已准备就绪。我们还可以列出系统中的所有Pods。这将导致以下输出：
- en: '[PRE27]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we can see our three expected pods. The names of the pods use the name
    of the ReplicaSet with a unique ID appended for each pod. In the `READY` column,
    we can see how many containers have been defined in the pod and how many of them
    are ready. In our case, we only have a single container per pod and, in each case,
    it is ready. Thus, the overall status of the pod is `Running`. We can also see
    how many times each pod had to be restarted. In our case, we don't have any restarts.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们期望的三个Pods。Pods的名称使用ReplicaSet的名称，并为每个Pod附加了唯一的ID。在`READY`列中，我们可以看到在Pod中定义了多少个容器以及其中有多少个是就绪的。在我们的情况下，每个Pod只有一个容器，并且每种情况下都已准备就绪。因此，Pod的整体状态是`Running`。我们还可以看到每个Pod需要重新启动的次数。在我们的情况下，我们没有任何重新启动。
- en: Self-healing
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自愈
- en: 'Now, let''s test the magic powers of the self-healing `ReplicaSet` by randomly
    killing one of its pods and observing what happens. Let''s delete the first pod
    from the previous list:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试自愈`ReplicaSet`的魔力，随机杀死其中一个Pod并观察发生了什么。让我们从前面的列表中删除第一个Pod：
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s list all the pods again. We expect to see only two pods, *right*?
    Wrong:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次列出所有的Pods。我们期望只看到两个Pods，*对吗*？错了：
- en: '![](assets/71366fbf-4638-484c-b2f6-dab56b41f5f4.png)List of pods after killing
    a pod of the ReplicaSet'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/71366fbf-4638-484c-b2f6-dab56b41f5f4.png)杀死ReplicaSet中一个Pod后的Pod列表'
- en: 'OK; evidently, the second pod in the list has been recreated, as we can see
    from the `AGE` column. This is auto-healing in action. Let''s see what we discover
    if we describe the ReplicaSet:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 好的；显然，列表中的第二个Pod已经被重新创建，我们可以从`AGE`列中看到。这就是自动修复的工作。让我们看看如果我们描述ReplicaSet会发现什么：
- en: '![](assets/5f1f58fd-fe0d-46a6-8cdb-df8bf3bfc159.png)Describe the ReplicaSet'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 描述ReplicaSet
- en: And indeed, we find an entry under `Events` that tells us that the `ReplicaSet` created
    the new pod called `rs-web-q6cr7`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，在“事件”下我们找到了一个条目，告诉我们ReplicaSet创建了名为rs-web-q6cr7的新pod。
- en: Kubernetes deployment
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes部署
- en: Kubernetes takes the single-responsibility principle very seriously. All Kubernetes
    objects are designed to do one thing and one thing only, and they are designed
    to do this one thing very well. In this regard, we have to understand Kubernetes
    **ReplicaSets** and **Deployments**. A **ReplicaSet**, as we have learned, is
    responsible for achieving and reconciling the desired state of an application
    service. This means that the **ReplicaSet** manages a set of pods.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes非常严肃地遵循单一责任原则。所有Kubernetes对象都被设计成只做一件事，并且它们被设计得非常出色。在这方面，我们必须了解Kubernetes的ReplicaSets和Deployments。正如我们所学到的，ReplicaSet负责实现和协调应用服务的期望状态。这意味着ReplicaSet管理一组pod。
- en: '**Deployment** augments a **ReplicaSet** by providing rolling updates and rollback
    functionality on top of it. In Docker Swarm, the Swarm service incorporates the
    functionality of both **ReplicaSet** and **Deployment.** In this regard, SwarmKit
    is much more monolithic than Kubernetes. The following diagram shows the relationship
    of a **Deployment** to a **ReplicaSet**:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 部署通过在ReplicaSet的基础上提供滚动更新和回滚功能来增强ReplicaSet。在Docker Swarm中，Swarm服务结合了ReplicaSet和部署的功能。在这方面，SwarmKit比Kubernetes更加单片化。下图显示了部署与ReplicaSet的关系：
- en: '![](assets/d6de18ec-b3ad-45c3-bc88-555afcaea7dd.png)Kubernetes deployment'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes部署
- en: In the preceding diagram, the **ReplicaSet** is defining and governing a set
    of identical pods. The main characteristics of the **ReplicaSet** are that it
    is **self-healing**, **scalable**, and always does its best to reconcile the **desired**
    **state**. Kubernetes Deployment, in turn, adds rolling updates and rollback functionality
    to this. In this regard, a deployment is really a wrapper object to a ReplicaSet.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，ReplicaSet定义和管理一组相同的pod。ReplicaSet的主要特点是它是自愈的、可扩展的，并且始终尽最大努力协调期望状态。而Kubernetes部署则为此添加了滚动更新和回滚功能。在这方面，部署实际上是对ReplicaSet的包装对象。
- en: We will learn more about rolling updates and rollbacks in the [Chapter 16](cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml),
    *Deploying, Updating, and Securing an Application with Kubernetes*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第16章《使用Kubernetes部署、更新和保护应用程序》中学习滚动更新和回滚。
- en: In the next section, we will learn more about Kubernetes services and how they
    enable service discovery and routing.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更多地了解Kubernetes服务以及它们如何实现服务发现和路由。
- en: Kubernetes service
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes服务
- en: 'The moment we start to work with applications consisting of more than one application
    service, we need service discovery. The following diagram illustrates this problem:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们开始处理由多个应用服务组成的应用程序，我们就需要服务发现。下图说明了这个问题：
- en: '![](assets/559a71f8-789e-4f46-ac6f-5366093fdbea.png)Service discovery'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现
- en: In the preceding diagram, we have a **Web API** service that needs access to
    three other services: **payments**, **shipping**, and **ordering**. The **Web
    API** should never have to care about how and where to find those three services.
    In the API code, we just want to use the name of the service we want to reach
    and its port number. A sample would be the following URL `http://payments:3000`,
    which is used to access an instance of the payments service.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们有一个需要访问其他三个服务的**Web API**服务：**支付**，**运输**和**订购**。**Web API**不应该关心如何以及在哪里找到这三个服务。在API代码中，我们只想使用我们想要到达的服务的名称和端口号。一个示例是以下URL
    `http://payments:3000`，用于访问支付服务的一个实例。
- en: In Kubernetes, the payments application service is represented by a ReplicaSet
    of pods. Due to the nature of highly distributed systems, we cannot assume that
    pods have stable endpoints. A pod can come and go on a whim. But that's a problem
    if we need to access the corresponding application service from an internal or
    external client. If we cannot rely on pod endpoints being stable, *what else can
    we do?*
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，支付应用程序服务由一组Pod的ReplicaSet表示。由于高度分布式系统的性质，我们不能假设Pod具有稳定的端点。一个Pod可能随心所欲地出现和消失。但是，如果我们需要从内部或外部客户端访问相应的应用程序服务，这就是一个问题。如果我们不能依赖于Pod端点的稳定性，*我们还能做什么呢？*
- en: 'This is where Kubernetes services come into play. They are meant to provide
    stable endpoints to ReplicaSets or Deployments, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Kubernetes服务发挥作用的地方。它们旨在为ReplicaSets或Deployments提供稳定的端点，如下所示：
- en: '![](assets/09cdd19a-e492-480c-9df1-f65ffc600f9d.png)Kubernetes service providing
    stable endpoints to clients'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/09cdd19a-e492-480c-9df1-f65ffc600f9d.png)Kubernetes服务为客户端提供稳定的端点'
- en: In the preceding diagram, in the center, we can see such a Kubernetes **Service**.
    It provides a **reliable** cluster-wide **IP** address, also called a **virtual
    IP** (**VIP**), as well as a **reliable** **Port** that's unique in the whole
    cluster. The pods that the Kubernetes service is proxying are determined by the
    **Selector** defined in the service specification. Selectors are always based
    on labels. Every Kubernetes object can have zero to many labels assigned to it.
    In our case, the **Selector** is **app=web**; that is, all pods that have a label
    called app with a value of web are proxied.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，中心位置有一个这样的Kubernetes **Service**。它提供了一个**可靠的**集群范围**IP**地址，也称为**虚拟IP**（**VIP**），以及整个集群中唯一的**可靠**端口。Kubernetes服务代理的Pod由服务规范中定义的**选择器**确定。选择器总是基于标签。每个Kubernetes对象都可以分配零个或多个标签。在我们的情况下，**选择器**是**app=web**；也就是说，所有具有名为app且值为web的标签的Pod都被代理。
- en: In the next section, we will learn more about context-based routing and how
    Kubernetes alleviates this task.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习更多关于基于上下文的路由以及Kubernetes如何减轻这项任务。
- en: Context-based routing
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于上下文的路由
- en: 'Often, we want to configure context-based routing for our Kubernetes cluster.
    Kubernetes offers us various ways to do this. The preferred and most scalable
    way at this time is to use an **IngressController**. The following diagram tries
    to illustrate how this ingress controller works:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望为我们的Kubernetes集群配置基于上下文的路由。Kubernetes为我们提供了各种方法来做到这一点。目前，首选和最可扩展的方法是使用**IngressController**。以下图尝试说明这个IngressController是如何工作的：
- en: '![](assets/bc913d65-7d30-4339-b876-5d1c0201e3f5.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/bc913d65-7d30-4339-b876-5d1c0201e3f5.png)'
- en: Context-based routing using a Kubernetes ingress controller
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes Ingress Controller进行基于上下文的路由
- en: In the preceding diagram, we can see how context-based (or layer 7) routing
    works when using an **IngressController**, such as Nginx. Here, we have the deployment
    of an application service called **web**. All the pods of this application service
    have the following label: **app=web**. Then, we have a Kubernetes service called
    **web** that provides a stable endpoint to those pods. The service has a (virtual)
    **IP** of `52.14.0.13` and exposes port `30044`. That is, if a request comes to
    any node of the Kubernetes cluster for the name **web** and port `30044`, then
    it is forwarded to this service. The service then load-balances the request to
    one of the pods.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到当使用**IngressController**（如Nginx）时，基于上下文（或第7层）的路由是如何工作的。在这里，我们部署了一个名为**web**的应用服务。该应用服务的所有pod都具有以下标签：**app=web**。然后，我们有一个名为**web**的Kubernetes服务，为这些pod提供了一个稳定的端点。该服务具有一个（虚拟）**IP**为`52.14.0.13`，并暴露端口`30044`。也就是说，如果任何Kubernetes集群的节点收到对**web**名称和端口`30044`的请求，那么它将被转发到该服务。然后该服务将请求负载均衡到其中一个pod。
- en: 'So far, so good, *but how is an ingress request from a client to the *`http[s]://example.com/web`* URL routed
    to our web service?* First, we have to define routing from a context-based request
    to a corresponding `<service name>/<port> request`. This is done through an **Ingress** object:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很好，*但是客户端对`http[s]://example.com/web`*的Ingress请求是如何路由到我们的web服务的呢？*首先，我们必须定义从基于上下文的请求到相应的`<service
    name>/<port>请求`的路由。这是通过一个**Ingress**对象完成的：
- en: In the **Ingress** object, we define the **Host** and **Path** as the source
    and the (service) name, and the port as the target. When this Ingress object is
    created by the Kubernetes API server, then a process that runs as a sidecar in `IngressController`
    picks this change up.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**Ingress**对象中，我们将**Host**和**Path**定义为源和（服务）名称，端口定义为目标。当Kubernetes API服务器创建此Ingress对象时，运行在`IngressController`中的一个进程会捕捉到这个变化。
- en: The process modifies the configuration the configuration file of the Nginx reverse
    proxy.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该进程修改了Nginx反向代理的配置文件。
- en: By adding the new route, Nginx is then asked to reload its configuration and
    thus will be able to correctly route any incoming requests to `http[s]://example.com/web`.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加新路由，然后要求Nginx重新加载其配置，从而能够正确地将任何传入请求路由到`http[s]://example.com/web`。
- en: In the next section, we are going to compare Docker SwarmKit with Kubernetes
    by contrasting some of the main resources of each orchestration engine.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将通过对比每个编排引擎的一些主要资源来比较Docker SwarmKit和Kubernetes。
- en: Comparing SwarmKit with Kubernetes
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较SwarmKit和Kubernetes
- en: 'Now that we have learned a lot of details about the most important resources
    in Kubernetes, it is helpful to compare the two orchestrators, SwarmKit and Kubernetes,
    by matching important resources. Let''s take a look:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了关于Kubernetes中最重要的资源的许多细节，通过匹配重要资源来比较两个编排器SwarmKit和Kubernetes是有帮助的。让我们来看一下：
- en: '| **SwarmKit** | **Kubernetes** | **Description** |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| **SwarmKit** | **Kubernetes** | **描述** |'
- en: '| Swarm | Cluster | Set of servers/nodes managed by the respective orchestrator.
    |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Swarm | 集群 | 由各自编排器管理的一组服务器/节点。 |'
- en: '| Node | Cluster member | Single host (physical or virtual) that''s a member
    of the Swarm/cluster. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 节点 | 集群成员 | Swarm/集群的单个主机（物理或虚拟）。 |'
- en: '| Manager node | Master | Node managing the Swarm/cluster. This is the control
    plane. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 管理节点 | 主节点 | 管理Swarm/集群的节点。这是控制平面。 |'
- en: '| Worker node | Node | Member of the Swarm/cluster running application workload.
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 工作节点 | 节点 | 运行应用程序工作负载的Swarm/集群成员。 |'
- en: '| Container | Container** | An instance of a container image running on a node.
    **Note: In a Kubernetes cluster, we cannot run a container directly. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 容器 | 容器** | 在节点上运行的容器镜像的实例。**注意：在Kubernetes集群中，我们不能直接运行容器。 |'
- en: '| Task | Pod | An instance of a service (Swarm) or ReplicaSet (Kubernetes)
    running on a node. A task manages a single container while a Pod contains one
    to many containers that all share the same network namespace. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | Pod | 在节点上运行的服务（Swarm）或ReplicaSet（Kubernetes）的实例。一个任务管理一个容器，而一个Pod包含一个到多个共享相同网络命名空间的容器。
    |'
- en: '| Service | ReplicaSet | Defines and reconciles the desired state of an application
    service consisting of multiple instances. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | 副本集 | 定义并协调由多个实例组成的应用服务的期望状态。 |'
- en: '| Service | Deployment | A deployment is a ReplicaSet augmented with rolling
    updates and rollback capabilities. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | 部署 | 部署是一个带有滚动更新和回滚功能的ReplicaSet。|'
- en: '| Routing Mesh | Service | The Swarm Routing Mesh provides L4 routing and load
    balancing using IPVS. A Kubernetes service is an abstraction that defines a logical
    set of pods and a policy that can be used to access them. It is a stable endpoint
    for a set of pods. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 路由网格 | 服务 | Swarm路由网格使用IPVS提供L4路由和负载平衡。Kubernetes服务是一个抽象，定义了一组逻辑pod和可用于访问它们的策略。它是一组pod的稳定端点。
    |'
- en: '| Stack | Stack ** | The definition of an application consisting of multiple
    (Swarm) services.**Note: While stacks are not native to Kubernetes, Docker''s
    tool, Docker for Desktop, will translate them for deployment onto a Kubernetes
    cluster. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 堆栈 | 堆栈 ** | 由多个（Swarm）服务组成的应用程序的定义。**注意：虽然堆栈不是Kubernetes的本机功能，但Docker的工具Docker
    for Desktop将它们转换为部署到Kubernetes集群上的功能。 |'
- en: '| Network | Network policy | Swarm **software-defined networks** (**SDNs**)
    are used to firewall containers. Kubernetes only defines a single flat network.
    Every pod can reach every other pod and/or node, unless network policies are explicitly
    defined to constrain inter-pod communication. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | 网络策略 | Swarm的软件定义网络（SDN）用于防火墙容器。Kubernetes只定义了一个单一的平面网络。除非明确定义了网络策略来限制pod之间的通信，否则每个pod都可以访问每个其他pod和/或节点。'
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the basics of Kubernetes. We took an overview
    of its architecture and introduced the main resources that are used to define
    and run applications in a Kubernetes cluster. We also introduced Minikube and
    Kubernetes support in Docker for Desktop.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了Kubernetes的基础知识。我们概述了其架构，并介绍了在Kubernetes集群中定义和运行应用程序的主要资源。我们还介绍了Minikube和Docker
    for Desktop中的Kubernetes支持。
- en: In the next chapter, we're going to deploy an application into a Kubernetes
    cluster. Then, we're going to be updating one of the services of this application
    using a zero downtime strategy. Finally, we're going to instrument application
    services running in Kubernetes with sensitive data using secrets. Stay tuned!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在Kubernetes集群中部署一个应用程序。然后，我们将使用零停机策略更新此应用程序的其中一个服务。最后，我们将使用机密信息对在Kubernetes中运行的应用程序服务进行仪器化。敬请关注！
- en: Questions
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please answer the following questions to assess your learning progress:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请回答以下问题以评估您的学习进度：
- en: Explain in a few short sentences what the role of a Kubernetes master is.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用几句简短的话解释一下Kubernetes主节点的作用。
- en: List the elements that need to be present on each Kubernetes (worker) node.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出每个Kubernetes（工作）节点上需要存在的元素。
- en: We cannot run individual containers in a Kubernetes cluster.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能在Kubernetes集群中运行单独的容器。
- en: A. Yes
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: A. 是
- en: B. No
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: B. 否
- en: Explain the reason why the containers in a pod can use `localhost` to communicate
    with each other.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释为什么pod中的容器可以使用`localhost`相互通信。
- en: What is the purpose of the so-called pause container in a pod?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所谓的暂停容器在pod中的目的是什么？
- en: 'Bob tells you "Our application consists of three Docker images: `web`, `inventory`,
    and `db`. Since we can run multiple containers in a Kubernetes pod, we are going
    to deploy all the services of our application in a single pod." List three to
    four reasons why this is a bad idea.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鲍勃告诉你：“我们的应用由三个Docker镜像组成：`web`、`inventory`和`db`。由于我们可以在Kubernetes pod中运行多个容器，我们将在一个单独的pod中部署我们应用的所有服务。”列出三到四个这样做是个坏主意的原因。
- en: Explain in your own words why we need Kubernetes ReplicaSets.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用自己的话解释为什么我们需要Kubernetes ReplicaSets。
- en: Under which circumstances do we need Kubernetes deployments?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在什么情况下我们需要Kubernetes部署？
- en: List at least three types of Kubernetes service and explain their purposes and
    their differences.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出至少三种Kubernetes服务类型，并解释它们的目的和区别。
- en: Further reading
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Here is a list of articles that contain more detailed information about the
    various topics that we discussed in this chapter:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些包含更多关于我们在本章讨论的各种主题的详细信息的文章列表：
- en: The Raft Consensus Algorithm: [https://raft.github.io/](https://raft.github.io/)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raft一致性算法：[https://raft.github.io/](https://raft.github.io/)
- en: Docker Compose and Kubernetes with Docker for Desktop: [https://dockr.ly/2G8Iqb9](https://dockr.ly/2G8Iqb9)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker桌面版的Docker Compose和Kubernetes：[https://dockr.ly/2G8Iqb9](https://dockr.ly/2G8Iqb9)
- en: Kubernetes Documentation: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes文档：[https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)
