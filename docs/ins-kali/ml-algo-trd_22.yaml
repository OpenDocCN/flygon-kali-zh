- en: '22'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '22'
- en: Deep Reinforcement Learning – Building a Trading Agent
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习-构建一个交易代理
- en: In this chapter, we'll introduce **reinforcement learning** (**RL**), which
    takes a different approach to **machine learning** (**ML**) than the supervised
    and unsupervised algorithms we have covered so far. RL has attracted enormous
    attention as it has been the main driver behind some of the most exciting AI breakthroughs,
    like AlphaGo. David Silver, AlphaGo's creator and the lead RL researcher at Google-owned
    DeepMind, recently won the prestigious 2019 ACM Prize in Computing "for breakthrough
    advances in computer game-playing." We will see that the interactive and online
    nature of RL makes it particularly well-suited to the trading and investment domain.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**强化学习**（**RL**），它与迄今为止我们所涵盖的监督和无监督算法有所不同。RL引起了巨大的关注，因为它是一些最激动人心的人工智能突破的主要推动力，比如AlphaGo。David
    Silver，AlphaGo的创造者，也是Google旗下DeepMind的首席RL研究员，最近因“在计算机游戏中取得突破性进展”而赢得了备受推崇的2019年ACM计算奖。我们将看到RL的互动和在线特性使其特别适合交易和投资领域。
- en: RL models **goal-directed learning by an agent** that interacts with a typically
    stochastic environment that the agent has incomplete information about. RL aims
    to automate how the agent makes decisions to achieve a long-term objective by
    learning the value of states and actions from a reward signal. The ultimate goal
    is to derive a policy that encodes behavioral rules and maps states to actions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RL模型通过与代理交互的通常是随机的环境来**目标导向地学习**，代理对该环境的信息不完整。RL旨在通过从奖励信号中学习状态和行动的价值来自动化代理的决策，最终目标是得出一个编码行为规则并将状态映射到行动的策略。
- en: RL is considered **most similar to human learning** that results from taking
    actions in the real world and observing the consequences. It differs from supervised
    learning because it optimizes the agent's behavior one trial-and-error experience
    at a time based on a scalar reward signal, rather than by generalizing from correctly
    labeled, representative samples of the target concept. Moreover, RL does not stop
    at making predictions. Instead, it takes an end-to-end perspective on goal-oriented
    decision-making by including actions and their consequences.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: RL被认为是**最类似于人类学习**，是通过在现实世界中采取行动并观察结果而产生的学习。它与监督学习不同，因为它通过一次次的试错经验基于标量奖励信号来优化代理的行为，而不是通过从正确标记的代表性样本中进行泛化。此外，RL不仅仅是进行预测。相反，它采取端到端的目标导向决策视角，包括行动及其后果。
- en: In this chapter, you will learn how to formulate an RL problem and apply various
    solution methods. We will cover model-based and model-free methods, introduce
    the OpenAI Gym environment, and combine deep learning with RL to train an agent
    that navigates a complex environment. Finally, we'll show you how to adapt RL
    to algorithmic trading by modeling an agent that interacts with the financial
    market to optimize its profit objective.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何制定RL问题并应用各种解决方法。我们将涵盖基于模型和无模型的方法，介绍OpenAI Gym环境，并将深度学习与RL相结合，以训练一个在复杂环境中导航的代理。最后，我们将向您展示如何通过建模与金融市场互动的代理来调整RL，以优化其利润目标。
- en: 'More specifically, after reading this chapter, you will be able to:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在阅读本章后，您将能够：
- en: Define a **Markov decision problem** (**MDP**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义**马尔可夫决策问题**（**MDP**）
- en: Use value and policy iteration to solve an MDP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用值迭代和策略迭代来解决MDP
- en: Apply Q-learning in an environment with discrete states and actions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在具有离散状态和行动的环境中应用Q-learning
- en: Build and train a deep Q-learning agent in a continuous environment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连续环境中构建和训练深度Q学习代理
- en: Use OpenAI Gym to train an RL trading agent
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenAI Gym来训练RL交易代理
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub存储库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: Elements of a reinforcement learning system
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习系统的要素
- en: 'RL problems feature several elements that set them apart from the ML settings
    we have covered so far. The following two sections outline the key features required
    for defining and solving an RL problem by learning a policy that automates decisions.
    We''ll use the notation and generally follow *Reinforcement Learning: An Introduction*
    (Sutton and Barto 2018) and David Silver''s UCL Courses on RL ([https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/)),
    which are recommended for further study beyond the brief summary that the scope
    of this chapter permits.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RL问题具有几个元素，使它们与迄今为止我们所涵盖的ML设置有所不同。以下两节概述了定义和解决RL问题所需的关键特征，即通过学习自动化决策的策略。我们将使用符号，并通常遵循*强化学习：一种介绍*（Sutton和Barto
    2018）和David Silver的UCL课程关于RL（[https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/)），这些课程建议进一步学习，超出了本章的范围允许的简要总结。
- en: RL problems aim to solve for actions that **optimize the agent's objective,
    given some observations about the environment**. The environment presents information
    about its state to the agent, assigns rewards for actions, and transitions the
    agent to new states, subject to probability distributions the agent may or may
    not know. It may be fully or partially observable, and it may also contain other
    agents. The structure of the environment has a strong impact on the agent's ability
    to learn a given task, and typically requires significant up-front design effort
    to facilitate the training process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RL问题旨在解决**优化代理目标的行动**，给出有关环境的一些观察。环境向代理提供其状态的信息，为行动分配奖励，并将代理转移到新状态，受代理可能或可能不知道的概率分布的影响。它可能是完全可观察的，也可能包含其他代理。环境的结构对代理学习给定任务的能力有很大影响，并且通常需要大量的前期设计工作来促进训练过程。
- en: RL problems differ based on the complexity of the environment's state and agent's
    action spaces, which can be either discrete or continuous. Continuous actions
    and states, unless discretized, require machine learning to approximate a functional
    relationship between states, actions, and their values. They also require generalization
    because the agent almost certainly experiences only a subset of the potentially
    infinite number of states and actions during training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于环境状态和代理动作空间的复杂性，RL问题有所不同，可以是离散的，也可以是连续的。连续的动作和状态，除非离散化，否则需要机器学习来逼近状态、动作和价值之间的功能关系。它们还需要泛化，因为代理在训练过程中几乎肯定只会经历潜在无限数量的状态和动作的子集。
- en: 'Solving complex decision problems usually requires a simplified model that
    isolates the key aspects. *Figure 22.1* highlights the **salient features of an
    RL problem**. These typically include:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解决复杂的决策问题通常需要一个简化的模型，它隔离了关键方面。*图22.1*突出了**RL问题的显著特征**。这些通常包括：
- en: Observations by the agent on the state of the environment
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理对环境状态的观察
- en: A set of actions available to the agent
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理可用的一组动作
- en: A policy that governs the agent's decisions
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指导代理决策的策略
- en: '![](img/B15439_22_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_01.png)'
- en: 'Figure 22.1: Components of an RL system'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.1：RL系统的组成部分
- en: In addition, the environment emits a **reward signal** (that may be negative)
    as the agent's action leads to a transition to a new state. At its core, the agent
    usually learns a **value function** that informs its judgment of the available
    actions. The agent's objective function processes the reward signal and translates
    the value judgments into an optimal policy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，环境发出一个**奖励信号**（可能为负），因为代理的动作导致转移到新状态。在其核心，代理通常学习一个**价值函数**，它通知其对可用动作的判断。代理的目标函数处理奖励信号，并将价值判断转化为最优策略。
- en: The policy – translating states into actions
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 - 将状态转化为动作
- en: At any point in time, **the policy defines the agent's behavior**. It maps any
    state the agent may encounter to one or several actions. In an environment with
    a limited number of states and actions, the policy can be a simple lookup table
    that's filled in during training.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时间点，**策略定义了代理的行为**。它将代理可能遇到的任何状态映射到一个或多个动作。在状态和动作数量有限的环境中，策略可以是一个在训练过程中填充的简单查找表。
- en: With continuous states and actions, the policy takes the form of a function
    that machine learning can help to approximate. The policy may also involve significant
    computation, as in the case of AlphaZero, which uses tree search to decide on
    the best action for a given game state. The policy may also be stochastic and
    assign probabilities to actions, given a state.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续状态和动作，策略采用机器学习可以帮助逼近的函数形式。策略也可能涉及大量计算，如AlphaZero的情况，它使用树搜索来决定给定游戏状态的最佳动作。策略也可能是随机的，并为给定状态分配动作的概率。
- en: Rewards – learning from actions
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励 - 从行动中学习
- en: The reward signal is a single value that the environment sends to the agent
    at each time step. The agent's objective is typically to **maximize the total
    reward received over time**. Rewards can also be a stochastic function of the
    state and the actions. They are typically discounted to facilitate convergence
    and reflect the time decay of value.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励信号是环境在每个时间步发送给代理的单个值。代理的目标通常是**最大化随时间收到的总奖励**。奖励也可能是状态和动作的随机函数。它们通常被折现以促进收敛并反映价值的时间衰减。
- en: Rewards are the **only way for the agent to learn** about the value of its decisions
    in a given state and to modify the policy accordingly. Due to its critical impact
    on the agent's learning, the reward signal is often the most challenging part
    of designing an RL system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励是代理了解其在给定状态下决策价值并相应修改策略的**唯一方式**。由于对代理学习的关键影响，奖励信号通常是设计RL系统中最具挑战性的部分。
- en: Rewards need to clearly communicate what the agent should accomplish (as opposed
    to how it should do so) and may require domain knowledge to properly encode this
    information. For example, the development of a trading agent may need to define
    rewards for buy, hold, and sell decisions. These may be limited to profit and
    loss, but also may need to include volatility and risk considerations, such as
    drawdown.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励需要清晰地传达代理应该完成的任务（而不是如何完成），可能需要领域知识来正确编码这些信息。例如，交易代理的开发可能需要为买入、持有和卖出决策定义奖励。这些可能仅限于利润和损失，但也可能需要包括波动性和风险考虑，如回撤。
- en: The value function – optimal choice for the long run
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 价值函数 - 长期最佳选择
- en: 'The reward provides immediate feedback on actions. However, solving an RL problem
    requires decisions that create value in the long run. This is where the value
    function comes in: it summarizes the utility of states or of actions in a given
    state in terms of their long-term reward.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励提供了对行动的即时反馈。然而，解决RL问题需要长期创造价值的决策。这就是价值函数的作用：它总结了状态或给定状态下动作的效用，以其长期奖励为衡量标准。
- en: In other words, the value of a state is the total reward an agent can expect
    to obtain in the future when starting in that state. The immediate reward may
    be a good proxy of future rewards, but the agent also needs to account for cases
    where low rewards are followed by much better outcomes that are likely to follow
    (or the reverse).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，状态的价值是代理在未来可以期望获得的总奖励，当代理从该状态开始时。即时奖励可能是未来奖励的良好代理，但代理还需要考虑低奖励后可能出现的更好结果（或相反）。
- en: Hence, **value estimates aim to predict future rewards**. Rewards are the key
    inputs, and the goal of making value estimates is to achieve more rewards. However,
    RL methods focus on learning accurate values that enable good decisions while
    efficiently leveraging the (often limited) experience.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**价值估计旨在预测未来奖励**。奖励是关键输入，而进行价值估计的目标是获得更多奖励。然而，RL方法侧重于学习准确的价值，以便在有效利用（通常有限的）经验的同时做出良好的决策。
- en: There are also RL approaches that do not rely on value functions, such as randomized
    optimization methods like genetic algorithms or simulated annealing, which aim
    to find optimal behaviors by efficiently exploring the policy space. The current
    interest in RL, however, is mostly driven by methods that directly or indirectly
    estimate the value of states and actions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些不依赖于价值函数的强化学习方法，例如基因算法或模拟退火等随机优化方法，旨在通过有效地探索策略空间来找到最佳行为。然而，目前对强化学习的兴趣主要是由直接或间接估计状态和行动价值的方法驱动的。
- en: '**Policy gradient methods** are a new development that relies on a parameterized,
    differentiable policy that can be directly optimized with respect to the objective
    using gradient descent (Sutton et al. 2000). See the resources on GitHub that
    include abstracts of key papers and algorithms beyond the scope of this chapter.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略梯度方法**是一种新的发展，依赖于参数化的、可微分的策略，可以直接通过梯度下降来优化目标（Sutton等人，2000年）。请参阅GitHub上的资源，其中包括本章节范围之外的关键论文和算法的摘要。'
- en: With or without a model – look before you leap?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有模型或无模型——三思而后行？
- en: '**Model-based RL** approaches learn a model of the environment to allow the
    agent to plan ahead by predicting the consequences of its actions. Such a model
    may be used, for example, to predict the next state and reward based on the current
    state and action. This is the **basis for planning**, that is, deciding on the
    best course of action by considering possible futures before they materialize.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于模型的强化学习**方法学习环境的模型，使代理能够通过预测其行动的后果来提前规划。例如，这样的模型可以用来预测基于当前状态和行动的下一个状态和奖励。这是**规划的基础**，即在未来可能发生之前考虑最佳行动。'
- en: Simpler **model-free methods**, in contrast, learn from **trial and error**.
    Modern RL methods span the gamut from low-level trial-and-error methods to high-level,
    deliberative planning. The right approach depends on the complexity and learnability
    of the environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，更简单的**无模型方法**是通过**反复试验**学习的。现代强化学习方法涵盖了从低层次的试错方法到高层次的深思熟虑的规划。正确的方法取决于环境的复杂性和可学习性。
- en: How to solve reinforcement learning problems
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何解决强化学习问题
- en: RL methods aim to learn from experience how to take actions that achieve a long-term
    goal. To this end, the agent and the environment interact over a sequence of discrete
    time steps via the interface of actions, state observations, and rewards described
    in the previous section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习方法旨在通过经验学习如何采取行动以实现长期目标。为此，代理和环境通过一系列离散时间步骤的接口进行交互，这些时间步骤通过前一节中描述的行动、状态观察和奖励来实现。
- en: Key challenges in solving RL problems
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决强化学习问题的关键挑战
- en: 'Solving RL problems requires addressing two unique challenges: the credit-assignment
    problem and the exploration-exploitation trade-off.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 解决强化学习问题需要解决两个独特的挑战：信用分配问题和探索-利用权衡。
- en: Credit assignment
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信用分配
- en: In RL, reward signals can occur significantly later than actions that contributed
    to the result, complicating the association of actions with their consequences.
    For example, when an agent takes 100 different positions and trades repeatedly,
    how does it realize that certain holdings performed much better than others if
    it only learns about the portfolio return?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，奖励信号可能比导致结果的行动发生得晚得多，使得将行动与其后果联系起来变得复杂。例如，当一个代理人多次进行100次不同的持仓和交易时，如果它只了解了投资组合回报，它如何意识到某些持仓的表现要比其他的好得多？
- en: The **credit-assignment problem** is the challenge of accurately estimating
    the benefits and costs of actions in a given state, despite these delays. RL algorithms
    need to find a way to distribute the credit for positive and negative outcomes
    among the many decisions that may have been involved in producing it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**信用分配问题**是准确估计在给定状态下行动的利益和成本的挑战，尽管存在这些延迟。强化学习算法需要找到一种方法来在许多可能涉及到产生它的决策中分配积极和消极结果的信用。'
- en: Exploration versus exploitation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索与利用
- en: The dynamic and interactive nature of RL implies that the agent needs to estimate
    the value of the states and actions before it has experienced all relevant trajectories.
    While it is able to select an action at any stage, these decisions are based on
    incomplete learning, yet generate the agent's first insights into the optimal
    choices of its behavior.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的动态和交互性意味着代理需要在经历所有相关轨迹之前估计状态和行动的价值。虽然它能够在任何阶段选择行动，但这些决策是基于不完全的学习，但却为代理的行为的最佳选择带来了第一手的见解。
- en: Partial visibility into the value of actions creates the risk of decisions that
    only exploit past (successful) experience rather than exploring uncharted territory.
    Such choices limit the agent's exposure and prevent it from learning an optimal
    policy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对行动价值的部分可见性会导致只利用过去（成功的）经验而不探索未知领域的决策的风险。这些选择限制了代理的暴露，并阻止其学习最佳策略。
- en: An RL algorithm needs to balance this **exploration-exploitation trade-off**—too
    little exploration will likely produce biased value estimates and suboptimal policies,
    whereas too little exploitation prevents learning from taking place in the first
    place.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法需要平衡**探索-利用权衡**——过少的探索可能会产生偏见的价值估计和次优的策略，而过少的利用则阻止了学习的发生。
- en: Fundamental approaches to solving RL problems
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决强化学习问题的基本方法
- en: 'There are numerous approaches to solving RL problems, all of which involve
    finding rules for the agent''s optimal behavior:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 解决强化学习问题有许多方法，所有这些方法都涉及找到代理的最佳行为规则：
- en: '**Dynamic programming** (**DP**) methods make the often unrealistic assumption
    of complete knowledge of the environment, but they are the conceptual foundation
    for most other approaches.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态规划**（**DP**）方法往往假设对环境有完全的了解，这是不现实的，但它们是大多数其他方法的概念基础。'
- en: '**Monte Carlo** (**MC**) methods learn about the environment and the costs
    and benefits of different decisions by sampling entire state-action-reward sequences.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡洛（MC）方法通过对整个状态-动作-奖励序列进行抽样来学习环境和不同决策的成本和收益。
- en: '**Temporal difference** (**TD**) learning significantly improves sample efficiency
    by learning from shorter sequences. To this end, it relies on **bootstrapping**,
    which is defined as refining its estimates based on its own prior estimates.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时序差分（TD）学习通过从更短的序列中学习显著提高了样本效率。为此，它依赖于引导，即根据自己先前的估计来改进其估计。
- en: When an RL problem includes well-defined transition probabilities and a limited
    number of states and actions, it can be framed as a finite **Markov decision process**
    (**MDP**) for which DP can compute an exact solution. Much of the current RL theory
    focuses on finite MDPs, but practical applications are used for (and require)
    more general settings. Unknown transition probabilities require efficient sampling
    to learn about their distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当强化学习问题包括明确定义的转移概率和有限数量的状态和动作时，可以将其构建为有限马尔可夫决策过程（MDP），动态规划可以计算出精确解。当前大部分强化学习理论都集中在有限MDP上，但实际应用需要更一般的设置。未知的转移概率需要有效的抽样来学习它们的分布。
- en: 'Approaches to continuous state and/or action spaces often leverage **machine
    learning** to approximate a value or policy function. They integrate supervised
    learning and, in particular, deep learning methods like those discussed in the
    previous four chapters. However, these methods face **distinct challenges** in
    the RL context:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 连续状态和/或动作空间的方法通常利用机器学习来近似值或策略函数。它们整合了监督学习，特别是像前四章中讨论的深度学习方法。然而，这些方法在强化学习环境中面临着独特的挑战：
- en: The **reward signal** does not directly reflect the target concept, like a labeled
    training sample.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励信号不直接反映目标概念，如标记的训练样本。
- en: The **distribution of the observations** depends on the agent's actions and
    the policy, which is itself the subject of the learning process.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察结果的分布取决于代理的行为和策略，策略本身是学习过程的主题。
- en: The following sections will introduce and demonstrate various solution methods.
    We'll start with the DP methods value iteration and policy iteration, which are
    limited to finite MDP with known transition probabilities. As we will see in the
    following section, they are the foundation for Q-learning, which is based on TD
    learning and does not require information about transition probabilities. It aims
    for similar outcomes as DP but with less computation and without assuming a perfect
    model of the environment. Finally, we'll expand the scope to continuous states
    and introduce deep Q-learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将介绍和演示各种解决方法。我们将从值迭代和策略迭代的DP方法开始，这些方法仅适用于已知转移概率的有限MDP。正如我们将在下一节中看到的，它们是基于TD学习的Q学习的基础，不需要关于转移概率的信息。它的目标是与DP类似的结果，但计算量更小，而且不需要假设环境的完美模型。最后，我们将扩展范围到连续状态，并介绍深度Q学习。
- en: Solving dynamic programming problems
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决动态规划问题
- en: Finite MDPs are a simple yet fundamental framework. We will introduce the trajectories
    of rewards that the agent aims to optimize, define the policy and value functions
    used to formulate the optimization problem, and the Bellman equations that form
    the basis for the solution methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有限MDP是一个简单但基本的框架。我们将介绍代理旨在优化的奖励轨迹，定义用于制定优化问题的策略和值函数，以及构成解决方法基础的贝尔曼方程。
- en: Finite Markov decision problems
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有限马尔可夫决策问题
- en: MDPs frame the agent-environment interaction as a sequential decision problem
    over a series of time steps *t* =1, …, *T* that constitute an episode. Time steps
    are assumed as discrete, but the framework can be extended to continuous time.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MDP将代理-环境交互框定为一个随时间步骤*t*=1，…，*T*的序贯决策问题，构成一个情节。时间步骤被假定为离散的，但该框架可以扩展到连续时间。
- en: The abstraction afforded by MDPs makes its application easily adaptable to many
    contexts. The time steps can be at arbitrary intervals, and actions and states
    can take any form that can be expressed numerically.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: MDP的抽象性使其在许多情境中易于适应。时间步骤可以是任意间隔，动作和状态可以采用可以用数字表示的任何形式。
- en: The Markov property implies that the current state completely describes the
    process, that is, the process has no memory. Information from past states adds
    no value when trying to predict the process's future. Due to these properties,
    the framework has been used to model asset prices subject to the efficient market
    hypothesis discussed in *Chapter 5*, *Portfolio Optimization and Performance Evaluation*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫性质意味着当前状态完全描述了过程，即过程没有记忆。当试图预测过程的未来时，来自过去状态的信息不增加价值。由于这些特性，该框架已被用于模拟资产价格，受到第5章《投资组合优化和绩效评估》中讨论的有效市场假说的影响。
- en: Sequences of states, actions, and rewards
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态、动作和奖励的序列
- en: 'MDPs proceed in the following fashion: at each step *t*, the agent observes
    the environment''s state ![](img/B15439_22_001.png) and selects an action ![](img/B15439_22_002.png),
    where *S* and *A* are the sets of states and actions, respectively. At the next
    time step *t+1*, the agent receives a reward ![](img/B15439_22_003.png) and transitions
    to state *S*[t][+1]. Over time, the MDP gives rise to a trajectory *S*[0], *A*[0],
    *R*[1], *S*[1], *A*[1], *R*[1], … that continues until the agent reaches a terminal
    state and the episode ends.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MDP的进行方式如下：在每一步*t*，代理观察环境的状态![](img/B15439_22_001.png)并选择一个动作![](img/B15439_22_002.png)，其中*S*和*A*分别是状态和动作的集合。在下一个时间步*t+1*，代理接收奖励![](img/B15439_22_003.png)并转移到状态*S*[t][+1]。随着时间的推移，MDP产生了一个轨迹*S*[0]，*A*[0]，*R*[1]，*S*[1]，*A*[1]，*R*[1]，…，直到代理达到终端状态并结束情节。
- en: Finite MDPs with a limited number of actions *A*, states *S*, and rewards *R*
    include well-defined discrete probability distributions over these elements. Due
    to the Markov property, these distributions only depend on the previous state
    and action.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 具有有限数量的动作*A*、状态*S*和奖励*R*的有限MDP包括这些元素的明确定的离散概率分布。由于马尔可夫性质，这些分布仅取决于先前的状态和动作。
- en: 'The probabilistic nature of trajectories implies that the agent maximizes the
    expected sum of future rewards. Furthermore, rewards are typically discounted
    using a factor ![](img/B15439_22_004.png) to reflect their time value. In the
    case of tasks that are not episodic but continue indefinitely, a discount factor
    strictly less than 1 is necessary to avoid infinite rewards and ensure convergence.
    Therefore, the agent maximizes the discounted, expected sum of future returns
    *R*[t], denoted as *G*[t]:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹的概率性意味着代理最大化未来奖励的期望总和。此外，奖励通常使用因子![](img/B15439_22_004.png)进行折现，以反映它们的时间价值。对于不是周期性但持续无限的任务，需要一个严格小于1的折现因子，以避免无限奖励并确保收敛。因此，代理最大化折现的未来回报的期望总和*R*[t]，表示为*G*[t]：
- en: '![](img/B15439_22_005.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_005.png)'
- en: 'This relationship can also be defined recursively because the sum starting
    at the second step is the same as *G*[t][+1] discounted once:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种关系也可以被递归地定义，因为从第二步开始的总和与*G*[t][+1]折现一次相同：
- en: '![](img/B15439_22_006.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_006.png)'
- en: We will see later that this type of recursive relationship is frequently used
    to formulate RL algorithms.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面看到，这种类型的递归关系经常用于制定RL算法。
- en: Value functions – how to estimate the long-run reward
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 价值函数-如何估计长期回报
- en: As introduced previously, a policy ![](img/B15439_22_009.png) maps all states
    to probability distributions over actions so that the probability of choosing
    action *A*[t] in state *S*[t] can be expressed as ![](img/B15439_22_007.png).
    The value function estimates the long-run return for each state or state-action
    pair. It is fundamental to find the policy that is the optimal mapping of states
    to actions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，策略![](img/B15439_22_009.png)将所有状态映射到动作的概率分布，以便在状态*S*[t]中选择动作*A*[t]的概率可以表示为![](img/B15439_22_007.png)。价值函数估计每个状态或状态-动作对的长期回报。找到将状态映射到动作的最佳策略是基本的。
- en: 'The state-value function ![](img/B15439_22_008.png) for policy ![](img/B15439_22_009.png)
    gives the long-term value *v* of a specific state *s* as the expected return *G*
    for an agent that starts in *s* and then always follows policy ![](img/B15439_22_009.png).
    It is defined as follows, where ![](img/B15439_22_011.png) refers to the expected
    value when the agent follows policy ![](img/B15439_22_009.png):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 策略![](img/B15439_22_009.png)的状态值函数![](img/B15439_22_008.png)给出了特定状态*s*的长期价值*v*，作为代理在*s*开始然后始终遵循策略![](img/B15439_22_009.png)的预期回报*G*。它的定义如下，其中![](img/B15439_22_011.png)是指代理遵循策略![](img/B15439_22_009.png)时的期望值：
- en: '![](img/B15439_22_013.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_013.png)'
- en: 'Similarly, we can compute the **state-action value function** *q*(*s*,*a*)
    as the expected return of starting in state *s*, taking action, and then always
    following the policy ![](img/B15439_22_009.png):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以计算**状态-动作值函数** *q*(*s*,*a*)，作为从状态*s*开始，采取行动，然后始终遵循策略![](img/B15439_22_009.png)的预期回报：
- en: '![](img/B15439_22_014.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_014.png)'
- en: The Bellman equations
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: 'The Bellman equations define a recursive relationship between the value functions
    for all states *s* in *S* and any of their successor states *s′* under a policy
    ![](img/B15439_22_009.png). They do so by decomposing the value function into
    the immediate reward and the discounted value of the next state:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程为*S*中所有状态的值函数和它们在策略![](img/B15439_22_009.png)下的任何后继状态*s′*之间定义了递归关系。它通过将值函数分解为即时奖励和下一个状态的折现值来实现这一点：
- en: '![](img/B15439_22_016.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_016.png)'
- en: This equation says that for a given policy, the value of a state must equal
    the expected value of its successor states under the policy, plus the expected
    reward earned from arriving at that successor state.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表明，对于给定的策略，状态的值必须等于在该策略下其后继状态的期望值，加上到达该后继状态所获得的期望奖励。
- en: This implies that, if we know the values of the successor states for the currently
    available actions, we can look ahead one step and compute the expected value of
    the current state. Since it holds for all states *S*, the expression defines a
    set of ![](img/B15439_22_017.png) equations. An analogous relationship holds for
    ![](img/B15439_22_018.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们知道当前可用动作的后继状态的值，我们可以向前看一步并计算当前状态的期望值。由于它适用于所有状态*S*，该表达式定义了一组![](img/B15439_22_017.png)方程。类似的关系也适用于![](img/B15439_22_018.png)。
- en: '*Figure 22.2* summarizes this recursive relationship: in the current state,
    the agent selects an action *a* based on the policy ![](img/B15439_22_009.png).
    The environment responds by assigning a reward that depends on the resulting new
    state *s′*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*图22.2*总结了这种递归关系：在当前状态下，代理根据策略![](img/B15439_22_009.png)选择一个动作*a*。环境通过分配奖励来做出回应，这取决于产生的新状态*s′*：'
- en: '![](img/B15439_22_02.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_02.png)'
- en: 'Figure 22.2: The recursive relationship expressed by the Bellman equation'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.2：贝尔曼方程表达的递归关系
- en: From a value function to an optimal policy
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从价值函数到最优策略
- en: 'The solution to an RL problem is a policy that optimizes the cumulative reward.
    Policies and value functions are closely connected: an optimal policy yields a
    value estimate for each state ![](img/B15439_22_020.png) or state-action pair
    ![](img/B15439_22_021.png) that is at least as high as for any other policy since
    the value is the cumulative reward under the given policy. Hence, the optimal
    value functions ![](img/B15439_22_022.png) and ![](img/B15439_22_023.png) implicitly
    define optimal policies and solve the MDP.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RL问题的解决方案是优化累积奖励的策略。策略和值函数密切相关：最优策略为每个状态![](img/B15439_22_020.png)或状态-动作对![](img/B15439_22_021.png)提供了值估计，该值至少与任何其他策略的值一样高，因为该值是在给定策略下的累积奖励。因此，最优值函数![](img/B15439_22_022.png)和![](img/B15439_22_023.png)隐含地定义了最优策略并解决了MDP问题。
- en: 'The optimal value functions ![](img/B15439_22_024.png) and ![](img/B15439_22_025.png)
    also satisfy the Bellman equations from the previous section. These Bellman optimality
    equations can omit the explicit reference to a policy as it is implied by ![](img/B15439_22_024.png)
    and ![](img/B15439_22_025.png). For ![](img/B15439_22_028.png), the recursive
    relationship equates the current value to the sum of the immediate reward from
    choosing the best action in the current state, as well as the expected discounted
    value of the successor states:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最优值函数![](img/B15439_22_024.png)和![](img/B15439_22_025.png)也满足前一节中的贝尔曼方程。这些贝尔曼最优方程可以省略对策略的显式引用，因为它由![](img/B15439_22_024.png)和![](img/B15439_22_025.png)隐含表示。对于![](img/B15439_22_028.png)，递归关系将当前值等同于选择当前状态中最佳动作的即时奖励的和后继状态的折现值的期望：
- en: '![](img/B15439_22_029.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_029.png)'
- en: 'For the optimal state-action value function ![](img/B15439_22_030.png), the
    Bellman optimality equation decomposes the current state-action value into the
    sum of the reward for the implied current action and the discounted expected value
    of the best action in all successor states:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最优状态-动作值函数![](img/B15439_22_030.png)，贝尔曼最优方程将当前状态-动作值分解为暗示的当前动作的奖励和所有后继状态中最佳动作的折现预期值的和：
- en: '![](img/B15439_22_031.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_031.png)'
- en: The optimality conditions imply that the best policy is to always select the
    action that maximizes the expected value in a greedy fashion, that is, to only
    consider the result of a single time step.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最优条件意味着最佳策略是始终以贪婪方式选择最大化期望值的动作，即仅考虑单个时间步的结果。
- en: The optimality conditions defined by the two previous expressions are nonlinear
    due to the max operator and lack a closed-form solution. Instead, MDP solutions
    rely on an iterative solution - like policy and value iteration or Q-learning,
    which we will cover next.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由前两个表达式定义的最优条件是非线性的，因为存在max运算符，并且缺乏封闭形式的解。相反，MDP的解依赖于迭代解决方案 - 如策略和值迭代或Q学习，我们将在下一节中介绍。
- en: Policy iteration
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略迭代
- en: DP is a general method for solving problems that can be decomposed into smaller,
    overlapping subproblems with a recursive structure that permit the reuse of intermediate
    results. MDPs fit the bill due to the recursive Bellman optimality equations and
    the cumulative nature of the value function. More specifically, the **principle
    of optimality** applies because an optimal policy consists of picking an optimal
    action and then following an optimal policy.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DP是解决可以分解为具有递归结构的较小、重叠子问题的一般方法，这些子问题允许重复使用中间结果。由于递归贝尔曼最优方程和值函数的累积性质，MDP符合这一原则。更具体地说，**最优原则**适用于最优策略，因为最优策略包括选择最优动作，然后遵循最优策略。
- en: DP requires knowledge of the MDP's transition probabilities. This is often not
    the case, but many methods for more general cases follow an approach similar to
    DP and learn the missing information from the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DP需要了解MDP的转移概率。这通常不是情况，但许多更一般情况下的方法遵循类似DP的方法，并从数据中学习缺失的信息。
- en: DP is useful for **prediction tasks** that estimate the value function and the
    control task that focuses on optimal decisions and outputs a policy (while also
    estimating a value function in the process).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: DP对于估计值函数的**预测任务**和专注于最优决策并输出策略的控制任务非常有用（同时也在过程中估计值函数）。
- en: 'The policy iteration algorithm to find an optimal policy repeats the following
    two steps until the policy has converged, that is, no longer changes more than
    a given threshold:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最优策略的策略迭代算法重复以下两个步骤，直到策略收敛，即不再变化超过给定的阈值：
- en: '**Policy evaluation**: Update the value function based on the current policy.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略评估**：根据当前策略更新值函数。'
- en: '**Policy improvement**: Update the policy so that actions maximize the expected
    one-step value.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略改进**：更新策略，使动作最大化预期的一步价值。'
- en: Policy evaluation relies on the Bellman equation to estimate the value function.
    More specifically, it selects the action determined by the current policy and
    sums the resulting reward, as well as the discounted value of the next state,
    to update the value for the current state.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 策略评估依赖贝尔曼方程来估计值函数。更具体地说，它选择由当前策略确定的动作，并对当前状态的奖励以及下一个状态的折现值进行求和，以更新当前状态的值。
- en: Policy improvement, in turn, alters the policy so that for each state, the policy
    produces the action that produces the highest value in the next state. This improvement
    is called greedy because it only considers the return of a single time step. Policy
    iteration always converges to an optimal policy and often does so in relatively
    few iterations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 策略改进则改变策略，使得对于每个状态，策略产生下一个状态中产生最高价值的动作。这种改进被称为贪婪，因为它只考虑单个时间步的回报。策略迭代总是收敛到最优策略，并且通常在相对较少的迭代中实现收敛。
- en: Value iteration
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值迭代
- en: Policy iteration requires the evaluation of the policy for all states after
    each iteration. The evaluation can be costly, as discussed previously, for search-tree-based
    policies, for example.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代要求在每次迭代后评估所有状态的策略。评估可能很昂贵，例如对于基于搜索树的策略。
- en: '**Value iteration** simplifies this process by collapsing the policy evaluation
    and improvement step. At each time step, it iterates over all states and selects
    the best greedy action based on the current value estimate for the next state.
    Then, it uses the one-step lookahead implied by the Bellman optimality equation
    to update the value function for the current state.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**值迭代**通过合并策略评估和改进步骤简化了这个过程。在每个时间步，它遍历所有状态并基于下一个状态的当前值估计选择最佳的贪婪动作。然后，它使用贝尔曼最优方程隐含的一步展望来更新当前状态的值函数。'
- en: 'The corresponding update rule for the value function ![](img/B15439_22_032.png)
    is almost identical to the policy evaluation update; it just adds the maximization
    over the available actions:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数的相应更新规则![](img/B15439_22_032.png)几乎与策略评估更新相同；它只是添加了对可用动作的最大化：
- en: '![](img/B15439_22_033.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_033.png)'
- en: The algorithm stops when the value function has converged and outputs the greedy
    policy derived from its value function estimate. It is also guaranteed to converge
    to an optimal policy.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当值函数收敛并输出从其值函数估计派生的贪婪策略时，算法停止。它也保证收敛到最优策略。
- en: Generalized policy iteration
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义策略迭代
- en: In practice, there are several ways to truncate policy iteration; for example,
    by evaluating the policy *k* times before improving it. This just means that the
    *max* operator will only be applied at every *k*^(th) iteration.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，有几种截断策略迭代的方法；例如，在改进之前评估策略*k*次。这意味着*max*运算符仅在每次*k*^(th)迭代时应用。
- en: 'Most RL algorithms estimate value and policy functions and rely on the interaction
    of policy evaluation and improvement to converge to a solution, as illustrated
    in *Figure 22.3*. The general approach improves the policy with respect to the
    value function while adjusting the value function so that it matches the policy:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数RL算法估计值和策略函数，并依赖策略评估和改进的交互来收敛到解决方案，如*图22.3*所示。一般方法是根据值函数改进策略，同时调整值函数以使其与策略匹配：
- en: '![](img/B15439_22_03.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_03.png)'
- en: 'Figure 22.3: Convergence of policy evaluation and improvement'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.3：策略评估和改进的收敛
- en: Convergence requires that the value function be consistent with the policy,
    which, in turn, needs to stabilize while acting greedily with respect to the value
    function. Thus, both processes stabilize only when a policy has been found that
    is greedy with respect to its own evaluation function. This implies that the Bellman
    optimality equation holds, and thus that the policy and the value function are
    optimal.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛要求值函数与策略一致，反过来，策略需要在贪婪地行动时稳定。因此，只有找到了相对于自己的评估函数贪婪的策略时，这两个过程才会稳定。这意味着贝尔曼最优方程成立，因此策略和值函数是最优的。
- en: Dynamic programming in Python
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python中的动态规划
- en: 'In this section, we''ll apply value and policy iteration to a toy environment
    that consists of a ![](img/B15439_22_034.png) grid, as depicted in *Figure 22.4*,
    with the following features:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用值迭代和策略迭代到一个玩具环境，它由一个![](img/B15439_22_034.png)网格组成，如*图22.4*所示，具有以下特点：
- en: '**States**: 11 states represented as two-dimensional coordinates. One field
    is not accessible and the top two states in the right-most column are terminal,
    that is, they end the episode.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：11个状态表示为二维坐标。一个字段是不可访问的，右侧列的顶部两个状态是终端状态，即它们结束了该情节。'
- en: '**Actions**: Movements of one step up, down, left, or right. The environment
    is randomized so that actions can have unintended outcomes. For each action, there
    is an 80 percent probability of moving to the expected state, and 10 percent each
    of moving in an adjacent direction (for example, right or left instead of up,
    or up/down instead of right).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：向上、向下、向左或向右移动一步。环境是随机的，因此动作可能会产生意外的结果。对于每个动作，有80%的概率移动到预期状态，以及10%的概率移动到相邻方向（例如，向右或向左而不是向上，或向上/向下而不是向右）。'
- en: '**Rewards**: As depicted in the left panel, each state results in -.02 except
    the +1/-1 rewards in the terminal states.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：如左侧面板所示，每个状态的结果为-0.02，除了终端状态的+1/-1奖励。'
- en: '![](img/B15439_22_04.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_04.png)'
- en: 'Figure 22.4: 3×4 gridworld rewards, value function, and optimal policy'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.4：3×4网格世界奖励、值函数和最优策略
- en: Setting up the gridworld
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置网格世界
- en: 'We will begin by defining the environment parameters:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从定义环境参数开始：
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will frequently need to convert between 1D and 2D representations, so we
    will define two helper functions for this purpose; states are one-dimensional,
    and cells are the corresponding 2D coordinates:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要在1D和2D表示之间转换，因此我们将为此定义两个辅助函数；状态是一维的，单元格是相应的二维坐标：
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Furthermore, we will precompute some data points to make the code more concise:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将预先计算一些数据点，以使代码更简洁：
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We store the rewards for each state:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个状态存储奖励。
- en: '[PRE3]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To account for the probabilistic environment, we also need to compute the probability
    distribution over the actual move for a given action:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑概率环境，我们还需要计算给定动作的实际移动的概率分布：
- en: '[PRE4]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we are ready to compute the transition matrix, which is the key input to
    the MDP.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备计算转移矩阵，这是MDP的关键输入。
- en: Computing the transition matrix
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算转移矩阵
- en: The **transition matrix** defines the probability of ending up in a certain
    state *S* for each previous state and action *A* ![](img/B15439_22_035.png). We
    will demonstrate `pymdptoolbox` and use one of the formats available to specify
    transitions and rewards. For both transition probabilities, we will create a NumPy
    array with dimensions ![](img/B15439_22_036.png).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**转移矩阵**定义了每个先前状态和动作*A*结束在某个状态*S*的概率！[](img/B15439_22_035.png)。我们将演示`pymdptoolbox`并使用其中一种可用的格式来指定转移和奖励。对于转移概率，我们将创建一个具有维度![](img/B15439_22_036.png)的NumPy数组。'
- en: 'We first compute the target cell for each starting cell and move:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算每个起始单元格和移动的目标单元格：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following function uses the arguments starting `state`, `action`, and `outcome`
    to fill in the transition probabilities and rewards:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数使用起始`state`、`action`和`outcome`的参数来填充转移概率和奖励：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We generate the transition and reward values by creating placeholder data structures
    and iterating over the Cartesian product of ![](img/B15439_22_037.png), as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过创建占位数据结构并迭代![](img/B15439_22_037.png)的笛卡尔积来生成转移和奖励值，如下所示：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Implementing the value iteration algorithm
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施值迭代算法
- en: 'We first create the value iteration algorithm, which is slightly simpler because
    it implements policy evaluation and improvement in a single step. We capture the
    states for which we need to update the value function, excluding terminal states
    that have a value of 0 for lack of rewards (+1/-1 are assigned to the starting
    state), and skip the blocked cell:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建值迭代算法，它稍微简单一些，因为它在单个步骤中实现了政策评估和改进。我们捕获需要更新值函数的状态，排除了值为0的终端状态（+1/-1分配给起始状态），并跳过阻塞的单元格：
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we initialize the value function and set the discount factor gamma and
    the convergence threshold `epsilon`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化值函数，并设置折扣因子gamma和收敛阈值`epsilon`：
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The algorithm updates the value function using the Bellman optimality equation,
    as described previously, and terminates when the L1 norm of *V* changes to less
    than epsilon in absolute terms:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用贝尔曼最优方程更新值函数，如前所述，并在*V*的L1范数绝对变化小于epsilon时终止：
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The algorithm converges in 16 iterations and 0.0117s. It produces the following
    optimal value estimate, which, together with the implied optimal policy, is depicted
    in the right panel of *Figure 22.4*, earlier in this section:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在16次迭代和0.0117秒内收敛。它产生了以下最优值估计，连同隐含的最优政策，如本节前面*图22.4*中的右侧面板所示。
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Defining and running policy iteration
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义和运行政策迭代
- en: 'Policy iterations involve separate evaluation and improvement steps. We define
    the improvement part by selecting the action that maximizes the sum of the expected
    reward and next-state value. Note that we temporarily fill in the rewards for
    the terminal states to avoid ignoring actions that would lead us there:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代包括独立的评估和改进步骤。我们通过选择最大化预期奖励和下一个状态值之和的动作来定义改进部分。请注意，我们暂时填写终端状态的奖励，以避免忽略会导致我们到达那里的行动。
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We initialize the value function as before and also include a random starting
    policy:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样初始化值函数，并且还包括一个随机的起始策略：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The algorithm alternates between policy evaluation for a greedily selected
    action and policy improvement until the policy stabilizes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在贪婪选择的动作进行政策评估和政策改进之间交替，直到政策稳定：
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Policy iteration converges after only three iterations. The policy stabilizes
    before the algorithm finds the optimal value function, and the optimal policy
    differs slightly, most notably by suggesting "up" instead of the safer "left"
    for the field next to the negative terminal state. This can be avoided by tightening
    the convergence criteria, for example, by requiring a stable policy of several
    rounds or by adding a threshold for the value function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代仅经过三次迭代就收敛了。算法在找到最优值函数之前就稳定了，最优政策略略有不同，最显著的是在负终端状态旁边的区域，建议使用“上”而不是更安全的“左”。可以通过加强收敛标准来避免这种情况，例如，要求进行几轮稳定的政策或者为值函数添加阈值。
- en: Solving MDPs using pymdptoolbox
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用pymdptoolbox解决MDPs
- en: We can also solve MDPs using the Python library `pymdptoolbox`, which includes
    a few other algorithms, including Q-learning.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Python库`pymdptoolbox`来解决MDPs，其中包括一些其他算法，包括Q-learning。
- en: 'To run value iteration, just instantiate the corresponding object with the
    desired configuration options, rewards, and transition matrices before calling
    the `.run()` method:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行值迭代，只需使用所需的配置选项、奖励和转移矩阵实例化相应的对象，然后调用`.run()`方法：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The value function estimate matches the result in the previous section:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数估计与前一节的结果相匹配：
- en: '[PRE16]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Policy iteration works similarly:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代的工作方式类似：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It also yields the same policy, but the value function varies by run and does
    not need to achieve the optimal value before the policy converges.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 它也产生相同的政策，但值函数会因运行而异，并且在政策收敛之前不需要达到最优值。
- en: Lessons learned
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所学到的经验
- en: The right panel we saw earlier in *Figure 22.4* shows the optimal value estimate
    produced by value iteration and the corresponding greedy policy. The negative
    rewards, combined with the uncertainty in the environment, produce an optimal
    policy that involves moving away from the negative terminal state.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在*图22.4*中看到的右侧面板显示了值迭代产生的最优值估计和相应的贪婪政策。负奖励和环境的不确定性产生了一个最优政策，涉及远离负终端状态的移动。
- en: The results are sensitive to both the rewards and the discount factor. The cost
    of the negative state affects the policy in the surrounding fields, and you should
    modify the example in the corresponding notebook to identify threshold levels
    that alter the optimal action selection.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 结果对奖励和折扣因子都很敏感。负状态的成本影响周围区域的政策，并且您应该修改相应笔记本中的示例，以确定改变最优行动选择的阈值水平。
- en: Q-learning – finding an optimal policy on the go
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning - 边学习边找到最优政策
- en: Q-learning was an early RL breakthrough when developed by Chris Watkins for
    his PhD thesis ([http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf))
    (1989). It introduces incremental dynamic programming to learn to control an MDP
    without knowing or modeling the transition and reward matrices that we used for
    value and policy iteration in the previous section. A convergence proof followed
    3 years later (Christopher J. C. H. Watkins and Dayan 1992).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning是早期的强化学习突破，由克里斯·沃特金斯在他的博士论文中开发（[http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)）（1989年）。它引入了增量动态规划，以学习控制MDP，而不需要知道或建模我们在前一节中用于值和政策迭代的转移和奖励矩阵。3年后出现了收敛证明（Christopher
    J. C. H. Watkins和Dayan 1992）。
- en: Q-learning directly optimizes the action-value function *q* to approximate *q**.
    The learning proceeds "off-policy," that is, the algorithm does not need to select
    actions based on the policy implied by the value function alone. However, convergence
    requires that all state-action pairs continue to be updated throughout the training
    process. A straightforward way to ensure this is through an ![](img/B15439_22_038.png)-greedy
    policy.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning直接优化动作值函数*q*以逼近*q*。学习是“离策略”的，也就是说，该算法不需要仅基于值函数暗示的策略选择动作。然而，收敛要求在整个训练过程中继续更新所有状态-动作对。确保这一点的一种简单方法是通过![](img/B15439_22_038.png)-贪婪策略。
- en: Exploration versus exploitation – ![](img/B15439_22_039.png)-greedy policy
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索与利用-![](img/B15439_22_039.png)-贪婪策略
- en: An ![](img/B15439_22_0391.png)**-greedy policy** is a simple policy that ensures
    the exploration of new actions in a given state while also exploiting the learning
    experience . It does this by randomizing the selection of actions. An ![](img/B15439_22_038.png)-greedy
    policy selects an action randomly with a probability of ![](img/B15439_22_038.png),
    and the best action according to the value function otherwise.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一个![](img/B15439_22_0391.png)**-贪婪策略**是一个简单的策略，它确保在给定状态下探索新的动作，同时利用学习经验。它通过随机选择动作来实现这一点。一个![](img/B15439_22_038.png)-贪婪策略以概率![](img/B15439_22_038.png)随机选择一个动作，否则根据值函数选择最佳动作。
- en: The Q-learning algorithm
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q-learning算法
- en: 'The algorithm keeps improving a state-action value function after random initialization
    for a given number of episodes. At each time step, it chooses an action based
    on an ![](img/B15439_22_042.png)-greedy policy, and uses a learning rate ![](img/B15439_22_043.png)
    to update the value function, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在随机初始化状态-动作值函数后不断改进一定数量的情节。在每个时间步，它根据![](img/B15439_22_042.png)-贪婪策略选择一个动作，并使用学习率![](img/B15439_22_043.png)来更新值函数，如下所示：
- en: '![](img/B15439_22_044.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_044.png)'
- en: Note that the algorithm does not compute expected values based on the transition
    probabilities. Instead, it learns the *Q* function from the rewards *R*[t] produced
    by the ![](img/B15439_22_045.png)-greedy policy and its current estimate of the
    discounted value function for the next state.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该算法不会基于转移概率计算期望值。相反，它从由![](img/B15439_22_045.png)-贪婪策略产生的奖励*R*[t]和下一个状态的折现值函数的当前估计中学习*Q*函数。
- en: The use of the estimated value function to improve this very estimate is called
    **bootstrapping**. The Q-learning algorithm is part of the **temporal difference**
    (**TD**) **learning** algorithms. TD learning does not wait until receiving the
    final reward for an episode. Instead, it updates its estimates using the values
    of intermediate states that are closer to the final reward. In this case, the
    intermediate state is one time step ahead.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用估计的值函数来改进这个估计本身被称为**自举**。Q-learning算法是**时间差**（**TD**）**学习**算法的一部分。TD学习不会等到收到情节的最终奖励。相反，它使用更接近最终奖励的中间状态的值来更新其估计。在这种情况下，中间状态是向前一步的时间步。
- en: How to train a Q-learning agent using Python
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用Python训练Q-learning代理
- en: 'In this section, we will demonstrate how to build a Q-learning agent using
    the ![](img/B15439_22_046.png) grid of states from the previous section. We will
    train the agent for 2,500 episodes, using a learning rate of ![](img/B15439_22_047.png)
    and ![](img/B15439_22_048.png) for the ![](img/B15439_22_042.png)-greedy policy
    (see the notebook `gridworld_q_learning.ipynb` for details):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用上一节中的状态网格构建一个Q-learning代理。我们将训练代理进行2,500个情节，使用学习率![](img/B15439_22_047.png)和![](img/B15439_22_048.png)进行![](img/B15439_22_042.png)-贪婪策略（有关详细信息，请参阅笔记本`gridworld_q_learning.ipynb`）：
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we will randomly initialize the state-action value function as a NumPy
    array with dimensions *number of states × number of actions*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将随机初始化状态-动作值函数，作为一个NumPy数组，其维度为*状态数×动作数*：
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The algorithm generates 2,500 episodes that start at a random location and
    proceed according to the ![](img/B15439_22_045.png)-greedy policy until termination,
    updating the value function according to the Q-learning rule:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法生成2,500个从随机位置开始并根据![](img/B15439_22_045.png)-贪婪策略进行的情节，直到终止，根据Q-learning规则更新值函数：
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The episodes take 0.6 seconds and converge to a value function fairly close
    to the result of the value iteration example from the previous section. The `pymdptoolbox`
    implementation works analogously to previous examples (see the notebook for details).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情节需要0.6秒，并收敛到一个与上一节值迭代示例结果相当接近的值函数。`pymdptoolbox`的实现与以前的示例类似（有关详细信息，请参阅笔记本）。
- en: Deep RL for trading with the OpenAI Gym
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于OpenAI Gym交易的深度RL
- en: In the previous section, we saw how Q-learning allows us to learn the optimal
    state-action value function *q** in an environment with discrete states and discrete
    actions using iterative updates based on the Bellman equation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了Q-learning如何允许我们使用基于Bellman方程的迭代更新，在具有离散状态和离散动作的环境中学习最优状态-动作值函数*q*。
- en: In this section, we will take RL one step closer to the real world and upgrade
    the algorithm to **continuous states** (while keeping actions discrete). This
    implies that we can no longer use a tabular solution that simply fills an array
    with state-action values. Instead, we will see how to **approximate q* using a
    neural network** (**NN**), which results in a deep Q-network. We will first discuss
    how deep learning integrates with RL before presenting the deep Q-learning algorithm,
    as well as various refinements that accelerate its convergence and make it more
    robust.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将RL推进一步，接近真实世界，并将算法升级为**连续状态**（同时保持动作离散）。这意味着我们不能再使用简单地填充一个数组的表格解决方案来**近似q**。相反，我们将看到如何使用神经网络**（NN）**来**近似q**，从而得到一个深度Q网络。在介绍深度Q-learning算法之前，我们将首先讨论深度学习如何与RL集成，以及各种加速其收敛并使其更加稳健的改进。
- en: Continuous states also imply a **more complex environment**. We will demonstrate
    how to work with OpenAI Gym, a toolkit for designing and comparing RL algorithms.
    First, we'll illustrate the workflow by training a deep Q-learning agent to navigate
    a toy spaceship in the Lunar Lander environment. Then, we'll proceed to **customize
    OpenAI Gym** to design an environment that simulates a trading context where an
    agent can buy and sell a stock while competing against the market.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 连续状态也意味着更复杂的环境。我们将演示如何使用OpenAI Gym，这是一个用于设计和比较RL算法的工具包。首先，我们将通过训练一个深度Q学习代理来演示工作流程，以在月球着陆器环境中导航一个玩具飞船。然后，我们将继续定制OpenAI
    Gym，设计一个模拟交易环境的环境，代理可以在与市场竞争的同时买卖股票。
- en: Value function approximation with neural networks
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络进行价值函数逼近
- en: Continuous state and/or action spaces imply an **infinite number of transitions**
    that make it impossible to tabulate the state-action values, as in the previous
    section. Rather, we approximate the Q function by learning a continuous, parameterized
    mapping from training samples.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 连续状态和/或动作空间意味着有无限数量的转换，这使得像在前一节中那样制表状态-动作值变得不可能。相反，我们通过学习连续的参数化映射来逼近Q函数。
- en: 'Motivated by the success of NNs in other domains, which we discussed in the
    previous chapters in *Part 4*, deep NNs have also become popular for approximating
    value functions. However, **machine learning in the RL context**, where the data
    is generated by the interaction of the model with the environment using a (possibly
    randomized) policy, **faces distinct challenges**:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 受到在其他领域中神经网络的成功的启发，我们在第4部分的前几章中讨论过，深度神经网络也成为逼近价值函数的一种流行方法。然而，在RL环境中进行机器学习，其中数据是通过模型与环境相互作用并使用（可能是随机的）策略生成的，面临着独特的挑战：
- en: With continuous states, the agent will fail to visit most states and thus needs
    to generalize.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于连续状态，代理将无法访问大多数状态，因此需要进行泛化。
- en: Whereas supervised learning aims to generalize from a sample of independently
    and identically distributed samples that are representative and correctly labeled,
    in the RL context, there is only one sample per time step, so learning needs to
    occur online.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而监督学习旨在从独立且标记正确的样本中进行泛化，RL环境中每个时间步只有一个样本，因此学习需要在线进行。
- en: Furthermore, samples can be highly correlated when sequential states are similar
    and the behavior distribution over states and actions is not stationary, but rather
    changes as a result of the agent's learning.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，当连续状态相似且行为分布在状态和动作上不是固定的，而是随着代理学习的结果而改变时，样本可能高度相关。
- en: We will look at several techniques that have been developed to address these
    additional challenges.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看一些已经开发出来应对这些额外挑战的技术。
- en: The Deep Q-learning algorithm and extensions
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度Q学习算法及其扩展
- en: Deep Q-learning estimates the value of the available actions for a given state
    using a deep neural network. DeepMind introduced this technique in *Playing Atari
    with Deep Reinforcement Learning* (Mnih et al. 2013), where agents learned to
    play games solely from pixel input.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习使用深度神经网络来估计给定状态的可用动作的价值。DeepMind在《使用深度强化学习玩Atari》（Mnih等人，2013年）中介绍了这种技术，代理从像素输入中学会玩游戏。
- en: The Deep Q-learning algorithm approximates the action-value function *q* by
    learning a set of weights ![](img/B15439_10_006.png) of a multilayered **deep
    Q-network** (**DQN**) that maps states to actions so that ![](img/B15439_22_052.png).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习算法通过学习多层深度Q网络（DQN）的一组权重![](img/B15439_10_006.png)来逼近动作值函数*q*，该网络将状态映射到动作，使得![](img/B15439_22_052.png)。
- en: 'The algorithm applies gradient descent based on a loss function that computes
    the squared difference between the DQN''s estimate of the target:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法应用基于损失函数的梯度下降，计算DQN对目标的估计之间的平方差异：
- en: '![](img/B15439_22_053.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_053.png)'
- en: 'and its estimate of the action-value of the current state-action pair ![](img/B15439_22_054.png)
    to learn the network parameters:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 及其对当前状态-动作对的动作值的估计![](img/B15439_22_054.png)来学习网络参数：
- en: '![](img/B15439_22_055.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_055.png)'
- en: Both **the target and the current estimate depend on the DQN weights**, underlining
    the distinction from supervised learning where targets are fixed prior to training.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 目标和当前估计都依赖于DQN的权重，突出了与监督学习的区别，监督学习中目标在训练之前是固定的。
- en: Rather than computing the full gradient, the Q-learning algorithm uses **stochastic
    gradient descent** (**SGD**) and updates the weights ![](img/B15439_22_056.png)
    after each time step *i*. To explore the state-action space, the agent uses an
    ![](img/B15439_22_057.png)-greedy policy that selects a random action with probability
    ![](img/B15439_22_067.png) and follows a greedy policy that selects the action
    with the highest predicted *q*-value otherwise.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习算法使用随机梯度下降（SGD）而不是计算完整的梯度，并在每个时间步*i*之后更新权重![](img/B15439_22_056.png)。为了探索状态-动作空间，代理使用一个![](img/B15439_22_057.png)-贪婪策略，以概率![](img/B15439_22_067.png)选择一个随机动作，并遵循一个贪婪策略，否则选择具有最高预测*q*值的动作。
- en: The basic **DQN architecture has been refined** in several directions to make
    the learning process more efficient and improve the final result; Hessel et al.
    (2017) combined these innovations in the **Rainbow agent** and demonstrated how
    each contributes to significantly higher performance across the Atari benchmarks.
    The following subsections summarize some of these innovations.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的DQN架构已经在几个方向上得到改进，以使学习过程更加高效并改善最终结果；Hessel等人（2017年）将这些创新结合在一起，形成了“Rainbow代理”，并展示了每个创新对Atari基准测试的显着性能提升。以下小节总结了其中一些创新。
- en: (Prioritized) Experience replay – focusing on past mistakes
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （优先）经验重放-专注于过去的错误
- en: Experience replay stores a history of the state, action, reward, and next state
    transitions experienced by the agent. It randomly samples mini-batches from this
    experience to update the network weights at each time step before the agent selects
    an *ε*-greedy action.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 经验重播存储了代理经历的状态、动作、奖励和下一个状态的历史转换。它从这些经验中随机抽样小批量，以在代理选择*ε*-贪婪动作之前每个时间步更新网络权重。
- en: Experience replay increases sample efficiency, reduces the autocorrelation of
    samples collected during online learning, and limits the feedback due to current
    weights producing training samples that can lead to local minima or divergence
    (Lin and Mitchell 1992).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 经验重播增加了样本效率，减少了在线学习过程中收集样本的自相关性，并限制了由于当前权重产生的反馈，从而产生可能导致局部最小值或发散的训练样本（Lin和Mitchell，1992年）。
- en: This technique was later refined to prioritize experience that is more important
    from a learning perspective. Schaul et al. (2015) approximated the value of a
    transition by the size of the TD error that captures how "surprising" the event
    was for the agent. In practice, it samples historical state transitions using
    their associated TD error rather than uniform probabilities.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术后来被进一步改进，以优先考虑从学习角度更重要的经验。Schaul等人（2015年）通过TD误差的大小来近似转换的价值，该误差捕捉了事件对代理的“惊讶程度”。在实践中，它使用与它们相关的TD误差而不是均匀概率来对历史状态转换进行采样。
- en: The target network – decorrelating the learning process
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标网络-解耦学习过程
- en: To further weaken the feedback loop from the current network parameters on the
    NN weight updates, the algorithm was extended by DeepMind in *Human-level control
    through deep reinforcement learning* (Mnih et al. 2015) to use a slowly-changing
    target network.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步削弱当前网络参数对NN权重更新的反馈循环，DeepMind在《通过深度强化学习实现人类水平控制》（Mnih等人，2015年）中扩展了该算法，使用了一个缓慢变化的目标网络。
- en: 'The target network has the same architecture as the Q-network, but its weights
    ![](img/B15439_22_058.png) are only updated periodically after ![](img/B15439_22_059.png)
    steps when they are copied from the Q-network and held constant otherwise. The
    target network **generates the TD target predictions**, that is, it takes the
    place of the Q-network to estimate:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络与Q网络具有相同的架构，但它的权重![](img/B15439_22_058.png)只在![](img/B15439_22_059.png)步后定期更新，当它们从Q网络复制并保持恒定。目标网络**生成TD目标预测**，即它取代Q网络来估计：
- en: '![](img/B15439_22_060.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_060.png)'
- en: Double deep Q-learning – decoupling action and prediction
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双重深度Q学习-解耦动作和预测
- en: Q-learning has been shown to overestimate the action values because it purposely
    samples maximal estimated action values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明Q学习会高估动作值，因为它有意地对最大估计动作值进行采样。
- en: This bias can negatively affect the learning process and the resulting policy
    if it does not apply uniformly and alters action preferences, as shown in *Deep
    Reinforcement Learning with Double Q-learning* (van Hasselt, Guez, and Silver
    2015).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这种偏见不均匀地应用并改变动作偏好，它可能会对学习过程和最终的策略产生负面影响，如《使用双Q学习的深度强化学习》（van Hasselt，Guez和Silver，2015年）所示。
- en: 'To decouple the estimation of action values from the selection of actions,
    the **Double DQN** (**DDQN**) algorithm uses the weights ![](img/B15439_22_061.png)
    of one network to select the best action given the next state, as well as the
    weights ![](img/B15439_22_062.png) of another network, to provide the corresponding
    action value estimate:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将动作值的估计与动作的选择分离开来，**双重DQN**（**DDQN**）算法使用一个网络的权重![](img/B15439_22_061.png)来选择给定下一个状态的最佳动作，以及另一个网络的权重![](img/B15439_22_062.png)来提供相应的动作值估计：
- en: '![](img/B15439_22_063.png).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B15439_22_063.png).'
- en: One option is to randomly select one of two identical networks for training
    at each iteration so that their weights will differ. A more efficient alternative
    is to rely on the target network to provide ![](img/B15439_22_064.png) instead.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是在每次迭代中随机选择两个相同的网络进行训练，以使它们的权重不同。一个更有效的替代方案是依赖目标网络提供![](img/B15439_22_064.png)。
- en: Introducing the OpenAI Gym
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍OpenAI Gym
- en: OpenAI Gym is an RL platform that provides standardized environments to test
    and benchmark RL algorithms using Python. It is also possible to extend the platform
    and register custom environments.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个强化学习平台，提供标准化的环境来测试和基准RL算法，使用Python。还可以扩展平台并注册自定义环境。
- en: 'The **Lunar Lander v2** (**LL**) environment requires the agent to control
    its motion in two dimensions based on a discrete action space and low-dimensional
    state observations that include position, orientation, and velocity. At each time
    step, the environment provides an observation of the new state and a positive
    or negative reward. Each episode consists of up to 1,000 time steps. *Figure 22.5*
    shows selected frames from a successful landing after 250 episodes by the agent
    we will train later:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**月球着陆器v2**（**LL**）环境要求代理根据离散动作空间和低维状态观察来控制其在二维空间中的运动，包括位置、方向和速度。每个时间步，环境提供新状态的观察和正面或负面的奖励。每个episode包括最多1,000个时间步。*图22.5*显示了我们将在后面训练的代理在250个episode后成功着陆的选定帧：'
- en: '![](img/B15439_22_05.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_05.png)'
- en: 'Figure 22.5: RL agent''s behavior during the Lunar Lander episode'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.5：月球着陆器episode中RL代理的行为
- en: More specifically, the **agent observes eight aspects of the state**, including
    six continuous and two discrete elements. Based on the observed elements, the
    agent knows its location, direction, and speed of movement and whether it has
    (partially) landed. However, it does not know in which direction it should move,
    nor can it observe the inner state of the environment to understand the rules
    that govern its motion.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，**代理观察到状态的八个方面**，包括六个连续和两个离散元素。根据观察到的元素，代理知道自己的位置、方向和移动速度，以及是否已经（部分）着陆。然而，它不知道应该朝哪个方向移动，也无法观察环境的内部状态以了解规定其运动的规则。
- en: At each time step, the agent controls its motion using one of **four discrete
    actions**. It can do nothing (and continue on its current path), fire its main
    engine (to reduce downward motion), or steer toward the left or right using the
    respective orientation engines. There are no fuel limitations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步长，代理使用**四个离散动作**来控制其运动。它可以什么都不做（并继续沿着当前路径前进），发动主引擎（以减少向下运动），或者使用相应的定位引擎向左或向右转向。没有燃料限制。
- en: The goal is to land the agent between two flags on a landing pad at coordinates
    (0, 0), but landing outside of the pad is possible. The agent accumulates rewards
    in the range of 100-140 for moving toward the pad, depending on the exact landing
    spot. However, a move away from the target negates the reward the agent would
    have gained by moving toward the pad. Ground contact by each leg adds 10 points,
    while using the main engine costs -0.3 points.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是在坐标（0, 0）的着陆垫上将代理降落在两个旗帜之间，但是在垫子外着陆也是可能的。代理在朝向垫子移动时积累的奖励在100-140的范围内，具体着陆点取决于奖励。然而，远离目标的移动会抵消代理朝向垫子移动时本来会获得的奖励。每条腿的接地都会增加10分，而使用主引擎会减少-0.3分。
- en: An episode terminates if the agent lands or crashes, adding or subtracting 100
    points, respectively, or after 1,000 time steps. Solving LL requires achieving
    a cumulative reward of at least 200 on average over 100 consecutive episodes.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理着陆或坠毁，将分别添加或减去100分，或者在1,000个时间步长之后。解决LL需要在100个连续情节中平均获得至少200的累积奖励。
- en: How to implement DDQN using TensorFlow 2
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用TensorFlow 2实现DDQN
- en: The notebook `03_lunar_lander_deep_q_learning` implements a DDQN agent using
    TensorFlow 2 that learns to solve OpenAI Gym's **Lunar Lander** 2.0 (**LL**) environment.
    The notebook `03_lunar_lander_deep_q_learning` contains a TensorFlow 1 implementation
    that was discussed in the first edition and runs significantly faster because
    it does not rely on eager execution and also converges sooner. This section highlights
    key elements of the implementation; please see the notebook for much more extensive
    details.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`03_lunar_lander_deep_q_learning`实现了一个使用TensorFlow 2的DDQN代理，该代理学习解决OpenAI
    Gym的**Lunar Lander** 2.0 (**LL**)环境。笔记本`03_lunar_lander_deep_q_learning`包含了第一版中讨论的TensorFlow
    1实现，运行速度显著更快，因为它不依赖于急切执行，而且收敛更快。本节重点介绍了实现的关键要素；更多详细信息请参阅笔记本。
- en: Creating the DDQN agent
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建DDQN代理
- en: We create our `DDQNAgent` as a Python class to integrate the learning and execution
    logic with the key configuration parameters and performance tracking.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`DDQNAgent`创建为Python类，以将学习和执行逻辑与关键配置参数和性能跟踪集成在一起。
- en: 'The agent''s `__init__()` method takes, as arguments, information on:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的`__init__()`方法接受有关以下信息的参数：
- en: The **environment characteristics**, like the number of dimensions for the state
    observations and the number of actions available to the agent.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境特征**，例如状态观察的维度数量和代理可用的动作数量。'
- en: The decay of the randomized exploration for the **ε-greedy policy**.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ε-greedy策略**的随机探索的衰减。'
- en: The **neural network architecture** and the parameters for **training** and
    target network updates.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络架构**和**训练**以及目标网络更新的参数。'
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Adapting the DDQN architecture to the Lunar Lander
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将DDQN架构调整到Lunar Lander
- en: The DDQN architecture was first applied to the Atari domain with high-dimensional
    image observations and relied on convolutional layers. The LL's lower-dimensional
    state representation makes fully connected layers a better choice (see *Chapter
    17*, *Deep Learning for Trading*).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN架构首先应用于具有高维图像观察的Atari领域，并依赖于卷积层。LL的低维状态表示使得全连接层成为更好的选择（参见*第17章*，*交易的深度学习*）。
- en: More specifically, the network maps eight inputs to four outputs, representing
    the Q values for each action, so that it only takes a single forward pass to compute
    the action values. The DQN is trained on the previous loss function using the
    Adam optimizer. The agent's DQN uses three densely connected layers with 256 units
    each and L2 activity regularization. Using a GPU via the TensorFlow Docker image
    can significantly speed up NN training performance (see *Chapter 17* and *Chapter
    18*, *CNNs for Financial Time Series and Satellite Images*).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，网络将八个输入映射到四个输出，表示每个动作的Q值，因此只需要进行一次前向传递即可计算动作值。DQN使用Adam优化器对先前的损失函数进行训练。代理的DQN使用了三个具有256个单元的密集连接层和L2活动正则化。通过TensorFlow
    Docker镜像使用GPU可以显著加快NN训练性能（参见*第17章*和*第18章*，*用于金融时间序列和卫星图像的CNN*）。
- en: The `DDQNAgent` class's `build_model()` method creates the primary online and
    slow-moving target networks based on the `architecture` parameter, which specifies
    the number of layers and their number of units.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`DDQNAgent`类的`build_model()`方法基于`architecture`参数创建主要的在线和缓慢移动的目标网络，该参数指定了层的数量和它们的单元数量。'
- en: 'We set `trainable` to `True` for the primary online network and to `False`
    for the target network. This is because we simply periodically copy the online
    NN weights to update the target network:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要的在线网络设置为`True`，将目标网络设置为`False`。这是因为我们只需定期复制在线NN权重以更新目标网络：
- en: '[PRE22]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Memorizing transitions and replaying the experience
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记忆转换和重播经验
- en: To enable experience replay, the agent memorizes each state transition so it
    can randomly sample a mini-batch during training. The `memorize_transition()`
    method receives the observation on the current and next state provided by the
    environment, as well as the agent's action, the reward, and a flag that indicates
    whether the episode is completed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用经验重播，代理会记住每个状态转换，以便在训练期间随机抽样一个小批量。`memorize_transition()`方法接收环境提供的当前和下一个状态的观察，以及代理的动作、奖励和指示该情节是否已完成的标志。
- en: 'It tracks the reward history and length of each episode, applies exponential
    decay to epsilon at the end of each period, and stores the state transition information
    in a buffer:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 它跟踪奖励历史和每个情节的长度，在每个周期结束时对epsilon进行指数衰减，并将状态转换信息存储在缓冲区中：
- en: '[PRE23]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The replay of the memorized experience begins as soon as there are enough samples
    to create a full batch. The `experience_replay()` method predicts the Q values
    for the next states using the online network and selects the best action. It then
    selects the predicted *q* values for these actions from the target network to
    arrive at the TD `targets`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦有足够的样本创建一个完整的批次，记忆体验的重放就开始了。`experience_replay()`方法使用在线网络预测下一个状态的Q值并选择最佳动作。然后它从目标网络中选择这些动作的预测*q*值以得到TD`targets`。
- en: 'Next, it trains the primary network using a single batch of current state observations
    as input, the TD targets as the outcome, and the mean-squared error as the loss
    function. Finally, it updates the target network weights every ![](img/B15439_22_065.png)
    steps:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它使用当前状态观察的单个批次作为输入来训练主网络，使用TD目标作为结果，并使用均方误差作为损失函数。最后，它每隔![](img/B15439_22_065.png)步更新一次目标网络权重：
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The notebook contains additional implementation details for the ε-greedy policy
    and the target network weight updates.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中包含了ε-greedy策略和目标网络权重更新的额外实现细节。
- en: Setting up the OpenAI environment
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置OpenAI环境
- en: 'We will begin by instantiating and extracting key parameters from the LL environment:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实例化并提取LL环境的关键参数：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will also use the built-in wrappers that permit the periodic storing of
    videos that display the agent''s performance:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用内置的包装器，允许定期存储显示代理表现的视频：
- en: '[PRE26]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When running on a server or Docker container without a display, you can use
    `pyvirtualdisplay`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有显示器的服务器或Docker容器上运行时，可以使用`pyvirtualdisplay`。
- en: Key hyperparameter choices
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键超参数选择
- en: 'The agent''s performance is quite sensitive to several hyperparameters. We
    will start with the discount and learning rates:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的表现对几个超参数非常敏感。我们将从折扣和学习率开始：
- en: '[PRE27]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will update the target network every 100 time steps, store up to 1 million
    past episodes in the replay memory, and sample mini-batches of 1,024 from memory
    to train the agent:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每100个时间步更新一次目标网络，将过去100万个episode存储在回放内存中，并从内存中抽样1,024个小批次来训练代理：
- en: '[PRE28]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The ε-greedy policy starts with pure exploration at ![](img/B15439_22_066.png),
    linear decay to 0.01 over 250 episodes, and exponential decay thereafter:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ε-greedy策略从![](img/B15439_22_066.png)开始进行纯探索，线性衰减到250个episode后为0.01，之后进行指数衰减：
- en: '[PRE29]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The notebook contains the training loop, including experience replay, SGD, and
    slow target network updates.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中包含了训练循环，包括经验重放、SGD和慢目标网络更新。
- en: Lunar Lander learning performance
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Lunar Lander学习表现
- en: The preceding hyperparameter settings enable the agent to solve the environment
    in around 300 episodes using the TensorFlow 1 implementation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的超参数设置使得代理能够在大约300个episode内使用TensorFlow 1实现解决环境。
- en: 'The left panel of *Figure 22.6* shows the episode rewards and their moving
    average over 100 periods. The right panel shows the decay of exploration and the
    number of steps per episode. There is a stretch of some 100 episodes that often
    take 1,000 time steps each while the agent reduces exploration and "learns how
    to fly" before starting to land fairly consistently:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*图22.6*的左侧面板显示了episode奖励及其100个周期的移动平均值。右侧面板显示了探索的衰减和每个episode的步数。在一段大约100个episode的时间内，代理经常需要每个时间步长1000步，同时减少探索并且在开始相当一致地着陆之前“学会飞行”：'
- en: '![](img/B15439_22_06.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_06.png)'
- en: 'Figure 22.6: The DDQN agent''s performance in the Lunar Lander environment'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.6：Lunar Lander环境中DDQN代理的表现
- en: Creating a simple trading agent
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个简单的交易代理
- en: In this and the following sections, we will adapt the deep RL approach to design
    an agent that learns how to trade a single asset. To train the agent, we will
    set up a simple environment with a limited set of actions, a relatively low-dimensional
    state with continuous observations, and other parameters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节和接下来的几节中，我们将调整深度RL方法，设计一个学习如何交易单一资产的代理。为了训练代理，我们将建立一个简单的环境，其中包含有限的动作集、一个相对低维的连续观察状态和其他参数。
- en: More specifically, the **environment** samples a stock price time series for
    a single ticker using a random start date to simulate a trading period that, by
    default, contains 252 days or 1 year. Each **state observation** provides the
    agent with the historical returns for various lags and some technical indicators,
    like the **relative strength index** (**RSI**).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，**环境**使用随机开始日期对单个股票的股价时间序列进行抽样，以模拟一个交易期，默认情况下包含252天或1年。每个**状态观察**为代理提供了各种滞后和一些技术指标的历史收益，比如**相对强弱指数**（**RSI**）。
- en: 'The agent can choose from three **actions**:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以从三个**动作**中进行选择：
- en: '**Buy**: Invest all capital for a long position in the stock.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**买入**：将所有资金投资于股票的多头头寸。'
- en: '**Flat**: Hold cash only.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持平**：只持有现金。'
- en: '**Sell short**: Take a short position equal to the amount of capital.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卖空**：建立一个等于资本金额的空头头寸。'
- en: The environment accounts for **trading cost**, set to 10 basis points by default,
    and deducts one basis point per period without trades. The **reward** of the agent
    consists of the daily return minus trading costs.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 环境考虑**交易成本**，默认设置为10个基点，并且每个没有交易的周期扣除一个基点。代理的**奖励**由每日收益减去交易成本组成。
- en: The environment tracks the **net asset value** (**NAV**) of the agent's portfolio
    (consisting of a single stock) and compares it against the market portfolio, which
    trades frictionless to raise the bar for the agent.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 环境跟踪代理投资组合（由单一股票组成）的**净资产价值**（**NAV**）并将其与市场投资组合进行比较，后者无摩擦地交易以提高代理的门槛。
- en: 'An episode begins with a starting NAV of 1 unit of cash:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一个episode以1单位现金的起始NAV开始：
- en: If the NAV drops to 0, the episode ends with a loss.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果NAV下降到0，episode以失败结束。
- en: If the NAV hits 2.0, the agent wins.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果NAV达到2.0，代理就赢了。
- en: This setting limits complexity as it focuses on a single stock and abstracts
    from position sizing to avoid the need for continuous actions or a larger number
    of discrete actions, as well as more sophisticated bookkeeping. However, it is
    useful to demonstrate how to customize an environment and permits for extensions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置限制了复杂性，因为它专注于单一股票，并且抽象出了头寸大小，避免了对连续行动或更大数量的离散行动的需求，以及更复杂的簿记。然而，这对于演示如何定制环境并允许扩展是有用的。
- en: How to design a custom OpenAI trading environment
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何设计一个自定义的OpenAI交易环境
- en: To build an agent that learns how to trade, we need to create a market environment
    that provides price and other information, offers relevant actions, and tracks
    the portfolio to reward the agent accordingly. For a description of the efforts
    to build a large-scale, real-world simulation environment, see Byrd, Hybinette,
    and Balch (2019).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个学习交易的代理，我们需要创建一个市场环境，提供价格和其他信息，提供相关的行动，并跟踪投资组合以相应地奖励代理。有关构建大规模、真实世界模拟环境的努力的描述，请参见Byrd、Hybinette和Balch（2019）。
- en: OpenAI Gym allows for the design, registration, and utilization of environments
    that adhere to its architecture, as described in the documentation. The file `trading_env.py`
    contains the following code examples, which illustrate the process unless noted
    otherwise.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym允许设计、注册和利用符合其架构的环境，如文档中所述。文件`trading_env.py`包含以下代码示例，演示了该过程，除非另有说明。
- en: The trading environment consists of three classes that interact to facilitate
    the agent's activities. The `DataSource` class loads a time series, generates
    a few features, and provides the latest observation to the agent at each time
    step. `TradingSimulator` tracks the positions, trades and cost, and the performance.
    It also implements and records the results of a buy-and-hold benchmark strategy.
    `TradingEnvironment` itself orchestrates the process. We will briefly describe
    each in turn; see the script for implementation details.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 交易环境由三个类组成，它们相互作用以促进代理的活动。`DataSource`类加载时间序列，生成一些特征，并在每个时间步向代理提供最新的观察。`TradingSimulator`跟踪头寸、交易和成本，以及绩效。它还实现并记录了买入持有基准策略的结果。`TradingEnvironment`本身编排了整个过程。我们将依次简要描述每个类；有关实现细节，请参见脚本。
- en: Designing a DataSource class
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设计一个DataSource类
- en: First, we code up a `DataSource` class to load and preprocess historical stock
    data to create the information used for state observations and rewards. In this
    example, we will keep it very simple and provide the agent with historical data
    on a single stock. Alternatively, you could combine many stocks into a single
    time series, for example, to train the agent on trading the S&P 500 constituents.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们编写一个`DataSource`类来加载和预处理历史股票数据，以创建用于状态观察和奖励的信息。在这个例子中，我们将保持非常简单，为代理提供单一股票的历史数据。或者，您可以将许多股票合并成一个时间序列，例如，训练代理交易标准普尔500成分股。
- en: 'We will load the adjusted price and volume information for one ticker from
    the Quandl dataset, in this case for AAPL with data from the early 1980s until
    2018:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载Quandl数据集中一个股票的调整价格和成交量信息，本例中为AAPL，数据从1980年代初到2018年：
- en: '[PRE30]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `preprocess_data()` method creates several features and normalizes them.
    The most recent daily returns play two roles:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocess_data()`方法创建了几个特征并对其进行了归一化。最近的每日收益扮演了两个角色：'
- en: An element of the observations for the current state
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态的观察元素
- en: The net of trading costs and, depending on the position size, the reward for
    the last period
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易成本的净额，以及根据头寸大小，上一期的奖励
- en: 'The method takes the following steps, among others (refer to the *Appendix*
    for details on the technical indicators):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法采取以下步骤，其中包括（有关技术指标的详细信息，请参阅*附录*）：
- en: '[PRE31]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `DataSource` class keeps track of episode progress, provides fresh data
    to `TradingEnvironment` at each time step, and signals the end of the episodes:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataSource`类跟踪剧集进展，为`TradingEnvironment`在每个时间步提供新鲜数据，并标志着剧集的结束：'
- en: '[PRE32]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The TradingSimulator class
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TradingSimulator类
- en: The trading simulator computes the agent's reward and tracks the net asset values
    of the agent and "the market," which executes a buy-and-hold strategy with reinvestment.
    It also tracks the positions and the market return, computes trading costs, and
    logs the results.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 交易模拟器计算代理的奖励，并跟踪代理和“市场”的净资产价值，后者执行买入持有策略并进行再投资。它还跟踪头寸和市场回报，计算交易成本，并记录结果。
- en: 'The most important method of this class is the `take_step` method, which computes
    the agent''s reward based on its current position, the latest stock return, and
    the trading costs (slightly simplified; see the script for full details):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 该类最重要的方法是`take_step`方法，它根据当前位置、最新的股票回报和交易成本计算代理的奖励（稍微简化；有关完整细节，请参见脚本）：
- en: '[PRE33]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The TradingEnvironment class
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TradingEnvironment类
- en: 'The `TradingEnvironment` class subclasses `gym.Env` and drives the environment
    dynamics. It instantiates the `DataSource` and `TradingSimulator` objects and
    sets the action and state-space dimensionality, with the latter depending on the
    ranges of the features defined by `DataSource`:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`TradingEnvironment`类是`gym.Env`的子类，驱动环境动态。它实例化`DataSource`和`TradingSimulator`对象，并设置动作和状态空间的维度，后者取决于`DataSource`定义的特征范围：'
- en: '[PRE34]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The two key methods of `TradingEnvironment` are `.reset()` and `.step()`. The
    former initializes the `DataSource` and `TradingSimulator` instances, as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`TradingEnvironment`的两个关键方法是`.reset()`和`.step()`。前者初始化`DataSource`和`TradingSimulator`实例，如下所示：'
- en: '[PRE35]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Each time step relies on `DataSource` and `TradingSimulator` to provide a state
    observation and reward the most recent action:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时间步依赖于`DataSource`和`TradingSimulator`，以提供状态观察和奖励最新的行动：
- en: '[PRE36]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Registering and parameterizing the custom environment
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册和参数化自定义环境
- en: 'Before using the custom environment, just as for the Lunar Lander environment,
    we need to register it with the `gym` package, provide information about the `entry_point`
    in terms of module and class, and define the maximum number of steps per episode
    (the following steps occur in the `q_learning_for_trading` notebook):'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用自定义环境之前，就像对于月球着陆器环境一样，我们需要在`gym`包中注册它，提供关于`entry_point`的模块和类的信息，并定义每个episode的最大步数（以下步骤发生在`q_learning_for_trading`笔记本中）：
- en: '[PRE37]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can instantiate the environment using the desired trading costs and ticker:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用所需的交易成本和股票来实例化环境：
- en: '[PRE38]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Deep Q-learning on the stock market
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 股票市场上的深度Q学习
- en: The notebook `q_learning_for_trading` contains the DDQN agent training code;
    we will only highlight noteworthy differences from the previous example.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`q_learning_for_trading`包含DDQN代理训练代码；我们只会突出显示与之前示例的显着差异。
- en: Adapting and training the DDQN agent
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整和训练DDQN代理
- en: 'We will use the same DDQN agent but simplify the NN architecture to two layers
    of 64 units each and add dropout for regularization. The online network has 5,059
    trainable parameters:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的DDQN代理，但简化NN架构为每层64个单元，并添加了用于正则化的dropout。在线网络有5,059个可训练参数：
- en: '[PRE39]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The training loop interacts with the custom environment in a manner very similar
    to the Lunar Lander case. While the episode is active, the agent takes the action
    recommended by its current policy and trains the online network using experience
    replay after memorizing the current transition. The following code highlights
    the key steps:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环与自定义环境的交互方式与月球着陆器的情况非常相似。在episode活动期间，代理根据其当前策略采取推荐的行动，并在记忆当前转换后使用经验重放来训练在线网络。以下代码突出了关键步骤：
- en: '[PRE40]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We let exploration continue for 2,000 1-year trading episodes, corresponding
    to about 500,000 time steps; we use linear decay of ε from 1.0 to 0.1 over 500
    periods with exponential decay at a factor of 0.995 thereafter.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们让探索继续进行2,000个1年的交易周期，相当于约500,000个时间步；我们在500个周期内使用线性衰减的ε从1.0到0.1，之后以0.995的指数衰减因子进行指数衰减。
- en: Benchmarking DDQN agent performance
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准测试DDQN代理的表现
- en: To compare the DDQN agent's performance, we not only track the buy-and-hold
    strategy but also generate the performance of a random agent.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较DDQN代理的表现，我们不仅跟踪买入持有策略，还生成了随机代理的表现。
- en: '*Figure 22.7* shows the rolling averages over the last 100 episodes of three
    cumulative return values for the 2,000 training periods (left panel), as well
    as the share of the last 100 episodes when the agent outperformed the buy-and-hold
    period (right panel). It uses AAPL stock data, for which there are some 9,000
    daily price and volume observations:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '*图22.7*显示了最后100个周期的滚动平均值，共2,000个训练周期的三个累积回报值（左侧面板），以及代理超过买入持有期的最后100个周期的份额（右侧面板）。它使用了AAPL股票数据，其中有大约9,000个每日价格和成交量观察值：'
- en: '![](img/B15439_22_07.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_22_07.png)'
- en: 'Figure 22.7: Trading agent performance relative to the market'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.7：交易代理相对于市场的表现
- en: This shows how the agent's performance improves steadily after 500 episodes,
    from the level of a random agent, and starts to outperform the buy-and-hold strategy
    toward the end of the experiment more than half of the time.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了代理的表现在500个周期后稳步提高，从随机代理的水平开始，并在实验结束时超过买入持有策略超过一半的时间。
- en: Lessons learned
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所学到的教训
- en: This relatively simple agent uses no information beyond the latest market data
    and the reward signal compared to the machine learning models we covered elsewhere
    in this book. Nonetheless, it learns to make a profit and achieve performance
    similar to that of the market (after training on 2,000 years' worth of data, which
    takes only a fraction of the time on a GPU).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相对简单的代理除了最新的市场数据和奖励信号之外，不使用其他信息，与本书其他地方介绍的机器学习模型相比。尽管如此，它学会了盈利，并实现了与市场类似的表现（在2000年的数据上进行训练，只需要GPU上的一小部分时间）。
- en: Keep in mind that using a single stock also increases the risk of overfitting
    to the data—by a lot. You can test your trained agent on new data using the saved
    model (see the notebook for Lunar Lander).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，只使用一支股票也会增加对数据过度拟合的风险—非常大。您可以使用保存的模型在新数据上测试训练好的代理（请参阅月球着陆器的笔记本）。
- en: In summary, we have demonstrated the mechanics of setting up an RL trading environment
    and experimented with a basic agent that uses a small number of technical indicators.
    You should try to extend both the environment and the agent, for example, to choose
    from several assets, size the positions, and manage risks.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们已经演示了建立RL交易环境的机制，并尝试了一个使用少量技术指标的基本代理。您应该尝试扩展环境和代理，例如选择多种资产、确定头寸大小和管理风险。
- en: Reinforcement learning is often considered the **most promising approach to
    algorithmic trading** because it most accurately models the task an investor is
    facing. However, our dramatically simplified examples illustrate that creating
    a realistic environment poses a considerable challenge. Moreover, deep reinforcement
    learning that has achieved impressive breakthroughs in other domains may face
    greater obstacles given the noisy nature of financial data, which makes it even
    harder to learn a value function based on delayed rewards.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习通常被认为是**算法交易最有前途的方法**，因为它最准确地模拟了投资者面临的任务。然而，我们大大简化的示例表明，创建一个真实的环境是一个相当大的挑战。此外，深度强化学习在其他领域取得了令人印象深刻的突破，但由于金融数据的嘈杂性，可能面临更大的障碍，这使得基于延迟奖励学习价值函数变得更加困难。
- en: Nonetheless, the substantial interest in this subject makes it likely that institutional
    investors are working on larger-scale experiments that may yield tangible results.
    An interesting complementary approach beyond the scope of this book is **Inverse
    Reinforcement Learning**, which aims to identify the reward function of an agent
    (for example, a human trader) given its observed behavior; see Arora and Doshi
    (2019) for a survey and Roa-Vicens et al. (2019) for an application on trading
    in the limit-order book context.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，对这一主题的重大兴趣使得机构投资者很可能正在进行规模更大的实验，这些实验可能会产生实质性的结果。这本书范围之外的一个有趣的补充方法是**逆强化学习**，它旨在确定代理人（例如人类交易员）的奖励函数，给定其观察到的行为；参见Arora和Doshi（2019）的调查以及Roa-Vicens等人（2019）在限价订单簿环境中的应用。
- en: Summary
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced a different class of machine learning problems
    that focus on automating decisions by agents that interact with an environment.
    We covered the key features required to define an RL problem and various solution
    methods.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一类不同的机器学习问题，重点是通过与环境互动的代理人自动化决策。我们涵盖了定义RL问题和各种解决方法所需的关键特征。
- en: We saw how to frame and analyze an RL problem as a finite Markov decision problem,
    as well as how to compute a solution using value and policy iteration. We then
    moved on to more realistic situations, where the transition probabilities and
    rewards are unknown to the agent, and saw how Q-learning builds on the key recursive
    relationship defined by the Bellman optimality equation in the MDP case. We saw
    how to solve RL problems using Python for simple MDPs and more complex environments
    with Q-learning.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何将RL问题框定和分析为有限马尔可夫决策问题，以及如何使用值和策略迭代来计算解决方案。然后，我们转向更现实的情况，其中转移概率和奖励对于代理人是未知的，并看到了Q学习如何建立在MDP情况下由贝尔曼最优方程定义的关键递归关系之上。我们看到了如何使用Python解决简单MDP和更复杂环境的RL问题。
- en: We then expanded our scope to continuous states and applied the Deep Q-learning
    algorithm to the more complex Lunar Lander environment. Finally, we designed a
    simple trading environment using the OpenAI Gym platform, and also demonstrated
    how to train an agent to learn how to make a profit while trading a single stock.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将我们的范围扩大到连续状态，并将深度Q学习算法应用于更复杂的月球着陆器环境。最后，我们使用OpenAI Gym平台设计了一个简单的交易环境，并演示了如何训练一个代理人学习如何在交易单一股票时获利。
- en: In the next and final chapter, we'll present a few conclusions and key takeaways
    from our journey through this book and lay out some steps for you to consider
    as you continue building your skills to use machine learning for trading.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的最后一章中，我们将从我们在本书中的旅程中得出一些结论和关键要点，并为您提出一些步骤，以便您在继续建立机器学习用于交易的技能时进行考虑。
