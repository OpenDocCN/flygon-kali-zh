- en: 5\. Handling common failures in AKS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 在AKS中处理常见故障
- en: Kubernetes is a distributed system with many working parts. AKS abstracts most
    of it for you, but it is still your responsibility to know where to look and how
    to respond when bad things happen. Much of the failure handling is done automatically
    by Kubernetes; however, you will encounter situations where manual intervention
    is required.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个具有许多工作部分的分布式系统。AKS为您抽象了大部分内容，但您仍有责任知道在发生不良情况时应该去哪里寻找以及如何做出响应。Kubernetes会自动处理大部分故障；然而，您会遇到需要手动干预的情况。
- en: There are two areas where things can go wrong in an application that is deployed
    on top of AKS. Either the cluster itself has issues, or the application deployed
    on top of the cluster has issues. This chapter focuses specifically on cluster
    issues. There are several things that can go wrong with a cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署在AKS之上的应用程序中，有两个可能出现问题的领域。要么是集群本身出现问题，要么是部署在集群之上的应用程序出现问题。本章专注于集群问题。集群可能出现多种问题。
- en: The first thing that can go wrong is a node in the cluster can become unavailable.
    This can happen either due to an Azure infrastructure outage or due to an issue
    with the virtual machine itself, such as an operating system crash. Either way,
    Kubernetes monitors the cluster for node failures and will recover automatically.
    You will see this process in action in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种可能出现的问题是集群中的节点可能变得不可用。这可能是由于Azure基础设施故障或虚拟机本身出现问题，例如操作系统崩溃。无论哪种情况，Kubernetes都会监视集群中的节点故障，并将自动恢复。您将在本章中看到这个过程。
- en: A second common issue in a Kubernetes cluster is out-of-resource failures. This
    means that the workload you are trying to deploy requires more resources than
    are available on your cluster. You will learn how to monitor these signals and
    how you can solve them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群中的第二个常见问题是资源不足的故障。这意味着您尝试部署的工作负载需要的资源超过了集群上可用的资源。您将学习如何监视这些信号以及如何解决这些问题。
- en: Another common issue is problems with mounting storage, which happens when a
    node becomes unavailable. When a node in Kubernetes becomes unavailable, Kubernetes
    will not detach the disks attached to this failed node. This means that those
    disks cannot be used by workloads on other nodes. You will see a practical example
    of this and learn how to recover from this failure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见问题是挂载存储出现问题，这发生在节点变得不可用时。当Kubernetes中的节点变得不可用时，Kubernetes不会分离附加到此失败节点的磁盘。这意味着这些磁盘不能被其他节点上的工作负载使用。您将看到一个实际的例子，并学习如何从这种故障中恢复。
- en: 'We will look into the following failure modes in depth in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究以下故障模式：
- en: Handling node failures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理节点故障
- en: Solving out-of-resource failures
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决资源不足的故障
- en: Handling storage mount issues
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理存储挂载问题
- en: In this chapter, you will learn about common failure scenarios, as well as solutions
    to those scenarios. To start, we will introduce node failures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您将了解常见的故障场景，以及针对这些场景的解决方案。首先，我们将介绍节点故障。
- en: 'Note:'
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意：
- en: Refer to *Kubernetes the Hard Way* ([https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)),
    an excellent tutorial, to get an idea about the blocks on which Kubernetes is
    built. For the Azure version, refer to *Kubernetes the Hard Way – Azure Translation*
    ([https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure](https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 参考*Kubernetes the Hard Way*（[https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)），一个优秀的教程，了解Kubernetes构建的基础。对于Azure版本，请参考*Kubernetes
    the Hard Way – Azure Translation*（[https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure](https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure)）。
- en: Handling node failures
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理节点故障
- en: 'Intentionally (to save costs) or unintentionally, nodes can go down. When that
    happens, you don''t want to get the proverbial 3 a.m. call that your system is
    down. Kubernetes can handle moving workloads on failed nodes automatically for
    you instead. In this exercise, we are going to deploy the guestbook application
    and are going to bring a node down in our cluster and see what Kubernetes does
    in response:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有意（为了节省成本）或无意中，节点可能会宕机。当这种情况发生时，您不希望在凌晨3点接到系统宕机的电话。Kubernetes可以自动处理节点故障时的工作负载迁移。在这个练习中，我们将部署guestbook应用程序，并将在我们的集群中关闭一个节点，看看Kubernetes的响应是什么：
- en: 'Ensure that your cluster has at least two nodes:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的集群至少有两个节点：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This should generate an output as shown in *Figure 5.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该生成一个如*图5.1*所示的输出：
- en: '![Executing the kubectl get nodes command displays an output withtwo nodes.
    The status of these two nodes is Ready.](image/Figure_5.1.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![执行kubectl get nodes命令会显示一个带有两个节点的输出。这两个节点的状态为Ready。](image/Figure_5.1.jpg)'
- en: 'Figure 5.1: Ensure you have two nodes running in your cluster'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.1：确保您的集群中有两个正在运行的节点
- en: 'If you don''t have two nodes in your cluster, look for your cluster in the
    Azure portal, navigate to **Node pools**, and click on **Node count**. You can
    scale this to **2** nodes as shown in *Figure 5.2*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群中没有两个节点，请在Azure门户中查找您的集群，导航到**节点池**，然后单击**节点计数**。您可以将其扩展到**2**个节点，如*图5.2*所示：
- en: '![Click on the Node pools tab in the Navigation pane located on the left side
    of the Azure portal. This will open show you several options. Go to the Node count
    option. Click on it to scale this count to two nodes.](image/Figure_5.2.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![单击Azure门户左侧的导航窗格中的节点池选项卡。这将显示给您几个选项。转到节点计数选项。单击它以将此计数扩展到两个节点。](image/Figure_5.2.jpg)'
- en: 'Figure 5.2: Scaling the cluster'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.2：扩展集群
- en: 'As an example application in this chapter, we will work with the guestbook
    application. The YAML file to deploy this has been provided in the source code
    for this chapter (`guestbook-all-in-one.yaml`). To deploy the guestbook application,
    use the following command:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为本章的示例应用程序，我们将使用guestbook应用程序。部署此应用程序的YAML文件已在本章的源代码中提供（`guestbook-all-in-one.yaml`）。要部署guestbook应用程序，请使用以下命令：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will give the service a public IP again, as we did in the previous chapters.
    To start the edit, execute the following command:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将再次为服务提供公共IP，就像在之前的章节中一样。要开始编辑，请执行以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will open a `vi` environment. Navigate to the line that now says `type:
    ClusterIP` (line 27) and change that to `type: LoadBalancer`. To make that change,
    hit the *I*button to enter insert mode, type your changes, then hit the *Esc*
    button, type `:wq!`, and hit *Enter* to save the changes.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '这将打开一个`vi`环境。导航到现在显示为`type: ClusterIP`（第27行）的行，并将其更改为`type: LoadBalancer`。要进行更改，请按*I*按钮进入插入模式，输入更改，然后按*Esc*按钮，输入`:wq!`，然后按*Enter*保存更改。'
- en: 'Once the changes are saved, you can watch the `service` object until the public
    IP becomes available. To do this, type the following:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改保存后，您可以观察`service`对象，直到公共IP变为可用。要做到这一点，请输入以下内容：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will take a couple of minutes to show you the updated IP. *Figure 5.3*
    represents the service's public IP. Once you see the right public IP, you can
    exit the watch command by hitting *Ctrl* + *C* (*command* + *C* on Mac):![The
    output displays frontend service changing its External-IP from <pending> to an
    actual IP.](image/Figure_5.3.jpg)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将花费几分钟的时间来显示更新后的IP。*图5.3*代表了服务的公共IP。一旦您看到正确的公共IP，您可以通过按*Ctrl* + *C*（Mac上为*command*
    + *C*）退出watch命令：![输出显示前端服务将其外部IP从<pending>更改为实际IP。](image/Figure_5.3.jpg)
- en: 'Figure 5.3: Get the service''s public IP'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.3：获取服务的公共IP
- en: Go to `http://<EXTERNAL-IP>` as shown in *Figure 5.4*:![Once you enter the public
    IP in the address bar of your browser, it will open a white screen with the word
    Guestbook written in bold. This is a sign that your application is now running.](image/Figure_5.4.jpg)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`http://<EXTERNAL-IP>`，如*图5.4*所示：![一旦在浏览器的地址栏中输入公共IP，它将打开一个带有粗体字“Guestbook”的白屏。这表明您的应用程序现在正在运行。](image/Figure_5.4.jpg)
- en: 'Figure 5.4: Ensure the application is running'
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.4：确保应用程序正在运行
- en: 'Let''s see where the Pods are currently running using the following code:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看当前正在运行的Pods使用以下代码：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will generate an output as shown in *Figure 5.5*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如*图5.5*所示的输出：
- en: '![When you execute the kubectl get pods -o wide command, the output displayed
    will show that the Pods are spread between nodes 0 and 1.](image/Figure_5.5.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![当您执行kubectl get pods -o wide命令时，显示的输出将显示Pods分布在节点0和1之间。](image/Figure_5.5.jpg)'
- en: 'Figure 5.5: Our Pods are spread between node 0 and node 1'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.5：我们的Pods分布在节点0和节点1之间
- en: This shows us that we have the workload spread between node 0 and node 1.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示我们的工作负载分布在节点0和节点1之间。
- en: 'In this example, we want to demonstrate how Kubernetes handles a node failure.
    To demonstrate this, we will shut down a node in the cluster. In this case, we
    are going for maximum damage, so let''s shut down node 1 (you can choose whichever
    node you want – for illustration purposes, it doesn''t really matter):'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们想演示Kubernetes如何处理节点故障。为了演示这一点，我们将关闭集群中的一个节点。在这种情况下，我们要造成最大的破坏，所以让我们关闭节点1（您可以选择任何节点
    - 出于说明目的，这并不重要）：
- en: 'To shut down this node, look for `Virtual machine scale sets` back in our cluster
    in the Azure search bar, as shown in *Figure 5.6*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭此节点，请在Azure搜索栏中查找我们集群中的`虚拟机规模集`，如*图5.6*所示：
- en: '![Type vmss in the Azure search bar to shut down the node.](image/Figure_5.6.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: 请在Azure搜索栏中键入vmss以关闭节点。
- en: 'Figure 5.6: Looking for the VMSS hosting your cluster'
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.6：查找托管集群的VMSS
- en: 'After navigating to the blade for the scale set, go the **Instances** view,
    select the instance you want to shut down, and then hit the **Deallocate** button,
    as shown in *Figure 5.7*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到规模集的刀片后，转到**实例**视图，选择要关闭的实例，然后点击**分配**按钮，如*图5.7*所示：
- en: '![Click on the Instances tab in the Navigation pane in the Azure portal. This
    will display the number of instances. Select the one you want to shutdown. Click
    on the deallocate icon that appears in the address bar.](image/Figure_5.7.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![单击Azure门户中导航窗格中的“实例”选项卡。这将显示实例的数量。选择要关闭的实例。单击出现在地址栏中的取消分配图标。](image/Figure_5.7.jpg)'
- en: 'Figure 5.7: Shutdown node 1'
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.7：关闭节点1
- en: 'This will shut down our node. To see how Kubernetes will react with your Pods,
    you can watch the Pods in your cluster via the following command:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将关闭我们的节点。要查看Kubernetes如何与您的Pods交互，可以通过以下命令观察集群中的Pods：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To verify whether your application can continue to run, you can optionally
    run the following command to hit the guestbook front end every 5 seconds and get
    the HTML. It''s recommended to open this in a new Cloud Shell window:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要验证您的应用程序是否可以继续运行，可以选择运行以下命令，每5秒钟点击一次guestbook前端并获取HTML。建议在新的Cloud Shell窗口中打开此命令：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding command will keep calling your application till you press *Ctrl*
    + *C* (*command* + *C* on Mac). There might be intermittent times where you don't
    get a reply, which is to be expected as Kubernetes takes a couple of minutes to
    rebalance the system.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将一直调用您的应用程序，直到您按下*Ctrl* + *C*（Mac上的*command* + *C*）。可能会有间歇时间您收不到回复，这是可以预期的，因为Kubernetes需要几分钟来重新平衡系统。
- en: 'Add some guestbook entries to see what happens to them when you cause the node
    to shut down. This will display an output as shown in *Figure 5.8*:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一些留言板条目，看看当你导致节点关闭时它们会发生什么。这将显示一个如*图5.8*所示的输出：
- en: '![Use the guestbook application to write a couple of messages.](image/Figure_5.8.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![使用留言板应用程序写下几条消息。](image/Figure_5.8.jpg)'
- en: 'Figure 5.8: Writing a couple of messages in the guestbook'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.8：在留言板中写下几条消息
- en: What you can see is that all your precious messages are gone! This shows the
    importance of having **PersistentVolumeClaims** (**PVCs**) for any data that you
    want to survive in the case of a node failure, which is not the case in our application
    here. You will see an example of this in the last section of this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你所看到的是你所有珍贵的消息都消失了！这显示了在节点故障的情况下拥有**PersistentVolumeClaims**（**PVCs**）对于任何你希望存活的数据的重要性，在我们的应用程序中并非如此。你将在本章的最后一节中看到一个例子。
- en: After a while, the output of watching the Pods should have shown additional
    output, showing you that the Pods got rescheduled on the healthy host, as shown
    in *Figure 5.9*:![This will display an output that shows that the Pods from the
    failed node arerecreated.](image/Figure_5.9.jpg)
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过一会儿，观察Pods的输出应该显示额外的输出，告诉你Pods已经在健康的主机上重新调度，如*图5.9*所示：![这将显示一个输出，显示了从失败节点重新创建的Pods。](image/Figure_5.9.jpg)
- en: 'Figure 5.9: The Pods from the failed node getting recreated'
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.9：从失败节点重新创建的Pods
- en: 'What you see here is the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里看到的是：
- en: One of the front end Pods running on host 1 got terminated as the host became
    unhealthy.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个前端Pod在主机1上运行时被终止，因为主机变得不健康。
- en: A new front end Pod got created, on host 0\. This went through the stages **Pending**,
    **ContainerCreating**, and then **Running**.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个新的前端Pod被创建在主机0上。这经历了**Pending**、**ContainerCreating**，然后是**Running**这些阶段。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Kubernetes picked up that the host was unhealthy before it rescheduled the Pods.
    If you were to do `kubectl get nodes`, you would see node 1 in a `NotReady` state.
    There is a configuration in Kubernetes called pod-eviction-timeout that defines
    how long the system will wait to reschedule Pods on a healthy host. The default
    is 5 minutes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在重新调度Pods之前察觉到主机不健康。如果你运行`kubectl get nodes`，你会看到节点1处于`NotReady`状态。Kubernetes中有一个叫做pod-eviction-timeout的配置，它定义了系统等待在健康主机上重新调度Pods的时间。默认值是5分钟。
- en: In this section, you learned how Kubernetes automatically handles node failures
    by recreating Pods on healthy nodes. In the next section, you will learn how you
    can diagnose and solve out-of-resource issues.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学会了Kubernetes如何自动处理节点故障，通过在健康节点上重新创建Pods。在下一节中，你将学习如何诊断和解决资源耗尽的问题。
- en: Solving out-of-resource failures
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决资源耗尽的问题
- en: Another common issue that can come up with Kubernetes clusters is the cluster
    running out of resources. When the cluster doesn't have enough CPU power or memory
    to schedule additional Pods, Pods will become stuck in a `Pending` state.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群可能出现的另一个常见问题是集群资源耗尽。当集群没有足够的CPU或内存来调度额外的Pods时，Pods将被卡在`Pending`状态。
- en: 'Kubernetes uses `requests` to calculate how much CPU power or memory a certain
    Pod requires. Our guestbook application has requests defined for all the deployments.
    If you open the `guestbook-all-in-one.yaml` file, you''ll see the following for
    the `redis-slave` deployment:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用`requests`来计算某个Pod需要多少CPU或内存。我们的留言板应用程序为所有部署定义了请求。如果你打开`guestbook-all-in-one.yaml`文件，你会看到`redis-slave`部署的以下内容：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This section explains that every Pod for the `redis-slave` deployment requires
    `100m` of a CPU core (100 milli or 10%) and 100MiB (Mebibyte) of memory. In our
    1 CPU cluster (with node 1 shut down), scaling this to 10 Pods will cause issues
    with the available resources. Let''s look into this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了`redis-slave`部署的每个Pod都需要`100m`的CPU核心（100毫或10%）和100MiB（Mebibyte）的内存。在我们的1个CPU集群（关闭节点1），将其扩展到10个Pods将导致可用资源出现问题。让我们来看看这个：
- en: Note
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In Kubernetes, you can use either the binary prefix notation or the base 10
    notation to specify memory and storage. Binary prefix notation means using KiB
    (kibibyte) to represent 1,024 bytes, MiB (mebibyte) to represent 1,024 KiB, and
    Gib (gibibyte) to represent 1,024 MiB. Base 10 notation means using kB (kilobyte)
    to represent 1,000 bytes, MB (megabyte) to represent 1,000 kB, and GB (gigabyte)
    represents 1,000 MB.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，您可以使用二进制前缀表示法或基数10表示法来指定内存和存储。二进制前缀表示法意味着使用KiB（kibibyte）表示1024字节，MiB（mebibyte）表示1024
    KiB，Gib（gibibyte）表示1024 MiB。基数10表示法意味着使用kB（kilobyte）表示1000字节，MB（megabyte）表示1000
    kB，GB（gigabyte）表示1000 MB。
- en: 'Let''s start by scaling the `redis-slave` deployment to 10 Pods:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先将`redis-slave`部署扩展到10个Pods：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will cause a couple of new Pods to be created. We can check our Pods using
    the following:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将导致创建一对新的Pod。我们可以使用以下命令来检查我们的Pods：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will generate an output as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Using thekubectl get pods command, you can check your Pods. The output will
    generate some Pods whose status isPending.](image/Figure_5.10.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![使用kubectl get pods命令，您可以检查您的Pods。输出将生成一些状态为Pending的Pods。](image/Figure_5.10.jpg)'
- en: 'Figure 5.10: If the cluster is out of resources, Pods will go into the Pending
    state'
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.10：如果集群资源不足，Pods将进入Pending状态
- en: Highlighted here is one of the Pods that are in the `Pending` state.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里突出显示了一个处于`Pending`状态的Pod。
- en: 'We can get more information about these pending Pods using the following command:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令获取有关这些待处理Pods的更多信息：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will show you more details. At the bottom of the `describe` command, you
    should see something like what''s shown in *Figure 5.11*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示更多细节。在`describe`命令的底部，您应该看到类似于*图5.11*中显示的内容：
- en: '![The output displays an event with a FailedSchedulingmessage from the default-scheduler.
    A detailed message shows "0/2 nodes are available: 1 insufficient CPU, 1 node(s)
    had taints that the pod didn''t tolerate."](image/Figure_5.11.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![输出显示了来自default-scheduler的FailedSchedulingmessage事件。详细消息显示“0/2个节点可用：1个CPU不足，1个节点有Pod无法容忍的污点。”](image/Figure_5.11.jpg)'
- en: 'Figure 5.11: Kubernetes is unable to schedule this Pod'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.11：Kubernetes无法安排此Pod
- en: 'It explains two things to us:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 它向我们解释了两件事：
- en: One of the nodes is out of CPU resources.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一个节点的CPU资源已经用完。
- en: One of the nodes has a taint that the Pod didn't tolerate. This means that the
    node that is `NotReady` can't accept Pods.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其中一个节点有一个Pod无法容忍的污点。这意味着`NotReady`的节点无法接受Pods。
- en: We can solve this capacity issue, by starting up node 1 as shown in *Figure
    5.12*. This can be done in a way similar to the shutdown process:![You can start
    the node by using the same procedure that you used for the shutdown. Click on
    the Instances tab in the Navigation blade. Select the node that you want to start.
    Lastly, click on the Start button in the toolbar.](image/Figure_5.12.jpg)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过像*图5.12*中所示的方式启动节点1来解决这个容量问题。这可以通过类似于关闭过程的方式完成：![您可以使用与关闭相同的过程来启动节点。单击导航窗格中的实例选项卡。选择要启动的节点。最后，单击工具栏中的“启动”按钮。](image/Figure_5.12.jpg)
- en: 'Figure 5.12: Start node 1 again'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.12：重新启动节点1
- en: It will take a couple of minutes for the other node to become available again
    in Kubernetes. If we re-execute the `describe` command on the previous Pod, we'll
    see an output like what's shown in *Figure 5.13*:![The events output from describing
    the pod shows that after some time, the scheduler assigned the pod to the new
    node, and shows the process of pulling an image, as well as creating and starting
    the container.](image/Figure_5.13.jpg)
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他节点再次在Kubernetes中变得可用需要几分钟的时间。如果我们重新执行先前Pod上的`describe`命令，我们将看到类似于*图5.13*所示的输出：![描述Pod的事件输出显示，经过一段时间，调度程序将Pod分配给新节点，并显示拉取图像的过程，以及创建和启动容器的过程。](image/Figure_5.13.jpg)
- en: 'Figure 5.13: When the node is available again, the other Pods get started on
    the new node'
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.13：当节点再次可用时，其他Pod将在新节点上启动。
- en: This shows that after node 1 became available, Kubernetes scheduled our Pod
    on that node, and then started the container.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明在节点1再次变得可用后，Kubernetes将我们的Pod调度到该节点，然后启动容器。
- en: In this section, we learned how to diagnose out-of-resource errors. We were
    able to solve the error by adding another node to the cluster. Before we move
    on to the final failure mode, we will clean up the guestbook deployment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何诊断资源不足的错误。我们通过向集群添加另一个节点来解决了这个错误。在我们继续进行最终故障模式之前，我们将清理一下guestbook部署。
- en: Note
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In *Chapter 4*, *Scaling your Application*, we discussed the cluster autoscaler.
    The cluster autoscaler will monitor out-of-resource errors and add new nodes to
    the cluster automatically.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*中，*扩展您的应用程序*，我们讨论了集群自动缩放器。集群自动缩放器将监视资源不足的错误，并自动向集群添加新节点。
- en: 'Let''s clean up by running the following `delete` command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下`delete`命令来清理一下：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So far, we have discussed two failure modes for nodes in a Kubernetes cluster.
    First, we discussed how Kubernetes handles a node going offline and how the system
    reschedules Pods to a working node. After that, we saw how Kubernetes uses requests
    to schedule Pods on a node, and what happens when a cluster is out of resources.
    In the next section, we'll cover another failure mode in Kubernetes, namely what
    happens when Kubernetes moves Pods with PVCs attached.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了Kubernetes集群中节点的两种故障模式。首先，我们讨论了Kubernetes如何处理节点离线以及系统如何将Pod重新调度到工作节点上。之后，我们看到了Kubernetes如何使用请求来调度节点上的Pod，以及当集群资源不足时会发生什么。在接下来的部分，我们将讨论Kubernetes中的另一种故障模式，即当Kubernetes移动带有PVC的Pod时会发生什么。
- en: Fixing storage mount issues
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修复存储挂载问题
- en: Earlier in this chapter, you noticed how the guestbook application lost data
    when the Redis master was moved to another node. This happened because that sample
    application didn't use any persistent storage. In this section, we'll cover an
    example of how PVCs can be used to prevent data loss when Kubernetes moves a Pod
    to another node. We will show you a common error that occurs when Kubernetes moves
    Pods with PVCs attached, and we will show you how to fix this.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，您注意到当Redis主节点移动到另一个节点时，guestbook应用程序丢失了数据。这是因为该示例应用程序没有使用任何持久存储。在本节中，我们将介绍当Kubernetes将Pod移动到另一个节点时，如何使用PVC来防止数据丢失的示例。我们将向您展示Kubernetes移动带有PVC的Pod时会发生的常见错误，并向您展示如何修复此错误。
- en: 'For this, we will reuse the WordPress example from the previous chapter. Before
    we start, let''s make sure that the cluster is in a clean state:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将重用上一章中的WordPress示例。在开始之前，让我们确保集群处于干净的状态：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This shows us just the one Kubernetes service, as shown in *Figure 5.14*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这向我们展示了一个Kubernetes服务，如*图5.14*所示：
- en: '![Executing the kubectl get all command generates an output showing only one
    Kubernetes service running for now.](image/Figure_5.14.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![执行kubectl get all命令会生成一个输出，显示目前只有一个Kubernetes服务正在运行。](image/Figure_5.14.jpg)'
- en: 'Figure 5.14: You should only have the Kubernetes service running for now'
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.14：目前您应该只运行Kubernetes服务
- en: 'Let''s also ensure that both nodes are running and `Ready`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还确保两个节点都在运行并且处于“就绪”状态：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This should show us both nodes in a `Ready` state, as shown in *Figure 5.15*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示我们两个节点都处于“就绪”状态，如*图5.15*所示：
- en: '![You should now see the statuses of the two nodes are Ready.](image/Figure_5.15.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![现在您应该看到两个节点的状态都是Ready。](image/Figure_5.15.jpg)'
- en: 'Figure 5.15: You should have two nodes available in your cluster'
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.15：您的集群中应该有两个可用节点
- en: In the previous example, under the *Handling node failures* section, we saw
    that the messages stored in `redis-master` were lost if the Pod gets restarted.
    The reason for this is that `redis-master` stores all data in its container, and
    whenever it is restarted, it uses the clean image without the data. In order to
    survive reboots, the data has to be stored outside. Kubernetes uses PVCs to abstract
    the underlying storage provider to provide this external storage.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，在*处理节点故障*部分，我们看到如果Pod重新启动，存储在`redis-master`中的消息将丢失。原因是`redis-master`将所有数据存储在其容器中，每当重新启动时，它都会使用不带数据的干净镜像。为了在重新启动后保留数据，数据必须存储在外部。Kubernetes使用PVC来抽象底层存储提供程序，以提供这种外部存储。
- en: To start this example, we'll set up the WordPress installation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始此示例，我们将设置WordPress安装。
- en: Starting the WordPress installation
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始WordPress安装
- en: 'Let''s start by installing WordPress. We will demonstrate how it works and
    then verify that storage is still present after a reboot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装WordPress开始。我们将演示其工作原理，然后验证在重新启动后存储是否仍然存在：
- en: 'Begin reinstallation by using the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令开始重新安装：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will take a couple of minutes to process. You can follow the status of
    this installation by executing the following command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将花费几分钟的时间来处理。您可以通过执行以下命令来跟踪此安装的状态：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After a couple of minutes, this should show us Pods with a status of `Running`
    and with a ready status of `1/1` for both Pods, as shown in *Figure 5.16*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，这应该显示我们的Pod状态为`Running`，并且两个Pod的就绪状态为`1/1`，如*图5.16*所示：
- en: '![Using kubectl get pods -w, you will see the pods transition from ContainerCreating
    to Runningstatus, and you will see the numberof Ready pods change from 0/1 to
    1/1.](image/Figure_5.16.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![使用kubectl get pods -w命令，您将看到Pod从ContainerCreating转换为Running状态，并且您将看到Ready
    pods的数量从0/1变为1/1。](image/Figure_5.16.jpg)'
- en: 'Figure 5.16: All Pods will have the status of Running after a couple of minutes'
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.16：几分钟后，所有Pod都将显示为运行状态
- en: In this section, we saw how to install the WordPress. Now, we will see how to
    avoid data loss using the persistent volumes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何安装WordPress。现在，我们将看到如何使用持久卷来避免数据丢失。
- en: Using persistent volumes to avoid data loss
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用持久卷来避免数据丢失
- en: 'A **persistent volume** (**PV**) is the way to store persistent data in the
    cluster with Kubernetes. We explained PVs in more detail in *Chapter 3*, *Application
    deployment on AKS*. Let''s explore the PVs created for our WordPress deployment:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**持久卷**（**PV**）是在Kubernetes集群中存储持久数据的方法。我们在*第3章*的*在AKS上部署应用程序*中更详细地解释了PV。让我们探索为WordPress部署创建的PV：'
- en: 'In our case, run the following `describe nodes` command:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的情况下，运行以下`describe nodes`命令：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Scroll through the output until you see a section that is similar to *Figure
    5.17*. In our case, both WordPress Pods are running on node 0:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动查看输出，直到看到类似于*图5.17*的部分。在我们的情况下，两个WordPress Pod都在节点0上运行：
- en: '![When you execute the kubectl describe nodes command, you will see the information
    that indicates both the Pods are running on node 0.](image/Figure_5.17.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![当您执行kubectl describe nodes命令时，您将看到指示两个Pod正在节点0上运行的信息。](image/Figure_5.17.jpg)'
- en: 'Figure 5.17: In our case, both WordPress Pods are running on node 0'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.17：在我们的情况下，两个WordPress Pod都在节点0上运行
- en: Your Pod placement might vary.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您的Pod放置可能会有所不同。
- en: 'The next thing we can check is the status of our PVCs:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以检查的下一件事是我们的PVC的状态：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will generate an output as shown in *Figure 5.18*:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个如*图5.18*所示的输出：
- en: '![The output displays two PVCs. Alongside their names, you can also see the
    Status, Volume, Capacity, and the Access mode of these PVCs.](image/Figure_5.18.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![输出显示了两个PVC。除了它们的名称，您还可以看到这些PVC的状态、卷、容量和访问模式。](image/Figure_5.18.jpg)'
- en: 'Figure 5.18: Two PVCs are created by the WordPress deployment'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.18：WordPress部署创建了两个PVC
- en: 'The following command shows the actual PV that is bound to the Pods:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令显示了绑定到Pod的实际PV：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will show you the details of both volumes. We will show you one of those
    in *Figure 5.19*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这将向您显示两个卷的详细信息。我们将在*图5.19*中向您展示其中一个：
- en: '![Using the kubectl describe pvc command, you can see the two PVCs in detail.
    The picture highlights the claim as default/data-wp-mariadb-0, and it highlights
    the diskURI in Azure.](image/Figure_5.19.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: 使用kubectl describe pvc命令，您可以详细查看两个PVC。图片突出显示了默认/data-wp-mariadb-0的声明，并突出显示了Azure中的diskURI。![使用kubectl
    describe pvc命令，您可以详细查看两个PVC。图片突出显示了默认/data-wp-mariadb-0的声明，并突出显示了Azure中的diskURI。](image/Figure_5.19.jpg)
- en: 'Figure 5.19: The details of one of the PVCs'
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.19：一个PVC的详细信息
- en: Here, we can see which Pod has claimed this volume and what the **DiskURI**
    is in Azure.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到哪个Pod声明了这个卷，以及Azure中的**DiskURI**是什么。
- en: 'Verify that your site is actually working:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证您的网站是否实际在运行：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will show us the public IP of our WordPress site, as shown in *Figure
    5.20*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示我们的WordPress网站的公共IP，如*图5.20*所示：
- en: '![The output screen displays the External-IP for the wp-WordPress service only.](image/Figure_5.20.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![输出屏幕仅显示了wp-WordPress服务的External-IP。](image/Figure_5.20.jpg)'
- en: 'Figure 5.20: Get the service''s public IP'
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.20：获取服务的公共IP
- en: 'If you remember from *Chapter 3*, *Application deployment of AKS*, Helm showed
    us the commands we need to get the admin credentials for our WordPress site. Let''s
    grab those commands and execute them to log on to the site as follows:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您还记得*第3章*，*AKS的应用部署*，Helm向我们显示了获取WordPress网站管理员凭据所需的命令。让我们获取这些命令并执行它们以登录到网站，如下所示：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will show you the `username` and `password`, as displayed in *Figure 5.21*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这将向您显示`用户名`和`密码`，如*图5.21*所示：
- en: '![The output shows how to get the username and password through Helm. The username
    in the screenshot is user, and the password in the screenshot is lcsUSJTk8e. The
    password in your case will differ.](image/Figure_5.21.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![输出显示如何通过Helm获取用户名和密码。截图中的用户名是user，密码是lcsUSJTk8e。在您的情况下，密码将不同。](image/Figure_5.21.jpg)'
- en: 'Figure 5.21: Getting the username and password for the WordPress application'
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.21：获取WordPress应用程序的用户名和密码
- en: Note
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You might notice that the commands we use in the book are slightly different
    than the ones that `helm` returned. The command that `helm` returned for the password
    didn't work for us, and we provided you with the working commands.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，我们在书中使用的命令与`helm`返回的命令略有不同。`helm`为密码返回的命令对我们不起作用，我们为您提供了有效的命令。
- en: 'We can log in to our site via the following address: `http://<external-ip>/admin`.
    Log in here with the credentials from the previous step. Then you can go ahead
    and add a post to your website. Click the **Write your first blog post** button,
    and then create a short post, as shown in *Figure 5.22*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下地址登录到我们的网站：`http://<external-ip>/admin`。在这里使用前一步骤中的凭据登录。然后，您可以继续添加一篇文章到您的网站。点击**撰写您的第一篇博客文章**按钮，然后创建一篇简短的文章，如*图5.22*所示：
- en: '![You will see a dashboard that welcomes you to WordPress. Here, you will see
    a button that says- Write your first blog post. Click on it to begin writing.](image/Figure_5.22.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![您将看到一个欢迎您来到WordPress的仪表板。在这里，您将看到一个按钮，上面写着-写下您的第一篇博客文章。单击它开始写作。](image/Figure_5.22.jpg)'
- en: 'Figure 5.22: Writing your first blog post'
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.22：撰写您的第一篇博客文章
- en: 'Type a little text now and hit the **Publish** button, as shown in *Figure
    5.23*. The text itself isn''t important; we are writing this to verify that data
    is indeed persisted to disk:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在输入一些文本，然后单击**发布**按钮，就像*图5.23*中所示。文本本身并不重要；我们写这个来验证数据确实被保留到磁盘上：
- en: '![Let''s say you typed the word "test" at random. Once you''ve written the
    text, click on the Publish button,which is located in the top right corner of
    the screen.](image/Figure_5.23.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![假设您随机输入了单词“测试”。在写完文字后，单击屏幕右上角的“发布”按钮。](image/Figure_5.23.jpg)'
- en: 'Figure 5.23: Publishing a post with random text'
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.23：发布包含随机文本的文章
- en: If you now head over to the main page of your website at `http://<external-ip>`,
    you'll see your test post as shown in *Figure 5.23*. We will verify whether this
    post survives a reboot in the next section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在转到您网站的主页`http://<external-ip>`，您将会看到您的测试文章，就像*图5.23*中所示。我们将在下一节验证此文章是否经得起重启。
- en: '**Handling Pod failure with PVC involvement**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理涉及PVC的Pod故障**'
- en: 'The first test we''ll do with our PVC is to kill the Pods and verify whether
    the data has indeed persisted. To do this, let''s do two things:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对我们的PVC进行的第一个测试是杀死Pod并验证数据是否确实被保留。为此，让我们做两件事：
- en: '**Watch our Pods in our application**. To do this, we''ll use our current Cloud
    Shell and execute the following command:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**观察我们应用程序中的Pod**。为此，我们将使用当前的Cloud Shell并执行以下命令：'
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Kill the two Pods that have the PVC mounted**. To do this, we''ll create
    a new Cloud Shell window by clicking on the icon shown in *Figure 5.24*:![Click
    on the icon located to the left side of the curly braces icon on the toolbar.
    Click on it to open a new Cloud Shell.](image/Figure_5.24.jpg)'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 杀死已挂载PVC的两个Pod。为此，我们将通过单击工具栏上显示的图标创建一个新的Cloud Shell窗口，如*图5.24*所示：![单击工具栏左侧花括号图标旁边的图标，单击以打开新的Cloud
    Shell。](image/Figure_5.24.jpg)
- en: 'Figure 24: Opening a new Cloud Shell instance'
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图24：打开新的Cloud Shell实例
- en: 'Once you open a new Cloud Shell, execute the following command:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您打开一个新的Cloud Shell，执行以下命令：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If you follow along with the `watch` command, you should see an output like
    what''s shown in *Figure 5.25*:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`watch`命令，您应该会看到类似于*图5.25*所示的输出：
- en: '![Running kubectl get pods -w shows you that the old pods get terminated and
    new pods are created. The new pods transition from the Pending state to ContainerCreating
    to Running.](image/Figure_5.25.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![运行kubectl get pods -w会显示旧的Pod被终止并创建新的Pod。新的Pod会从Pending状态过渡到ContainerCreating再到Running。](image/Figure_5.25.jpg)'
- en: 'Figure 5.25: After deleting the Pods, Kubernetes will automatically recreate
    both Pods'
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.25：删除Pod后，Kubernetes将自动重新创建两个Pod
- en: As you can see, Kubernetes quickly started creating new Pods to recover from
    the Pod outage. The Pods went through a similar life cycle as the original ones,
    going from **Pending** to **ContainerCreating** to **Running**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Kubernetes迅速开始创建新的Pod来从Pod故障中恢复。这些Pod经历了与原始Pod相似的生命周期，从**Pending**到**ContainerCreating**再到**Running**。
- en: If you head on over to your website, you should see that your demo post has
    been persisted. This is how PVCs can help you prevent data loss, as they persist
    data that would not have been persisted in the container itself.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您转到您的网站，您应该会看到您的演示文章已经被保留。这就是PVC如何帮助您防止数据丢失的方式，因为它们保留了容器本身无法保留的数据。
- en: '*Figure 5.26* shows that the blog post was persisted even though the Pod was
    recreated:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.26*显示，即使Pod被重新创建，博客文章仍然被保留：'
- en: '![You can see that your data is persisted and the blog post with the word "test"
    is still available.](image/Figure_5.26.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![您可以看到您的数据被持久保存，带有“test”字样的博客帖子仍然可用。](image/Figure_5.26.jpg)'
- en: 'Figure 5.26: Your data is persisted, and your blog post is still there'
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.26：您的数据被持久保存，您的博客帖子仍然存在
- en: 'One final interesting data point to watch here is the Kubernetes event stream.
    If you run the following command, you can see events related to volumes:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里观察的最后一个有趣的数据点是Kubernetes事件流。如果运行以下命令，您可以看到与卷相关的事件：
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will generate an output as shown in *Figure 5.27*:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如*图5.27*所示的输出。
- en: '![Output screen will show a FailedAttachVolumewarning. Below it, it will show
    that the state is now normal with a SuccessfulAttachVolumemessage. This shows
    that Kubernetes was initially unable to mount the Volume, but was able to mount
    it on the next try.](image/Figure_5.27.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![输出屏幕将显示一个FailedAttachVolume警告。在其下方，它将显示状态现在正常，并带有SuccessfulAttachVolume消息。这表明Kubernetes最初无法挂载卷，但在下一次尝试时成功挂载了它。](image/Figure_5.27.jpg)'
- en: 'Figure 5.27: Kubernetes dealt with a FailedAttachVolume error'
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.27：Kubernetes处理了FailedAttachVolume错误
- en: 'This shows you the events related to volumes. There are two interesting messages
    to explain in more detail: `FailedAttachVolume` and `SuccessfulAttachVolume`.
    This shows us how Kubernetes handles volumes that have a read-write-once configuration.
    Since the characteristic is to only read and write from a single Pod, Kubernetes
    will only mount the volume to the new Pod when it is successfully unmounted from
    the current Pod. Hence, initially, when the new Pod was scheduled, it showed the
    `FailedAttachVolume` message as the volume was still attached to the Pod that
    was being deleted. Afterward, the Pod could successfully mount the volume and
    showed this with the message `SuccessfulAttachVolume`.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了与卷相关的事件。有两条有趣的消息需要详细解释：`FailedAttachVolume`和`SuccessfulAttachVolume`。这向我们展示了Kubernetes如何处理具有read-write-once配置的卷。由于特性是只能从单个Pod中读取和写入，Kubernetes只会在成功从当前Pod卸载卷后，将卷挂载到新的Pod上。因此，最初，当新的Pod被调度时，它显示了`FailedAttachVolume`消息，因为卷仍然附加到正在删除的Pod上。之后，Pod成功挂载了卷，并通过`SuccessfulAttachVolume`消息显示了这一点。
- en: In this section, we've learned how PVCs can help when Pods get recreated on
    the same node. In the next section, we'll see how PVCs are used when a node has
    a failure.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经学习了PVC在Pod在同一节点上重新创建时可以起到的作用。在下一节中，我们将看到当节点发生故障时PVC的使用情况。
- en: '**Handling node failure with PVC involvement**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用PVC处理节点故障**'
- en: 'In the previous example, we saw how Kubernetes can handle Pod failures when
    those Pods have a PV attached. In this example, we''ll look at how Kubernetes
    handles node failures when a volume is attached:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们看到了Kubernetes如何处理具有PV附加的Pod故障。在这个示例中，我们将看看Kubernetes在卷附加时如何处理节点故障：
- en: 'Let''s first check which node is hosting our application, using the following
    command:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先检查哪个节点托管了我们的应用程序，使用以下命令：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We found that, in our cluster, node 1 was hosting MariaDB, and node 0 was hosting
    the WordPress site, as shown in *Figure 5.28*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在我们的集群中，节点1托管了MariaDB，节点0托管了WordPress网站，如*图5.28*所示：
- en: '![Running kubectl get pods -o wide shows you which pod is running on which
    node. In the screenshot,one pod is running on each host.](image/Figure_5.28.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![运行kubectl get pods -o wide会显示哪个pod在哪个节点上运行。在截图中，一个pod在每个主机上运行。](image/Figure_5.28.jpg)'
- en: 'Figure 5.28: We have two Pods running in our deployment – one on node 1, one
    on node 0'
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.28：我们的部署中有两个正在运行的Pod - 一个在节点1上，一个在节点0上
- en: We are going to introduce a failure and stop the node that could cause the most
    damage by shutting down node 0 on the Azure portal. We'll do this the same way
    as in the earlier example. First, look for the scale set backing our cluster,
    as shown in *Figure 5.29*:![Typing vmss in the search bar on the Azure portal
    will show you the full name of the VMSS hosting your AKS cluster.](image/Figure_5.29.jpg)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将引入一个故障，并停止可能会造成最严重损害的节点，即关闭Azure门户上的节点0。我们将以与先前示例相同的方式进行。首先，查找支持我们集群的规模集，如*图5.29*所示：![在Azure门户的搜索栏中键入vmss将显示托管您AKS集群的VMSS的完整名称。](image/Figure_5.29.jpg)
- en: 'Figure 5.29: Finding the scale set backing our cluster'
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.29：查找支持我们集群的规模集
- en: Then shut down the node as shown in *Figure 5.30*:![To shutdown the node, click
    on the Instances tab located in the Navigation pane in the Azure portal. You will
    see two nodes. Select the first node and click on the Deallocate button on the
    toolbar.](image/Figure_5.30.jpg)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后按照*图5.30*中所示关闭节点：![要关闭节点，请在Azure门户中导航窗格中的“实例”选项卡上单击。您将看到两个节点。选择第一个节点，然后单击工具栏上的“停用”按钮。](image/Figure_5.30.jpg)
- en: 'Figure 5.30: Shutting down the node'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.30：关闭节点
- en: 'After this action, we''ll once again watch our Pods to see what is happening
    in the cluster:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成此操作后，我们将再次观察我们的Pod，以了解集群中正在发生的情况：
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As in the previous example, it is going to take 5 minutes before Kubernetes
    will start taking action against our failed node. We can see that happening in
    *Figure 5.31*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的示例一样，Kubernetes将在5分钟后开始采取行动来应对我们失败的节点。我们可以在*图5.31*中看到这一情况：
- en: '![When you execute the kubectl get pods -o wide -w command, you will see that
    there are Pods with a status ofPending that are not assigned to a node.](image/Figure_5.31.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![当您执行kubectl get pods -o wide -w命令时，您将看到状态为Pending的Pod未分配到节点。](image/Figure_5.31.jpg)'
- en: 'Figure 5.31: A Pod in a pending state'
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.31：处于挂起状态的Pod
- en: 'We are seeing a new issue here. Our new Pod is stuck in a `Pending` state and
    has not been assigned to a new node. Let''s figure out what is happening here.
    First, we''ll `describe` our Pod:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里遇到了一个新问题。我们的新Pod处于“挂起”状态，尚未分配到新节点。让我们弄清楚这里发生了什么。首先，我们将“描述”我们的Pod：
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You will get an output as shown in *Figure 5.32*:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到一个如*图5.32*所示的输出：
- en: '![You will see the details of why this Pod is stuck in Pending state, which
    is due to insufficient CPU.](image/Figure_5.32.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![您将看到此Pod处于挂起状态的原因的详细信息，这是由于CPU不足造成的。](image/Figure_5.32.jpg)'
- en: 'Figure 5.32: Output displaying the Pod in a pending state'
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.32：显示处于挂起状态的Pod的输出
- en: 'This shows us that we don''t have sufficient CPU resources in our cluster to
    host the new Pod. We can fix this by using the `kubectl edit deploy/...` command
    to fix any insufficient CPU/memory errors. We''ll change the CPU requests from
    300 to 3 for our example to continue:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这表明我们的集群中没有足够的CPU资源来托管新的Pod。我们可以使用`kubectl edit deploy/...`命令来修复任何不足的CPU/内存错误。我们将将CPU请求从300更改为3，以便我们的示例继续进行：
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This brings us to a `vi` environment. We can quickly find the section referring
    to the CPU by typing the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带我们进入一个`vi`环境。我们可以通过输入以下内容快速找到与CPU相关的部分：
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once there, move the cursor over the two zeros and hit the `x` key twice to
    delete the zeros. Finally, type `:wq!` to save our changes so that we can continue
    our example.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦到达那里，将光标移动到两个零上，然后按两次“x”键删除零。最后，键入“:wq!”以保存我们的更改，以便我们可以继续我们的示例。
- en: 'This will result in a new ReplicaSet being created with a new Pod. We can get
    the name of the new Pod by entering the following command:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将导致创建一个新的ReplicaSet和一个新的Pod。我们可以通过输入以下命令来获取新Pod的名称：
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Look for the Pod with the status `ContainerCreating`, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 查找状态为“ContainerCreating”的Pod，如下所示：
- en: '![The output screen displays four Pods with different statuses. Look for the
    Pod with the ContainerCreatingstatus.](image/Figure_5.33.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![输出屏幕显示了四个具有不同状态的Pod。查找具有ContainerCreating状态的Pod。](image/Figure_5.33.jpg)'
- en: 'Figure 5.33: The new Pod is stuck in a ContainerCreating state'
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.33：新的Pod被卡在ContainerCreating状态
- en: 'Let''s have a look at the details for that Pod with the `describe` command:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用`describe`命令查看该Pod的详细信息：
- en: '[PRE30]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In the `Events` section of this `describe` output, you can see an error message
    as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`describe`输出的“事件”部分，您可以看到以下错误消息：
- en: '![Executing kubectl describe on the pod with the ContainerCreating status shows
    a detailed error message with the reason FailedMount. The message says that Kubernetes
    was unable to mount the volume.](image/Figure_5.34.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![在具有ContainerCreating状态的Pod上执行kubectl describe命令会显示一个详细的错误消息，其中包含FailedMount的原因。消息表示Kubernetes无法挂载卷。](image/Figure_5.34.jpg)'
- en: 'Figure 5.34: The new Pod has a new error message, describing volume mounting
    issues'
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.34：新的Pod有一个新的错误消息，描述了卷挂载问题
- en: This tells us that the volume that our new Pod wants to mount is still mounted
    to the Pod that is stuck in a `Terminating` state. We can solve this by manually
    detaching the disk from the node we shut down and forcefully removing the Pod
    stuck in the `Terminating` state.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这告诉我们，我们的新Pod想要挂载的卷仍然挂载到了被卡在“终止”状态的Pod上。我们可以通过手动从我们关闭的节点上分离磁盘并强制删除被卡在“终止”状态的Pod来解决这个问题。
- en: Note
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The behavior of the Pod stuck in the `Terminating` state is not a bug. This
    is default Kubernetes behavior. The Kubernetes documentation states the following:
    "Kubernetes (versions 1.5 or newer) will not delete Pods just because a Node is
    unreachable. The Pods running on an unreachable Node enter the `Terminating` or
    `Unknown` state after a timeout. Pods may also enter these states when the user
    attempts the graceful deletion of a Pod on an unreachable Node." You can read
    more at [https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 处于“终止”状态的Pod的行为不是一个错误。这是默认的Kubernetes行为。Kubernetes文档中指出：“Kubernetes（1.5版本或更新版本）不会仅仅因为节点不可达而删除Pods。在不可达节点上运行的Pods在超时后进入“终止”或“未知”状态。当用户尝试在不可达节点上优雅地删除Pod时，Pods也可能进入这些状态。”您可以在[https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/)阅读更多内容。
- en: To do that, we'll need the scale set's name and the resource group name. To
    find those, look for the scale set in the portal as shown in *Figure 5.35*:![Type
    vmss in the Azure search bar to look for the ScaleSet that is backing your cluster](image/Figure_5.35.jpg)
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，我们需要规模集的名称和资源组的名称。要找到这些信息，请在门户中查找规模集，如*图5.35*所示：![在Azure搜索栏中键入vmss以查找支持您集群的ScaleSet](image/Figure_5.35.jpg)
- en: 'Figure 5.35: Look for the scale set backing your cluster'
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.35：查找支持您集群的规模集
- en: 'In the scale set view, copy and paste the scale set name and the resource group.
    Edit the following command to detach the disk from your failed node and then run
    this command in Cloud Shell:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在规模集视图中，复制并粘贴规模集名称和资源组。编辑以下命令以从失败的节点分离磁盘，然后在Cloud Shell中运行此命令：
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will detach the disk from node 0\. The second step required here is the
    forceful removal of the Pod from the cluster, while it is stuck in the terminating
    state:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将从节点0中分离磁盘。这里需要的第二步是在Pod被卡在终止状态时，强制将其从集群中移除：
- en: '[PRE32]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will bring our new Pod to a healthy state. It will take a couple of minutes
    for the system to pick up the changes and then mount and schedule the new Pod.
    Let''s get the details of the Pod again using the following command:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将使我们的新Pod恢复到健康状态。系统需要几分钟来接受更改，然后挂载和调度新的Pod。让我们再次使用以下命令获取Pod的详细信息：
- en: '[PRE33]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will generate an output as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![The output will display the event types for both the Pods as Normal. The
    reason is SuccessfulAttachVolume and Pulled for the second Pod.](image/Figure_5.36.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![输出将显示两个Pod的事件类型为Normal。原因是SuccessfulAttachVolume和Pulled的第二个Pod。](image/Figure_5.36.jpg)'
- en: 'Figure 5.36: Our new Pod is now attaching the volume and pulling the container
    image'
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.36：我们的新Pod现在正在挂载卷并拉取容器镜像
- en: 'This shows us that the new Pod successfully got the volume attached and that
    the container image got pulled. Let''s verify that the Pod is actually running:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这表明新的Pod成功挂载了卷，并且容器镜像已被拉取。让我们验证一下Pod是否真的在运行：
- en: '[PRE34]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will show the Pods running as shown in *Figure 5.37*:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示Pod正在运行，如*图5.37*所示：
- en: '![The status of the two Pods is Running.](image/Figure_5.37.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![两个Pod的状态为Running。](image/Figure_5.37.jpg)'
- en: 'Figure 5.37: Both Pods are successfully running'
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.37：两个Pod都成功运行
- en: And this should make your WordPress website available again.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使您的WordPress网站再次可用。
- en: 'Before continuing, let''s clean up our deployment using the following command:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们使用以下命令清理我们的部署：
- en: '[PRE35]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s also restart the node that was shut down, as shown in *Figure 5.38*:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还重新启动关闭的节点，如*图5.38*所示：
- en: '![To restart the node that was shutdown, click on the Instances tab in the
    Navigation pane. Select the node that is deallocated/stopped. Click on the Start
    button in the toolbar alongside the search bar. Your node should now be restarted.](image/Figure_5.38.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![要重新启动关闭的节点，请单击导航窗格中的实例选项卡。选择已分配/已停止的节点。单击搜索栏旁边的工具栏中的启动按钮。您的节点现在应该已重新启动。](image/Figure_5.38.jpg)'
- en: 'Figure 5.38: Starting node 1 again'
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.38：重新启动节点1
- en: In this section, we covered how you can recover from a node failure when PVCs
    aren't mounted to new Pods. We needed to manually unmount the disk and then forcefully
    delete the Pod that was stuck in a `Terminating` state.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了当PVC未挂载到新的Pod时，您如何从节点故障中恢复。我们需要手动卸载磁盘，然后强制删除处于“Terminating”状态的Pod。
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about common Kubernetes failure modes and how you
    can recover from these. We started this chapter with an example on how Kubernetes
    automatically detects node failures and how it will start new Pods to recover
    the workload. After that, you scaled out your workload and had your cluster run
    out of resources. You recovered from that situation by starting the failed node
    again to add new resources to the cluster.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了常见的Kubernetes故障模式以及如何从中恢复。我们从一个示例开始，介绍了Kubernetes如何自动检测节点故障并启动新的Pod来恢复工作负载。之后，您扩展了工作负载，导致集群资源耗尽。您通过重新启动故障节点来为集群添加新资源，从这种情况中恢复了过来。
- en: Next, you saw how PVs are useful to store data outside of a Pod. You shut down
    all Pods on the cluster and saw how the PV ensured that no data was lost in your
    application. In the final example in this chapter, you saw how you can recover
    from a node failure when PVs are attached. You were able to recover the workload
    by unmounting the disk from the node and forcefully deleting the terminating Pod.
    This brought your workload back to a healthy state.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您看到了PV是如何有用地将数据存储在Pod之外的。您关闭了集群上的所有Pod，并看到PV确保应用程序中没有数据丢失。在本章的最后一个示例中，您看到了当PV被附加时，您如何从节点故障中恢复。您通过从节点卸载磁盘并强制删除终止的Pod来恢复工作负载。这将使您的工作负载恢复到健康状态。
- en: This chapter has explained common failure modes in Kubernetes. In the next chapter,
    we will introduce HTTPS support to our services and introduce authentication with
    Azure Active Directory.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已经解释了Kubernetes中常见的故障模式。在下一章中，我们将为我们的服务引入HTTPS支持，并介绍与Azure活动目录的身份验证。
