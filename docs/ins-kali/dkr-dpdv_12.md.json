["```\n    $ docker swarm init `\\`\n      --advertise-addr `10`.0.0.1:2377 `\\`\n      --listen-addr `10`.0.0.1:2377\n\n    Swarm initialized: current node `(`d21lyz...c79qzkx`)` is now a manager. \n    ```", "```\n    $ docker node ls\n    ID            HOSTNAME   STATUS  AVAILABILITY  MANAGER STATUS\n    d21...qzkx *  mgr1       Ready   Active        Leader \n    ```", "```\n    $ docker swarm join-token worker\n    To add a manager to this swarm, run the following command:\n       docker swarm join `\\`\n       --token SWMTKN-1-0uahebax...c87tu8dx2c `\\`\n       `10`.0.0.1:2377\n\n    $ docker swarm join-token manager\n    To add a manager to this swarm, run the following command:\n       docker swarm join `\\`\n       --token SWMTKN-1-0uahebax...ue4hv6ps3p `\\`\n       `10`.0.0.1:2377 \n    ```", "```\n    $ docker swarm join `\\`\n        --token SWMTKN-1-0uahebax...c87tu8dx2c `\\`\n        `10`.0.0.1:2377 `\\`\n        --advertise-addr `10`.0.0.4:2377 `\\`\n        --listen-addr `10`.0.0.4:2377\n\n    This node joined a swarm as a worker. \n    ```", "```\n    $ docker swarm join `\\`\n        --token SWMTKN-1-0uahebax...ue4hv6ps3p `\\`\n        `10`.0.0.1:2377 `\\`\n        --advertise-addr `10`.0.0.2:2377 `\\`\n        --listen-addr `10`.0.0.1:2377\n\n    This node joined a swarm as a manager. \n    ```", "```\n    $ docker node ls\n    ID               HOSTNAME     STATUS  AVAILABILITY  MANAGER STATUS\n    0g4rl...babl8 *  mgr2         Ready   Active        Reachable\n    2xlti...l0nyp    mgr3         Ready   Active        Reachable\n    8yv0b...wmr67    wrk1         Ready   Active\n    9mzwf...e4m4n    wrk3         Ready   Active\n    d21ly...9qzkx    mgr1         Ready   Active        Leader\n    e62gf...l5wt6    wrk2         Ready   Active \n    ```", "````` \n\n ```Congratulations! You\u2019ve just created a 6-node swarm with 3 managers and 3 workers. As part of the process you put the Docker Engine on each node into *swarm mode*. As a bonus, the *swarm* is automatically secured with TLS.\n\nIf you look in the `MANAGER STATUS` column you\u2019ll see that the three manager nodes are showing as either \u201cReachable\u201d or \u201cLeader\u201d. We\u2019ll learn more about leaders shortly. Nodes with nothing in the `MANAGER STATUS` column are *workers*. Also note the asterisk (`*`) after the ID on the line showing **mgr2**. This shows us which node we ran the `docker node ls` command from. In this instance the command was issued from **mgr2**.\n\n> **Note:** It\u2019s a pain to specify the `--advertise-addr` and `--listen-addr` flags every time you join a node to the swarm. However, it can be a much bigger pain if you get the network configuration of your swarm wrong. Also, manually adding nodes to a swarm is unlikely to be a daily task, so I think it\u2019s worth the extra up-front effort to use the flags. It\u2019s your choice though. In lab environments or nodes with only a single IP you probably don\u2019t need to use them.\n\nNow that we have a *swarm* up and running, let\u2019s take a look at manager high availability (HA).\n\n#### Swarm manager high availability (HA)\n\nSo far, we\u2019ve added three manager nodes to a swarm. Why did we add three, and how do they work together? We\u2019ll answer all of this, plus more in this section.\n\nSwarm *managers* have native support for high availability (HA). This means one or more can fail, and the survivors will keep the swarm running.\n\nTechnically speaking, swarm implements a form of active-passive multi-manager HA. This means that although you might \u2014 and should \u2014 have multiple *managers*, only one of them is ever considered *active*. We call this active manager the \u201c*leader*\u201d, and the leader\u2019s the only one that will ever issue live commands against the *swarm*. So it\u2019s only ever the leader that changes the config, or issues tasks to workers. If a passive (non-active) manager receives commands for the swarm, it proxies them across to the leader.\n\nThis process is shown in Figure 10.4\\. Step `1` is the command coming in to a *manager* from a remote Docker client. Step 2 is the non-leader manager proxying the command to the leader. Step 3 is the leader executing the command on the swarm.\n\n![Figure 10.4](images/figure10-4.png)\n\nFigure 10.4\n\nIf you look closely at Figure 10.4 you\u2019ll notice that managers are either *leaders* or *followers*. This is Raft terminology, because swarm uses an implementation of the [Raft consensus algorithm](https://raft.github.io/) to power manager HA. And on the topic of HA, the following two best practices apply:\n\n1.  Deploy an odd number of managers.\n2.  Don\u2019t deploy too many managers (3 or 5 is recommended)\n\nHaving an odd number of *managers* reduces the chances of split-brain conditions. For example, if you had 4 managers and the network partitioned, you could be left with two managers on each side of the partition. This is known as a split brain \u2014 each side knows there used to be 4 but can now only see 2\\. But crucially, neither side has any way of knowing if the other two are still alive and whether it holds a majority (quorum). The cluster continues to operate during split-brain conditions, but you are no longer able to alter the configuration or add and manage application workloads.\n\nHowever, if you had 3 or 5 managers and the same network partition occurred, it would be impossible to have the same number of managers on both sides of the partition. This means that one side achieve quorum and cluster management would remain available. The example on the right side of Figure 10.5 shows a partitioned cluster where the left side of the split knows it has a majority of managers.\n\n![Figure 10.5](images/figure10-5.png)\n\nFigure 10.5\n\nAs with all consensus algorithms, more participants means more time required to achieve consensus. It\u2019s like deciding where to eat \u2014 it\u2019s always quicker and easier to decide with 3 people than it is with 33! With this in mind, it\u2019s a best practice to have either 3 or 5 managers for HA. 7 might work, but it\u2019s generally accepted that 3 or 5 is optimal. You definitely don\u2019t want more than 7, as the time taken to achieve consensus will be longer.\n\nA final word of caution regarding manager HA. While it\u2019s obviously a good practice to spread your managers across availability zones within your network, you need to make sure that the networks connecting them are reliable! Network partitions can be a royal pain in the backside! This means, at the time of writing, the nirvana of hosting your active production applications and infrastructure across multiple cloud providers such as AWS and Azure is a bit of a daydream. Take time to make sure your managers are connected via reliable high-speed networks!\n\n##### Built-in Swarm security\n\nSwarm clusters have a ton of built-in security that\u2019s configured out-of-the-box with sensible defaults \u2014 CA settings, join tokens, mutual TLS, encrypted cluster store, encrypted networks, cryptographic node ID\u2019s and more. See **Chapter 15: Security in Docker** for a detailed look at these.\n\n##### Locking a Swarm\n\nDespite all of this built-in native security, restarting an older manager or restoring an old backup has the potential to compromise the cluster. Old managers re-joining a swarm automatically decrypt and gain access to the Raft log time-series database \u2014 this can pose security concerns. Restoring old backups can wipe the current swarm configuration.\n\nTo prevent situations like these, Docker allows you to lock a swarm with the Autolock feature. This forces managers that have been restarted to present the cluster unlock key before being permitted back into the cluster.\n\nIt\u2019s possible to apply a lock directly to a new swarm you are creating by passing the `--autolock` flag to the `docker swarm init` command. However, we\u2019ve already built a swarm, so we\u2019ll lock our existing swarm with the `docker swarm update` command.\n\nRun the following command from a swarm manager.\n\n```\n$ docker swarm update --autolock`=``true`\nSwarm updated.\nTo unlock a swarm manager after it restarts, run the ```docker swarm\nunlock````command` and provide the following key:\n\n    SWMKEY-1-5+ICW2kRxPxZrVyBDWzBkzZdSd0Yc7Cl2o4Uuf9NPU4\n\nPlease remember to store this key in a password manager, since without\nit you will not be able to restart the manager. \n```\n\n `Be sure to keep the unlock key in a secure place!\n\nRestart one of your manager nodes to see if it automatically re-joins the cluster. You may need to prepend the command with `sudo`.\n\n```\n$ service docker restart \n```\n\n `Try and list the nodes in the swarm.\n\n```\n$ docker node ls\nError response from daemon: Swarm is encrypted and needs to be unlocked befo`\\`\nre\nit can be used. \n```\n\n `Although the Docker service has restarted on the manager, it has not been allowed to re-join the cluster. You can prove this even further by running the `docker node ls` command on another manager node. The restarted manager will show as `down` and `unreachable`.\n\nUse the `docker swarm unlock` command to unlock the swarm for the restarted manager. You\u2019ll need to run this command on the restarted manager, and you\u2019ll need to provide the unlock key.\n\n```\n$ docker swarm unlock\nPlease enter unlock key: <enter your key> \n```\n\n `The node will be allowed to re-join the swarm, and will show as `ready` and `reachable` if you run another `docker node ls`.\n\nLocking your swarm and protecting the unlock key is recommended for production environments.\n\nNow that we\u2019ve got our *swarm* built, and we understand the concepts of *leaders* and *manager HA*, let\u2019s move on to *services*.\n\n#### Swarm services\n\nEverything we do in this section of the chapter gets improved on by Docker Stacks (Chapter 14). However, it\u2019s important that you learn the concepts here so that you\u2019re prepared for Chapter 14.\n\nLike we said in the *swarm primer*\u2026 *services* are a new construct introduced with Docker 1.12, and they only exist in *swarm mode*.\n\nThey let us specify most of the familiar container options, such as *name, port mappings, attaching to networks,* and *images*. But they add things, like letting us declare the *desired state* for an application service, feed that to Docker, and let Docker take care of deploying it and managing it. For example, assume you\u2019ve got an app with a web front-end. You have an image for it, and testing has shown that you\u2019ll need 5 instances to handle normal daily traffic. You would translate this requirement into a single *service* declaring the image the containers should use, and that the service should always have 5 running replicas.\n\nWe\u2019ll see some of the other things that can be declared as part of a service in a minute, but before we do that, let\u2019s see how to create what we just described.\n\nYou create a new service with the `docker service create` command.\n\n> **Note:** The command to create a new service is the same on Windows. However, the image used in this example is a Linux image and will not work on Windows. You can substitute the image for a Windows web server image and the command will work. Remember, if you are typing Windows commands from a PowerShell terminal you will need to use the backtick (`) to indicate continuation on the next line.\n\n```\n$ docker service create --name web-fe `\\`\n   -p `8080`:8080 `\\`\n   --replicas `5` `\\`\n   nigelpoulton/pluralsight-docker-ci\n\nz7ovearqmruwk0u2vc5o7ql0p \n```\n\n `Notice that many of the familiar `docker container run` arguments are the same. In the example, we specified `--name` and `-p` which work the same for standalone containers as well as services.\n\nLet\u2019s review the command and output.\n\nWe used `docker service create` to tell Docker we are declaring a new service, and we used the `--name` flag to name it **web-fe**. We told Docker to map port 8080 on every node in the swarm to 8080 inside of each service replica. Next, we used the `--replicas` flag to tell Docker that there should always be 5 replicas of this service. Finally, we told Docker which image to use for the replicas \u2014 it\u2019s important to understand that all service replicas use the same image and config!\n\nAfter we hit `Return`, the manager acting as leader instantiated 5 replicas across the *swarm* \u2014 remember that swarm managers also act as workers. Each worker or manager then pulled the image and started a container from it running on port 8080\\. The swarm leader also ensured a copy of the service\u2019s desired state was stored on the cluster and replicated to every manager in the swarm.\n\nBut this isn\u2019t the end. All *services* are constantly monitored by the swarm \u2014 the swarm runs a background *reconciliation loop* that constantly compares the *actual state* of the service to the *desired state*. If the two states match, the world is a happy place and no further action is needed. If they don\u2019t match, swarm takes actions so that they do. Put another way, the swarm is constantly making sure that *actual state* matches *desired state*.\n\nAs an example, if a *worker* hosting one of the 5 **web-fe** replicas fails, the *actual state* for the **web-fe** service will drop from 5 replicas to 4\\. This will no longer match the *desired state* of 5, so Docker will start a new **web-fe** replica to bring *actual state* back in line with *desired state*. This behavior is very powerful and allows the service to self-heal in the event of node failures and the likes.\n\n#### Viewing and inspecting services\n\nYou can use the `docker service ls` command to see a list of all services running on a swarm.\n\n```\n$ docker service ls\nID        NAME     MODE        REPLICAS   IMAGE               PORTS\nz7o...uw  web-fe   replicated  `5`/5        nigel...ci:latest   *:8080->8080/t`\\`\ncp \n```\n\n `The output above shows a single running service as well as some basic information about state. Among other things, we can see the name of the service and that 5 out of the 5 desired replicas are in the running state. If you run this command soon after deploying the service it might not show all tasks/replicas as running. This is often due to the time it takes to pull the image on each node.\n\nYou can use the `docker service ps` command to see a list of service replicas and the state of each.\n\n```\n$ docker service ps web-fe\nID         NAME      IMAGE             NODE  DESIRED  CURRENT\n`817`...f6z  web-fe.1  nigelpoulton/...  mgr2  Running  Running `2` mins\na1d...mzn  web-fe.2  nigelpoulton/...  wrk1  Running  Running `2` mins\ncc0...ar0  web-fe.3  nigelpoulton/...  wrk2  Running  Running `2` mins\n6f0...azu  web-fe.4  nigelpoulton/...  mgr3  Running  Running `2` mins\ndyl...p3e  web-fe.5  nigelpoulton/...  mgr1  Running  Running `2` mins \n```\n\n `The format of the command is `docker service ps <service-name or service-id>`. The output displays each replica (container) on its own line, shows which node in the swarm it\u2019s executing on, and shows desired state and actual state.\n\nFor detailed information about a service, use the `docker service inspect` command.\n\n```\n$ docker service inspect --pretty web-fe\nID:             z7ovearqmruwk0u2vc5o7ql0p\nName:           web-fe\nService Mode:   Replicated\n Replicas:      `5`\nPlacement:\nUpdateConfig:\n Parallelism:   `1`\n On failure:    pause\n Monitoring Period: 5s\n Max failure ratio: `0`\n Update order:      stop-first\nRollbackConfig:\n Parallelism:   `1`\n On failure:    pause\n Monitoring Period: 5s\n Max failure ratio: `0`\n Rollback order:    stop-first\nContainerSpec:\n Image:   nigelpoulton/pluralsight-docker-ci:latest@sha256:7a6b01...d8d3d\nResources:\nEndpoint Mode:  vip\nPorts:\n `PublishedPort` `=` `8080`\n  `Protocol` `=` tcp\n  `TargetPort` `=` `8080`\n  `PublishMode` `=` ingress \n```\n\n `The example above uses the `--pretty` flag to limit the output to the most interesting items printed in an easy-to-read format. Leaving off the `--pretty` flag will give a more verbose output. I highly recommend you read through the output of `docker inspect` commands as they\u2019re a great source of information and a great way to learn what\u2019s going on under the hood.\n\nWe\u2019ll come back to some of these outputs later.\n\n#### Replicated vs global services\n\nThe default replication mode of a service is `replicated`. This will deploy a desired number of replicas and distribute them as evenly as possible across the cluster.\n\nThe other mode is `global`, which runs a single replica on every node in the swarm.\n\nTo deploy a *global service* you need to pass the `--mode global` flag to the `docker service create` command.\n\n#### Scaling a service\n\nAnother powerful feature of *services* is the ability to easily scale them up and down.\n\nLet\u2019s assume business is booming and we\u2019re seeing double the amount of traffic hitting the web front-end. Fortunately, scaling the **web-fe** service is as simple as running the `docker service scale` command.\n\n```\n$ docker service scale web-fe`=``10`\nweb-fe scaled to `10` \n```\n\n `This command will scale the number of service replicas from 5 to 10\\. In the background it\u2019s updating the service\u2019s *desired state* from 5 to 10\\. Run another `docker service ls` command to verify the operation was successful.\n\n```\n$ docker service ls\nID        NAME     MODE        REPLICAS   IMAGE               PORTS\nz7o...uw  web-fe   replicated  `10`/10      nigel...ci:latest   *:8080->8080/t`\\`\ncp \n```\n\n `Running a `docker service ps` command will show that the service replicas are balanced across all nodes in the swarm evenly.\n\n```\n$ docker service ps web-fe\nID         NAME      IMAGE             NODE  DESIRED  CURRENT\nnwf...tpn  web-fe.1  nigelpoulton/...  mgr1  Running  Running `7` mins\nyb0...e3e  web-fe.2  nigelpoulton/...  wrk3  Running  Running `7` mins\nmos...gf6  web-fe.3  nigelpoulton/...  wrk2  Running  Running `7` mins\nutn...6ak  web-fe.4  nigelpoulton/...  wrk3  Running  Running `7` mins\n2ge...fyy  web-fe.5  nigelpoulton/...  mgr3  Running  Running `7` mins\n64y...m49  web-fe.6  igelpoulton/...   wrk3  Running  Running about a min\nild...51s  web-fe.7  nigelpoulton/...  mgr1  Running  Running about a min\nvah...rjf  web-fe.8  nigelpoulton/...  wrk2  Running  Running about a mins\nxe7...fvu  web-fe.9  nigelpoulton/...  mgr2  Running  Running `45` seconds ago\nl7k...jkv  web-fe.10 nigelpoulton/...  mgr2  Running  Running `46` seconds ago \n```\n\n `Behind the scenes, swarm runs a scheduling algorithm that defaults to balancing replicas as evenly as possible across the nodes in the swarm. At the time of writing, this amounts to running an equal number of replicas on each node without taking into consideration things like CPU load etc.\n\nRun another `docker service scale` command to bring the number back down from 10 to 5.\n\n```\n$ docker service scale web-fe`=``5`\nweb-fe scaled to `5` \n```\n\n `Now that we know how to scale a service, let\u2019s see how we remove one.\n\n#### Removing a service\n\nRemoving a service is simple \u2014 may be too simple.\n\nThe following `docker service rm` command will delete the service deployed earlier.\n\n```\n$ docker service rm web-fe\nweb-fe \n```\n\n `Confirm it\u2019s gone with the `docker service ls` command.\n\n```\n$ docker service ls\nID      NAME    MODE   REPLICAS    IMAGE      PORTS \n```\n\n `Be careful using the `docker service rm` command, as it deletes all service replicas without asking for confirmation.\n\nNow that the service is deleted from the system, let\u2019s look at how to push rolling updates to one.\n\n#### Rolling updates\n\nPushing updates to deployed applications is a fact of life. And for the longest time it\u2019s been really painful. I\u2019ve lost more than enough weekends to major application updates, and I\u2019ve no intention of doing it again.\n\nWell\u2026 thanks to Docker *services*, pushing updates to well-designed apps just got a lot easier!\n\nTo see this, we\u2019re going to deploy a new service. But before we do that we\u2019re going to create a new overlay network for the service. This isn\u2019t necessary, but I want you to see how it is done and how to attach the service to it.\n\n```\n$ docker network create -d overlay uber-net\n43wfp6pzea470et4d57udn9ws \n```\n\n `This creates a new overlay network called \u201cuber-net\u201d that we\u2019ll be able to leverage with the service we\u2019re about to create. An overlay network creates a new layer 2 network that we can place containers on, and all containers on it will be able to communicate. This works even if the Docker hosts the containers are running on are on different underlying networks. Basically, the overlay network creates a new layer 2 container network on top of potentially multiple different underlying networks.\n\nFigure 10.6 shows two underlay networks connected by a layer 3 router. There is then a single overlay network across both. Docker hosts are connected to the two underlay networks and containers are connected to the overlay. All containers on the overlay can communicate even if they are on Docker hosts plumbed into different underlay networks.\n\n![Figure 10.6](images/figure10-6.png)\n\nFigure 10.6\n\nRun a `docker network ls` to verify that the network created properly and is visible on the Docker host.\n\n```\n$ docker network ls\nNETWORK ID          NAME                DRIVER      SCOPE\n<Snip>\n43wfp6pzea47        uber-net            overlay     swarm \n```\n\n `The `uber-net` network was successfully created with the `swarm` scope and is *currently* only visible on manager nodes in the swarm.\n\nLet\u2019s create a new service and attach it to the network.\n\n```\n$ docker service create --name uber-svc `\\`\n   --network uber-net `\\`\n   -p `80`:80 --replicas `12` `\\`\n   nigelpoulton/tu-demo:v1\n\ndhbtgvqrg2q4sg07ttfuhg8nz \n```\n\n `Let\u2019s see what we just declared with that `docker service create` command.\n\nThe first thing we did was name the service and then use the `--network` flag to tell it to place all replicas on the new `uber-net` network. We then exposed port 80 across the entire swarm and mapped it to port 80 inside of each of the 12 replicas we asked it to run. Finally, we told it to base all replicas on the nigelpoulton/tu-demo:v1 image.\n\nRun a `docker service ls` and a `docker service ps` command to verify the state of the new service.\n\n```\n$ docker service ls\nID            NAME      REPLICAS  IMAGE\ndhbtgvqrg2q4  uber-svc  `12`/12     nigelpoulton/tu-demo:v1\n\n$ docker service ps uber-svc\nID        NAME          IMAGE                NODE  DESIRED   CURRENT STATE\n0v...7e5  uber-svc.1    nigelpoulton/...:v1  wrk3  Running   Running `1` min\nbh...wa0  uber-svc.2    nigelpoulton/...:v1  wrk2  Running   Running `1` min\n`23`...u97  uber-svc.3    nigelpoulton/...:v1  wrk2  Running   Running `1` min\n`82`...5y1  uber-svc.4    nigelpoulton/...:v1  mgr2  Running   Running `1` min\nc3...gny  uber-svc.5    nigelpoulton/...:v1  wrk3  Running   Running `1` min\ne6...3u0  uber-svc.6    nigelpoulton/...:v1  wrk1  Running   Running `1` min\n`78`...r7z  uber-svc.7    nigelpoulton/...:v1  wrk1  Running   Running `1` min\n2m...kdz  uber-svc.8    nigelpoulton/...:v1  mgr3  Running   Running `1` min\nb9...k7w  uber-svc.9    nigelpoulton/...:v1  mgr3  Running   Running `1` min\nag...v16  uber-svc.10   nigelpoulton/...:v1  mgr2  Running   Running `1` min\ne6...dfk  uber-svc.11   nigelpoulton/...:v1  mgr1  Running   Running `1` min\ne2...k1j  uber-svc.12   nigelpoulton/...:v1  mgr1  Running   Running `1` min \n```\n\n `Passing the service the `-p 80:80` flag will ensure that a **swarm-wide** mapping is created that maps all traffic, coming in to any node in the swarm on port 80, through to port 80 inside of any service replica.\n\nThis mode of publishing a port on every node in the swarm \u2014 even nodes not running service replicas \u2014 is called *ingress mode* and is the default. The alternative mode is *host mode* which only publishes the service on swarm nodes running replicas. Publishing a service in *host mode* requires the long-form syntax and looks like the following:\n\n```\ndocker service create --name uber-svc \\\n   --network uber-net \\\n   --publish published=80,target=80,mode=host \\\n   --replicas 12 \\\n   nigelpoulton/tu-demo:v1 \n```\n\n `Open a web browser and point it to the IP address of any of the nodes in the swarm on port 80 to see the service running.\n\n![Figure 10.7](images/figure10-7.png)\n\nFigure 10.7\n\nAs you can see, it\u2019s a simple voting application that will register votes for either \u201cfootball\u201d or \u201csoccer\u201d. Feel free to point your web browser to other nodes in the swarm. You\u2019ll be able to reach the web service from any node because the `-p 80:80` flag creates an *ingress mode* mapping on every swarm node. This is true even on nodes that are not running a replica for the service \u2014 **every node gets a mapping and can therefore redirect your request to a node that runs the service**.\n\nNow let\u2019s assume that this particular vote has come to an end and your company is wants to run a new poll. A new image has been created for the new poll and has been added to the same Docker Hub repository, but this one is tagged as `v2` instead of `v1`.\n\nLet\u2019s also assume that you\u2019ve been tasked with pushing the updated image to the swarm in a staged manner \u2014 2 replicas at a time with a 20 second delay between each. We can use the following `docker service update` command to accomplish this.\n\n```\n$ docker service update `\\`\n   --image nigelpoulton/tu-demo:v2 `\\`\n   --update-parallelism `2` `\\`\n   --update-delay 20s uber-svc \n```\n\n `Let\u2019s review the command. `docker service update` lets us make updates to running services by updating the service\u2019s desired state. This time we gave it a new image tag `v2` instead of `v1`. And we used the `--update-parallelism` and the `--update-delay` flags to make sure that the new image was pushed to 2 replicas at a time with a 20 second cool-off period in between each. Finally, we told Docker to make these changes to the `uber-svc` service.\n\nIf we run a `docker service ps` against the service we\u2019ll see that some of the replicas are at `v2` while some are still at `v1`. If we give the operation enough time to complete (4 minutes) all replicas will eventually reach the new desired state of using the `v2` image.\n\n```\n$ docker service ps uber-svc\nID        NAME              IMAGE        NODE   DESIRED   CURRENT STATE\n7z...nys  uber-svc.1    nigel...v2   mgr2  Running   Running `13` secs\n0v...7e5  `\\_`uber-svc.1  nigel...v1   wrk3  Shutdown  Shutdown `13` secs\nbh...wa0  uber-svc.2    nigel...v1   wrk2  Running   Running `1` min\ne3...gr2  uber-svc.3    nigel...v2   wrk2  Running   Running `13` secs\n`23`...u97  `\\_`uber-svc.3  nigel...v1   wrk2  Shutdown  Shutdown `13` secs\n`82`...5y1  uber-svc.4    nigel...v1   mgr2  Running   Running `1` min\nc3...gny  uber-svc.5    nigel...v1   wrk3  Running   Running `1` min\ne6...3u0  uber-svc.6    nigel...v1   wrk1  Running   Running `1` min\n`78`...r7z  uber-svc.7    nigel...v1   wrk1  Running   Running `1` min\n2m...kdz  uber-svc.8    nigel...v1   mgr3  Running   Running `1` min\nb9...k7w  uber-svc.9    nigel...v1   mgr3  Running   Running `1` min\nag...v16  uber-svc.10   nigel...v1   mgr2  Running   Running `1` min\ne6...dfk  uber-svc.11   nigel...v1   mgr1  Running   Running `1` min\ne2...k1j  uber-svc.12   nigel...v1   mgr1  Running   Running `1` min \n```\n\n `You can witness the update happening in real-time by opening a web browser to any node in the swarm and hitting refresh several times. Some of the requests will be serviced by replicas running the old version and some will be serviced by replicas running the new version. After enough time, all requests will be serviced by replicas running the updated version of the service.\n\nCongratulations. You\u2019ve just pushed a rolling update to a live containerized application. Remember, Docker Stacks take all of this to the next level in Chapter 14.\n\nIf you run a `docker inspect --pretty` command against the service, you\u2019ll see the update parallelism and update delay settings are now part of the service definition. This means future updates will automatically use these settings unless you override them as part of the `docker service update` command.\n\n```\n$ docker service inspect --pretty uber-svc\nID:             mub0dgtc8szm80ez5bs8wlt19\nName:           uber-svc\nService Mode:   Replicated\n Replicas:      `12`\nUpdateStatus:\n State:         updating\n Started:       About a minute\n Message:       update in progress\nPlacement:\nUpdateConfig:\n Parallelism:   `2`\n Delay:         20s\n On failure:    pause\n Monitoring Period: 5s\n Max failure ratio: `0`\n Update order:      stop-first\nRollbackConfig:\n Parallelism:   `1`\n On failure:    pause\n Monitoring Period: 5s\n Max failure ratio: `0`\n Rollback order:    stop-first\nContainerSpec:\n Image:    nigelpoulton/tu-demo:v2@sha256:d3c0d8c9...cf0ef2ba5eb74c\nResources:\nNetworks: uber-net\nEndpoint Mode:  vip\nPorts:\n `PublishedPort` `=` `80`\n  `Protocol` `=` tcp\n  `TargetPort` `=` `80`\n  `PublishMode` `=` ingress \n```\n\n `You should also note a couple of things about the service\u2019s network config. All nodes in the swarm that are running a replica for the service will have the `uber-net` overlay network that we created earlier. We can verify this by running `docker network ls` on any node running a replica.\n\nYou should also note the `Networks` portion of the `docker inspect` output. This shows the `uber-net` network as well as the swarm-wide `80:80` port mapping.\n\n#### Troubleshooting\n\nSwarm Service logs can be viewed with the `docker service logs` command. However, not all logging drivers support the command.\n\nBy default, Docker nodes configure services to use the `json-file` log driver, but other drivers exist, including:\n\n*   `journald` (only works on Linux hosts running `systemd`)\n*   `syslog`\n*   `splunk`\n*   `gelf`\n\n`json-file` and `journald` are the easiest to configure, and both work with the `docker service logs` command. The format of the command is `docker service logs <service-name>`.\n\nIf you\u2019re using 3rd-party logging drivers you should view those logs using the logging platform\u2019s native tools.\n\nThe following snippet from a `daemon.json` configuration file shows a Docker host configured to use `syslog`.\n\n```\n{\n  \"log-driver\": \"syslog\"\n} \n```\n\n `You can force individual services to use a different driver by passing the `--log-driver` and `--log-opts` flags to the `docker service create` command. These will override anything set in `daemon.json`.\n\nService logs work on the premise that your application is running as PID 1 in its container and sending logs to `STDOUT`, and errors to `STDERR`. The logging driver forwards these \u201clogs\u201d to the locations configured via the logging driver.\n\nThe following `docker service logs` command shows the logs for all replicas in the `svc1` service that experienced a couple of failures starting a replica.\n\n```\n$ docker service logs seastack_reverse_proxy\nsvc1.1.zhc3cjeti9d4@wrk-2 `|` `[`emerg`]` `1``#1: host not found...`\nsvc1.1.6m1nmbzmwh2d@wrk-2 `|` `[`emerg`]` `1``#1: host not found...`\nsvc1.1.6m1nmbzmwh2d@wrk-2 `|` nginx: `[`emerg`]` host not found..\nsvc1.1.zhc3cjeti9d4@wrk-2 `|` nginx: `[`emerg`]` host not found..\nsvc1.1.1tmya243m5um@mgr-1 `|` `10`.255.0.2 `\"GET / HTTP/1.1\"` `302` \n```\n\n `The output is trimmed to fit the page, but you can see that logs from all three service replicas are shown (the two that failed and the one that\u2019s running). Each line starts with the name of the replica, which includes the service name, replica number, replica ID, and name of host that it\u2019s scheduled on. Following that is the log output.\n\nIt\u2019s hard to tell because it\u2019s trimmed to fit the book, but it looks like the first two replicas failed because they were trying to connect to another service that was still starting (a sort of race condition when dependent services are starting).\n\nYou can follow the logs (`--follow`), tail them (`--tail`), and get extra details (`--details`).\n\n### Docker Swarm - The Commands\n\n*   `docker swarm init` is the command to create a new swarm. The node that you run the command on becomes the first manager and is switched to run in *swarm mode*.\n*   `docker swarm join-token` reveals the commands and tokens needed to join workers and managers to existing swarms. To expose the command to join a new manager, use the `docker swarm join-token manager` command. To get the command to join a worker, use the `docker swarm join-token worker` command.\n*   `docker node ls` lists all nodes in the swarm including which are managers and which is the leader.\n*   `docker service create` is the command to create a new service.\n*   `docker service ls` lists running services in the swarm and gives basic info on the state of the service and any replicas it\u2019s running.\n*   `docker service ps <service>` gives more detailed information about individual service replicas.\n*   `docker service inspect` gives very detailed information on a service. It accepts the `--pretty` flag to limit the information returned to the most important information.\n*   `docker service scale` lets you scale the number of replicas in a service up and down.\n*   `docker service update` lets you update many of the properties of a running service.\n*   `docker service logs` lets you view the logs of a service.\n*   `docker service rm` is the command to delete a service from the swarm. Use it with caution as it deletes all service replicas without asking for confirmation.\n\n### Chapter summary\n\nDocker swarm is key to the operation of Docker at scale.\n\nAt its core, swarm has a secure clustering component, and an orchestration component.\n\nThe secure clustering component is enterprise-grade and offers a wealth of security and HA features that are automatically configured and extremely simple to modify.\n\nThe orchestration component allows you to deploy and manage microservices applications in a simple declarative manner. Native Docker Swarm apps are supported, and so are Kubernetes apps.\n\nWe\u2019ll dig deeper into deploying microservices apps in a declarative manner in Chapter 14.`````", "`````````````````````"]