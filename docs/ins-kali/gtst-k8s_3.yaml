- en: Chapter 3. Core Concepts – Networking, Storage, and Advanced Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 核心概念-网络、存储和高级服务
- en: In this chapter, we will be covering how the Kubernetes cluster handles networking
    and how it differs from other approaches. We will be describing the three requirements
    for Kubernetes networking solutions and exploring why these are key to ease of
    operations. Further, we will take a deeper dive into services and how the Kubernetes
    proxy works on each node. Towards the end, we will take a look at storage concerns
    and how we can persist data across pods and the container life cycle. Finishing
    up, we will see a brief overview of some higher level isolation features for multitenancy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍Kubernetes集群如何处理网络以及与其他方法的不同之处。我们将描述Kubernetes网络解决方案的三个要求，并探讨为什么这些对操作的便利性至关重要。此外，我们将深入探讨服务以及Kubernetes代理在每个节点上的工作原理。最后，我们将看一下存储问题，以及如何在pod和容器生命周期中持久保存数据。最后，我们将简要概述一些用于多租户的更高级别隔离特性。
- en: 'This chapter will discuss the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下内容：
- en: Kubernetes networking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络
- en: Advanced services concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级服务概念
- en: Service discovery
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现
- en: DNS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS
- en: Persistent storage
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久存储
- en: Namespace limits and quotas
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名空间限制和配额
- en: Kubernetes networking
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络
- en: Networking is a vital concern for production-level operations. At a service
    level, we need a reliable way for our application components to find and communicate
    with each other. Introduce containers and clustering into the mix and things get
    more complex as we now have multiple networking namespaces to bear in mind. Communication
    and discovery now becomes a feat that must traverse container IP space, host networking,
    and sometimes even multiple data center network topologies.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是生产级操作的一个重要关注点。在服务级别上，我们需要一种可靠的方式让我们的应用组件找到并与彼此通信。引入容器和集群后，事情变得更加复杂，因为现在我们有多个网络命名空间需要考虑。通信和发现现在变成了一个必须穿越容器IP空间、主机网络，甚至有时甚至是多个数据中心网络拓扑的壮举。
- en: Kubernetes benefits here from getting its ancestry from the clustering tools
    used by Google for the past decade. Networking is one area where Google has outpaced
    the competition with one of the largest networks on the planet. Early on, Google
    built its own hardware switches and **Software-defined Networking** (**SDN**)
    to give them more control, redundancy, and efficiency in their day-to-day network
    operations¹. Many of the lessons learned from running and networking two billion
    containers per week have been distilled into Kubernetes and informed how K8s networking
    is done.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在这里受益于过去十年谷歌使用的集群工具的祖先。网络是谷歌在全球拥有最大网络之一的领域之一。早期，谷歌建立了自己的硬件交换机和软件定义网络（SDN），以便在日常网络操作中获得更多的控制、冗余和效率。从每周运行和网络化20亿个容器中学到的许多经验教训已经被提炼成了Kubernetes，并影响了K8s网络的实施方式。
- en: Networking in Kubernetes requires that each pod have its own IP address. Implementation
    details may vary based on the underlying infrastructure provider. However, all
    implementations must adhere to some basic rules. First and second, Kubernetes
    does not allow the use of **Network Address Translation** (**NAT**) for container-to-container
    or for container-to-node (minion) traffic. Further, the internal container IP
    address must match the IP address that is used to communicate with it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的网络需要每个pod有自己的IP地址。实施细节可能会根据基础设施提供商的不同而有所不同。然而，所有实施都必须遵守一些基本规则。首先，Kubernetes不允许使用网络地址转换（NAT）来进行容器对容器或容器对节点（minion）的流量。此外，内部容器IP地址必须与用于与其通信的IP地址匹配。
- en: These rules keep much of the complexity out of our networking stack and ease
    the design of the applications. Further, it eliminates the need to redesign network
    communication in legacy applications that are migrated from existing infrastructure.
    Finally, in greenfield applications, it allows for greater scale in handling hundreds,
    or even thousands, of services and application communication.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则使我们的网络堆栈中的大部分复杂性消失，并简化了应用程序的设计。此外，它消除了需要重新设计从现有基础设施迁移的遗留应用程序的网络通信。最后，在全新的应用程序中，它允许更大规模地处理数百甚至数千个服务和应用程序通信。
- en: K8s achieves this pod-wide IP magic by using a **placeholder**. Remember that
    `pause` container we saw in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    under the *Services running on the master* section. That is often referred to
    as a **pod infrastructure container**, and it has the important job of reserving
    the network resources for our application containers that will be started later
    on. Essentially, the pause container holds the networking namespace and IP address
    for the entire pod and can be used by all the containers running within.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: K8s通过使用**占位符**来实现这种整个pod的IP魔术。记得我们在[第1章](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第1章。Kubernetes和容器操作")中看到的`pause`容器吗，在*在主节点上运行的服务*部分。这经常被称为**pod基础设施容器**，它有一个重要的工作，为稍后启动的应用容器保留网络资源。基本上，暂停容器保存了整个pod的网络命名空间和IP地址，并且可以被所有运行其中的容器使用。
- en: Networking comparisons
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络比较
- en: In getting a better understanding of networking in containers, it can be instructive
    to look at other approaches to container networking.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解容器网络的过程中，看看其他容器网络方法可能会很有启发。
- en: Docker
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker
- en: The **Docker Engine** by default uses a *bridged* networking mode. In this mode,
    the container has its own networking namespace and is then bridged via virtual
    interfaces to the host (or node in the case of K8s) network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker Engine**默认使用*桥接*网络模式。在这种模式下，容器有自己的网络命名空间，然后通过虚拟接口桥接到主机（或者在K8s情况下是节点）网络。'
- en: In the *bridged* mode, two containers can use the same IP range because they
    are completely isolated. Therefore, service communication requires some additional
    port mapping through the host side of network interfaces.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在*桥接*模式下，两个容器可以使用相同的IP范围，因为它们是完全隔离的。因此，服务通信需要通过主机端的网络接口进行一些额外的端口映射。
- en: Docker also supports a *host* mode, which allows the containers to use the host
    network stack. Performance is greatly benefited since it removes a level of network
    virtualization; however, you lose the security of having an isolated network namespace.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Docker还支持*主机*模式，允许容器使用主机网络堆栈。性能大大受益，因为它消除了一层网络虚拟化；然而，您失去了具有隔离网络命名空间的安全性。
- en: Finally, Docker supports a *container* mode, which shares a network namespace
    between two containers. The containers will share the namespace and IP address,
    so containers cannot use the same ports.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Docker支持*容器*模式，它在两个容器之间共享网络命名空间。容器将共享命名空间和IP地址，因此容器不能使用相同的端口。
- en: In all these scenarios, we are still on a single machine, and outside of a host
    mode, the container IP space is not available outside that machine. Connecting
    containers across two machines then requires **Network Address Translation** (**NAT**)
    and **port mapping** for communication.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些场景中，我们仍然在单台机器上，除了主机模式外，容器IP空间在该机器之外是不可用的。跨两台机器连接容器然后需要**网络地址转换**（**NAT**）和**端口映射**进行通信。
- en: Docker plugins (libnetwork)
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker插件（libnetwork）
- en: In order to address the cross-machine communication issue, Docker has released
    new network plugins, which just moved out of experimental support as we went to
    press. This plugin allows networks to be created independent of the containers
    themselves. In this way, containers can join the same existing *networks*. Through
    the new plugin architecture, various drivers can be provided for different network
    use cases.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决跨机器通信问题，Docker发布了新的网络插件，这些插件在我们发行时刚刚退出了实验性支持。该插件允许独立于容器本身创建网络。通过这种方式，容器可以加入相同的*网络*。通过新的插件架构，可以为不同的网络用例提供各种驱动程序。
- en: The first of these is the **overlay** driver. In order to coordinate across
    multiple hosts, they must all agree on the available networks and their topologies.
    The overlay driver uses a distributed key-value store to synchronize the network
    creation across multiple hosts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中之一是**overlay**驱动程序。为了在多个主机之间协调，它们必须就可用的网络及其拓扑达成一致。overlay驱动程序使用分布式键值存储来在多个主机之间同步网络创建。
- en: It's important to note that the plugin mechanism will allow a wide range of
    networking possibilities in Docker. In fact, many of the third-party options such
    as Weave are already creating their own Docker network plugins.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，插件机制将允许在Docker中实现各种网络可能性。事实上，许多第三方选项，如Weave，已经在创建自己的Docker网络插件。
- en: Weave
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Weave
- en: '**Weave** provides an overlay network for Docker containers. It can be used
    as a plugin with the new Docker network plugin interface, and it is also compatible
    with Kubernetes. Like many overlay networks, many criticize the performance impact
    of the encapsulation overhead. Note that they have recently added a preview release
    with **Virtual Extensible LAN** (**VXLAN**) encapsulation support, which greatly
    improves performance. For more information, visit:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Weave**为Docker容器提供了一个overlay网络。它可以作为新的Docker网络插件接口的插件使用，并且与Kubernetes兼容。像许多overlay网络一样，许多人批评封装开销对性能的影响。请注意，他们最近添加了一个具有**Virtual
    Extensible LAN** (**VXLAN**)封装支持的预览版本，这大大提高了性能。有关更多信息，请访问：'
- en: '[http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/)'
- en: Flannel
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Flannel
- en: '**Flannel** comes from CoreOS and is an etcd-backed overlay. Flannel gives
    a full subnet to each host/node enabling a similar pattern to the Kubernetes practice
    of a routable IP per pod or group of containers. Flannel includes an in-kernel
    VXLAN encapsulation mode for better performance and has an experimental multinetwork
    mode similar to the overlay Docker plugin. For more information, visit:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Flannel**来自CoreOS，是一个基于etcd的overlay。Flannel为每个主机/节点提供了一个完整的子网，实现了类似于Kubernetes实践的每个pod或一组容器的可路由IP。Flannel包括一个内核中的VXLAN封装模式，以获得更好的性能，并且具有类似于overlay
    Docker插件的实验性多网络模式。有关更多信息，请访问：'
- en: '[https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
- en: Project Calico
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Project Calico
- en: '**Project Calico** is a layer 3-based networking model that uses the built-in
    routing functions of the Linux kernel. Routes are propagated to virtual routers
    on each host via **Border Gateway Protocol** (**BGP**). Calico can be used for
    anything from small-scale deploys to large Internet-scale installations. Because
    it works at a lower level on the network stack, there is no need for additional
    NAT, tunneling, or overlays. It can interact directly with the underlying network
    infrastructure. Additionally, it has a support for network-level ACLs to provide
    additional isolation and security. For more information visit:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**Project Calico**是一个基于第3层的网络模型，它使用Linux内核的内置路由功能。路由通过**边界网关协议**（**BGP**）传播到每个主机上的虚拟路由器。Calico可用于从小规模部署到大型互联网规模的安装。由于它在网络堆栈的较低级别工作，因此无需额外的NAT、隧道或覆盖。它可以直接与底层网络基础设施进行交互。此外，它支持网络级ACL以提供额外的隔离和安全性。欲了解更多信息，请访问：'
- en: '[http://www.projectcalico.org/](http://www.projectcalico.org/)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.projectcalico.org/](http://www.projectcalico.org/)'
- en: Balanced design
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平衡的设计
- en: It's important to point out the balance Kubernetes is trying to achieve by placing
    the IP at the pod level. Using unique IP addresses at the host level is problematic
    as the number of containers grow. Ports must be used to expose services on specific
    containers and allow external communication. In addition to this, the complexity
    of running multiple services that may or may not know about each other (and their
    custom ports), and managing the port space becomes a big issue.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes试图在pod级别放置IP地址来实现平衡是很重要的。在主机级别使用唯一的IP地址在容器数量增长时会出现问题。必须使用端口来暴露特定容器上的服务并允许外部通信。除此之外，运行可能知道彼此（及其自定义端口）的多个服务的复杂性，以及管理端口空间成为一个大问题。
- en: However, assigning an IP address to each container can be overkill. In cases
    of sizable scale, overlay networks and NATs are needed in order to address each
    container. Overlay networks add latency, and IP addresses would be taken up by
    backend services as well since they need to communicate with their frontend counterparts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为每个容器分配一个IP地址可能有些过度。在规模较大的情况下，需要覆盖网络和NAT，以便为每个容器分配IP地址。覆盖网络会增加延迟，并且由于后端服务需要与其前端对应方进行通信，IP地址也会被占用。
- en: Here, we really see an advantage in the abstractions that Kubernetes provides
    at the application and service level. If I have a web server and a database, we
    can keep them on the same pod and use a single IP address. The web server and
    database can use the local interface and standard ports to communicate, and no
    custom setup is required. Further, services on the backend are not needlessly
    exposed to other application stacks running elsewhere in the cluster (but possibly
    on the same host). Since the pod sees the same IP address that the applications
    running within it see, service discovery does not require any additional translation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们真的看到了Kubernetes在应用程序和服务级别提供的抽象的优势。如果我有一个Web服务器和一个数据库，我们可以将它们放在同一个pod中并使用单个IP地址。Web服务器和数据库可以使用本地接口和标准端口进行通信，而不需要自定义设置。此外，后端的服务不会被不必要地暴露给集群中其他应用程序堆栈（但可能在同一主机上）。由于pod看到的IP地址与其内部运行的应用程序看到的IP地址相同，因此服务发现不需要任何额外的转换。
- en: If you need the flexibility of an overlay network, you can still use an overlay
    at the pod level. Both Weave and Flannel overlays, as well as the BGP routing
    Project Calico, can be used with Kubernetes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要覆盖网络的灵活性，仍然可以在pod级别使用覆盖网络。Weave和Flannel覆盖网络，以及BGP路由项目Calico，都可以与Kubernetes一起使用。
- en: This is also very helpful in the context of scheduling the workloads. It is
    a key to have a simple and standard structure for the scheduler to match constraints
    and understand where space exists on the cluster's network at any given time.
    This is a dynamic environment with a variety of applications and tasks running,
    so any additional complexity here will have rippling effects.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这在调度工作负载的上下文中也非常有帮助。对于调度器来说，拥有一个简单和标准的结构来匹配约束并了解集群网络上的空间存在的位置是关键的。这是一个具有各种应用程序和任务运行的动态环境，因此在这里增加任何额外的复杂性都会产生连锁效应。
- en: There are also implications for service discovery. New services coming online
    must determine and register an IP address on which the rest of the world, or at
    least cluster, can reach them. If NAT is used, the services will need an additional
    mechanism to learn their externally facing IP.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这也对服务发现有影响。新上线的服务必须确定并注册一个IP地址，以便世界其他地方，或者至少是集群，可以访问它们。如果使用NAT，服务将需要额外的机制来学习它们的外部IP。
- en: Advanced services
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级服务
- en: Let's explore the IP strategy as it relates to Services and communication between
    containers. If you recall, in [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 2. Kubernetes – Core Concepts and Constructs"), *Kubernetes – Core Concepts
    and Constructs* **,** under the *Services* section, you learned that Kubernetes
    is using kube-proxy to determine the proper pod IP address and port serving each
    request. Behind the scenes, kube-proxy is actually using virtual IPs and **iptables**
    to make all this magic work.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨与服务和容器之间的通信相关的IP策略。如果你还记得，在[第2章](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第2章。Kubernetes-核心概念和构造")中，*Kubernetes-核心概念和构造*，在*服务*部分，你学到了Kubernetes正在使用kube-proxy来确定每个请求所服务的正确的pod
    IP地址和端口。在幕后，kube-proxy实际上是使用虚拟IP和**iptables**来使所有这些魔术生效。
- en: Recall that kube-proxy is running on every host. Its first duty is to monitor
    the API from the Kubernetes master. Any updates to services will trigger an update
    to iptables from kube-proxy. For example, when a new service is created, a virtual
    IP address is chosen and a rule in iptables is set, which will direct its traffic
    to kube-proxy via a random port. Thus, we now have a way to capture service-destined
    traffic on this node. Since kube-proxy is running on all nodes, we have cluster-wide
    resolution for the service VIP. Additionally, DNS records can point to this virtual
    IP as well.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，kube-proxy在每个主机上运行。它的第一个职责是监视来自Kubernetes主节点的API。任何对服务的更新都会触发kube-proxy对iptables的更新。例如，当创建新服务时，会选择一个虚拟IP地址，并设置iptables中的规则，这将通过一个随机端口将其流量定向到kube-proxy。因此，我们现在有一种方法来在此节点上捕获服务定向的流量。由于kube-proxy在所有节点上运行，我们对服务VIP有整个集群范围的解析。此外，DNS记录也可以指向这个虚拟IP。
- en: 'Now that we have a *hook* created in iptables, we still need to get the traffic
    to the servicing pods; however, the rule is only sending traffic to the service
    entry in kube-proxy at this point. Once kube-proxy receives the traffic for a
    particular service, it must then forward it to a pod in the service''s pool of
    candidates. It does this using a random port that was selected during service
    creation. Refer to the following figure (Figure 3.1) for an overview of the flow:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在iptables中创建了一个*hook*，我们仍然需要将流量发送到服务的pod；然而，这条规则目前只是将流量发送到kube-proxy中的服务入口。一旦kube-proxy接收到特定服务的流量，它必须将其转发到服务候选池中的一个pod。它使用在服务创建过程中选择的随机端口来执行此操作。请参考以下图（图3.1）概述流程：
- en: '![Advanced services](../images/00033.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![高级服务](../images/00033.jpeg)'
- en: Figure 3.1\. Kube-proxy communication
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1。kube-proxy通信
- en: At the time of writing this book, there are plans in the upcoming version 1.1
    to include a kube-proxy, which does not rely on service entry and uses only iptable
    rules.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，计划在即将推出的1.1版本中包括一个kube-proxy，它不依赖于服务条目，只使用iptable规则。
- en: Tip
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is also possible to always forward traffic from the same client IP to same
    backend pod/container using the `sessionAffinity` element in your service definition.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用服务定义中的`sessionAffinity`元素，始终将来自相同客户端IP的流量转发到相同的后端pod/container。
- en: External services
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部服务
- en: 'In the last chapter, we saw a few service examples. For testing and demonstration
    purposes, we wanted all the services to be externally accessible. This was configured
    by the `type: LoadBalancer` element in our service definition. The `LoadBalancer`
    type creates an external load balancer on the cloud provider. We should note that
    support for external load balancers varies by provider as does the implementation.
    In our case, we are using GCE, so integration is pretty smooth. The only additional
    setup needed is to open firewall rules for the external service ports.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在上一章中，我们看到了一些服务示例。出于测试和演示目的，我们希望所有服务都可以外部访问。这是通过我们服务定义中的`type: LoadBalancer`元素进行配置的。`LoadBalancer`类型在云提供商上创建外部负载均衡器。我们应该注意，对外部负载均衡器的支持因提供商而异，实现也不同。在我们的情况下，我们使用的是GCE，因此集成非常顺利。唯一需要额外设置的是为外部服务端口打开防火墙规则。'
- en: Let's dig a little deeper and do a `describe` on one of the services from the
    [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7 "Chapter 2. Kubernetes
    – Core Concepts and Constructs"), *Kubernetes – Core Concepts and Constructs*,
    under the *More on labels* section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入一点，对[第2章](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第2章。Kubernetes-核心概念和构造")中的服务进行`describe`，*Kubernetes-核心概念和构造*，在*更多关于标签*部分。
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的结果：
- en: '![External services](../images/00034.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![外部服务](../images/00034.jpeg)'
- en: Figure 3.2\. Service description
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2\. 服务描述
- en: In the output, in Figure 3.2, you'll note several key elements. Our namespace
    is set to default, **Type:** is `LoadBalancer`, and we have the external IP listed
    under **LoadBalancer Ingress:**. Further, we see **Endpoints:**, which shows us
    the IPs of the pods available to answer service requests.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.2的输出中，您会注意到几个关键元素。我们的命名空间设置为默认值，**类型：**为`LoadBalancer`，并且在**LoadBalancer
    Ingress：**下列出了外部IP。此外，我们看到了**Endpoints：**，它显示了可用于响应服务请求的pod的IP。
- en: Internal services
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内部服务
- en: 'Let''s explore the other types of services we can deploy. First, by default,
    services are internally facing only. You can specify a type of `clusterIP` to
    achieve this, but if no type is defined, `clusterIP` is the assumed type. Let''s
    take a look at an example, note the lack of the `type` element:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索我们可以部署的其他类型的服务。首先，默认情况下，服务只面向内部。您可以指定`clusterIP`类型来实现这一点，但如果没有定义类型，`clusterIP`将被假定为类型。让我们看一个例子，注意缺少`type`元素：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 3-1*: `nodejs-service-internal.yaml`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-1*：`nodejs-service-internal.yaml`'
- en: 'Use this listing to create the service definition file. You''ll need a healthy
    version of the `node-js` RC (*Listing 2-7*: `nodejs-health-controller-2.yaml`).
    As you can see, the selector matches on the pods named `node-js` that our RC launched
    in the last chapter. We will create the service and then list the currently running
    services with a filter:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此清单创建服务定义文件。您将需要一个健康的`node-js` RC版本（*清单2-7*：`nodejs-health-controller-2.yaml`）。正如您所看到的，选择器匹配上一章中我们的RC启动的名为`node-js`的pod。我们将创建服务，然后使用过滤器列出当前运行的服务：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的结果：
- en: '![Internal services](../images/00035.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![内部服务](../images/00035.jpeg)'
- en: Figure 3.3\. Internal service listing
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3\. 内部服务列表
- en: 'As you can see, we have a new service, but only one IP. Further, the IP address
    is not externally accessible. We won''t be able to test the service from a web
    browser this time. However, we can use the handy `kubectl exec` command and attempt
    to connect from one of the other pods. You will need `node-js-pod` (*Listing 2-1*:
    `nodejs-pod.yaml`) running. Then, you can execute the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '正如您所看到的，我们有一个新的服务，但只有一个IP。此外，该IP地址无法外部访问。这次我们将无法从Web浏览器测试该服务。但是，我们可以使用方便的 `kubectl
    exec` 命令，并尝试从其他一个pod连接。您需要运行 `node-js-pod` (*清单2-1*: `nodejs-pod.yaml`)。然后，您可以执行以下命令：'
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This allows us to run a `docker exec` command as if we had a shell in the `node-js-pod`
    container. It then hits the internal service URL, which forwards to any pods with
    the `node-js` label.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够像在 `node-js-pod` 容器中拥有shell一样运行 `docker exec` 命令。然后它会命中内部服务URL，该URL会转发到具有
    `node-js` 标签的任何pod。
- en: If all is well, you should get the raw HTML output back. So, you've successfully
    created an internal-only service. This can be useful for backend services that
    you want to make available to other containers running in your cluster, but not
    open to the world at large.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您应该会收到原始的HTML输出。因此，您已成功创建了一个仅内部可用的服务。这对于您希望向集群中运行的其他容器提供的后端服务可能很有用，但不对外开放。
- en: Custom load balancing
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义负载均衡
- en: 'A third type of service K8s allows is the `NodePort` type. This type allows
    us to expose a service through the host or minion on a specific port. In this
    way, we can use the IP address of any node (minion) and access our service on
    the assigned node port. Kubernetes will assign a node port by default in the range
    of 3000–32767, but you can also specify your own custom port. In the example in
    *Listing 3-2*: `nodejs-service-nodeport.yaml`, we choose port `30001` as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'K8s允许的第三种服务类型是 `NodePort` 类型。这种类型允许我们通过主机或minion在特定端口上公开服务。通过这种方式，我们可以使用任何节点（minion）的IP地址，并在分配的节点端口上访问我们的服务。Kubernetes将默认分配一个节点端口在3000-32767的范围内，但您也可以指定自己的自定义端口。在*清单3-2*:
    `nodejs-service-nodeport.yaml`中的示例中，我们选择端口`30001`如下：'
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 3-2*: `nodejs-service-nodeport.yaml`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-2*: `nodejs-service-nodeport.yaml`'
- en: 'Once again, create this YAML definition file and create your service as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 再次创建这个YAML定义文件，并按以下方式创建您的服务：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output should have a message like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该有类似这样的消息：
- en: '![Custom load balancing](../images/00036.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![自定义负载均衡](../images/00036.jpeg)'
- en: Figure 3.4\. New GCP firewall rule
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4\. 新的GCP防火墙规则
- en: You'll note a message about opening firewall ports. Similar to the external
    load balancer type, `NodePort` is exposing your service externally using ports
    on the nodes. This could be useful if, for example, you want to use your own load
    balancer in front of the nodes. Let's make sure that we open those ports on GCP
    before we test our new service.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到有关打开防火墙端口的消息。与外部负载均衡器类型类似，`NodePort`使用节点上的端口外部公开您的服务。例如，如果您想在节点前使用自己的负载均衡器，这可能很有用。在测试新服务之前，让我们确保在GCP上打开这些端口。
- en: 'From the GCE VM instance console, click on the network for any of your nodes
    (minions). In my case, it was default. Under firewall rules, we can add a rule
    by clicking **Add firewall rule**. Create a rule like the one shown in Figure
    3.5:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从GCE VM实例控制台，单击任何节点（minion）的网络。在我的情况下，它是默认的。在防火墙规则下，我们可以通过单击**添加防火墙规则**来添加规则。创建一个如图3.5所示的规则：
- en: '![Custom load balancing](../images/00037.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![自定义负载均衡](../images/00037.jpeg)'
- en: Figure 3.5\. New GCP firewall rule
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5\. 新的GCP防火墙规则
- en: 'We can now test our new service out, by opening a browser and using an IP address
    of any node (minion) in your cluster. The format to test the new service is:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过打开浏览器并使用集群中任何节点（minion）的IP地址来测试我们的新服务。测试新服务的格式是：
- en: '`http://`**`<Minoion IP Address>`**`:``<NodePort>``/`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://`**`<Minoion IP地址>`**`:``<NodePort>``/`'
- en: Cross-node proxy
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨节点代理
- en: Remember that kube-proxy is running on all the nodes, so even if the pod is
    not running there, traffic will be given a proxy to the appropriate host. Refer
    to Figure 3.6 for a visual on how the traffic flows. A user makes a request to
    an external IP or URL. The request is serviced by **Node 1** in this case. However,
    the pod does not happen to run on this node. This is not a problem because the
    pod IP addresses are routable. So, **Kube-proxy** simply passes traffic on to
    the pod IP for this service. The network routing then completes on **Node 2**,
    where the requested application lives.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，kube-proxy在所有节点上运行，因此即使该pod在那里没有运行，流量也会被代理到适当的主机。有关流量流动方式的可视化，请参考图3.6。用户向外部IP或URL发出请求。在这种情况下，请求由**Node
    1**处理。但是，该pod恰好没有在此节点上运行。这不是问题，因为pod IP地址是可路由的。因此，**Kube-proxy**只是将流量传递给此服务的pod
    IP。然后网络路由在**Node 2**上完成，请求的应用程序就在那里。
- en: '![Cross-node proxy](../images/00038.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![跨节点代理](../images/00038.jpeg)'
- en: Figure 3.6\. Cross-node traffic
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6。跨节点流量
- en: Custom ports
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义端口
- en: 'Services also allow you to map your traffic to different ports, then the containers
    and pods themselves expose. We will create a service that exposes `port 90` and
    forwards traffic to `port 80` on the pods. We will call the `node-js-90` pod to
    reflect the custom port number. Create the following two definition files:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 服务还允许您将流量映射到不同的端口，然后容器和Pod本身会暴露。我们将创建一个服务，暴露`端口90`并将流量转发到Pod上的`端口80`。我们将调用`node-js-90`
    pod来反映自定义端口号。创建以下两个定义文件：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 3-3*: `nodejs-customPort-controller.yaml`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-3*：`nodejs-customPort-controller.yaml`'
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 3-4*: `nodejs-customPort-service.yaml`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-4*：`nodejs-customPort-service.yaml`'
- en: You'll note that in the service definition, we have a `targetPort` element.
    This element tells the service the port to use for pods/containers in the pool.
    As we saw in previous examples, if you do not specify `targetPort`, it assumes
    that it's the same port as the service. Port is still used as the service port,
    but in this case, we are going to expose the service on port `90` while the containers
    serve content on port `80`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到在服务定义中，我们有一个`targetPort`元素。该元素告诉服务在池中使用的端口。正如我们在之前的示例中看到的，如果您不指定`targetPort`，它会假定与服务相同的端口。端口仍然用作服务端口，但在这种情况下，我们将在端口`90`上公开服务，而容器在端口`80`上提供内容。
- en: 'Create this RC and service and open the appropriate firewall rules, as we did
    in the last example. It may take a moment for the external load balancer IP to
    propagate to the `get service` command. Once it does, you should be able to open
    and see our familiar web application in a browser using the following format:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 创建此RC和服务，并打开适当的防火墙规则，就像我们在上一个示例中所做的那样。可能需要一段时间才能将外部负载均衡器IP传播到`get service`命令。一旦完成，您应该能够以以下格式在浏览器中打开并查看我们熟悉的Web应用程序：
- en: '`http://`**`<external service IP>`**`:90/`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://`**`<外部服务IP>`**`:90/`'
- en: Multiple ports
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个端口
- en: 'Another custom port use case is that of multiple ports. Many applications expose
    multiple ports, such as HTTP on port `80` and port `8888` for web servers. The
    following example shows our app responding on both ports. Once again, we''ll also
    need to add a firewall rule for this port, as we did for *Listing 3-2*: `nodejs-service-nodeport.yaml`
    previously:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个自定义端口用例是多个端口。许多应用程序暴露多个端口，例如端口`80`上的HTTP和端口`8888`上的Web服务器。以下示例显示我们的应用程序在两个端口上响应。再次，我们还需要为此端口添加防火墙规则，就像我们之前为*清单3-2*：`nodejs-service-nodeport.yaml`所做的那样：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 3-5*: `nodejs-multicontroller.yaml`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-5*：`nodejs-multicontroller.yaml`'
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Listing 3-6*: `nodejs-multiservice.yaml`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-6*：`nodejs-multiservice.yaml`'
- en: Note
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the application and container itself must be listening on both ports
    for this to work. In this example, port `8888` is used to represent a fake admin
    interface.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，应用程序和容器本身必须同时侦听两个端口才能正常工作。在本例中，端口`8888`用于表示一个虚假的管理界面。
- en: If, for example, you want to listen on port 443, you would need a proper SSL
    socket listening on the server.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想在端口443上侦听，您需要在服务器上侦听适当的SSL套接字。
- en: Migrations, multicluster, and more
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移、多集群等
- en: As you've seen so far, Kubernetes offers a high level of flexibility and customization
    to create a service abstraction around your containers running in the cluster.
    However, there may be times where you want to point to something outside your
    cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到，Kubernetes提供了高度的灵活性和定制化，以在集群中运行的容器周围创建服务抽象。然而，可能会有时候您想要指向集群外的东西。
- en: An example of this would be working with legacy systems, or even applications
    running on another cluster. In the case of the former, this is a perfectly good
    strategy in order to migrate to Kubernetes and containers in general. We can begin
    to manage the service endpoints in Kubernetes while stitching the stack together
    using the K8s orchestration concepts. Additionally, we can even start bringing
    over pieces of the stack, as the frontend, one at a time as the organization refactors
    applications for microservices and/or containerization.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是与传统系统一起工作，甚至是在另一个集群上运行的应用程序。在前一种情况下，这是一种非常好的策略，可以迁移到Kubernetes和容器。我们可以开始在Kubernetes中管理服务端点，同时使用K8s编排概念将整个堆栈组合在一起。此外，我们甚至可以开始逐步将堆栈的部分，如前端，迁移到组织为微服务和/或容器化而重构应用程序。
- en: 'To allow access to non-pod–based applications, the services construct allows
    you to use endpoints that are outside the cluster. Kubernetes is actually creating
    an endpoint resource every time you create a service that uses selectors. The
    `endpoints` object keeps track of the pod IPs in the load balancing pool. You
    can see this by running a `get endpoints` command as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许非基于pod的应用程序访问，服务构造允许您使用集群外的端点。实际上，每次创建使用选择器的服务时，Kubernetes都会创建一个端点资源。`endpoints`对象跟踪负载平衡池中的pod
    IP。您可以通过运行`get endpoints`命令来查看这一点：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should see something similar to this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于这样的东西：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You'll note an entry for all the services we currently have running on our cluster.
    For most, the endpoints are just the IP of each pod running in a RC. As I mentioned,
    Kubernetes does this automatically based on the selector. As we scale the replicas
    in a controller with matching labels, Kubernetes will update the endpoints automatically.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到我们当前在集群上运行的所有服务的条目。对于大多数服务，端点只是在RC中运行的每个pod的IP。正如我所提到的，Kubernetes会根据选择器自动执行此操作。当我们扩展具有匹配标签的控制器中的副本时，Kubernetes将自动更新端点。
- en: 'If we want to create a service for something that is not a pod and therefore
    has no labels to select, we can easily do this with both a service and endpoint
    definition as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想为不是pod的东西创建一个服务，因此没有标签可供选择，我们可以通过服务和端点定义轻松实现：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Listing 3-7*: `nodejs-custom-service.yaml`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-7*：`nodejs-custom-service.yaml`'
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Listing 3-8*: `nodejs-custom-endpoint.yaml`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-8*：`nodejs-custom-endpoint.yaml`'
- en: In the preceding example, you'll need to replace the `<X.X.X.X>` with a real
    IP address where the new service can point. In my case, I used the public load
    balancer IP from `node-js-multiservice` we created earlier. Go ahead and create
    these resources now.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，您需要将`<X.X.X.X>`替换为新服务可以指向的真实IP地址。在我的情况下，我使用了之前创建的`node-js-multiservice`的公共负载均衡器IP。现在可以创建这些资源了。
- en: If we now run a `get endpoints` command, we will see this IP address at port
    `80` associated with the `custom-service` endpoint. Further, if we look at the
    service details, we will see the IP listed in the `Endpoints` section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在运行`get endpoints`命令，我们将看到与`custom-service`端点关联的IP地址在端口`80`上。此外，如果我们查看服务详情，我们将在`Endpoints`部分看到列出的IP地址。
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can test out this new service by opening the `custom-service` external IP
    from a browser.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在浏览器中打开`custom-service`的外部IP来测试这项新服务。
- en: Custom addressing
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义寻址
- en: 'Another option to customize services is with the `clusterIP` element. In our
    examples this far, we''ve not specified an IP address, which means that it chooses
    the internal address of the service for us. However, we can add this element and
    choose the IP address in advance with something like `clusterip: 10.0.125.105`.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个自定义服务的选项是使用`clusterIP`元素。在我们迄今为止的示例中，我们没有指定IP地址，这意味着它会为我们选择服务的内部地址。但是，我们可以添加这个元素，并预先选择IP地址，比如`clusterip:
    10.0.125.105`。'
- en: 'There may be times when you don''t want to load balance and would rather have
    DNS with *A* records for each pod. For example, software that needs to replicate
    data evenly to all nodes may rely on *A* records to distribute data. In this case,
    we can use an example like the following one and set `clusterip` to `None`. Kubernetes
    will not assign an IP address and instead only assign *A* records in DNS for each
    of the pods. If you are using DNS, the service should be available at `node-js-none`
    or `node-js-none.default.cluster.local` from within the cluster. We have the following
    code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你可能不想进行负载均衡，而是更愿意使用每个pod的*A*记录的DNS。例如，需要将数据均匀复制到所有节点的软件可能依赖*A*记录来分发数据。在这种情况下，我们可以使用以下示例，并将`clusterip`设置为`None`。Kubernetes不会分配IP地址，而是只为每个pod在DNS中分配*A*记录。如果你在使用DNS，服务应该可以在集群内的`node-js-none`或`node-js-none.default.cluster.local`访问。我们有以下代码：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Listing 3-9*: `nodejs-headless-service.yaml`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-9*：`nodejs-headless-service.yaml`'
- en: 'Test it out after you create this service with the trusty `exec` command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了这个服务之后，可以使用`exec`命令进行测试：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Service discovery
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'As we discussed earlier, the Kubernetes master keeps track of all service definitions
    and updates. Discovery can occur in one of three ways. The first two methods use
    Linux environment variables. There is support for the Docker link style of environment
    variables, but Kubernetes also has its own naming convention. Here is an example
    of what our `node-js` service example might look like using K8s environment variables
    (note IPs will vary):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，Kubernetes主节点会跟踪所有服务定义和更新。发现可以通过三种方式之一进行。前两种方法使用Linux环境变量。虽然Docker链接风格的环境变量得到支持，但Kubernetes也有自己的命名约定。以下是使用K8s环境变量的`node-js`服务示例的示例（注意IP地址会有所不同）：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Listing 3-10*: *Service environment variables*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-10*：*服务环境变量*'
- en: Another option for discovery is through DNS. While environment variables can
    be useful when DNS is not available, it has drawbacks. The system only creates
    variables at creation time, so services that come online later will not be discovered
    or would require some additional tooling to update all the system environments.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过DNS进行发现的另一个选项。虽然环境变量在DNS不可用时可能很有用，但它也有缺点。系统只在创建时创建变量，因此稍后上线的服务将无法被发现，或者需要一些额外的工具来更新所有系统环境。
- en: DNS
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNS
- en: DNS solves the issues seen with environment variables by allowing us to reference
    the services by their name. As services restart, scale out, or appear anew, the
    DNS entries will be updating and ensuring that the service name always points
    to the latest infrastructure. DNS is set up by default in most of the supported
    providers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: DNS通过允许我们通过名称引用服务来解决环境变量的问题。随着服务的重新启动、扩展或出现新的，DNS条目将更新并确保服务名称始终指向最新的基础架构。DNS默认在大多数受支持的提供者中设置。
- en: Tip
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'If DNS is supported by your provider, but not setup, you can configure the
    following variables in your default provider `config` when you create your Kubernetes
    cluster:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的提供者支持DNS，但尚未设置，您可以在创建Kubernetes集群时在默认提供者`config`中配置以下变量：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With DNS active, services can be accessed in one of two forms—either the service
    name itself, `<service-name>`, or a fully qualified name that includes the namespace,
    `<service-name>.<namespace-name>.cluster.local`. In our examples, it would look
    similar to `node-js-90` or `node-js-90.default.cluster.local`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有了活动的DNS，服务可以以两种形式之一访问——要么是服务名称本身`<service-name>`，要么是包括命名空间的完全限定名称`<service-name>.<namespace-name>.cluster.local`。在我们的示例中，它看起来类似于`node-js-90`或`node-js-90.default.cluster.local`。
- en: Persistent storage
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久存储
- en: 'Let''s switch gears for a moment and talk about another core concept: persistent
    storage. When you start moving from development to production, one of the most
    obvious challenges you face is the transient nature of containers themselves.
    If you recall our discussion of layered file systems in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    the top layer is writable. (It''s also frosting, which is delicious.) However,
    when the container dies, the data goes with it. The same is true for crashed containers
    that Kubernetes restarts.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转换一下思路，谈谈另一个核心概念：持久存储。当您从开发转向生产时，您面临的最明显的挑战之一是容器本身的瞬态性。如果您回忆一下我们在[第1章](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第1章。Kubernetes和容器操作")中对分层文件系统的讨论，*Kubernetes和容器操作*，顶层是可写的。（它也是奶油，非常美味。）然而，当容器死亡时，数据也随之消失。对于Kubernetes重新启动的崩溃容器也是如此。
- en: This is where **persistent disks** (**PDs**), or volumes, come into play. A
    persistent volume that exists outside the container allows us to save our important
    data across containers outages. Further, if we have a volume at the pod level,
    data can be shared between containers in the same application stack and within
    the same pod.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**持久磁盘**（**PDs**）或卷发挥作用的地方。存在于容器之外的持久卷允许我们在容器宕机时保存重要数据。此外，如果我们在pod级别有一个卷，数据可以在同一应用程序堆栈中的容器之间和同一pod内共享。
- en: Docker itself has some support for volumes, but Kubernetes gives us persistent
    storage that lasts beyond the lifetime of a single container. The volumes are
    tied to pods and live and die with those pods. Additionally, a pod can have multiple
    volumes from a variety of sources. Let's take a look at some of these sources.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Docker本身对卷有一些支持，但Kubernetes为我们提供了持久存储，可以持续超出单个容器的生命周期。这些卷与pod相关联，并随着pod的生死而生死。此外，一个pod可以有多个来自各种来源的卷。让我们来看看其中一些来源。
- en: Temporary disks
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 临时磁盘
- en: One of the easiest ways to achieve improved persistence amid container crashes
    and data sharing within a pod is to use the `emptydir` volume. This volume type
    can be used with either the storage volumes of the node machine itself or an optional
    RAM disk for higher performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器崩溃和pod内数据共享方面实现改进的最简单方法之一是使用`emptydir`卷。这种卷类型可以与节点机器本身的存储卷或用于提高性能的可选RAM磁盘一起使用。
- en: Again, we improve our persistence beyond a single container, but when a pod
    is removed, the data will be lost. Machine reboot will also clear any data from
    RAM-type disks. There may be times when we just need some shared temporary space
    or have containers that process data and hand it off to another container before
    they die. Whatever the case, here is a quick example of using this temporary disk
    with the RAM-backed option.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将持久性提升到单个容器之外，但当一个pod被移除时，数据将丢失。机器重启也会清除RAM类型磁盘中的任何数据。有时我们可能只需要一些共享的临时空间，或者有一些处理数据并在它们死亡之前将其传递给另一个容器的容器。无论情况如何，这里是一个使用这个临时磁盘和RAM支持选项的快速示例。
- en: 'Open your favorite editor and create a file like the one in *Listing 3-11*:
    `storage-memory.yaml` here:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您喜欢的编辑器，并创建一个类似于*清单3-11*：`storage-memory.yaml`的文件：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Listing 3-11*: `storage-memory.yaml`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-11*：`storage-memory.yaml`'
- en: 'It''s probably second nature by now, but we will once again issue a `create`
    command followed by an `exec` command to see the folders in the container:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能已经成为第二天性了，但我们将再次发出`create`命令，然后是`exec`命令来查看容器中的文件夹：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will give us a bash shell in the container itself. The `ls` command shows
    us a `memory-pd` folder at the top level. We use `grep` to filter the output,
    but you can run the command without `| grep memory-pd` to see all folders.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在容器本身中为我们提供一个bash shell。`ls`命令显示顶层有一个`memory-pd`文件夹。我们使用`grep`来过滤输出，但您也可以运行不带`|
    grep memory-pd`的命令来查看所有文件夹。
- en: '![Temporary disks](../images/00039.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![临时磁盘](../images/00039.jpeg)'
- en: Figure 3.7\. Temporary storage inside a container
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7\. 容器内的临时存储
- en: Again, this folder is quite temporary as everything is stored in the minion's
    RAM. When the node gets restarted, all the files will be erased. We will look
    at a more permanent example next.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这个文件夹是相当临时的，因为一切都存储在minion的RAM中。当节点重新启动时，所有文件都将被删除。接下来我们将看一个更加永久的例子。
- en: Cloud volumes
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云卷
- en: Many companies will already have significant infrastructure running in the public
    cloud. Luckily, Kubernetes has native support for the persistent volume types
    provided by two of the most popular providers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司可能已经在公共云中运行重要的基础设施。幸运的是，Kubernetes原生支持两个最受欢迎提供者提供的持久卷类型。
- en: GCE persistent disks
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GCE持久磁盘
- en: Let's create a new **GCE persistent volume**. From the console, under **Compute**,
    go to **Disks**. On this new screen, click on the **New disk** button.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的**GCE持久卷**。从控制台，在**计算**下，转到**磁盘**。在这个新屏幕上，点击**新建磁盘**按钮。
- en: We'll be presented with a screen similar to Figure 3.8\. Choose a name for this
    volume and give it a brief description. Make sure that the zone is the same as
    the nodes in your cluster. GCE PDs can only be attached to machines in the same
    zone.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一个类似于图3.8\.的屏幕。为这个卷选择一个名称并给它一个简短的描述。确保区域与集群中的节点相同。GCE PDs只能附加到同一区域中的机器上。
- en: Enter `mysite-volume-1` for the **Name**. Choose a **Source type** of **None
    (blank disk)** and give `10` (10 GB) as value in **Size (GB)**. Finally, click
    on **Create**.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输入`mysite-volume-1`作为**名称**。选择**源类型**为**无（空白磁盘）**，并在**大小（GB）**中输入`10`（10 GB）作为值。最后，点击**创建**。
- en: '![GCE persistent disks](../images/00040.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![GCE持久磁盘](../images/00040.jpeg)'
- en: Figure 3.8\. GCE new persistent disk
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8\. GCE新持久磁盘
- en: 'The nice thing about PDs on GCE is that they allow for mounting to multiple
    machines (nodes in our case). However, when mounting to multiple machines, the
    volume must be in read-only mode. So, let''s first mount this to a single pod,
    so we can create some files. Use *Listing 3-12*: `storage-gce.yaml` as follows
    to create a pod that will mount the disk in read/write mode:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: GCE上PD的好处是它们允许挂载到多台机器（在我们的情况下是节点）。但是，当挂载到多台机器时，卷必须处于只读模式。因此，让我们首先将其挂载到单个pod，以便我们可以创建一些文件。使用*清单3-12*：`storage-gce.yaml`如下创建一个将以读/写模式挂载磁盘的pod：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Listing 3-12*: `storage-gce.yaml`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-12*：`storage-gce.yaml`'
- en: First, let's issue a `create` command followed by a describe to find out which
    node it is running on. Note the node and save the pod IP address for later. Then,
    open an SSH session into the node.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们发出一个`create`命令，然后进行描述以找出它正在哪个节点上运行。注意节点并保存pod IP地址以备后用。然后，打开一个SSH会话进入节点。
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since we''ve already looked at the volume from inside the running container,
    let''s access it directly from the minion node itself this time. We will run a
    `df` command to see where it is mounted:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经从正在运行的容器内部查看了卷，这次让我们直接从节点本身访问它。我们将运行`df`命令来查看它挂载在哪里：
- en: '[PRE23]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can see, the GCE volume is mounted directly to the node itself. We can
    use the mount path listed in the output of the earlier `df` command. Use `cd`
    to change to the folder now. Then, create a new file named `index.html` with your
    favorite editor:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，GCE卷直接挂载到节点本身。我们可以使用早期`df`命令的输出中列出的挂载路径。现在使用`cd`切换到该文件夹。然后，使用您喜欢的编辑器创建一个名为`index.html`的新文件：
- en: '[PRE24]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Enter a quaint message such as `Hello from my GCE PD!`. Now save the file and
    exit the editor. If you recall from *Listing 3-12*: `storage-gce.yaml`, the PD
    is mounted directly to the NGINX html directory. So, let''s test this out while
    we still have the SSH session open on the node. Do a simple `curl` command to
    the pod IP we wrote down earlier.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输入一条古雅的消息，比如`来自我的GCE PD的问候！`。现在保存文件并退出编辑器。如果您回忆起*清单3-12*：`storage-gce.yaml`，PD直接挂载到NGINX的html目录。因此，在我们仍然在节点上保持SSH会话的情况下，让我们测试一下。对我们之前记下的pod
    IP执行一个简单的`curl`命令。
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You should see **Hello from my GCE PD!** or whatever message you saved in the
    `index.html` file. In a real-world scenario, we could use the volume for an entire
    website or any other central storage. Let's take a look at running a set of load
    balanced web servers all pointing to the same volume.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到**来自我的GCE PD的问候！**或者您在`index.html`文件中保存的任何消息。在实际情况下，我们可以将该卷用于整个网站或任何其他中央存储。让我们看看如何运行一组指向相同卷的负载平衡Web服务器。
- en: First, leave the SSH session with `exit`. Before we proceed, we will need to
    remove our `test-gce` pod so that the volume can be mounted read-only across a
    number of nodes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`exit`离开SSH会话。在继续之前，我们需要删除`test-gce` pod，以便可以将卷以只读方式挂载到多个节点上。
- en: '[PRE26]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we can create a RC that will run three web servers all mounting the same
    persistent volume as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个RC，它将运行三个Web服务器，所有这些服务器都挂载相同的持久卷，如下所示：
- en: '[PRE27]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*Listing 3-13*: `http-pd-controller.yaml`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-13*：`http-pd-controller.yaml`'
- en: 'Let''s also create an external service, so we can see it from outside the cluster:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还创建一个外部服务，这样我们就可以从集群外部看到它：
- en: '[PRE28]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Listing 3-14*: `http-pd-service.yaml`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-14*：`http-pd-service.yaml`'
- en: 'Go ahead and create these two resources now. Wait a few moments for the external
    IP to get assigned. After this, a `describe` command will give us the IP we can
    use in a browser:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建这两个资源。等待一段时间以便分配外部IP。之后，`describe`命令将给出我们可以在浏览器中使用的IP：
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![GCE persistent disks](../images/00041.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![GCE持久磁盘](../images/00041.jpeg)'
- en: Figure 3.9\. K8s service with GCE PD shared across three pods
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9。K8s服务与GCE PD跨三个pod共享
- en: Type the IP address into a browser, and you should see your familiar `index.html`
    file show up with the text we entered previously!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中输入IP地址，您应该看到您熟悉的`index.html`文件显示出我们之前输入的文本！
- en: AWS Elastic Block Store
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS弹性块存储
- en: K8s also supports AWS **Elastic Block Store** (**EBS**) volumes. Like the GCE
    PDs, EBS volumes are required to be attached to an instance running in the same
    availability zone. A further limitation is that EBS can only be mounted to a single
    instance at one time.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: K8s还支持AWS **弹性块存储**（**EBS**）卷。与GCE PDs一样，EBS卷需要附加到在同一可用区运行的实例上。进一步的限制是，EBS一次只能挂载到一个实例上。
- en: For brevity, we will not walk through an AWS example, but a sample YAML file
    is included to get you started. Again, remember to create the EBS volume before
    your pod.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们不会演示AWS的例子，但是包含了一个示例YAML文件，可以帮助你开始。再次提醒，在创建pod之前记得创建EBS卷。
- en: '[PRE30]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*Listing 3-15*: `storage-aws.yaml`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-15*：`storage-aws.yaml`'
- en: Other PD options
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他PD选项
- en: 'Kubernetes supports a variety of other types of persistent storage. A full
    list can be found here:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持各种其他类型的持久存储。完整列表可以在这里找到：
- en: '[http://kubernetes.io/v1.0/docs/user-guide/volumes.html#types-of-volumes](http://kubernetes.io/v1.0/docs/user-guide/volumes.html#types-of-volumes)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://kubernetes.io/v1.0/docs/user-guide/volumes.html#types-of-volumes](http://kubernetes.io/v1.0/docs/user-guide/volumes.html#types-of-volumes)'
- en: 'Here are a few that may be of particular interest:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可能特别感兴趣的选项：
- en: '`nfs`: This type allows us to mount a **Network File Share** (**NFS**), which
    can be very useful for both persisting the data and sharing it across the infrastructure'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nfs`：这种类型允许我们挂载一个**网络文件共享**（**NFS**），这对于持久化数据和在基础设施中共享数据非常有用'
- en: '`gitrepo`: As you might have guessed, this option clones a Git repo into an
    a new and empty folder'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gitrepo`：正如你可能猜到的，这个选项会将一个Git仓库克隆到一个新的空文件夹中'
- en: Multitenancy
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多租户
- en: Kubernetes also has an additional construct for isolation at the cluster level.
    In most cases, you can run Kubernetes and never worry about namespaces; everything
    will run in the default namespace if not specified. However, in cases where you
    run multitenancy communities or want broad-scale segregation and isolation of
    the cluster resources, namespaces can be used to this end.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还在集群级别有一个额外的隔离构造。在大多数情况下，你可以运行Kubernetes而不必担心命名空间；如果没有指定，一切都将在默认命名空间中运行。然而，在运行多租户社区或者想要对集群资源进行广泛的隔离和隔离的情况下，可以使用命名空间来实现这一目的。
- en: 'To start, Kubernetes has two namespaces: `default` and `kube-system`. `kube-system`
    is used for all the system-level containers we saw in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    under the *Services running on the minions* section. The UI, logging, DNS, and
    so on are all run under `kube-system`. Everything else the user creates runs in
    the default namespace. However, our resource definition files can optionally specify
    a custom namespace. For the sake of experimenting, let''s take a look at how to
    build a new namespace.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Kubernetes有两个命名空间：`default`和`kube-system`。`kube-system`用于我们在[第1章](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第1章。Kubernetes和容器操作")中看到的所有系统级容器，*Kubernetes和容器操作*，在*运行在minions上的服务*部分。UI、日志、DNS等都在`kube-system`下运行。用户创建的其他所有内容都在默认命名空间中运行。然而，我们的资源定义文件可以选择指定一个自定义的命名空间。为了实验的目的，让我们看看如何构建一个新的命名空间。
- en: 'First, we''ll need to create a namespace definition file like the one in this
    listing:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个命名空间定义文件，就像这个清单中的一个：
- en: '[PRE31]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*Listing 3-16*: `test-ns.yaml`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-16*：`test-ns.yaml`'
- en: 'We can go ahead and create this file with our handy `create` command:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续使用我们方便的`create`命令来创建这个文件：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we can create resources that use the `test` namespace. The following is
    an example of a pod using this new namespace. We have the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建使用`test`命名空间的资源。以下是一个使用这个新命名空间的pod的示例。我们有以下内容：
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Listing 3-17*: `ns-pod.yaml`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-17*：`ns-pod.yaml`'
- en: 'While the pod can still access services in other namespaces, it will need to
    use the long DNS form of `<service-name>.<namespace-name>.cluster.local`. For
    example, if you were to run command from inside the container in *Listing 3-17*:
    `ns-pod.yaml`, you could use `http-pd.default.cluster.local` to access the PD
    example from *Listing 3-14*: `http-pd-service.yaml`.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然pod仍然可以访问其他命名空间中的服务，但它需要使用`<service-name>.<namespace-name>.cluster.local`的长DNS形式。例如，如果您要从*清单3-17*：`ns-pod.yaml`中的容器内运行命令，您可以使用`http-pd.default.cluster.local`来访问*清单3-14*：`http-pd-service.yaml`中的PD示例。
- en: Limits
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'Let''s inspect our new namespace a bit more. Run the `describe` command as
    follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地检查一下我们的新命名空间。按照以下方式运行`describe`命令：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![Limits](../images/00042.jpeg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![Limits](../images/00042.jpeg)'
- en: Figure 3.10\. Namespace describe
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10。命名空间描述
- en: Kubernetes allows you to both limit the resources used by individual pods or
    containers and the resources used by the overall namespace using quotas. You'll
    note that there are no resource **limits** or **quotas** currently set on the
    test namespace.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes允许您限制单个pod或容器使用的资源，也可以使用配额限制整个命名空间使用的资源。您会注意到测试命名空间当前没有设置资源**限制**或**配额**。
- en: 'Suppose we want to limit the footprint of this new namespace; we can set quotas
    such as the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要限制这个新命名空间的占用空间，我们可以设置以下配额：
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '*Listing 3-18*: `quota.yaml`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-18*：`quota.yaml`'
- en: Note
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that in reality, namespaces would be for larger application communities
    and would probably never have quotas this low. I am using this in order to ease
    illustration of the capability in the example.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，实际上，命名空间将用于更大的应用程序社区，可能永远不会有这么低的配额。我之所以使用这个例子，是为了便于说明这个功能。
- en: 'Here, we will create a quota of 3 pods, 1 RC, and 1 service for the test namespace.
    As you probably guessed, this is executed once again by our trusty `create` command:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将为测试命名空间创建一个配额，包括3个pod、1个RC和1个service。正如您可能猜到的那样，这是通过我们可靠的`create`命令执行的：
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we have that in place, let''s use `describe` on the namespace as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经做好了准备，让我们使用`describe`命令来查看命名空间：
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![Limits](../images/00043.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![Limits](../images/00043.jpeg)'
- en: Figure 3.11\. Namespace describe after quota is set
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11。设置配额后的命名空间描述
- en: 'You''ll note that we now have some values listed in the quota section and the
    limits section is still blank. We also have a **Used** column, which lets us know
    how close to the limits we are at the moment. Let''s try to spin up a few pods
    using the following definition:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，现在在配额部分列出了一些值，而限制部分仍然是空白的。我们还有一个**Used**列，它让我们知道我们目前离限制有多近。让我们尝试使用以下定义来启动一些pod：
- en: '[PRE38]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '*Listing 3-19*: `busybox-ns.yaml`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-19*：`busybox-ns.yaml`'
- en: You'll note that we are creating four replicas of this basic pod. After using
    `create` to build this RC, run the `describe` command on the `test` namespace
    once more. You'll note that the `used` values for pods and RCs are at their max.
    However, we asked for four replicas and only see three pods in use.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到我们正在创建这个基本pod的四个副本。在使用`create`构建这个RC之后，再次在`test`命名空间上运行`describe`命令。您会注意到pod和RC的`used`值已经达到了最大值。但是，我们要求四个副本，但只看到三个正在使用的pod。
- en: 'Let''s see what''s happening with our RC. You might tempt to do that with the
    command here:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的RC发生了什么。您可以使用以下命令来尝试：
- en: '[PRE39]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: However, if you try, you'll be disparaged to see a **not found** message from
    the server. This is because we created this RC in a new namespace and `kubectl`
    assumes the default namespace if not specified. This means that we need to specify
    `--namepsace=test` with every command when we wish to access resources in the
    `test` namespace.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你尝试，你会看到一个来自服务器的**未找到**消息而感到沮丧。这是因为我们在一个新的命名空间中创建了这个RC，如果没有指定，`kubectl`会假设默认的命名空间。这意味着当我们希望访问`test`命名空间中的资源时，我们需要在每个命令中指定`--namepsace=test`。
- en: Tip
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'We can also set the current namespace by working with the context settings.
    First, we need to find our current context, which is found with the following
    command:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过处理上下文设置来设置当前命名空间。首先，我们需要找到我们当前的上下文，可以使用以下命令找到：
- en: '[PRE40]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we can take that context and set the namespace variable like the following:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以采用这个上下文，并设置命名空间变量如下：
- en: '[PRE41]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now you can run the `kubectl` command without the need to specify the namespace.
    Just remember to switch back when you want to look at the resources running in
    your `default` namespace.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以运行`kubectl`命令而不需要指定命名空间。只需记住，当你想查看在你的`default`命名空间中运行的资源时，切换回去即可。
- en: 'Run the command with the namespace specified like so. If you''ve set your current
    namespace as demonstrated in the tip box, you can leave off the `--namespace`
    argument:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样使用指定命名空间运行命令。如果你已经按照提示框中演示的设置了当前的命名空间，你可以省略`--namespace`参数：
- en: '[PRE42]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![Limits](../images/00044.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![Limits](../images/00044.jpeg)'
- en: Figure 3.12\. Namespace quotas
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12. 命名空间配额
- en: As you can see in the preceding image, the first three pods were successfully
    created, but our final one fails with the error **Limited to 3 pods**.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图像所示，前三个pod已成功创建，但我们的最后一个失败，并显示错误**Limited to 3 pods**。
- en: This is an easy way to set limits for resources partitioned out at a community
    scale. It's worth noting that you can also set quotas for CPU, memory, persistent
    volumes, and secrets. Additionally, limits work similar to quota, but they set
    the limit for each pod or container within the namespace.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在社区规模上设置资源限制的简单方法。值得注意的是，您还可以为CPU、内存、持久卷和秘密设置配额。此外，限制与配额类似，但它们为命名空间内的每个pod或容器设置了限制。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We took a deeper look into networking and services in Kubernetes. You should
    now understand how networking communications are designed in K8s and feel comfortable
    accessing your services internally and externally. We saw how kube-proxy balances
    traffic both locally and across the cluster. We also looked briefly at how DNS
    and service discovery is achieved in Kubernetes. In the later portion of the chapter,
    we explored a variety of persistent storage options. We finished off with quick
    look at namespace and isolation for multitenancy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入研究了Kubernetes中的网络和服务。现在你应该了解了K8s中设计的网络通信方式，并且可以轻松地在内部和外部访问你的服务。我们看到了kube-proxy如何在本地和整个集群中平衡流量。我们还简要地看了一下Kubernetes中如何实现DNS和服务发现。在本章的后半部分，我们探讨了各种持久存储选项。最后，我们简要地看了一下多租户的命名空间和隔离。
- en: Footnotes
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚注
- en: ¹[http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/](http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ¹[http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/](http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/)
