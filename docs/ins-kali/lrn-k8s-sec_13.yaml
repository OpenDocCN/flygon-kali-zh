- en: '*Chapter 11*: Defense in Depth'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：深度防御'
- en: 'Defense in depth is an approach in cybersecurity that applies multiple layers
    of security controls to protect valuable assets. In a traditional or monolithic
    IT environment, we can list quite a few: authentication, encryption, authorization,
    logging, intrusion detection, antivirus, a **virtual private network** (**VPN**),
    firewalls, and so on. You may find that these security controls also exist in
    the Kubernetes cluster (and they should).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度防御是一种在网络安全中应用多层安全控制来保护有价值资产的方法。在传统或单片式IT环境中，我们可以列举出许多：认证、加密、授权、日志记录、入侵检测、防病毒、**虚拟私人网络**（**VPN**）、防火墙等等。您可能会发现这些安全控制也存在于Kubernetes集群中（而且应该存在）。
- en: 'We''ve discussed topics such as authentication, authorization, admission controllers,
    securing Kubernetes components, securing a configuration, hardening images, and
    Kubernetes workloads in the previous chapters. All these build up different security
    control layers to protect your Kubernetes cluster. In this chapter, we''re going
    to discuss topics that build up additional security control layers, and these
    are most related to runtime defense in a Kubernetes cluster. These are the questions
    we''re going to address in this chapter: Does your cluster expose any sensitive
    data? If an attack happens in the Kubernetes cluster, can you detect the attack?
    Can your Kubernetes cluster sustain the attack? How do you respond to the attack?'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经讨论了认证、授权、准入控制器、保护Kubernetes组件、保护配置、加固镜像和Kubernetes工作负载等主题。所有这些都构建了不同的安全控制层，以保护您的Kubernetes集群。在本章中，我们将讨论构建额外安全控制层的主题，这些主题与Kubernetes集群中的运行时防御最相关。以下是本章将要解决的问题：您的集群是否暴露了任何敏感数据？如果Kubernetes集群发生攻击，您能否检测到攻击？您的Kubernetes集群能够承受攻击吗？您如何应对攻击？
- en: In this chapter, we will talk about Kubernetes auditing, then we will introduce
    the concept of high availability and talk about how we can apply high availability
    in the Kubernetes cluster. Next, we will introduce Vault, a handy secrets management
    product for the Kubernetes cluster. Then, we will talk about how to use Falco
    to detect anomalous activities in the Kubernetes cluster. Last but not least,
    we will introduce Sysdig Inspect and **Checkpoint and Resource In Userspace**
    (also known as **CRIU**) for forensics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论Kubernetes审计，然后介绍高可用性的概念，并讨论如何在Kubernetes集群中应用高可用性。接下来，我们将介绍Vault，这是一个方便的秘密管理产品，适用于Kubernetes集群。然后，我们将讨论如何使用Falco来检测Kubernetes集群中的异常活动。最后但同样重要的是，我们将介绍Sysdig
    Inspect和**用户空间的检查点和资源**（也称为**CRIU**）用于取证。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing Kubernetes auditing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Kubernetes审计
- en: Enabling high availability in a Kubernetes cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中启用高可用性
- en: Managing secrets with Vault
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Vault管理秘密
- en: Detecting anomalies with Falco
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Falco检测异常
- en: Conducting forensics with Sysdig Inspect and CRIU
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Sysdig Inspect和CRIU进行取证
- en: Introducing Kubernetes auditing
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kubernetes审计
- en: 'Kubernetes auditing was introduced in the 1.11 version. Kubernetes auditing
    records events such as creating a deployment, patching pods, deleting namespaces,
    and more in a chronological order. With auditing, a Kubernetes cluster administrator
    is able to answer questions such as the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes审计是在1.11版本中引入的。Kubernetes审计记录事件，例如创建部署，修补pod，删除命名空间等，按照时间顺序进行记录。通过审计，Kubernetes集群管理员能够回答以下问题：
- en: What happened? (A pod is created and what kind of pod it is)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发生了什么？（创建了一个pod，是什么类型的pod）
- en: Who did it? (From user/admin)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁做的？（来自用户/管理员）
- en: When did it happen? (The timestamp of the event)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发生在什么时候？（事件的时间戳）
- en: Where did it happen? (In which namespace is the pod created?)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它发生在哪里？（Pod是在哪个命名空间中创建的？）
- en: From a security standpoint, auditing enables DevOps and the security team to
    do better anomaly detection and prevention by tracking events happening inside
    the Kubernetes cluster.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从安全的角度来看，审计使DevOps团队和安全团队能够通过跟踪Kubernetes集群内发生的事件来更好地检测和预防异常。
- en: 'In a Kubernetes cluster, it is `kube-apiserver` that does the auditing. When
    a request (for example, create a namespace) is sent to `kube-apiserver`, the request
    may go through multiple stages. There will be an event generated per stage. The
    following are the known stages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中，是`kube-apiserver`进行审计。当请求（例如，创建一个命名空间）发送到`kube-apiserver`时，请求可能会经过多个阶段。每个阶段将生成一个事件。已知的阶段如下：
- en: '`RequestReceived`: The event is generated as soon as the request is received
    by the audit handler without processing it.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RequestReceived`：在审计处理程序接收请求而不处理它时生成事件。'
- en: '`RequestStarted`: The event is generated between the time that the response
    header is sent and the response body is sent, and only applies for long-running
    requests such as `watch`.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RequestStarted`：在发送响应头并发送响应正文之间生成事件，仅适用于长时间运行的请求，如`watch`。'
- en: '`RequestComplete`: The event is generated when the response body is sent.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RequestComplete`：在发送响应正文时生成事件。'
- en: '`Panic`: The event is generated when panic occurs.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Panic`：当发生紧急情况时生成事件。'
- en: In this section, we will first introduce the Kubernetes audit policy, and then
    show you how to enable a Kubernetes audit and a couple of ways to persist audit
    records.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍Kubernetes审计策略，然后向您展示如何启用Kubernetes审计以及持久化审计记录的几种方法。
- en: Kubernetes audit policy
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes审计策略
- en: 'As it is not realistic to record everything happening inside the Kubernetes
    cluster, an audit policy allows users to define rules about what kind of event
    should be recorded and how much detail of the event should be recorded. When an
    event is processed by `kube-apiserver`, it compares the list of rules in the audit
    policy in order. The first matching rules also dictate the audit level of the
    event. Let''s take a look at what an audit policy looks like. Here is an example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于记录Kubernetes集群内发生的一切事情并不现实，审计策略允许用户定义关于应记录何种事件以及应记录事件的多少细节的规则。当`kube-apiserver`处理事件时，它将按顺序比较审计策略中的规则列表。第一个匹配的规则还决定了事件的审计级别。让我们看看审计策略是什么样子。以下是一个示例：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can configure multiple audit rules in the audit policy. Each audit rule
    will be configured by the following fields:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在审计策略中配置多个审计规则。每个审计规则将由以下字段配置：
- en: '`level`: The audit level that defines the verbosity of the audit event.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level`：定义审计事件详细程度的审计级别。'
- en: '`resources`: The Kubernetes objects under audit. Resources can be specified
    by an **A****pplication Programming Interface** (**API**) group and an object
    type.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resources`：审计的Kubernetes对象。资源可以通过**应用程序编程接口**（**API**）组和对象类型来指定。'
- en: '`nonResourcesURL`: A non-resource **Uniform Resource Locator** (**URL**) path
    that is not associated with any resources under audit.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nonResourcesURL`：与审计的任何资源不相关的非资源**统一资源定位符**（**URL**）路径。'
- en: '`namespace`: Decides which Kubernetes objects from which namespaces will be
    under audit. An empty string will be used to select non-namespaced objects, and
    an empty list implies every namespace.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespace`：决定哪个命名空间中的Kubernetes对象将接受审计。空字符串将用于选择非命名空间对象，空列表意味着每个命名空间。'
- en: '`verb`: Decides the specific operation of Kubernetes objects that will be under
    audit—for example, `create`, `update`, or `delete`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verb`：决定将接受审计的Kubernetes对象的具体操作，例如`create`，`update`或`delete`。'
- en: '`users`: Decides the authenticated user the audit rule applies to'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users`：决定审计规则适用于的经过身份验证的用户'
- en: '`userGroups`: Decides the authenticated user group the audit rule applies to.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`userGroups`：决定认证用户组适用于的审计规则。'
- en: '`omitStages`: Skips generating events on the given stages. This can also be
    set at the policy level.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`omitStages`：跳过在给定阶段生成事件。这也可以在策略级别设置。'
- en: 'The audit policy allows you to configure a policy at a fine-grained level by
    specifying `verb`, `namespace`, `resources`, and more. It is the audit level of
    the rule that defines how much detail of the event should be recorded. There are
    four audit levels, detailed as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 审计策略允许您通过指定`verb`、`namespace`、`resources`等在细粒度级别上配置策略。规则的审计级别定义了应记录事件的详细程度。有四个审计级别，如下所述：
- en: '`None`: Do not log events that match the audit rule.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`None`：不记录与审计规则匹配的事件。'
- en: '`Metadata`: When an event matches the audit rule, log the metadata (such as
    `user`, `timestamp`, `resource`, `verb`, and more) of the request to `kube-apiserver`.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Metadata`：当事件匹配审计规则时，记录请求到`kube-apiserver`的元数据（如`user`、`timestamp`、`resource`、`verb`等）。'
- en: '`Request`: When an event matches the audit rule, log the metadata as well as
    the request body. This does not apply for the non-resource URL.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Request`：当事件匹配审计规则时，记录元数据以及请求正文。这不适用于非资源URL。'
- en: '`RequestResponse`: When an event matches the audit rule, log the metadata,
    request-and-response body. This does not apply for the non-resource request.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RequestResponse`：当事件匹配审计规则时，记录元数据、请求和响应正文。这不适用于非资源请求。'
- en: 'The request-level event is more verbose than the metadata level events, while
    the `RequestResponse` level event is more verbose than the request-level event.
    The high verbosity requires more **input/output** (**I/O**) throughputs and storage.
    It is quite necessary to understand the differences between the audit levels so
    that you can define audit rules properly, both for resource consumption and security.
    With an audit policy successfully configured, let''s take a look at what audit
    events look like. The following is a metadata-level audit event:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请求级别的事件比元数据级别的事件更详细，而`RequestResponse`级别的事件比请求级别的事件更详细。高详细度需要更多的输入/输出（I/O）吞吐量和存储。了解审计级别之间的差异非常必要，这样您就可以正确定义审计规则，既可以节约资源又可以保障安全。成功配置审计策略后，让我们看看审计事件是什么样子的。以下是一个元数据级别的审计事件：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding audit event shows the `user`, `timestamp`, the object being accessed,
    the authorization decision, and so on. A request-level audit event provides extra
    information within the `requestObject` field in the audit event. You will find
    out the specification of the workload in the `requestObject` field, as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的审计事件显示了`user`、`timestamp`、被访问的对象、授权决定等。请求级别的审计事件在审计事件中的`requestObject`字段中提供了额外的信息。您将在`requestObject`字段中找到工作负载的规范，如下所示：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `RequestResponse`-level audit event is the most verbose. The `responseObject`
    instance in the event is almost the same as `requestObject`, with extra information
    such as resource version and creation timestamp, as shown in the following code
    block:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`RequestResponse`级别的审计事件是最详细的。事件中的`responseObject`实例几乎与`requestObject`相同，但包含了额外的信息，如资源版本和创建时间戳，如下面的代码块所示：'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Please do choose the audit level properly. More verbose logs provide deeper
    insight into the activities being carried out. However, it does cost more in storage
    and time to process the audit events. One thing worth mentioning is that if you
    set a request or a `RequestResponse` audit level on Kubernetes secret objects,
    the secret content will be recorded in the audit events. If you set the audit
    level to be more verbose than metadata for Kubernetes objects containing sensitive
    data, you should use a sensitive data redaction mechanism to avoid secrets being
    logged in the audit events.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请务必正确选择审计级别。更详细的日志提供了对正在进行的活动更深入的洞察。然而，存储和处理审计事件的时间成本更高。值得一提的是，如果在Kubernetes秘密对象上设置了请求或`RequestResponse`审计级别，秘密内容将被记录在审计事件中。如果将审计级别设置为比包含敏感数据的Kubernetes对象的元数据更详细，您应该使用敏感数据遮蔽机制，以避免秘密被记录在审计事件中。
- en: The Kubernetes auditing functionality offers a lot of flexibility to audit Kubernetes
    objects by object kind, namespace, operations, user, and so on. As Kubernetes
    auditing is not enabled by default, next, let's look at how to enable Kubernetes
    auditing and store audit records.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes审计功能通过对象类型、命名空间、操作、用户等提供了对Kubernetes对象的审计灵活性。由于Kubernetes审计默认情况下未启用，接下来，让我们看看如何启用Kubernetes审计并存储审计记录。
- en: Configuring the audit backend
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置审计后端
- en: 'In order to enable Kubernetes auditing, you need to pass the `--audit-policy-file`
    flag with your audit policy file when starting `kube-apiserver`. There are two
    types of audit backends that can be configured to use process audit events: a
    log backend and a webhook backend. Let''s have a look at them.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用Kubernetes审计，您需要在启动`kube-apiserver`时传递`--audit-policy-file`标志和您的审计策略文件。可以配置两种类型的审计后端来处理审计事件：日志后端和webhook后端。让我们来看看它们。
- en: Log backend
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志后端
- en: 'The log backend writes audit events to a file on the master node. The following
    flags are used to configure the log backend within `kube-apiserver`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 日志后端将审计事件写入主节点上的文件。以下标志用于在`kube-apiserver`中配置日志后端：
- en: '`--log-audit-path`: Specify the log path on the master node. This is the flag
    to turn ON or OFF the log backend.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--log-audit-path`：指定主节点上的日志路径。这是打开或关闭日志后端的标志。'
- en: '`--audit-log-maxage`: Specify the maximum number of days to keep the audit
    records.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--audit-log-maxage`：指定保留审计记录的最大天数。'
- en: '`--audit-log-maxbackup`: Specify the maximum number of audit files to keep
    on the master node.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--audit-log-maxbackup`：指定主节点上要保留的审计文件的最大数量。'
- en: '`--audit-log-maxsize`: Specify the maximum size in megabytes of an audit log
    file before it gets rotated.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--audit-log-maxsize`：指定在日志文件被轮换之前的最大兆字节大小。'
- en: Let's take a look at the webhook backend.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看webhook后端。
- en: Webhook backend
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: webhook后端
- en: 'The webhook backend writes audit events to the remote webhook registered to
    `kube-apiserver`. To enable the webhook backend, you need to set the `--audit-webhook-config-file`
    flag with the webhook configuration file. This flag is also specified when starting
    `kube-apiserver`. The following is an example of a webhook configuration to register
    a webhook backend for the Falco service, which will be introduced later in more
    detail:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: webhook后端将审计事件写入注册到`kube-apiserver`的远程webhook。要启用webhook后端，您需要使用webhook配置文件设置`--audit-webhook-config-file`标志。此标志也在启动`kube-apiserver`时指定。以下是一个用于为稍后将更详细介绍的Falco服务注册webhook后端的webhook配置的示例：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The URL specified in the `server` field (`http://$FALCO_SERVICE_CLUSTERIP:8765/k8s_audit`)
    is the remote endpoint that the audit events will be sent to. Since version 1.13
    of Kubernetes, the webhook backend can be configured dynamically via the `AuditSink`
    object, which is still in the alpha stage.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`server`字段中指定的URL（`http://$FALCO_SERVICE_CLUSTERIP:8765/k8s_audit`）是审计事件将要发送到的远程端点。自Kubernetes
    1.13版本以来，可以通过`AuditSink`对象动态配置webhook后端，该对象仍处于alpha阶段。'
- en: In this section, we talked about Kubernetes auditing by introducing the audit
    policy and audit backends. In the next section, we will talk about high availability
    in the Kubernetes cluster.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了Kubernetes审计，介绍了审计策略和审计后端。在下一节中，我们将讨论Kubernetes集群中的高可用性。
- en: Enabling high availability in a Kubernetes cluster
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中启用高可用性
- en: 'Availability refers to the ability of the user to access the service or system.
    The high availability of a system ensures an agreed level of uptime of the system.
    For example, if there is only one instance to serve the service and the instance
    is down, users can no longer access the service. A service with high availability
    is served by multiple instances. When one instance is down, the standby instance
    or backup instance can still provide the service. The following diagram describes
    services with and without high availability:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性指的是用户访问服务或系统的能力。系统的高可用性确保了系统的约定的正常运行时间。例如，如果只有一个实例来提供服务，而该实例宕机，用户将无法再访问该服务。具有高可用性的服务由多个实例提供。当一个实例宕机时，备用实例仍然可以提供服务。以下图表描述了具有和不具有高可用性的服务：
- en: '![Figure 11.1 – Services with and without high availability'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 - 具有和不具有高可用性的服务'
- en: '](image/B15566_11_001.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_001.jpg)'
- en: Figure 11.1 – Services with and without high availability
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 - 具有和不具有高可用性的服务
- en: 'In a Kubernetes cluster, there will usually be more than one worker node. The
    high availability of the cluster is guaranteed as even if one worker node is down,
    there are some other worker nodes to host the workload. However, high availability
    is more than running multiple nodes in the cluster. In this section, we will look
    at high availability in Kubernetes clusters from three levels: workloads, Kubernetes
    components, and cloud infrastructure.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中，通常会有多个工作节点。集群的高可用性得到了保证，即使一个工作节点宕机，仍然有其他工作节点来承载工作负载。然而，高可用性不仅仅是在集群中运行多个节点。在本节中，我们将从三个层面来看Kubernetes集群中的高可用性：工作负载、Kubernetes组件和云基础设施。
- en: Enabling high availability of Kubernetes workloads
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用Kubernetes工作负载的高可用性
- en: For Kubernetes workloads such as a deployment and a StatefulSet, you can specify
    the `replicas` field in the specification for how many replicated pods are running
    for the microservice, and controllers will ensure there will be `x` number of
    pods running on different worker nodes in the cluster, as specified in the `replicas`
    field. A DaemonSet is a special workload; the controller will ensure there will
    be one pod running on every node in the cluster, assuming your Kubernetes cluster
    has more than one node. So, specifying more than one replica in the deployment
    or the StatefulSet, or using a DaemonSet, will ensure the high availability of
    your workload. In order to ensure the high availability of the workload, the high
    availability of Kubernetes components needs to be ensured as well.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kubernetes工作负载，比如部署和StatefulSet，您可以在规范中指定`replicas`字段，用于指定微服务运行多少个复制的pod，并且控制器将确保在集群中的不同工作节点上有`x`个pod运行，如`replicas`字段中指定的那样。DaemonSet是一种特殊的工作负载；控制器将确保在集群中的每个节点上都有一个pod运行，假设您的Kubernetes集群有多个节点。因此，在部署或StatefulSet中指定多个副本，或者使用DaemonSet，将确保您的工作负载具有高可用性。为了确保工作负载的高可用性，还需要确保Kubernetes组件的高可用性。
- en: Enabling high availability of Kubernetes components
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用Kubernetes组件的高可用性
- en: 'High availability also applies to the Kubernetes components. Let''s review
    a few critical Kubernetes components, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性也适用于Kubernetes组件。让我们来回顾一下几个关键的Kubernetes组件，如下所示：
- en: '`kube-apiserver`: The Kubernetes API server (`kube-apiserver`) is a control
    plane component that validates and configures data for objects such as pods, services,
    and controllers. It interacts with the objects using **REepresentational State
    Transfer** (**REST**) requests.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`：Kubernetes API服务器（`kube-apiserver`）是一个控制平面组件，用于验证和配置诸如pod、服务和控制器之类的对象的数据。它使用**REepresentational
    State Transfer**（**REST**）请求与对象进行交互。'
- en: '`etcd`: `etcd` is a high-availability key-value store used to store data such
    as configuration, state, and metadata. Its `watch` functionality provides Kubernetes
    with the ability to listen for updates to a configuration and make changes accordingly.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`：`etcd`是一个高可用性的键值存储，用于存储配置、状态和元数据等数据。其`watch`功能使Kubernetes能够监听配置的更新并相应地进行更改。'
- en: '`kube-scheduler`: `kube-scheduler` is a default scheduler for Kubernetes. It
    watches for newly created pods and assigns the pods to the nodes.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`：`kube-scheduler`是Kubernetes的默认调度程序。它会观察新创建的pod并将pod分配给节点。'
- en: '`kube-controller-manager`: The Kubernetes controller manager is a combination
    of the core controllers that watch for state updates and make changes to the cluster
    accordingly.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`：Kubernetes控制器管理器是观察状态更新并相应地对集群进行更改的核心控制器的组合。'
- en: 'If the `kube-apiserver` is down, then basically your cluster is down, as users
    or other Kubernetes components rely on communicating to the `kube-apiserver` to
    perform their tasks. If `etcd` is down, no states of the cluster and objects are
    available to be consumed. `kube-scheduler` and `kube-controller-manager` are also
    important to make sure the workloads are running properly in the cluster. All
    these components are running on the master node, to ensure the high availability
    of the components. One straightforward way is to bring up multiple master nodes
    for your Kubernetes cluster, either via `kops` or `kubeadm`. You will find something
    like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`kube-apiserver`宕机，那么基本上您的集群也会宕机，因为用户或其他Kubernetes组件依赖于与`kube-apiserver`通信来执行其任务。如果`etcd`宕机，那么集群和对象的状态将无法被消费。`kube-scheduler`和`kube-controller-manager`也很重要，以确保工作负载在集群中正常运行。所有这些组件都在主节点上运行，以确保组件的高可用性。一个简单的方法是为您的Kubernetes集群启动多个主节点，可以通过`kops`或`kubeadm`来实现。您会发现类似以下的内容：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now you have multiple `kube-apiserver` pods, `etcd` pods, `kube-controller-manager`
    pods, and `kube-scheduler` pods running in the `kube-system` namespace, and they're
    running on different master nodes. There are some other components such as `kubelet`
    and `kube-proxy` that are running on every node, so, their availability is guaranteed
    by the availability of the nodes, and `kube-dns` are spun up with more than one
    pod by default, so their high availability is ensured. No matter if your Kubernetes
    cluster is running on the public cloud or in a private data center—the infrastructure
    is the pillar to support the availability of the Kubernetes cluster. Next, we
    will talk about the high availability of a cloud infrastructure and use cloud
    providers as an example.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有多个`kube-apiserver` pod、`etcd` pod、`kube-controller-manager` pod和`kube-scheduler`
    pod在`kube-system`命名空间中运行，并且它们在不同的主节点上运行。还有一些其他组件，如`kubelet`和`kube-proxy`，它们在每个节点上运行，因此它们的可用性由节点的可用性保证，并且`kube-dns`默认情况下会启动多个pod，因此它们的高可用性是得到保证的。无论您的Kubernetes集群是在公共云上运行还是在私有数据中心中运行——基础设施都是支持Kubernetes集群可用性的支柱。接下来，我们将讨论云基础设施的高可用性，并以云提供商为例。
- en: Enabling high availability of a cloud infrastructure
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用云基础设施的高可用性
- en: 'Cloud providers offers cloud services all over the world through multiple data
    centers located in different areas. Cloud users can choose in which region and
    zone (the actual data center) to host their service. Regions and zones provide
    isolation from most types of physical infrastructure and infrastructure software
    service failures. Note that the availability of a cloud infrastructure also impacts
    the services running on your Kubernetes cluster if the cluster is hosted in the
    cloud. You should leverage the high availability of the cloud and ultimately ensure
    the high availability of the service running on the Kubernetes cluster. The following
    code block provides an example of specifying zones using `kops` to leverage the
    high availability of a cloud infrastructure:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 云提供商通过位于不同地区的多个数据中心提供全球范围的云服务。云用户可以选择在哪个地区和区域（实际数据中心）托管他们的服务。区域和区域提供了对大多数类型的物理基础设施和基础设施软件服务故障的隔离。请注意，云基础设施的可用性也会影响托管在云中的Kubernetes集群上运行的服务。您应该利用云的高可用性，并最终确保在Kubernetes集群上运行的服务的高可用性。以下代码块提供了使用`kops`指定区域的示例，以利用云基础设施的高可用性：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The nodes of the Kubernetes clusters look like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群的节点如下所示：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding code block shows three master nodes running on the `us-east-1a`,
    `us-east-1b`, and `us-east-1c` availability zones respectively. So, as worker
    nodes, even if one of the data centers is down or under maintenance, both master
    nodes and worker nodes can still function in other data centers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块显示了分别在`us-east-1a`、`us-east-1b`和`us-east-1c`可用区运行的三个主节点。因此，作为工作节点，即使其中一个数据中心宕机或正在维护，主节点和工作节点仍然可以在其他数据中心中运行。
- en: 'In this section, we''ve talked about the high availability of Kubernetes workloads,
    Kubernetes components, and a cloud infrastructure. Let''s use the following diagram
    to recap on the high availability of a Kubernetes cluster:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经讨论了Kubernetes工作负载、Kubernetes组件和云基础设施的高可用性。让我们使用以下图表来总结Kubernetes集群的高可用性：
- en: '![Figure 11.2 – High availability of Kubernetes cluster in the cloud'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.2-云中Kubernetes集群的高可用性'
- en: '](image/B15566_11_002.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_002.jpg)'
- en: Figure 11.2 – High availability of Kubernetes cluster in the cloud
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2-云中Kubernetes集群的高可用性
- en: Now, let's move to the next topic about managing secrets in the Kubernetes cluster.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转到下一个关于在Kubernetes集群中管理秘密的主题。
- en: Managing secrets with Vault
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Vault管理秘密
- en: Secrets management is a big topic, and many open source and proprietary solutions
    have been developed to help solve the secrets management problem on different
    platforms. So, in Kubernetes, its built-in `Secret` object is used to store secret
    data, and the actual data is stored in `etcd` along with other Kubernetes objects.
    By default, the secret data is stored in plaintext (encoded format) in `etcd`.
    `etcd` can be configured to encrypt secrets at rest. Similarly, if `etcd` is not
    configured to encrypt communication using **Transport Layer Security** (**TLS**),
    secret data is transferred in plaintext too. Unless the security requirement is
    very low, it is recommended to use a third-party solution to manage secrets in
    a Kubernetes cluster.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密管理是一个重要的话题，许多开源和专有解决方案已经被开发出来，以帮助解决不同平台上的秘密管理问题。因此，在Kubernetes中，它的内置`Secret`对象用于存储秘密数据，并且实际数据与其他Kubernetes对象一起存储在`etcd`中。默认情况下，秘密数据以明文（编码格式）存储在`etcd`中。`etcd`可以配置为在静止状态下加密秘密。同样，如果`etcd`未配置为使用**传输层安全性**（**TLS**）加密通信，则秘密数据也以明文传输。除非安全要求非常低，否则建议在Kubernetes集群中使用第三方解决方案来管理秘密。
- en: In this section, we're going to introduce Vault, a **Cloud Native Computing
    Foundation** (**CNCF**) secrets management project. Vault supports secure storage
    of secrets, dynamic secrets' generation, data encryption, key revocation, and
    so on. In this section, we will focus on the use case of how to store and provision
    secrets for applications in the Kubernetes cluster using Vault. Now, let's see
    how to set up Vault for the Kubernetes cluster.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍Vault，这是一个**Cloud Native Computing Foundation**（**CNCF**）秘密管理项目。Vault支持安全存储秘密、动态秘密生成、数据加密、密钥吊销等。在本节中，我们将重点介绍如何在Kubernetes集群中为应用程序存储和提供秘密。现在，让我们看看如何为Kubernetes集群设置Vault。
- en: Setting up Vault
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Vault
- en: 'You can deploy Vault in the Kubernetes cluster using `helm`, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`helm`在Kubernetes集群中部署Vault，如下所示：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that `server.dev.enabled=true` is set. This is good for a development
    environment but is not recommended to be set in a production environment. You
    should see two pods are running, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，设置了`server.dev.enabled=true`。这对开发环境很好，但不建议在生产环境中设置。您应该看到有两个正在运行的pod，如下所示：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `vault-0` pod is the one to manage and store secrets, while the `vault-agent-injector-7fd6b9588b-fgsnj`
    pod is responsible for injecting secrets into pods with special vault annotation,
    which we will show in more detail in the *Provisioning and rotating secrets* section.
    Next, let''s create an example secret for a `postgres` database connection, like
    this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`vault-0` pod是用于管理和存储秘密的pod，而`vault-agent-injector-7fd6b9588b-fgsnj` pod负责将秘密注入带有特殊vault注释的pod中，我们将在*提供和轮换秘密*部分中更详细地展示。接下来，让我们为`postgres`数据库连接创建一个示例秘密，如下所示：'
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that the preceding command needs to be executed inside the `vault-0` pod.
    Since you want to restrict only the relevant application in the Kubernetes cluster
    to access the secret, you may want to define a policy to achieve that, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的命令需要在`vault-0` pod内执行。由于您希望限制Kubernetes集群中仅有相关应用程序可以访问秘钥，您可能希望定义一个策略来实现，如下所示：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, you have a policy defining a privilege to read the secret under the `secret`
    path, such as `secret`/`postgres`. Next, you want to associate the policy with
    allowed entities, such as a service account in Kubernetes. This can be done by
    executing the following commands:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您有一个定义了在`secret`路径下读取秘密权限的策略，比如`secret`/`postgres`。接下来，您希望将策略与允许的实体关联，比如Kubernetes中的服务账户。这可以通过执行以下命令来完成：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Vault can leverage naive authentication from Kubernetes and then bind the secret
    access policy to the service account. Now, the service account app in the namespace
    demo can access the `postgres` secret. Now, let''s deploy a demo application in
    the `vault-app.yaml` file, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Vault可以利用Kubernetes的天真认证，然后将秘密访问策略绑定到服务账户。现在，命名空间demo中的服务账户app可以访问`postgres`秘密。现在，让我们在`vault-app.yaml`文件中部署一个演示应用程序，如下所示：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that in the preceding `.yaml` file, there is no annotation added yet,
    so the secret is not injected, nor is the sidecar container added when the application
    is created. The code can be seen in the following snippet:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述的`.yaml`文件中，尚未添加注释，因此在创建应用程序时，秘密不会被注入，也不会添加sidecar容器。代码可以在以下片段中看到：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, we will show how secret injection works.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示秘密注入的工作原理。
- en: Provisioning and rotating secrets
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提供和轮换秘密
- en: 'The reason we don''t show secret injection when the application is deployed
    is that we want to show you the detailed difference before and after injection
    to the demo application pod. Now, let''s patch the deployment with the following
    Vault annotations:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在部署应用程序时不展示秘密注入的原因是，我们想向您展示在注入到演示应用程序pod之前和之后的详细差异。现在，让我们使用以下Vault注释来补丁部署：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding annotation dictates which secret will be injected, and in what
    format and using which role. Once we update the demo application deployment, we
    will find the secret has been injected, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 上述注释规定了将注入哪个秘密，以及以什么格式和使用哪个角色。一旦我们更新了演示应用程序的部署，我们将发现秘密已经被注入，如下所示：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And let''s look at the specification of the pod (not the patched deployment)—you
    will find the following (marked in bold) were added, compared to the specification
    of the patched deployment:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下pod的规范（而不是补丁后的部署）-与补丁后的部署规范相比，您会发现以下内容（用粗体标记）已经添加：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A few things worth mentioning from the preceding changes listed: one `init`
    container named `vault-agent-init` and one sidecar container named `vault-agent`
    have been injected, as well as an `emptyDir` type volume named `vault-secrets`.
    That''s why you saw two containers are running in the demo application pod after
    the patch. Also, the `vault-secrets` volume is mounted in the `init` container,
    the `sidecar` container, and the `app` container with the `/vault/secrets/` directory.
    The secret is stored in the `vault-secrets` volume. The pod specification modification
    is done by the `vault-agent-injector` pod through a predefined mutating webhook
    configuration (installed via `helm`), as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述列出的变化中值得一提的几件事情：注入了一个名为`vault-agent-init`的`init`容器和一个名为`vault-agent`的sidecar容器，以及一个名为`vault-secrets`的`emptyDir`类型卷。这就是为什么在补丁之后，你会看到演示应用程序pod中运行了两个容器。此外，`vault-secrets`卷被挂载在`init`容器、`sidecar`容器和`app`容器的`/vault/secrets/`目录中。秘密存储在`vault-secrets`卷中。通过预定义的变异webhook配置（通过`helm`安装）来完成pod规范的修改，如下所示：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The mutating webhook configuration registered with `kube-apiserver` basically
    tells `kube-apiserver` to redirect any pods, create or update the request to the
    `vault-agent-injector-svc` service in the `demo` namespace. Behind the service
    is the `vault-agent-injector` pod. Then, the `vault-agent-injector` pod will look
    up the relevant annotations and inject the `init` container and the `sidecar`
    container, as well as the volume that stores the secret, to the specification
    of the pod on request. Why do we need one `init` container and one `sidecar` container?
    The `init` container is to prepopulate our secret, and the `sidecar` container
    is to keep that secret data in sync throughout our application's life cycle.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注册到`kube-apiserver`的变异webhook配置基本上告诉`kube-apiserver`将任何pod的创建或更新请求重定向到`demo`命名空间中的`vault-agent-injector-svc`服务。服务的后面是`vault-agent-injector`
    pod。然后，`vault-agent-injector` pod将查找相关的注释，并根据请求将`init`容器和`sidecar`容器以及存储秘密的卷注入到pod的规范中。为什么我们需要一个`init`容器和一个`sidecar`容器？`init`容器是为了预先填充我们的秘密，而`sidecar`容器是为了在整个应用程序生命周期中保持秘密数据同步。
- en: 'Now, let''s update the secret by running the following code and see what happens:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行以下代码来更新秘密，并看看会发生什么：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, the password has been updated to `changeme` from `pass` in the `vault`
    pod. And, on the `demo` application side, we can see from the following code block
    that it is updated as well, after waiting a few seconds:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，密码已从`pass`更新为`changeme`在`vault` pod中。并且，在`demo`应用程序方面，我们可以看到在等待几秒钟后，它也已经更新了：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Vault is a powerful secrets management solution and a lot of its features cannot
    be covered in a single section. I would encourage you to read the documentation
    and try it out to understand Vault better. Next, let's talk about runtime threat
    detection in Kubernetes with Falco.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Vault是一个强大的秘密管理解决方案，它的许多功能无法在单个部分中涵盖。我鼓励你阅读文档并尝试使用它来更好地了解Vault。接下来，让我们谈谈在Kubernetes中使用Falco进行运行时威胁检测。
- en: Detecting anomalies with Falco
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Falco检测异常
- en: Falco is a CNCF open source project that detects anomalous behavior or runtime
    threats in cloud-native environments, such as a Kubernetes cluster. It is a rule-based
    runtime detection engine with about 100 out-of-the-box detection rules. In this
    section, we will first take an overview of Falco, and then we will show you how
    to write Falco rules so that you can build your own Falco rules to protect your
    Kubernetes cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Falco是一个CNCF开源项目，用于检测云原生环境中的异常行为或运行时威胁，比如Kubernetes集群。它是一个基于规则的运行时检测引擎，具有约100个现成的检测规则。在本节中，我们将首先概述Falco，然后向您展示如何编写Falco规则，以便您可以构建自己的Falco规则来保护您的Kubernetes集群。
- en: An overview of Falco
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falco概述
- en: Falco is widely used to detect anomalous behavior in cloud-native environments,
    especially in the Kubernetes cluster. So, what is anomaly detection? Basically,
    it uses behavioral signals to detect security abnormalities, such as leaked credentials
    or unusual activity, and the behavioral signals can be derived from your knowledge
    of the entities in terms of what the normal behavior is.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Falco被广泛用于检测云原生环境中的异常行为，特别是在Kubernetes集群中。那么，什么是异常检测？基本上，它使用行为信号来检测安全异常，比如泄露的凭据或异常活动，行为信号可以从你对实体的了解中得出正常行为是什么。
- en: Challenges faced
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面临的挑战
- en: 'To identify what normal behaviors are in the Kubernetes cluster is not easy.
    From a running application''s perspective, we may group them into three categories,
    as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定Kubernetes集群中的正常行为并不容易。从运行应用程序的角度来看，我们可以将它们分为三类，如下所示：
- en: '**Kubernetes components**: `kube-apiserver`, `kube-proxy`, `kubelet`, the **Container
    Runtime Interface** (**CRI**) plugin, the **Container Networking Interface** (**CNI**)
    plugin, and so on'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes组件**：`kube-apiserver`、`kube-proxy`、`kubelet`、**容器运行时接口**（**CRI**）插件、**容器网络接口**（**CNI**）插件等'
- en: '**Self-hosted applications**: Java, Node.js, Golang, Python, and so on'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自托管应用程序**：Java、Node.js、Golang、Python等'
- en: '**Vendor services**: Cassandra, Redis, MySQL, NGINX, Tomcat, and so on'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**供应商服务**：Cassandra、Redis、MySQL、NGINX、Tomcat等'
- en: 'Or, from a system''s perspective, we have the following types of activities:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，从系统的角度来看，我们有以下类型的活动：
- en: File activities such as open, read, and write
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件活动，如打开、读取和写入
- en: Process activities such as `execve` and `clone` system calls
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程活动，如`execve`和`clone`系统调用
- en: Network activities such as accept, connect, and send
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络活动，如接受、连接和发送
- en: 'Or, from a Kubernetes object''s perspective: `pod`, `secret`, `deployment`,
    `namespace`, `serviceaccount`, `configmap`, and so on'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，从Kubernetes对象的角度来看：`pod`、`secret`、`deployment`、`namespace`、`serviceaccount`、`configmap`等
- en: In order to cover all these activities or behaviors happening in the Kubernetes
    cluster, we will need rich sources of information. Next, let's talk about the
    event sources that Falco relies on to do anomalous detection, and how the sources
    cover the preceding activities and behaviors.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了覆盖Kubernetes集群中发生的所有这些活动或行为，我们将需要丰富的信息来源。接下来，让我们谈谈Falco依赖的事件来源，以进行异常检测，以及这些来源如何涵盖前述的活动和行为。
- en: Event sources for anomaly detection
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常检测的事件来源
- en: Falco relies on two event sources to do anomalous detection. One is system calls
    and the other is the Kubernetes audit events. For system call events, Falco uses
    a kernel module to tap into the stream of system calls on a machine, and then
    passes those system calls to a user space (`ebpf` is recently supported as well).
    Within the user space, Falco also enriches the raw system call events with more
    context such as the process name, container ID, container name, image name, and
    so on. For Kubernetes audit events, users need to enable the Kubernetes audit
    policy and register the Kubernetes audit webhook backend with the Falco service
    endpoint. Then, the Falco engine checks any of the system call events or Kubernetes
    audit events matching any Falco rules loaded in the engine.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Falco依赖两个事件来源进行异常检测。一个是系统调用，另一个是Kubernetes审计事件。对于系统调用事件，Falco使用内核模块来监听机器上的系统调用流，并将这些系统调用传递到用户空间（最近也支持了`ebpf`）。在用户空间，Falco还会丰富原始系统调用事件的上下文，如进程名称、容器ID、容器名称、镜像名称等。对于Kubernetes审计事件，用户需要启用Kubernetes审计策略，并将Kubernetes审计webhook后端注册到Falco服务端点。然后，Falco引擎检查引擎中加载的任何Falco规则匹配的任何系统调用事件或Kubernetes审计事件。
- en: 'It''s also important to talk about the rationale for using system calls and
    Kubernetes audit events as event sources to do anomalous detection. System calls
    are a programmatic way for applications to interact with the operating system
    in order to access resources such as files, devices, the network, and so on. Considering
    containers are a bunch of processes with their own dedicated namespaces and that
    they share the same operating system on the node, a system call is the one unified
    event source that can be used to monitor activities from containers. It doesn''t
    matter what programming language the application is written in; ultimately, all
    the functions will be translated into system calls to interact with the operating
    system. Take a look at the following diagram:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论使用系统调用和Kubernetes审计事件作为事件源进行异常检测的原因也很重要。系统调用是应用程序与操作系统交互以访问文件、设备、网络等资源的编程方式。考虑到容器是一组具有自己专用命名空间的进程，并且它们共享节点上相同的操作系统，系统调用是可以用来监视容器活动的统一事件源。应用程序使用什么编程语言并不重要；最终，所有函数都将被转换为系统调用以与操作系统交互。看一下下面的图表：
- en: '![Figure 11.3 – Containers and system calls'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.3 - 容器和系统调用'
- en: '](image/B15566_11_003.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_003.jpg)'
- en: Figure 11.3 – Containers and system calls
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 - 容器和系统调用
- en: In the preceding diagram, there are four containers running different applications.
    These applications may be written in different programming languages, and all
    of them call a function to open a file with a different function name (for example,
    `fopen`, `open`, and `os.Open`). However, from the operating system's perspective,
    all these applications call the same system call, `open`, but maybe with different
    parameters. Falco is able to retrieve events from system calls so that it doesn't
    matter what kind of applications they are or what kind of programming language
    is in use.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，有四个运行不同应用程序的容器。这些应用程序可能使用不同的编程语言编写，并且它们都调用一个函数来以不同的函数名打开文件（例如，`fopen`、`open`和`os.Open`）。然而，从操作系统的角度来看，所有这些应用程序都调用相同的系统调用`open`，但可能使用不同的参数。Falco能够从系统调用中检索事件，因此无论应用程序是什么类型或使用什么编程语言都不重要。
- en: On the other hand, with the help of Kubernetes audit events, Falco has full
    visibility into a Kubernetes object's life cycle. This is also important for anomalous
    detection. For example, it may be abnormal that there is a pod with a `busybox`
    image launched as a privileged pod in a production environment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，借助Kubernetes审计事件，Falco可以完全了解Kubernetes对象的生命周期。这对于异常检测也很重要。例如，在生产环境中，以特权方式启动一个带有`busybox`镜像的pod可能是异常的。
- en: Overall, the two event sources—system calls and Kubernetes audit events—are
    sufficient to cover all the meaningful activities happening in the Kubernetes
    cluster. Now, with an understanding of Falco event sources, let's wrap up our
    overview on Falco with a high-level architecture diagram.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，两个事件源——系统调用和Kubernetes审计事件——足以覆盖Kubernetes集群中发生的所有重要活动。现在，通过对Falco事件源的理解，让我们用一个高级架构图总结一下Falco的概述。
- en: High-level architecture
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级架构
- en: 'Falco is mainly composed of a few components, as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Falco主要由几个组件组成，如下：
- en: '**Falco rules**: Rules that are defined to detect whether an event is an anomaly.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Falco规则**：定义用于检测事件是否异常的规则。'
- en: '**Falco engine**: Evaluate an incoming event with Falco rules and throw an
    output if an event matches any of the rules.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Falco引擎**：使用Falco规则评估传入事件，并在事件匹配任何规则时产生输出。'
- en: '**Kernel module/Sysdig libraries**: Tag system call events and enrich them
    before sending to the Falco engine for evaluation.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核模块/Sysdig库**：在发送到Falco引擎进行评估之前，标记系统调用事件并丰富它们。'
- en: '**Web server**: Listen on Kubernetes audit events and pass on to the Falco
    engine for evaluation.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web服务器：监听Kubernetes审计事件并传递给Falco引擎进行评估。
- en: 'The following diagram shows Falco''s internal architecture:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Falco的内部架构：
- en: '![Figure 11.4 – Falco''s internal architecture'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 - Falco的内部架构
- en: '](image/B15566_11_004.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_004.jpg)'
- en: Figure 11.4 – Falco's internal architecture
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 - Falco的内部架构
- en: Now, we have wrapped up our overview of Falco. Next, let's try to create some
    Falco rules and detect any anomalous behavior.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经总结了Falco的概述。接下来，让我们尝试创建一些Falco规则并检测任何异常行为。
- en: Creating Falco rules to detect anomalies
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Falco规则以检测异常
- en: 'Before we dive into Falco rules, make sure you have Falco installed by running
    the following command:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究Falco规则之前，请确保已通过以下命令安装了Falco：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The Falco DaemonSet should be running in your Kubernetes cluster, as illustrated
    in the following code block:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Falco DaemonSet应该在您的Kubernetes集群中运行，如下面的代码块所示：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: To enable the Kubernetes audit and register Falco as the webhook backend, please
    follow the instructions in the Falco repository ([https://github.com/falcosecurity/evolution/tree/master/examples/k8s_audit_config](https://github.com/falcosecurity/evolution/tree/master/examples/k8s_audit_config)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用Kubernetes审计并将Falco注册为webhook后端，请按照Falco存储库中的说明进行操作（[https://github.com/falcosecurity/evolution/tree/master/examples/k8s_audit_config](https://github.com/falcosecurity/evolution/tree/master/examples/k8s_audit_config)）。
- en: 'There are three types of elements in the Falco rules, as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Falco规则中有三种类型的元素，如下所示：
- en: '**Rule**: A condition under which an alert will be triggered. A rule has the
    following attributes: rule name, description, condition, priority, source, tags,
    and output. When an event matches any rule''s condition, an alert is generated
    based on the output definition of the rule.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规则：触发警报的条件。规则具有以下属性：规则名称、描述、条件、优先级、来源、标签和输出。当事件匹配任何规则的条件时，根据规则的输出定义生成警报。
- en: '**Macro**: A rule condition snippet that can be reused by other rules or macros.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宏：可以被其他规则或宏重复使用的规则条件片段。
- en: '**List**: A collection of items that can be used by macros and rules.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表：可以被宏和规则使用的项目集合。
- en: To facilitate Falco users in building their own rules, Falco provides a handful
    of default lists and macros.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便Falco用户构建自己的规则，Falco提供了一些默认列表和宏。
- en: Creating the system call rule
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建系统调用规则
- en: Falco system call rules evaluate system call events—more precisely, the enriched
    system calls. System call event fields are provided by the kernel module and are
    identical to the Sysdig (an open source tool built by the Sysdig company) filter
    fields. The policy engine uses Sysdig's filter to extract information such as
    the process name, container image, and file path from system call events and evaluate
    them with Falco rules.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Falco系统调用规则评估系统调用事件 - 更准确地说是增强的系统调用。系统调用事件字段由内核模块提供，并且与Sysdig（Sysdig公司构建的开源工具）过滤字段相同。策略引擎使用Sysdig的过滤器从系统调用事件中提取信息，如进程名称、容器映像和文件路径，并使用Falco规则进行评估。
- en: 'The following are the most common Sysdig filter fields that can be used to
    build Falco rules:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以用于构建Falco规则的最常见的Sysdig过滤字段：
- en: '**proc.name**: Process name'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: proc.name：进程名称
- en: '**fd.name**: File name that is written to or read from'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fd.name：写入或读取的文件名
- en: '**container.id**: Container ID'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: container.id：容器ID
- en: '**container.image.repository**: Container image name without tag'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: container.image.repository：不带标签的容器映像名称
- en: '**fd.sip and fd.sport**: Server **Internet Protocol** (**IP**) address and
    server port'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fd.sip和fd.sport：服务器**Internet Protocol**（**IP**）地址和服务器端口
- en: '**fd.cip and fd.cport**: Client IP and client port'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fd.cip和fd.cport：客户端IP和客户端端口
- en: '**evt.type**: System call event (`open`, `connect`, `accept`, `execve`, and
    so on)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**evt.type**: 系统调用事件（`open`、`connect`、`accept`、`execve`等）'
- en: 'Let''s try to build a simple Falco rule. Assume that you have a `nginx` pod
    that serves static files from the `/usr/share/nginx/html/` directory only. So,
    you can create a Falco rule to detect any anomalous file read activities as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个简单的Falco规则。假设您有一个`nginx` pod，仅从`/usr/share/nginx/html/`目录提供静态文件。因此，您可以创建一个Falco规则来检测任何异常的文件读取活动，如下所示：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding rule used two default macros: `open_read` and `container`. The
    `open_read` macro checks if the system call event is open in read mode only, while
    the `container` macro checks if the system call event happened inside a container.
    Then, the rule applies to containers running the `kaizheh/insecure-nginx` image
    only, and the `fd.directory` filter retrieves the file directory information from
    the system call event. In this rule, it checks if there is any file read outside
    of the `/usr/share/nginx/html/` directory. So, what if there is misconfiguration
    of `nginx` that leads to file path traversal (reading files under arbitrary directories)?
    An example of this is shown in the following code block:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的规则使用了两个默认宏：`open_read`和`container`。`open_read`宏检查系统调用事件是否仅以读模式打开，而`container`宏检查系统调用事件是否发生在容器内。然后，该规则仅适用于运行`kaizheh/insecure-nginx`镜像的容器，并且`fd.directory`过滤器从系统调用事件中检索文件目录信息。在此规则中，它检查是否有任何文件读取超出`/usr/share/nginx/html/`目录。那么，如果`nginx`的配置错误导致文件路径遍历（在任意目录下读取文件）会怎么样？以下代码块显示了一个示例：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'At the same time, Falco detects file access beyond the designated directory,
    with the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，Falco检测到超出指定目录的文件访问，输出如下：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, let's look at how to use K8s audit rules.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用K8s审计规则。
- en: Creating K8s audit rules
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建K8s审计规则
- en: 'K8s audit rules evaluate Kubernetes audit events. We''ve already shown what
    a Kubernetes audit event record looks like, earlier in this chapter. Similar to
    Sysdig filters, there are two ways to retrieve the information out of a Kubernetes
    audit event. One is to use the **JavaScript Object Notation** (**JSON**) pointer;
    the other is to use Falco built-in filters. The following are a few commonly used
    Falco built-in filters to retrieve the information of Kubernetes audit events:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: K8s审计规则评估Kubernetes审计事件。在本章的前面部分，我们已经展示了Kubernetes审计事件记录的样子。与Sysdig过滤器类似，有两种方法可以从Kubernetes审计事件中检索信息。一种是使用**JavaScript对象表示法**（**JSON**）指针；另一种是使用Falco内置过滤器。以下是用于检索Kubernetes审计事件信息的一些常用Falco内置过滤器：
- en: '`ka.verb`: The verb field of the Kubernetes audit event. `jevt.value[/verb]`
    is its corresponding JSON pointer.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ka.verb`: Kubernetes审计事件的动词字段。`jevt.value[/verb]`是其对应的JSON指针。'
- en: '`ka.target.resource`: The resource field of the Kubernetes audit event. `jevt.value[/objectRef/resource]`
    is its corresponding JSON pointer.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ka.target.resource`: Kubernetes审计事件的资源字段。`jevt.value[/objectRef/resource]`是其对应的JSON指针。'
- en: '`ka.user.name`: The username field of the Kubernetes audit event. `jevt.value[/user/username]`
    is its corresponding JSON pointer.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ka.user.name`: Kubernetes审计事件的用户名字段。`jevt.value[/user/username]`是其对应的JSON指针。'
- en: '`ka.uri`: The `requestURI` field of the Kubernetes audit event. `jet.value[/requestURI]`
    is its corresponding JSON pointer.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ka.uri`: Kubernetes审计事件的`requestURI`字段。`jet.value[/requestURI]`是其对应的JSON指针。'
- en: 'Let''s try to build a simple K8s audit rule. Assume that you don''t want to
    deploy images in the `kube-system` namespaces except a few trusted images for
    services such as `kube-apiserver`, `etcd-manager`, and more. So, you can create
    a Falco rule, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个简单的K8s审计规则。假设您不希望在`kube-system`命名空间中部署除了一些受信任的服务镜像（如`kube-apiserver`、`etcd-manager`等）之外的镜像。因此，您可以创建一个Falco规则，如下所示：
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'First, we define a list of trusted images that will be allowed to be deployed
    in the `kube-system` namespace. In the rule, we use two default macros: `pod`
    and `kcreate`. The `pod` macro checks if the target resource is a pod, while `kcreate`
    checks if the verb is `create`. We also check if the target namespace is `kube-system`
    and that the deploying image is not in the `trusted_images` list. The `k8s_audit`
    value from the `source` field of the rule indicates this rule evaluates the Kubernetes
    audit events. Then, if we try to deploy a `busybox` image pod in the `kube-system`
    namespace, we will see the following alert from Falco:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了一个受信任的镜像列表，这些镜像将被允许部署到“kube-system”命名空间中。在规则中，我们使用了两个默认宏：“pod”和“kcreate”。
    “pod”宏检查目标资源是否为Pod，而“kcreate”检查动词是否为“create”。我们还检查目标命名空间是否为“kube-system”，并且部署的镜像不在“trusted_images”列表中。规则的“source”字段中的“k8s_audit”值表示此规则评估Kubernetes审计事件。然后，如果我们尝试在“kube-system”命名空间中部署“busybox”镜像的Pod，我们将从Falco看到以下警报：
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that in order for this rule to work, the audit level for a pod's creation
    needs to be at least at the `Request` level, with which the audit events include
    the pod's specification information, such as the image.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了使此规则起作用，需要将Pod创建的审计级别至少设置为“请求”级别，其中审计事件包括Pod的规范信息，例如镜像。
- en: 'In this section, we introduced Falco and showed you how to create Falco rules
    from both event sources: system calls and Kubernetes audit events. Both rules
    are used to detect anomalous activities based on the known benign activities of
    the workload or cluster. Next, let''s talk about how to do forensics in the Kubernetes
    cluster.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了Falco，并向您展示了如何从系统调用和Kubernetes审计事件两个事件源创建Falco规则。这两个规则都用于基于工作负载或集群已知良性活动来检测异常活动。接下来，让我们谈谈如何在Kubernetes集群中进行取证工作。
- en: Conducting forensics with Sysdig Inspect and CRIU
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Sysdig Inspect和CRIU进行取证。
- en: Forensics in cybersecurity means collecting, processing, and analyzing information
    in support of vulnerability mitigation and/or fraud, counterintelligence, or law
    enforcement investigations. The more data you can preserve and the faster the
    analysis you can conduct on the collected data, the quicker you will trace down
    an attack and respond to the incident better. In this section, we will show you
    how to use the CRIU and Sysdig open source tools to collect data, and then introduce
    Sysdig Inspect, an open source tool for analyzing data collected by Sysdig.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全中，取证意味着收集、处理和分析信息，以支持漏洞缓解和/或欺诈、反情报或执法调查。您可以保存的数据越多，对收集的数据进行的分析越快，您就越能追踪攻击并更好地应对事件。在本节中，我们将向您展示如何使用CRIU和Sysdig开源工具来收集数据，然后介绍Sysdig
    Inspect，这是一个用于分析Sysdig收集的数据的开源工具。
- en: Using CRIU to collect data
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CRIU收集数据
- en: '**CRIU** is the abbreviation of **Checkpoint and Restore In Userspace**. It
    is a tool that can freeze a running container and capture the container''s state
    on disk. Later on, the container''s and application''s data saved on the disk
    can be restored to the state it was at the time of the freeze. It is useful for
    container snapshots, migration, and remote debugging. From a security standpoint,
    it is especially useful to capture malicious activities in action in the container
    (so that you may kill the container right after the checkpoint) and then restore
    the state in a sandboxed environment for further analysis.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**CRIU**是**Checkpoint and Restore In Userspace**的缩写。它是一个可以冻结运行中的容器并在磁盘上捕获容器状态的工具。稍后，可以将磁盘上保存的容器和应用程序数据恢复到冻结时的状态。它对于容器快照、迁移和远程调试非常有用。从安全的角度来看，它特别有用于捕获容器中正在进行的恶意活动（以便您可以在检查点后立即终止容器），然后在沙盒环境中恢复状态以进行进一步分析。'
- en: CRIU works as a Docker plugin and is still in experimental mode, and there is
    a known issue that CRIU is not working properly in the most recent few versions
    ([https://github.com/moby/moby/issues/37344](https://github.com/moby/moby/issues/37344)).
    For demo purposes, I have used an older Docker version (Docker CE 17.03) and will
    show how to use CRIU to checkpoint a running container and restore the state back
    as a new container.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: CRIU作为Docker插件工作，仍处于实验阶段，已知问题是CRIU在最近的几个版本中无法正常工作（[https://github.com/moby/moby/issues/37344](https://github.com/moby/moby/issues/37344)）。出于演示目的，我使用了较旧的Docker版本（Docker
    CE 17.03），并将展示如何使用CRIU对运行中的容器进行检查点，并将状态恢复为新容器。
- en: 'To enable CRIU, you will need to enable the `experimental` mode in the Docker
    daemon, as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用CRIU，您需要在Docker守护程序中启用`experimental`模式，如下所示：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And then, after restarting the Docker daemon, you should be able to execute
    the `docker checkpoint` command successfully, like this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在重新启动Docker守护程序后，您应该能够成功执行`docker checkpoint`命令，就像这样：
- en: '[PRE29]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, follow the instructions to install CRIU ([https://criu.org/Installation](https://criu.org/Installation)).
    Next, let''s see a simple example to show how powerful CRIU is. I have a simple
    `busybox` container running to increase the counter by `1` every second, as illustrated
    in the following code snippet:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，按照说明安装CRIU（[https://criu.org/Installation](https://criu.org/Installation)）。接下来，让我们看一个简单的示例，展示CRIU的强大之处。我有一个简单的`busybox`容器在运行，每秒增加`1`，如下面的代码片段所示：
- en: '[PRE30]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After sleeping for a few seconds, I then see the output of the counter increasing,
    as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 睡了几秒钟后，我看到计数器的输出在增加，如下所示：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, I would like to checkpoint the container and store the state to the local
    filesystem, like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我想对容器进行检查点，并将状态存储到本地文件系统，就像这样：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, the `checkpoint` state has been saved under the `/tmp` directory. Note
    that the container looper will be killed after the checkpoint unless you specify
    a `--leave-running` flag when creating the checkpoint.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`checkpoint`状态已保存在`/tmp`目录下。请注意，除非在创建检查点时指定了`--leave-running`标志，否则容器looper将在检查点后被杀死。
- en: 'Then, create a mirror container without running it, like this:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建一个镜像容器，但不运行它，就像这样：
- en: '[PRE33]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we can start the new `looper-clone` container with the stored state. Let''s
    wait another few seconds and see what happens. The result can be seen in the following
    code snippet:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以启动具有存储状态的新`looper-clone`容器。让我们再等几秒钟，看看会发生什么。结果可以在下面的代码片段中看到：
- en: '[PRE34]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The new `looper-clone` container starts counting at `6`, which means the state
    (the counter was `5`) was successfully restored and used.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 新的`looper-clone`容器从`6`开始计数，这意味着状态（计数器为`5`）已成功恢复并使用。
- en: CRIU is very useful for container forensics, especially when there are some
    suspicious activities happening in a container. You can checkpoint the container
    (assuming you have multiple replicas running within the cluster), let CRIU kill
    the suspicious container, and then restore the suspicious state of the container
    in a sandboxed environment for further analysis. Next, let's talk about another
    way to capture data for forensics.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: CRIU对容器取证非常有用，特别是当容器中发生可疑活动时。您可以对容器进行检查点（假设在集群中有多个副本运行），让CRIU杀死可疑容器，然后在沙盒环境中恢复容器的可疑状态以进行进一步分析。接下来，让我们谈谈另一种获取取证数据的方法。
- en: Using Sysdig and Sysdig Inspect
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Sysdig和Sysdig Inspect
- en: Sysdig is an open source tool for Linux system exploration and troubleshooting
    with support for containers. Sysdig can also be used to create trace files for
    system activity through instrumenting into the Linux kernel and capturing system
    calls and other operating system events. The capture capability makes it an awesome
    forensics tool for a containerized environment. To support capture system calls
    in the Kubernetes cluster, Sysdig offers a `kubectl` plugin, `kubectl-capture`,
    which enables you to capture system calls of the target pods as simply as with
    some other `kubectl` commands. After the capture is finished, Sysdig Inspect,
    a powerful open source tool, can be used to do troubleshooting and security investigation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig是一个用于Linux系统探索和故障排除的开源工具，支持容器。Sysdig还可以用于通过在Linux内核中进行仪器化和捕获系统调用和其他操作系统事件来创建系统活动的跟踪文件。捕获功能使其成为容器化环境中的一种出色的取证工具。为了支持在Kubernetes集群中捕获系统调用，Sysdig提供了一个`kubectl`插件，`kubectl-capture`，它使您可以像使用其他`kubectl`命令一样简单地捕获目标pod的系统调用。捕获完成后，可以使用强大的开源工具Sysdig
    Inspect进行故障排除和安全调查。
- en: 'Let''s continue to use `insecure-nginx` as an example, since we''ve got a Falco
    alert, as illustrated in the following code snippet:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续以`insecure-nginx`为例，因为我们收到了Falco警报，如下面的代码片段所示：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: By the time the alert was triggered, it is still possible the `nginx` pod was
    undergoing an attack. There are a few things you can do to respond. Starting a
    capture and then analyzing more context out of the Falco alert is one of them.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在触发警报时，`nginx` pod仍然可能正在遭受攻击。您可以采取一些措施来应对。启动捕获，然后分析Falco警报的更多上下文是其中之一。
- en: 'To trigger a capture, download `kubectl-capture` from [https://github.com/sysdiglabs/kubectl-capture](https://github.com/sysdiglabs/kubectl-capture)
    and place it with the other `kubectl` plugins, like this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要触发捕获，请从[https://github.com/sysdiglabs/kubectl-capture](https://github.com/sysdiglabs/kubectl-capture)下载`kubectl-capture`并将其放置在其他`kubectl`插件中，就像这样：
- en: '[PRE36]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, start a capture on the `nginx` pod, like this:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像这样在`nginx` pod上启动捕获：
- en: '[PRE37]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Under the hood, `kubectl-capture` starts a new pod to do the capture on the
    host where the suspected victim pod is running, with a `120`-second capture duration,
    so that we can see everything that is happening right now and in the next `120`
    seconds in that host. Once the capture is done, the zipped capture file will be
    created in the current working directory. You can bring in Sysdig Inspect as a
    Docker container to start a security investigation, like this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`kubectl-capture`在运行疑似受害者pod的主机上启动一个新的pod进行捕获，持续时间为`120`秒，这样我们就可以看到主机上正在发生的一切以及接下来`120`秒内的情况。捕获完成后，压缩的捕获文件将在当前工作目录中创建。您可以将Sysdig
    Inspect作为Docker容器引入，以开始安全调查，就像这样：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, log in to `http://localhost:3000`, and you should see the login **user
    interface** (**UI**). Remember to unzip the `scap` file so that you should be
    able to see the overview page of the capture file, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，登录到`http://localhost:3000`，您应该看到登录**用户界面**（**UI**）。记得解压`scap`文件，这样您就可以看到捕获文件的概述页面，如下所示：
- en: '![Figure 11.5 – Sysdig Inspect overview'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.5 - Sysdig Inspect概述'
- en: '](image/B15566_11_005.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_005.jpg)'
- en: Figure 11.5 – Sysdig Inspect overview
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 - Sysdig Inspect概述
- en: 'Sysdig Inspect provides a full-blown insight into the activities happening
    inside the containers from the following angles:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig Inspect从以下角度提供了对容器内发生活动的全面洞察：
- en: Executed commands
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行的命令
- en: File access
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件访问
- en: Network connections
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络连接
- en: System calls
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统调用
- en: 'Let''s do a little more digging than just the Falco alert. From the alert,
    we may suspect this is a file path traversal issue as it is the `nginx` process
    accessing the `/etc/passwd` file, and we know that this pod serves static files
    only so that the `nginx` process should never access any files outside of the
    `/usr/share/nginx/html/` directory. Now, let''s take a look at the following screenshot
    to see what the network requests sent to `nginx` pod were:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们不仅仅限于Falco警报进行更深入的挖掘。根据警报，我们可能怀疑这是一个文件路径遍历问题，因为是`nginx`进程访问`/etc/passwd`文件，我们知道这个pod只提供静态文件服务，所以`nginx`进程不应该访问`/usr/share/nginx/html/`目录之外的任何文件。现在，让我们看一下以下截图，看看发送给`nginx`
    pod的网络请求是什么：
- en: '![Figure 11.6 – Sysdig Inspect investigating network connections to nginx'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.6 – Sysdig Inspect调查连接到nginx的网络连接'
- en: '](image/B15566_11_006.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_006.jpg)'
- en: Figure 11.6 – Sysdig Inspect investigating network connections to nginx
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – Sysdig Inspect调查连接到nginx的网络连接
- en: 'After looking into the connections, we see that the requests came from a single
    IP, `100.123.226.66`, which looks like a pod IP. Could it be from the same cluster?
    Click the **Containers** view on the left panel and specify `fd.cip=100.123.226.66`
    in the filter. Then, you will find out it is from the `anchore-cli` container,
    as shown in the following screenshot:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看连接后，我们发现请求来自单个IP，`100.123.226.66`，看起来像是一个pod IP。它可能来自同一个集群吗？在左侧面板上点击**Containers**视图，并在过滤器中指定`fd.cip=100.123.226.66`。然后，你会发现它来自`anchore-cli`容器，如下截图所示：
- en: '![Figure 11.7 – Sysdig Inspect investigating a container sending a request
    to nginx'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.7 – Sysdig Inspect调查一个容器向nginx发送请求'
- en: '](image/B15566_11_007.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_007.jpg)'
- en: Figure 11.7 – Sysdig Inspect investigating a container sending a request to
    nginx
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – Sysdig Inspect调查一个容器向nginx发送请求
- en: 'The `anchore-cli` pod actually happens to run on the same node as the `nginx`
    pod, as shown in the following code block:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，`anchore-cli` pod碰巧运行在与`nginx` pod相同的节点上，如下面的代码块所示：
- en: '[PRE39]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now we know that there might be some file path traversal attack launched from
    the `anchore-cli` pod, let''s look at what this is (just double-click on the entry
    in the preceding **Sysdig Inspect** page), as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道可能有一些文件路径遍历攻击是从`anchore-cli` pod发起的，让我们看看这是什么（只需在前面的**Sysdig Inspect**页面中双击条目），如下所示：
- en: '![Figure 11.8 – Sysdig Inspect investigating path traversal attack commands'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.8 – Sysdig Inspect调查路径遍历攻击命令'
- en: '](image/B15566_11_008.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_11_008.jpg)'
- en: Figure 11.8 – Sysdig Inspect investigating path traversal attack commands
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 – Sysdig Inspect调查路径遍历攻击命令
- en: 'We found that there is list of file path traversal commands executed in the
    `anchore-cli` pod, detailed as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现在`anchore-cli` pod中执行了一系列文件路径遍历命令，详细如下：
- en: '`curl 100.71.138.95/files../etc/`'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用curl命令访问100.71.138.95上的文件../etc/
- en: '`curl 100.71.138.95/files../`'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用curl命令访问100.71.138.95上的文件../
- en: '`curl 100.71.138.95/files../etc/passwd`'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用curl命令访问100.71.138.95上的文件../etc/passwd
- en: '`curl 100.71.138.95/files../etc/shadow`'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用curl命令访问100.71.138.95上的文件../etc/shadow
- en: We're now able to get a step closer to the attacker, and the next step is to
    try to investigate more into how the attacker landed in the `anchore-cli` pod.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在能够更接近攻击者了，下一步是尝试更深入地调查攻击者是如何进入`anchore-cli` pod的。
- en: Both CRIU and Sysdig are powerful tools to conduct forensics in a containerized
    environment. Hopefully, the CRIU issue can be fixed soon. And note that CRIU also
    requires the Docker daemon to be run in `experimental` mode, while Sysdig and
    Sysdig Inspect work more at the Kubernetes level. Sysdig Inspect provides a nice
    UI to help navigate through different activities that happened in the pods and
    containers.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: CRIU和Sysdig都是在容器化环境中进行取证的强大工具。希望CRIU问题能够很快得到解决。请注意，CRIU还需要Docker守护程序以`experimental`模式运行，而Sysdig和Sysdig
    Inspect更多地在Kubernetes级别工作。Sysdig Inspect提供了一个漂亮的用户界面，帮助浏览发生在Pod和容器中的不同活动。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this long chapter, we covered Kubernetes auditing, high availability for
    a Kubernetes cluster, managing secrets with Vault, detecting anomalous activities
    with Falco, and conducting forensics with CRIU and Sysdig. Though you may find
    it will take quite some time to get familiar with all the practices and tools,
    defense in depth is a huge topic and it is worth digging deeper into security
    so that you may build up a stronger fence for your Kubernetes cluster.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一长章中，我们涵盖了Kubernetes审计、Kubernetes集群的高可用性、使用Vault管理秘密、使用Falco检测异常活动以及使用CRIU和Sysdig进行取证。虽然您可能会发现需要花费相当长的时间来熟悉所有的实践和工具，但深度防御是一个庞大的主题，值得深入研究安全性，这样您就可以为Kubernetes集群建立更强大的防护。
- en: 'Most of the tools we talked about are easy to install and deploy. I would encourage
    you to try them out: add your own Kubernetes audit rules, use Vault to manage
    secrets in Kubernetes clusters, build your own Falco rules to detect anomalous
    behavior because you know your cluster better than anyone else, and use Sysdig
    to collect all the forensics data. Once you get familiar with all of these tools,
    you should feel confident that your Kubernetes cluster is a bit more under control.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们谈到的大多数工具都很容易安装和部署。我鼓励您尝试它们：添加自己的Kubernetes审计规则，使用Vault在Kubernetes集群中管理秘密，编写自己的Falco规则来检测异常行为，因为您比任何其他人都更了解您的集群，并使用Sysdig收集所有取证数据。一旦您熟悉了所有这些工具，您应该会对自己的Kubernetes集群更有信心。
- en: In the next chapter, we're going to talk about some known attacks, such as the
    crypto mining hack against Kubernetes clusters, and see how we can use the techniques
    we learned in this book to mitigate these attacks.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论一些已知的攻击，比如针对Kubernetes集群的加密挖矿攻击，看看我们如何利用本书中学到的技术来减轻这些攻击。
- en: Questions
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why should we not set the audit level to `Request` or `RequestResponse` for
    secret objects?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们不应该将审计级别设置为`Request`或`RequestResponse`用于秘密对象？
- en: What flag is used to set up multiple master nodes in `kops`?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`kops`中用什么标志设置多个主节点？
- en: What does the sidecar container do when a secret is updated in Vault?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当Vault中的秘密更新时，侧车容器会做什么？
- en: What are the event sources that Falco uses?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Falco使用哪些事件源？
- en: Which filter does Falco use to retrieve the process name from the system call
    event?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Falco使用哪个过滤器从系统调用事件中检索进程名称？
- en: What can CRIU do to a running container?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CRIU对正在运行的容器有什么作用？
- en: What can you do with Sysdig Inspect?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以用Sysdig Inspect做什么？
- en: Further references
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多参考资料
- en: 'Kubernetes auditing: [https://kubernetes.io/docs/tasks/debug-application-cluster/audit/](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes审计：[https://kubernetes.io/docs/tasks/debug-application-cluster/audit/](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)
- en: 'High availability with `kubeadm`: [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubeadm`实现高可用性：[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)
- en: 'Vault: [https://www.vaultproject.io/docs/internals/architecture](https://www.vaultproject.io/docs/internals/architecture)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vault：[https://www.vaultproject.io/docs/internals/architecture](https://www.vaultproject.io/docs/internals/architecture)
- en: 'Falco: https://falco.org/docs/'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falco：https://falco.org/docs/
- en: 'Sysdig filtering: [https://github.com/draios/sysdig/wiki/Sysdig-User-Guide#user-content-filtering](https://github.com/draios/sysdig/wiki/Sysdig-User-Guide#user-content-filtering)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sysdig过滤：[https://github.com/draios/sysdig/wiki/Sysdig-User-Guide#user-content-filtering](https://github.com/draios/sysdig/wiki/Sysdig-User-Guide#user-content-filtering)
- en: 'CRIU: [https://criu.org/Docker](https://criu.org/Docker)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CRIU：[https://criu.org/Docker](https://criu.org/Docker)
- en: 'Sysdig `kubectl-capture`: [https://sysdig.com/blog/tracing-in-kubernetes-kubectl-capture-plugin/](https://sysdig.com/blog/tracing-in-kubernetes-kubectl-capture-plugin/)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sysdig `kubectl-capture`：[https://sysdig.com/blog/tracing-in-kubernetes-kubectl-capture-plugin/](https://sysdig.com/blog/tracing-in-kubernetes-kubectl-capture-plugin/)
- en: 'Sysdig Inspect: [https://github.com/draios/sysdig-inspect](https://github.com/draios/sysdig-inspect)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sysdig Inspect：[https://github.com/draios/sysdig-inspect](https://github.com/draios/sysdig-inspect)
- en: 'Sysdig: [https://github.com/draios/sysdig](https://github.com/draios/sysdig)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sysdig：[https://github.com/draios/sysdig](https://github.com/draios/sysdig)
