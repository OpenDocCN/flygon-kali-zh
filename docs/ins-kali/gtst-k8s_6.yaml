- en: Chapter 6. Monitoring and Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。监控和日志记录
- en: This chapter will cover the usage and customization of both built-in and third-party
    monitoring tools on our Kubernetes cluster. We will cover how to use the tools
    to monitor health and performance of our cluster. In addition, we will look at
    built-in logging, the **Google Cloud Logging** service, and **Sysdig**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍在我们的Kubernetes集群上使用内置和第三方监控工具的用法和定制。我们将介绍如何使用这些工具来监控我们集群的健康和性能。此外，我们还将研究内置日志记录、**Google
    Cloud Logging**服务和**Sysdig**。
- en: 'This chapter will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主题：
- en: How Kuberentes uses cAdvisor, Heapster, InfluxDB, and Grafana
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuberentes如何使用cAdvisor、Heapster、InfluxDB和Grafana
- en: How to customize the default Grafana dashboard
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何自定义默认的Grafana仪表板
- en: How FluentD and Grafana are used
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用FluentD和Grafana
- en: How to install and use logging tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何安装和使用日志记录工具
- en: How to work with popular third-party tools, such as StackDriver and Sysdig,
    to extend our monitoring capabilities
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用流行的第三方工具，如StackDriver和Sysdig，来扩展我们的监控能力
- en: Monitoring operations
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控操作
- en: Real-world monitoring goes far beyond checking whether a system is up and running.
    Although health checks, like those you learned in [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 2. Kubernetes – Core Concepts and Constructs"), *Kubernetes – Core Concepts
    and Constructs*, under the *Health checks* section, can help us isolate problem
    applications. Operation teams can best serve the business when they can anticipate
    the issues and mitigate them before a system goes offline.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的监控远远不止是检查系统是否正常运行。尽管健康检查（就像您在[第2章](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第2章。Kubernetes-核心概念和构造")中学到的那样，*Kubernetes-核心概念和构造*，在*健康检查*部分，可以帮助我们隔离问题应用程序。当运营团队能够预测问题并在系统下线之前减轻问题时，他们可以最好地为业务服务。
- en: Best practices in monitoring are to measure the performance and usage of core
    resources and watch for trends that stray from the normal baseline. Containers
    are not different here, and a key component to managing our Kubernetes cluster
    is having a clear view into performance and availability of the OS, network, system
    (CPU and memory), and storage resources across all nodes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 监控中的最佳实践是衡量核心资源的性能和使用情况，并观察偏离正常基线的趋势。容器在这方面也不例外，管理我们的Kubernetes集群的关键组件是清晰地了解操作系统、网络、系统（CPU和内存）以及所有节点上的存储资源的性能和可用性。
- en: In this chapter, we will examine several options to monitor and measure the
    performance and availability of all our cluster resources. In addition, we will
    look at a few options for alerting and notifications when irregular trends start
    to emerge.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究几种选项，以监控和衡量我们集群资源的性能和可用性。此外，当出现异常趋势时，我们将研究一些警报和通知的选项。
- en: Built-in monitoring
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内置监控
- en: 'If you recall from [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    we noted that our nodes were already running a number of monitoring services.
    We can see these once again by running the `get pods` command with the `kube-system`
    namespace specified as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得[第1章](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7 "第1章。Kubernetes和容器操作")中提到的，我们注意到我们的节点已经运行了许多监控服务。我们可以通过使用`get
    pods`命令并指定`kube-system`命名空间来再次看到这些服务：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图是前面命令的结果：
- en: '![Built-in monitoring](../images/00058.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![内置监控](../images/00058.jpeg)'
- en: Figure 6.1\. System pod listing
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1。系统pod列表
- en: Again, we see a variety of services, but how does this all fit together? If
    you recall the *Node (formerly minions)* section from [Chapter 2](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 2. Kubernetes – Core Concepts and Constructs"), *Kubernetes – Core Concepts
    and Constructs*, each node is running a kublet. The kublet is the main interface
    for nodes to interact and update the API server. One such update is the **metrics**
    of the node resources. The actual reporting of the resource usage is performed
    by a program named cAdvisor.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们看到了各种各样的服务，但这一切是如何结合在一起的呢？如果你回忆一下[第2章](part0022_split_000.html#KVCC1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第2章 Kubernetes – 核心概念和构造")中的*Node（以前是minions）*部分，*Kubernetes – 核心概念和构造*，每个节点都在运行一个kublet。Kublet是节点与API服务器进行交互和更新的主要接口。其中一个更新是节点资源的**指标**。实际的资源使用情况报告是由一个名为cAdvisor的程序执行的。
- en: '**cAdvisor** is another open source project from Google, which provides various
    metrics on container resource use. Metrics include CPU, memory, and network statistics.
    There is no need to tell cAdvisor about individual containers; it collects the
    metrics for all containers on a node and reports this back to the kublet, which
    in turn reports to Heapster.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**cAdvisor**是谷歌的另一个开源项目，它提供了有关容器资源使用的各种指标。指标包括CPU、内存和网络统计。无需告诉cAdvisor有关单个容器；它收集节点上所有容器的指标，并将其报告给kublet，后者再报告给Heapster。'
- en: Note
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Google''s open source projects**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**谷歌的开源项目**'
- en: Google has a variety of open source projects related to Kubernetes. Check them
    out, use them, and even contribute your own code!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌有许多与Kubernetes相关的开源项目。查看它们，使用它们，甚至贡献你自己的代码！
- en: 'cAdvisor and Heapster are mentioned in the following section:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor和Heapster在以下部分中提到：
- en: '**cAdvisor**: [https://github.com/google/cadvisor](https://github.com/google/cadvisor)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cAdvisor**：[https://github.com/google/cadvisor](https://github.com/google/cadvisor)'
- en: '**Heapster**: [https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Heapster**：[https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)'
- en: '**Contrib** is a catch-all for a variety of components that are not part of
    core Kubernetes. It is found at [https://github.com/kubernetes/contrib](https://github.com/kubernetes/contrib).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Contrib**是一个包含各种不属于核心Kubernetes的组件的地方。它可以在[https://github.com/kubernetes/contrib](https://github.com/kubernetes/contrib)找到。'
- en: '**LevelDB** is a key store library that was used in the creation of InfluxDB.
    It is found at [https://github.com/google/leveldb](https://github.com/google/leveldb).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**LevelDB**是在创建InfluxDB时使用的键存储库。它可以在[https://github.com/google/leveldb](https://github.com/google/leveldb)找到。'
- en: '**Heapster** is yet another open source project from Google; you may start
    to see a theme emerging here (see the preceding information box). Heapster runs
    in a container on one of the minion nodes and aggregates the data from kublet.
    A simple REST interface is provided to query the data.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Heapster**是谷歌的另一个开源项目；你可能会开始看到一个主题在这里出现（参见前面的信息框）。Heapster在一个minion节点上的容器中运行，并聚合来自kublet的数据。提供了一个简单的REST接口来查询数据。'
- en: When using the GCE setup, a few additional packages are set up for us, which
    saves us time and gives us a complete package to monitor our container workloads.
    As we can see from Figure 6.1, there is another pod with `influx-grafana` in the
    title.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用GCE设置时，我们设置了一些额外的软件包，这为我们节省了时间，并为我们提供了一个完整的软件包来监视我们的容器工作负载。正如我们从图6.1中可以看到的，标题中还有另一个带有`influx-grafana`的pod。
- en: '**InfluxDB** is described at it''s official website as follows¹:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfluxDB**在其官方网站上被描述如下¹：'
- en: '*An open-source distributed time series database with no external dependencies.*'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*一个没有外部依赖的开源分布式时间序列数据库。*'
- en: It is based on a key store package (see the previous *Google's open source projects*
    information box) and is perfect to store and query event or time-based statistics
    such as those provided by Heapster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于一个键存储包（请参阅前面的*Google开源项目*信息框）,非常适合存储和查询事件或基于时间的统计数据，例如`Heapster`提供的数据。
- en: Finally, we have **Grafana**, which provides a dashboard and graphing interface
    for the data stored in InfluxDB. Using Grafana, users can create a custom monitoring
    dashboard and get immediate visibility into the health of their Kubernetes cluster
    and therefore their entire container infrastructure.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有**Grafana**，它为存储在InfluxDB中的数据提供了仪表板和图形界面。使用Grafana，用户可以创建自定义监控仪表板，并立即了解其Kubernetes集群的健康状况，从而了解其整个容器基础架构的健康状况。
- en: Exploring Heapster
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索Heapster
- en: 'Let''s quickly look at the REST interface by SSH''ing to the node with the
    Heapster pod. First, we can list the pods to find the one running Heapster as
    follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过SSH连接到具有Heapster pod的节点来快速查看REST接口。首先，我们可以列出pod以找到运行Heapster的pod，如下所示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The name of the pod should start with `monitoring-heapster`. Run a `describe`
    command to see which node it is running on as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: pod的名称应该以`monitoring-heapster`开头。执行`describe`命令查看它正在哪个节点上运行，如下所示：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: From the output in the following figure (Figure 6.2), we can see that the pod
    is running in `kubernetes-minion-merd`. Also note the IP for the pod, a few lines
    down, as we will need that in a moment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从下图的输出（图6.2）中，我们可以看到pod正在`kubernetes-minion-merd`中运行。还要注意一下pod的IP，因为我们一会儿会用到。
- en: '![Exploring Heapster](../images/00059.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![探索Heapster](../images/00059.jpeg)'
- en: Figure 6.2\. Heapster pod details
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2\. Heapster pod详情
- en: 'Next, we can SSH to this box with the familiar `gcloud ssh` command as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用熟悉的`gcloud ssh`命令SSH到这个节点：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From here, we can access the `Heapster` REST API directly using the pod's IP
    address. Remember that pod IPs are routable not only in the containers but also
    on the nodes themselves. The `Heapster` API is listening on port `8082`, and we
    can get a full list of metrics at `/api/v1/metric-export-schema/`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以直接使用pod的IP地址访问`Heapster` REST API。请记住，pod IP不仅在容器中可路由，而且在节点本身也可路由。`Heapster`
    API正在`8082`端口监听，我们可以在`/api/v1/metric-export-schema/`获取完整的指标列表。
- en: 'Let''s see the list now by issuing a `curl` command to the pod IP address we
    saved from the `describe` command as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过发出`curl`命令到我们从`describe`命令中保存的pod IP地址来查看列表：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will see a listing that is quite long. The first section shows all the metrics
    available. The last two sections list fields by which we can filter and group.
    For your convenience, I''ve added the following tables that are a little bit easier
    to read:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一个相当长的列表。第一部分显示所有可用的指标。最后两部分列出了我们可以按照哪些字段进行过滤和分组。为了方便起见，我添加了以下表格，这些表格稍微容易阅读一些：
- en: '| Metric | Description | Unit | Type |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 描述 | 单位 | 类型 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| uptime | The number of milliseconds since the container was started | ms
    | cumulative |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| uptime | 容器启动以来的毫秒数 | 毫秒 | 累积 |'
- en: '| cpu/usage | Cumulative CPU usage on all cores | ns | cumulative |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| cpu/usage | 所有核心的累积CPU使用率 | 纳秒 | 累积 |'
- en: '| cpu/limit | CPU limit in millicores | - | gauge |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| cpu/limit | CPU限制（以毫核为单位） | - | 计量 |'
- en: '| memory/usage | Total memory usage | bytes | gauge |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| memory/usage | 总内存使用量 | 字节 | 计量 |'
- en: '| memory/working_set | Total working set usage. Working set is the memory being
    used and not easily dropped by the kernel | bytes | gauge |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| memory/working_set | 总工作集使用量。工作集是内存被使用且内核不容易释放的部分 | 字节 | 计量 |'
- en: '| memory/limit | Memory limit | bytes | gauge |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| memory/limit | 内存限制 | 字节 | 计量 |'
- en: '| memory/page_faults | The number of page faults | - | cumulative |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| memory/page_faults | 页面错误数 | - | 累积 |'
- en: '| memory/major_page_faults | The number of major page faults | - | cumulative
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| memory/major_page_faults | 主要页面错误的数量 | - | 累积 |'
- en: '| network/rx | Cumulative number of bytes received over the network | bytes
    | cumulative |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| network/rx | 累积接收网络字节数 | 字节 | 累积 |'
- en: '| network/rx_errors | Cumulative number of errors while receiving over the
    network | - | cumulative |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| network/rx_errors | 接收网络时累积的错误数 | - | 累积 |'
- en: '| network/tx | Cumulative number of bytes sent over the network | bytes | cumulative
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| network/tx | 累积发送网络字节数 | 字节 | 累积 |'
- en: '| network/tx_errors | Cumulative number of errors while sending over the network
    | - | cumulative |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| network/tx_errors | 发送网络时累积的错误数 | - | 累积 |'
- en: '| filesystem/usage | Total number of bytes consumed on a filesystem | bytes
    | gauge |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| filesystem/usage | 文件系统上消耗的总字节数 | 字节 | 测量 |'
- en: '| filesystem/limit | The total size of filesystem in bytes | bytes | gauge
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| filesystem/limit | 文件系统的总字节大小 | 字节 | 测量 |'
- en: '*Table 6.1\. Available Heapster metrics*'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表6.1\. 可用的Heapster指标*'
- en: '| Field | Description | Label type |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 字段 | 描述 | 标签类型 |'
- en: '| --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `hostname` | The hostname where the container ran | Common |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `hostname` | 容器运行的主机名 | 通用 |'
- en: '| `host_id` | An identifier specific to a host, which is set by cloud provider
    or user | Common |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `host_id` | 主机的特定标识符，由云提供商或用户设置 | 通用 |'
- en: '| `container_name` | The user-provided name of the container or full container
    name for system containers | Common |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `container_name` | 容器的用户提供的名称或系统容器的完整容器名称 | 通用 |'
- en: '| `pod_name` | The name of the pod | Pod |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `pod_name` | Pod的名称 | Pod |'
- en: '| `pod_id` | The unique ID of the pod | Pod |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `pod_id` | Pod的唯一ID | Pod |'
- en: '| `pod_namespace` | The namespace of the pod | Pod |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `pod_namespace` | Pod的命名空间 | Pod |'
- en: '| `namespace_id` | The unique ID of the namespace of the pod | Pod |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `namespace_id` | Pod的命名空间的唯一ID | Pod |'
- en: '| `labels` | A comma-separated list of user-provided labels | Pod |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `labels` | 用户提供的标签的逗号分隔列表 | Pod |'
- en: '*Table 6.2\. Available Heapster fields*'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表6.2\. 可用的Heapster字段*'
- en: Customizing our dashboards
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义我们的仪表板
- en: 'Now that we have the fields, we can have some fun. Recall the Grafana page
    we looked at in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*.
    Let''s pull that up again by going our cluster''s monitoring URL. Note that you
    may need to log in with your cluster credentials. Refer to the following format
    of the link you need to use:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了字段，我们可以玩得开心了。回想一下我们在[第1章](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第1章。Kubernetes和容器操作")中看到的Grafana页面，*Kubernetes和容器操作*。让我们再次打开它，方法是转到我们集群的监控URL。请注意，您可能需要使用您的集群凭据登录。请参考您需要使用的链接的以下格式：
- en: '`https://`**`<your master IP>`**`/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`https://`**`<您的主节点IP>`**`/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana`'
- en: We'll see the default Kubernetes dashboard, and now we can add our own statistics
    to the board. Scroll all the way to the bottom and click on **Add a Row**. This
    should create a space for a new row and present a green tab on the left-hand side
    of the screen.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到默认的Kubernetes仪表板，现在我们可以向仪表板添加我们自己的统计信息。滚动到底部，点击**添加行**。这将为新行创建一个空间，并在屏幕左侧显示一个绿色标签。
- en: Let's start by adding a view into the filesystem usage for each node (minion).
    Click on the *green* tab to expand and then choose **Add Panel** and then **graph**.
    An empty graph should appear on the screen. If we click on the **graph** where
    it says **no title (click here)**, a context menu will appear. We can then click
    on **Edit**, and we'll be able to set up the query for our custom dashboard panel.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先为每个节点（minion）添加一个文件系统使用情况的视图。点击*绿色*选项卡展开，然后选择**添加面板**，然后选择**图表**。屏幕上应该会出现一个空图表。如果我们点击**图表**上写着**无标题（点击此处）**的地方，将会出现一个上下文菜单。然后我们可以点击**编辑**，我们就能设置自定义仪表板面板的查询。
- en: 'The **series** box allows us to use any of the Heapster metrics we saw in the
    previous tables. In the **series** box, enter `filesystem/usage_bytes_gauge` and
    select to **max(value)**. Then, enter `5s` for **group by time** and **hostname**
    in the box marked column next to the plus sign, as shown in the following screenshot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**系列**框允许我们使用我们在前面表格中看到的任何Heapster指标。在**系列**框中，输入`filesystem/usage_bytes_gauge`并选择**max(value)**。然后，在标有加号旁边的列框中输入`5s`作为**按时间分组**，并输入**主机名**，如下图所示：'
- en: '![Customizing our dashboards](../images/00060.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![自定义我们的仪表板](../images/00060.jpeg)'
- en: Figure 6.3\. Heapster pod details
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3. Heapster pod详细信息
- en: Next, let's click on the **Axes & Grid** tab, so that we can set the units and
    legend. Under **Left Y Axis**, set **Format** to **bytes** and **Label** to **Disk
    Space Used**. Under **Right Y Axis**, set **Format** to **none**. Next, under
    **Legend** styles, make sure to check **Show values, and table**. A **Legend Values**
    section should appear, and we can check the box for **Max** here.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们点击**轴和网格**选项卡，这样我们就可以设置单位和图例。在**左Y轴**下，将**格式**设置为**字节**，**标签**设置为**磁盘空间已用**。在**右Y轴**下，将**格式**设置为**无**。接下来，在**图例**样式下，确保勾选**显示数值和表格**。应该会出现一个**图例数值**部分，我们可以在这里勾选**最大值**的框。
- en: Now, let's quickly go to the **General** tab and choose a title. In my case,
    I named mine `Filesystem Disk Usage by Node (max)`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们快速转到**常规**选项卡并选择一个标题。在我的情况下，我将我的命名为`节点的文件系统磁盘使用情况（最大值）`。
- en: We don't want to lose this nice new graph we've created, so let's click on the
    save icon in the top right corner. It looks like a *floppy disk* (you can do a
    Google image search if you don't know what those are).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想丢失我们创建的这个漂亮的新图表，所以让我们点击右上角的保存图标。它看起来像一个*软盘*（如果你不知道这是什么，你可以做一个谷歌图片搜索）。
- en: After we click on the save icon, a dropdown will appear with several options.
    The first item should have the default dashboard title, which is **Kubernetes
    Cluster!** at the time of this writing. Also, click on the save icon on the right-hand
    side.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 点击保存图标后，将会出现一个下拉菜单，其中有几个选项。第一项应该是默认的仪表板标题，在撰写本文时是**Kubernetes Cluster!**。同时，也点击右侧的保存图标。
- en: It should take us back to the main dashboard where we will see our new graph
    at the bottom. Let's add another panel to that row. Again use the *green* tab
    and then select **Add Panel** and **singlestat**. Once again, an empty panel will
    appear, and we can click it where it says **no title (click here)** for the context
    menu and then click on **Edit**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该会带我们回到主仪表板，在那里我们将在底部看到我们的新图表。让我们在该行再添加一个面板。再次使用*绿色*选项卡，然后选择**添加面板**和**单个状态**。再次出现一个空面板，我们可以点击上面写着**无标题（点击此处）**的地方，然后点击**编辑**。
- en: Let's say, we want to watch a particular node and monitor memory usage. We can
    easily do this by setting the where clause in our query. First, choose **network/rx_bytes_cumulative**
    for **series** and **mean(value)** for **select**. Then, we can specify the hostname
    in the `where` clause with `hostname=kubernetes-minion-35ao` and **group by time**
    to `5s`. (Use one of your own hostnames if you are following along).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要监视特定节点并监控内存使用情况。我们可以通过设置查询中的where子句来轻松实现这一点。首先，选择**network/rx_bytes_cumulative**作为**series**，选择**mean(value)**作为**select**。然后，我们可以在`where`子句中使用`hostname=kubernetes-minion-35ao`指定主机名，并将**group
    by time**设置为`5s`。（如果您在跟随进行操作，请使用自己的主机名之一）。
- en: '![Customizing our dashboards](../images/00061.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![自定义我们的仪表板](../images/00061.jpeg)'
- en: Figure 6.4\. Singlestat options
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4\. Singlestat选项
- en: Under the **Options** tab, make sure that **Unit format** is set to **bytes**
    and check the **Spark line** box under **Spark lines**. The **sparkline** gives
    us a quick history view of the recent variation in the value. We can use the **Background**
    mode to take up the entire background; by default, it uses the area below the
    value.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在**选项**选项卡下，确保**单位格式**设置为**字节**，并在**Spark线**下勾选**Spark线**框。**sparkline**为我们提供了最近值的快速历史视图。我们可以使用**背景**模式来占据整个背景；默认情况下，它使用值下面的区域。
- en: Tip
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Under **Coloring**, we can optionally check the **Value** box. A **Thresholds**
    and **Colors** section will appear. This will allow us to choose different colors
    for the value based on the threshold tier we specify. Note that an unformatted
    version of the number must be used for threshold values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在**着色**下，我们可以选择勾选**值**框。将出现**阈值**和**颜色**部分。这将允许我们根据我们指定的阈值层选择不同的颜色值。请注意，阈值值必须使用未格式化的数字。
- en: 'Now, let''s go back to the **General** tab and choose a title as **Network
    bytes received (Node 35ao)**. Once again, let''s save our work and return to the
    dashboard. We should now have a row that looks like the following figure (Figure
    6.5):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到**常规**选项卡，并选择一个标题为**网络字节接收（节点35ao）**。再次保存我们的工作并返回到仪表板。我们现在应该有一行看起来像下图（图6.5）的样子：
- en: '![Customizing our dashboards](../images/00062.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![自定义我们的仪表板](../images/00062.jpeg)'
- en: Figure 6.5\. Custom dashboard panels
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5\. 自定义仪表板面板
- en: A third type of panel we didn't cover is **text**. It's pretty straightforward
    and allows us to place a block of text on the dashboard using HTML, markdown,
    or just plain text.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有涵盖的第三种面板类型是**文本**。这很简单，允许我们使用HTML、markdown或纯文本在仪表板上放置一块文本。
- en: As we can see, it is pretty easy to build a custom dashboard and monitor the
    health of our cluster at a glance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，构建自定义仪表板并一目了然地监视我们集群的健康状况非常容易。
- en: FluentD and Google Cloud Logging
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FluentD和Google Cloud Logging
- en: Looking back at Figure 6.1, you may have noted a number of pods starting with
    the words **fluentd-cloud-logging-kubernetes**. These pods appear when using the
    GCE provider for your K8s cluster. A pod like this exists on every node in our
    cluster and its sole purpose to handle the processing of Kubernetes logs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾图6.1，您可能已经注意到一些以**fluentd-cloud-logging-kubernetes**开头的pod。当使用GCE提供程序为您的K8s集群时，这些pod会出现。我们集群中的每个节点上都存在这样一个pod，它的唯一目的是处理Kubernetes日志的处理。
- en: If we log in to our Google Cloud Platform account, we can see some of the logs
    processed there. Simply navigate to our project page, and on the left, under **Monitoring**,
    click on **Logs**. (If you are using the beta console, it will be under **Operations**
    and then **Logging**.) This will take us to a log listing page with a number of
    drop-down menus on the top. If this is your first time visiting the page, you
    should see a log selection dropdown with the value **All Logs**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们登录到我们的Google Cloud Platform账户，我们可以在那里看到一些处理过的日志。只需导航到我们的项目页面，在左侧的**监控**下，点击**日志**。（如果您正在使用beta控制台，它将在**操作**下，然后是**日志**。）这将带我们到一个日志列表页面，顶部有许多下拉菜单。如果这是您第一次访问该页面，您应该会看到一个日志选择下拉菜单，其中值为**所有日志**。
- en: In this dropdown, we'll see a number of Kubernetes-related entries, including
    **kublet** and some entries with **kubernetes** at the beginning of the label.
    We can also filter by date and use the *play* button to watch events stream in
    live.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个下拉菜单中，我们会看到许多与Kubernetes相关的条目，包括**kublet**和一些以**kubernetes**开头的标签条目。我们还可以按日期筛选，并使用*播放*按钮观看实时事件流。
- en: '![FluentD and Google Cloud Logging](../images/00063.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![FluentD和Google Cloud Logging](../images/00063.jpeg)'
- en: Figure 6.6\. The Google Cloud Logging filter
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6。Google Cloud Logging过滤器
- en: FluentD
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FluentD
- en: Now we know that the `fluentd-cloud-logging-kubernetes` pods are sending the
    data to the Google Cloud, but why do we need FluentD? Simply put, **FluentD**
    is a collector. It can be configured to have multiple sources to collect and tag
    logs, which are then sent to various output points for analysis, alerting, or
    archiving. We can even transform data using plugins before it is passed on to
    its destination.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道`fluentd-cloud-logging-kubernetes` pods正在将数据发送到Google Cloud，但是为什么我们需要FluentD呢？简单地说，**FluentD**是一个收集器。它可以配置为具有多个来源来收集和标记日志，然后将这些日志发送到各种输出点进行分析、警报或归档。甚至在将数据传递到目的地之前，我们可以使用插件来转换数据。
- en: Not all provider setups have FluentD installed by default, but it is one of
    the recommended approaches to give us greater flexibility for future monitoring
    operations. The AWS Kubernetes setup also uses FluentD, but instead forwards events
    to **Elasticsearch**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的提供商设置都默认安装了FluentD，但这是一个推荐的方法，可以为我们未来的监控操作提供更大的灵活性。AWS Kubernetes设置也使用FluentD，但是将事件转发到**Elasticsearch**。
- en: Note
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Exploring FluentD**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索FluentD**'
- en: If you are curious about the inner workings of the FluentD setup or just want
    to customize the log collection, we can explore quite easily using the `kubectl
    exec` command.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对FluentD设置的内部工作方式感到好奇，或者只是想自定义日志收集，我们可以使用`kubectl exec`命令很容易地进行探索。
- en: 'First, let''s see if we can find the FluentD `config` file:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看是否能找到FluentD的`config`文件：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Usually, we would look in the `etc` folder for a `ta-agent` or `fluent` subfolder.
    However, if we run an `ls` command, we''ll see that there is no `ta-agent` or
    `fluent` subfolder, but there is a `google-fluentd` subfolder:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会在`etc`文件夹中寻找`ta-agent`或`fluent`子文件夹。然而，如果我们运行`ls`命令，我们会发现没有`ta-agent`或`fluent`子文件夹，但有一个`google-fluentd`子文件夹：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'While searching in this directory, we should see a `google-fluentd.conf` file.
    We can view that file with a simple `cat` command as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索这个目录时，我们应该看到一个`google-fluentd.conf`文件。我们可以使用简单的`cat`命令查看该文件，如下所示：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We should see a number of sources including the `kublet`, `containers`, `etcd`,
    and various other Kubernetes components.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到许多来源，包括`kublet`、`containers`、`etcd`和其他各种Kubernetes组件。
- en: Note that while we can make changes here, remember that is a running container
    and our changes won't be saved if the pod dies or is restarted. If we really want
    to customize, it's best to use this container as a base and build a new container
    that we can push to a repository for later use.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然我们可以在这里进行更改，但请记住这是一个正在运行的容器，如果Pod死机或重新启动，我们的更改将不会被保存。如果我们真的想自定义，最好使用此容器作为基础，并构建一个新的容器，以便将来使用。
- en: Maturing our monitoring operations
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完善我们的监控操作
- en: While Grafana gives us a great start to monitor our container operations, it
    is still a work in progress. In the real world of operations, having a complete
    dashboard view is great once we know there is a problem. However, in everyday
    scenarios, we'd prefer to be proactive and actually receive notifications when
    issues arise. This kind of alerting capability is a must to keep the operations
    team ahead of the curve and out of *reactive mode*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Grafana为我们提供了一个很好的开始来监视我们的容器操作，但它仍然是一个正在进行中的工作。在实际运营中，一旦我们知道有问题，拥有完整的仪表板视图是很好的。然而，在日常情况下，我们更希望是主动的，并在问题出现时实际收到通知。这种警报功能对于使运营团队保持领先并避免*被动模式*是必不可少的。
- en: 'There are many solutions available in this space, and we will take a look at
    two in particular: GCE monitoring (StackDriver) and Sysdig.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域有许多解决方案可用，我们将特别关注两个：GCE监控（StackDriver）和Sysdig。
- en: GCE (StackDriver)
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GCE（StackDriver）
- en: '**StackDriver** is a great place to start for infrastructure in the public
    cloud. It is actually owned by Google, so it''s integrated as the Google Cloud
    Platform monitoring service. Before your lock-in alarm bells start ringing, StackDriver
    also has solid integration with AWS. In addition, StackDriver has alerting capability
    with support for notification to a variety of platforms and webhooks for anything
    else.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**StackDriver**是公共云基础设施的绝佳起点。实际上，它由Google拥有，因此作为Google Cloud Platform监控服务进行集成。在您的锁定警报开始响起之前，请注意，StackDriver还与AWS有很好的集成。此外，StackDriver具有警报功能，支持通知到各种平台和用于其他任何事情的Webhook。'
- en: Sign-up for GCE monitoring
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册GCE监控
- en: In the GCE console, under the **Monitoring** section, there is a **Dashboard
    & alerts** link (or just the **Monitoring** link under **Operations** in the beta
    console). This will open a new window where we can enable the monitoring functionality
    (still in beta at the time of this writing). Once enabled, we'll be taken to a
    screen that has install instructions for each operating system (this will be under
    **Set up and monitor an endpoint** in the beta console). It will also show your
    API key, which is necessary for the installation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCE控制台中，在**监控**部分下，有一个**仪表板和警报**链接（或者在测试版控制台中的**操作**下只有**监控**链接）。这将打开一个新窗口，我们可以在其中启用监控功能（在撰写本文时仍处于测试版）。启用后，我们将进入一个屏幕，其中包含每个操作系统的安装说明（这将在测试版控制台中的**设置和监控端点**下）。它还会显示您的API密钥，这对安装是必要的。
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you want to do something similar in AWS, you can simply sign up for account
    at StackDriver''s main website:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在AWS中做类似的事情，您可以简单地在StackDriver的主要网站注册帐户：
- en: '[https://www.stackdriver.com/](https://www.stackdriver.com/)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.stackdriver.com/](https://www.stackdriver.com/)'
- en: Installation instructions for the more common installs can be found at [http://support.stackdriver.com/customer/en/portal/articles/1491726-what-is-the-stackdriver-agent](http://support.stackdriver.com/customer/en/portal/articles/1491726-what-is-the-stackdriver-agent).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 常见安装的安装说明可以在[http://support.stackdriver.com/customer/en/portal/articles/1491726-what-is-the-stackdriver-agent](http://support.stackdriver.com/customer/en/portal/articles/1491726-what-is-the-stackdriver-agent)找到。
- en: We can find our API key under **Account Settings** and **API Keys**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在**帐户设置**和**API密钥**下找到我们的API密钥。
- en: Click on **Go to Monitoring** to proceed. We'll be taken to the main dashboard
    page where we will see some basic statistics on our node in the cluster. If we
    go to **Infrastructure** and then **Instances**, we'll be taken to a page with
    all our nodes listed. By clicking on the individual node, we can again see some
    basic information even without an agent installed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**转到监控**继续。我们将被带到主仪表板页面，在那里我们将看到集群中节点的一些基本统计信息。如果我们转到**基础设施**然后**实例**，我们将被带到列出所有节点的页面。通过点击单个节点，即使没有安装代理，我们也可以再次看到一些基本信息。
- en: Configure detailed monitoring
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置详细的监控
- en: As we have seen, simply enabling monitoring will give us basic stats for all
    our machines in GCE, but if we want to get detailed results, we'll need the agent
    on each node. Let's walk through an install.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，简单地启用监控将为我们在GCE中的所有机器提供基本统计信息，但如果我们想要获得详细的结果，我们将需要在每个节点上安装代理。让我们来安装一下。
- en: 'As before, we''ll want to use the `gcloud compute ssh` command to get a shell
    on one of our minion nodes. Then, we can download and install the agent. If you
    need your API key, this can be found by clicking your user icon in the top-right
    corner and going to **Account Settings** and then on the next page, click on **API
    Keys** in the menu on the left:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们将使用`gcloud compute ssh`命令在其中一个从节点上获取shell。然后，我们可以下载并安装代理。如果您需要API密钥，可以通过点击右上角的用户图标，转到**帐户设置**，然后在下一页中点击左侧菜单中的**API密钥**来找到。
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If everything goes well, we should have an agent installed and ready. We can
    check this by running the `info` command as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，我们应该已经安装并准备好了代理。我们可以通过运行以下`info`命令来检查：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We should see a lot of information in the form of JSON on the screen. After
    you finish, give the agent a few minutes before going back to **Infrastructure**
    and **Instances**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在屏幕上看到大量的JSON格式的信息。完成后，给代理几分钟，然后返回到**基础设施**和**实例**。
- en: On the summary instance page, we'll note that all our GCE instances are showing
    CPU usage. However, only the instance with the agent installed will show the **Memory
    usage** statistic.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在摘要实例页面上，我们会注意到所有的GCE实例都显示了CPU使用情况。然而，只有安装了代理的实例才会显示**内存使用**统计信息。
- en: Click on the node with the agent installed, so we can inspect it a bit further.
    If we click on each one and look at the details page, we should note that the
    instance with the agent installed has a lot more information. Although all instances
    report CPU usage, Disk I/O, and network traffic, the instance with the agent has
    much more.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 点击安装了代理的节点，这样我们可以进一步检查它。如果我们点击每个节点并查看详细页面，我们应该注意到安装了代理的实例有更多的信息。尽管所有实例都报告CPU使用情况、磁盘I/O和网络流量，但安装了代理的实例有更多的信息。
- en: '![Configure detailed monitoring](../images/00064.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![配置详细监控](../images/00064.jpeg)'
- en: Figure 6.7\. Google Cloud Monitoring with agent installed
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7。安装了代理的Google云监控
- en: In Figure 6.7, we can see a variety of additional charts including Open TCP
    connections and processes as well as CPU steal (not pictured). We also have better
    visibility into the machine details such as network interfaces, file systems,
    and operating system information.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.7中，我们可以看到各种额外的图表，包括打开的TCP连接和进程，以及CPU偷取（未显示）。我们还可以更好地了解机器的详细信息，如网络接口、文件系统和操作系统信息。
- en: Now that we see how much information is available, we can install the agent
    on the remaining instances. You may also wish to install an agent on the master
    as it is a critical piece of your Kubernetes infrastructure.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到了有多少信息可用，我们可以在剩下的实例上安装代理。您可能还希望在主节点上安装代理，因为它是您的Kubernetes基础设施的关键部分。
- en: Alerts
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 警报
- en: Next, we can look at the alerting policies available as part of the monitoring
    service. From the instance details page, click on the **Create Alerting Policy**
    button in the **Incidents** section at the top of the page.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以查看作为监控服务一部分的警报策略。从实例详细信息页面，单击页面顶部**事件**部分的**创建警报策略**按钮。
- en: We'll name the policy as `Excessive CPU Load` and set a metric threshold. Under
    the section, in the **Metric Threshold** area, click on **Next** and then in the
    **TARGET** section, set **Resource Type** to **Instances**. Then, set **Applies
    To** to **Group** and **kubernetes**. Leave **Condition Triggers If** set to **Any
    Member Violates**.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将策略命名为`过高的CPU负载`并设置度量阈值。在**度量阈值**区域下，单击**下一步**，然后在**目标**部分中，将**资源类型**设置为**实例**。然后，将**适用于**设置为**组**和**kubernetes**。将**条件触发**设置为**任何成员违反**。
- en: Click on **Next** and leave **IF METRIC** as **CPU (agent)** and **CONDITION**
    as **above**. Now set **THRESHOLD (PERCENT)** to `80` and leave the time under
    **FOR** to **5 minutes**. Click on **Save Condition**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 单击**下一步**，将**如果度量**保留为**CPU（代理）**，将**条件**保留为**高于**。现在将**阈值（百分比）**设置为`80`，并将**持续时间**下的时间保留为**5分钟**。单击**保存条件**。
- en: '![Alerts](../images/00065.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![警报](../images/00065.jpeg)'
- en: Figure 6.8\. Google Cloud Monitoring alert policy
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8。Google Cloud监控警报策略
- en: Finally, we will add a notification. Under that section, leave **Method** as
    **Email** and click on **Add Notification**. Enter your e-mail address and then
    click on **Save Policy**.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加一个通知。在该部分下，将**方法**保留为**电子邮件**，然后单击**添加通知**。输入您的电子邮件地址，然后单击**保存策略**。
- en: Now whenever the CPU from one of our instances goes above 80 percent, we will
    receive an e-mail notification. If we ever need to review our policies, we can
    find them under the **Alerting** dropdown and **Policies Overview** at the menu
    on the top of the screen.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每当我们的实例之一的CPU超过80％，我们将收到电子邮件通知。如果我们需要审查我们的策略，我们可以在屏幕顶部的**警报**下拉菜单和**策略概述**中找到它们。
- en: Beyond system monitoring with Sysdig
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Sysdig进行系统监控
- en: Monitoring our cloud systems is a great start, but what about visibility into
    the containers themselves? Although there are a variety of cloud monitoring and
    visibility tools, Sysdig stands out for its ability to dive deep not only into
    system operations but specifically containers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 监控我们的云系统是一个很好的开始，但是对容器本身的可见性呢？虽然有各种各样的云监控和可见性工具，但Sysdig因其深入系统操作和特定容器的能力而脱颖而出。
- en: Sysdig is open source and is billed as *a universal system visibility tool with
    native support for containers**²*. It is a command-line tool, which provides insight
    into the areas we've looked at earlier such as storage, network, and system processes.
    What sets it apart is the level of detail and visibility it offers for these process
    and system activities. Furthermore, it has native support for containers, which
    gives us a full picture of our container operations. This is a highly recommended
    tool for your container operations arsenal. Their main website is [http://www.sysdig.org/](http://www.sysdig.org/).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig是开源的，并被称为*具有对容器的本机支持的通用系统可见性工具**²*。它是一个命令行工具，可以提供对我们之前查看过的领域（如存储、网络和系统进程）的洞察。它的独特之处在于它为这些进程和系统活动提供的详细和可见性。此外，它对容器有本机支持，这为我们提供了我们容器操作的全貌。这是一个非常推荐的工具，适用于您的容器操作工具库。他们的主要网站是[http://www.sysdig.org/](http://www.sysdig.org/)。
- en: Sysdig Cloud
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sysdig云
- en: We will take a look at the Sysdig tool and some of the useful command-line-based
    UIs in a moment. However, the team at Sysdig has also built a commercial product,
    named **Sysdig Cloud**, which provides the advanced dashboard, alerting, and notification
    services we discussed earlier in the chapter. Also, the differentiator here has
    high visibility into containers, including some nice visualizations of our application
    topology.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将马上看一下Sysdig工具和一些有用的基于命令行的UI。然而，Sysdig团队还构建了一个商业产品，名为**Sysdig Cloud**，它提供了我们在本章前面讨论的高级仪表板、警报和通知服务。此外，这里的不同之处在于对容器的高可见性，包括我们应用程序拓扑的一些漂亮的可视化效果。
- en: Note
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you'd rather skip the *Sysdig Cloud* section and just try out the command-line
    tool, simply skip to the *Sysdig command line* section later in this chapter.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您宁愿跳过*Sysdig Cloud*部分，只是想尝试命令行工具，那么可以直接跳到本章后面的*Sysdig命令行*部分。
- en: If you have not done so already, sign up for Sysdig Cloud at [http://www.sysdigcloud.com](http://www.sysdigcloud.com).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请在[http://www.sysdigcloud.com](http://www.sysdigcloud.com)注册Sysdig
    Cloud。
- en: After activating and logging in for the first time, we'll be taken to a welcome
    page. Clicking on **Next**, we are shown a page with various options to install
    the `sysdig` agents. For our example environment, we will use a Linux agent. The
    **Next** button will be disabled until we install at least one agent. The page
    should show the following command with our *access key* filled in.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次激活并登录后，我们将被带到一个欢迎页面。点击**下一步**，我们将看到一个页面，上面有各种选项来安装`sysdig`代理。在我们的示例环境中，我们将使用Linux代理。在安装至少一个代理之前，**下一步**按钮将被禁用。页面应该会显示以下命令，并填入我们的*访问密钥*。
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We'll need to SSH into our master and each node to run the installer. It will
    take a few minutes to install several packages and then set up the connection
    to the Sysdig Cloud.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要SSH登录到我们的主节点和每个节点来运行安装程序。安装几个软件包然后设置到Sysdig Cloud的连接需要几分钟时间。
- en: After our first install completes, the page should update with the text **You
    have one agent connected!** and the **Next** button will become active. Go ahead
    and install the rest of the agents and then come back to this page and click on
    **Next**.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次安装完成后，页面应该会更新，显示文本**您已连接一个代理！**，并且**下一步**按钮将变为可用状态。继续安装其余的代理，然后回到这个页面，点击**下一步**。
- en: We can skip the AWS setup for now and then click on **Let's Get Started** on
    the final screen.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以跳过AWS设置，然后在最后一个屏幕上点击**让我们开始吧**。
- en: We'll be taken to the main **sysdig cloud** dashboard screen. **kubernetes-master**
    and our various minion nodes should appear under the **Explore** tab. We should
    see something similar to Figure 6.9 with our cluster master and all four minion
    nodes (or the nodes we have already installed agents on).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将被带到主要的**sysdig cloud**仪表板屏幕。**kubernetes-master**和我们的各种minion节点应该会出现在**探索**选项卡下。我们应该会看到类似于图6.9的内容，显示我们的集群主节点和所有四个minion节点（或者我们已经安装了代理的节点）。
- en: '![Sysdig Cloud](../images/00066.jpeg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![Sysdig Cloud](../images/00066.jpeg)'
- en: Figure 6.9\. Sysdig Cloud Explore page
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9\. Sysdig Cloud 探索页面
- en: This page shows us a table view and the links on the left let us explore some
    key metrics for CPU, memory, networking, and so on. Although this is a great start,
    the detailed views will give us a much deeper look at each node.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个页面向我们展示了一个表格视图，左侧的链接让我们可以查看一些关于CPU、内存、网络等关键指标。虽然这是一个很好的开始，但详细视图将让我们更深入地了解每个节点。
- en: Detailed views
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 详细视图
- en: 'Let''s take a look at these views. Select **kubernetes-master** and then scroll
    down to the detail section that appears below. By default, we should see the **System:
    Overview by Process** view (If it''s not selected, just click on it in the list
    on the left.) If the chart is hard to read, simply use the maximize icon in the
    top-left corner of each graph for a larger view.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们来看看这些视图。选择**kubernetes-master**，然后向下滚动到下面出现的详细部分。默认情况下，我们应该看到**System: Overview
    by Process**视图（如果没有被选中，只需在左侧的列表中点击它）。如果图表难以阅读，只需使用每个图表左上角的最大化图标来获得更大的视图。'
- en: 'There are a variety of interesting views to explore. Just to call out a few
    others, **Application: HTTP** and **System: Overview** by container give us some
    great charts for inspection. In the later view, we can see stats for CPU, memory,
    network, and file usage by container.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '有各种有趣的视图可以探索。只是举几个例子，**Application: HTTP**和**System: Overview** by container
    为我们提供了一些很棒的图表供检查。在后一种视图中，我们可以看到容器的 CPU、内存、网络和文件使用情况的统计数据。'
- en: Topology views
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拓扑视图
- en: 'In addition, there are three topology views at the bottom. These views are
    perfect for helping us understand how our application is communicating. Click
    on **Topology: Network Traffic** and wait a few seconds for the view to fully
    populate. It should look similar to Figure 6.10:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，底部还有三个拓扑视图。这些视图非常适合帮助我们理解我们的应用程序是如何通信的。点击**Topology: Network Traffic**，等待几秒钟让视图完全填充。它应该看起来类似于图6.10：'
- en: '![Topology views](../images/00067.jpeg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![Topology views](../images/00067.jpeg)'
- en: Figure 6.10\. Sysdig Cloud network topology view
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10\. Sysdig Cloud 网络拓扑视图
- en: We note the view maps out the flow of communication between the minion nodes
    and the master in the cluster. On the right-hand side, there may be connections
    to servers with a **1e100.net** name and also **169.254.169.254**, which are both
    part of Google infrastructure.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到这个视图将集群中的 minion 节点与 master 之间的通信流程映射出来。在右侧，可能会有连接到带有**1e100.net**名称和**169.254.169.254**的服务器，这两者都是
    Google 基础设施的一部分。
- en: 'You may also note a **+** symbol in the top corner of the node boxes. Click
    on that in **kubernetes-master** and use the zoom tools at the top of the view
    area to zoom into the details, as you see in Figure 6.11:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以注意到节点框的右上角有一个**+**符号。点击**kubernetes-master**中的那个，并使用视图区域顶部的缩放工具来放大细节，就像您在图6.11中看到的那样：
- en: '![Topology views](../images/00068.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![Topology views](../images/00068.jpeg)'
- en: Figure 6.11\. The Sysdig Cloud network topology detailed view
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11\. Sysdig Cloud 网络拓扑详细视图
- en: Note that we can now see all the components of Kubernetes running inside the
    master. We can see how the various components work together. We will see **kubectl**
    and the **kublet** process running, as well as a number of boxes with the Docker
    whale, which indicate that they are containers. If we zoom in and use the plus
    icon, we will see that these are the containers for core Kubernetes process, as
    we saw in the services running on the master section in [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们现在可以看到 Kubernetes 内部运行在 master 节点上的所有组件。我们可以看到各个组件是如何一起工作的。我们将看到**kubectl**和**kublet**进程正在运行，以及一些带有
    Docker 鲸鱼标志的方框，这表示它们是容器。如果我们放大并使用加号图标，我们将看到这些是核心 Kubernetes 进程的容器，就像我们在[第1章](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations")中在 master 节点上运行的服务中看到的那样，*Kubernetes
    and Container Operations*。
- en: Also, if we pan over to the minion, we can also see **kublet**, which initiates
    communication, and follow it all the way through the `kube-apiserver` container
    in the master.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们移动到 minion 节点，我们还可以看到**kublet**，它发起通信，并跟随它一直到 master 节点中的`kube-apiserver`容器。
- en: We can even see the instance probing for GCE metadata on **169.254.169.254**.
    This view is great in order to get a mental picture of how our infrastructure
    and underlying containers are talking to one another.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以看到实例在**169.254.169.254**上探测GCE元数据。这个视图非常适合形象地了解我们的基础设施和底层容器是如何相互通信的。
- en: Metrics
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标
- en: Next, let's switch over to the **Metrics** tab in the left-hand menu next to
    **Views**. Here, there are also a variety of helpful views.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们切换到左侧菜单中的**Metrics**选项卡旁边的**Views**。在这里，还有各种有用的视图。
- en: Let's look at **capacity.estimated.request.total.count (avg)** under **System**.
    This view shows us an estimate of how many requests a node is capable of handling
    when fully loaded. This can be really useful for infrastructure planning.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看**System**下的**capacity.estimated.request.total.count (avg)**。这个视图向我们展示了一个节点在完全加载时能够处理多少请求的估计值。这对基础设施规划非常有用。
- en: '![Metrics](../images/00069.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![指标](../images/00069.jpeg)'
- en: Figure 6.12\. Sysdig Cloud capacity estimate view
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12\. Sysdig Cloud容量估算视图
- en: Alerting
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 警报
- en: Now that we have all this great information, let's create some notifications.
    Scroll back up to the top of the page and find the bell icon next to one of your
    minion entries. This will open a **New Alert** dialog. Here, we can set manual
    alerts similar to what we did earlier in the chapter. However, there is also the
    option to use **Baselines** and **Host comparison**.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有这些伟大的信息，让我们创建一些通知。滚动回页面顶部，找到一个随从条目旁边的铃铛图标。这将打开一个**New Alert**对话框。在这里，我们可以设置手动警报，类似于我们在本章前面所做的。但是，还有使用**Baselines**和**Host
    comparison**的选项。
- en: 'Using the **Baseline** option is extremely helpful as Sysdig will watch the
    historical patterns of the node and alert us whenever one of the metrics strays
    outside the expected metric thresholds. No manual settings are required, so this
    can really save time for the notification setup and help our operations team to
    be proactive before issues arise. Refer to the following image:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基线选项非常有帮助，因为Sysdig将监视节点的历史模式，并在其中一个指标偏离预期的指标阈值时向我们发出警报。不需要手动设置，因此这可以真正节省通知设置的时间，并帮助我们的运维团队在问题出现之前采取积极的措施。参考以下图片：
- en: '![Alerting](../images/00070.jpeg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![警报](../images/00070.jpeg)'
- en: Figure 6.13\. Sysdig Cloud new alert
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13\. Sysdig Cloud新警报
- en: The **Host Comparison** option is also a great help as it allows us to compare
    metrics with other hosts and alert whenever one host has a metric that differs
    significantly from the group. A great use case for this is monitoring resource
    usage across minion nodes to ensure that our scheduling constraints are not creating
    a bottleneck somewhere in the cluster.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 主机比较选项也是一个很大的帮助，因为它允许我们将指标与其他主机进行比较，并在一个主机的指标与组中的其他主机有显著差异时发出警报。这在监控随从节点上的资源使用情况以确保我们的调度约束没有在集群中的某个地方创建瓶颈时非常有用。
- en: You can choose whichever option you like, give it a name and description and
    choose a notification method. Sysdig supports e-mail, **SNS** (short for **Simple
    Notification Service**), and **PagerDuty** as notification methods. Once you have
    everything set, just click on **Create** and you will start to receive alerts
    as issues come up.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择任何您喜欢的选项，给它命名和描述，并选择通知方法。Sysdig支持电子邮件、**SNS**（简单通知服务）和**PagerDuty**作为通知方法。一切设置好后，只需点击**Create**，您将开始在问题出现时收到警报。
- en: Kubernetes support
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes支持
- en: An exciting new feature that has been recently released is support for integrating
    directly with the Kubernetes API. The agents make calls to K8s so that it is aware
    of metadata and the various constructs, such as pods and RCs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发布的一个令人兴奋的新功能是直接与Kubernetes API集成的支持。代理程序调用K8s，以便它了解元数据和各种构造，比如pod和RCs。
- en: We can check this out easily on the main dashboard by clicking the gear icon
    next to the word Show on the top bar. We should see some filter options as in
    the following figure (Figure 6.14). Click on the **Apply** button next to **Logical
    Apps Hierarchy - Kubernetes**. This will set a number of filters that organizes
    our list in order of namespace, RC, pods, and finally container ID.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过单击顶部栏上的显示单词旁边的齿轮图标轻松查看这一点。我们应该看到一些过滤选项，如下图所示（图6.14）。单击**逻辑应用程序层次结构 - Kubernetes**旁边的**应用**按钮。这将设置一些过滤器，按照命名空间、RC、pods，最后是容器ID来组织我们的列表。
- en: '![Kubernetes support](../images/00071.jpeg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes支持](../images/00071.jpeg)'
- en: Figure 6.14\. Sysdig Cloud Kubernetes filters
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14。Sysdig Cloud Kubernetes过滤器
- en: 'We can then select a default namespace from the list and use the detail views
    later, as we did before. By selecting the **Topology: Network Traffic** view,
    we can drill into the namespace and get a visual for each RC and the pods running
    within (see Figure 6.15):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以从列表中选择一个默认的命名空间，并在以后使用详细视图，就像以前一样。通过选择**拓扑：网络流量**视图，我们可以深入了解命名空间，并为每个RC和其中运行的pods获得可视化（参见图6.15）：
- en: '![Kubernetes support](../images/00072.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes支持](../images/00072.jpeg)'
- en: Figure 6.15\. Sysdig Cloud Kubernetes-aware topology view
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15。Sysdig Cloud Kubernetes感知拓扑视图
- en: The Sysdig command line
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sysdig命令行
- en: Whether you only use the open source tool or you are trying out the full Sysdig
    Cloud package, the command-line utility is a great companion to have to track
    down issues or get a deeper understanding of your system.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您只使用开源工具还是尝试完整的Sysdig Cloud套餐，命令行实用程序都是一个很好的伴侣，可以帮助您追踪问题或更深入地了解您的系统。
- en: In the core tool, there is the main `sysdig` utility and also a command-line
    style UI named `csysdig`. Let's take a look at a few useful commands.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心工具中，有主要的`sysdig`实用程序，还有一个名为`csysdig`的命令行样式UI。让我们看看一些有用的命令。
- en: 'We''ll need to SSH to the master or one of the minion nodes where we installed
    the Sysdig Cloud agents. It''s a single command to install the CLI tools as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要SSH到主节点或我们安装了Sysdig Cloud代理的从节点之一。安装CLI工具只需一个命令，如下所示：
- en: '[PRE11]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can find instructions for other OSes at [http://www.sysdig.org/install/](http://www.sysdig.org/install/).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://www.sysdig.org/install/](http://www.sysdig.org/install/)找到其他操作系统的安装说明。
- en: 'First, we can see the process with the most network activity by issuing the
    following command:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过发出以下命令来查看具有最多网络活动的进程：
- en: '[PRE12]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![The Sysdig command line](../images/00073.jpeg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![Sysdig命令行](../images/00073.jpeg)'
- en: Figure 6.16\. A Sysdig top process by network activity
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16。Sysdig网络活动的顶级进程
- en: 'This is an interactive view that will show us a top process in terms of network
    activity. Also, there are a plethora of commands to use with `sysdig`. A few other
    useful commands to try out include the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个交互式视图，将显示网络活动方面的顶级进程。此外，有大量的命令可以与`sysdig`一起使用。还有一些其他有用的命令可以尝试，包括以下内容：
- en: '[PRE13]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: More examples can be found at [http://www.sysdig.org/wiki/sysdig-examples/](http://www.sysdig.org/wiki/sysdig-examples/).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例可以在[http://www.sysdig.org/wiki/sysdig-examples/](http://www.sysdig.org/wiki/sysdig-examples/)找到。
- en: The csysdig command-line UI
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: csysdig命令行UI
- en: 'Because we are in a shell on one of our nodes doesn''t mean we can''t have
    a UI. Csysdig is a customizable UI to explore all the metrics and insight that
    Sysdig provides. Simply type `csysdig` at the prompt:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在一个节点的shell上，并不意味着我们不能有一个UI。Csysdig是一个可定制的UI，可以探索Sysdig提供的所有指标和见解。只需在提示符处键入`csysdig`：
- en: '[PRE14]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After entering csysdig, we see a real-time listing of all processes on the machine.
    At the bottom of the screen, you'll note a menu with various options. Click on
    **Views** or *F2* if you love to use your keyboard. On the left-hand menu, there
    are a variety of options, but we'll look at threads. Double-click to select **Threads**.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 进入csysdig后，我们可以看到机器上所有进程的实时列表。在屏幕底部，您会注意到一个带有各种选项的菜单。单击**Views**或*F2*（如果您喜欢使用键盘）。在左侧菜单中，有各种选项，但我们将查看线程。双击选择**Threads**。
- en: We can see all the threads currently running on the system and some information
    about the resource usage. By default, we see a big list that is updating often.
    If we click on the **Filter**, *F4* for the mouse challenged, we can slim down
    the list.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到当前在系统上运行的所有线程以及有关资源使用情况的一些信息。默认情况下，我们看到一个经常更新的大列表。如果我们单击**Filter**，对于鼠标操作有困难的人来说，我们可以缩减列表。
- en: Type `kube-apiserver`, if you are on the master, or `kube-proxy`, if you are
    on a (minion) node, in the filter box and press enter. The view now filters for
    only the threads in that command.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤框中输入`kube-apiserver`（如果您在主节点上）或`kube-proxy`（如果您在（从属）节点上），然后按Enter。现在视图仅过滤出该命令中的线程。
- en: '![The csysdig command-line UI](../images/00074.jpeg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![csysdig命令行UI](../images/00074.jpeg)'
- en: Figure 6.17\. Csysdig threads
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17。csysdig线程
- en: If we want to inspect a little further, we can simply select one of the threads
    in the list and click on **Dig** or *F6*. Now we see a detail listing of system
    calls from the command in real time. This can be a really useful tool to gain
    deep insight into the containers and processing running on our cluster.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想进一步检查，我们可以简单地选择列表中的一个线程，然后单击**Dig**或*F6*。现在我们可以实时看到该命令的系统调用的详细列表。这可以是一个非常有用的工具，可以深入了解集群中正在运行的容器和处理过程。
- en: Press **Back** or the *backspace* key to go back to the previous screen. Then,
    go to **Views** once more. This time, we will look at the **Containers** view.
    Once again, we can filter and also use the **Dig** view to get more in-depth visibility
    into what is happening at a system call level.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 按**Back**或*退格*键返回到上一个屏幕。然后，再次转到**Views**。这次，我们将查看**Containers**视图。再次，我们可以过滤并使用**Dig**视图来更深入地了解系统调用级别发生的情况。
- en: Another menu item you might note here is **Actions**, which is available in
    the newest release. These features allow us to go from process monitoring to action
    and response. It gives us the ability to perform a variety of actions from the
    various process views in csysdig. For example, the container view has actions
    to drop into a bash shell, kill containers, inspect logs, and more. It's worth
    getting to know the various actions and hotkeys and even add you own custom hotkeys
    for common operations.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到这里的另一个菜单项是**Actions**，它在最新版本中可用。这些功能使我们能够从进程监视转到操作和响应。它使我们能够从csysdig中的各种进程视图执行各种操作。例如，容器视图具有进入bash
    shell、杀死容器、检查日志等操作。值得了解各种操作和快捷键，甚至为常见操作添加自定义快捷键。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We took a quick look at monitoring and logging with Kubernetes. You should now
    be familiar with how Kubernetes uses cAdvisor and Heapster to collect metrics
    on all the resources in a given cluster. Furthermore, we saw how Kubernetes saves
    us time by providing InfluxDB and Grafana set up and configured out of the box.
    Dashboards are easily customizable for our everyday operational needs.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快速查看了Kubernetes的监控和日志记录。现在您应该熟悉Kubernetes如何使用cAdvisor和Heapster来收集给定集群中所有资源的指标。此外，我们看到Kubernetes通过提供预先设置和配置的InfluxDB和Grafana来节省我们的时间。仪表板可以轻松定制以满足我们日常的运营需求。
- en: In addition, we looked at the built-in logging capabilities with FluentD and
    the Google Cloud Logging service. Also, Kubernetes gives us great time savings
    by setting up the basics for us.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还研究了FluentD的内置日志功能和Google Cloud日志服务。此外，Kubernetes通过为我们设置基础，为我们节省了大量时间。
- en: Finally, you learned about the various third-party options available to monitor
    our containers and clusters. Using these tools will allow us to gain even more
    insight into the health and status of our applications. All these tools combine
    to give us a solid toolset to manage day-to-day operations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您了解了各种第三方选项，可用于监视我们的容器和集群。使用这些工具将使我们能够更深入地了解我们应用程序的健康状况和状态。所有这些工具结合在一起，为我们提供了一个稳固的工具集，用于管理日常运营。
- en: Footnotes
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚注
- en: ¹[http://stackdriver.com/](http://stackdriver.com/)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ¹[http://stackdriver.com/](http://stackdriver.com/)
- en: ²[http://www.sysdig.org/wiki/](http://www.sysdig.org/wiki/)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ²[http://www.sysdig.org/wiki/](http://www.sysdig.org/wiki/)
