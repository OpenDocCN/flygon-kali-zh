- en: The Limits of Scaling and the Workarounds
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展的限制和解决方法
- en: 'When you scale up your systems, every tool or framework you are using will
    reach a point where it will break or just not function as expected. For some things
    that point will be high and for some it will be low, and the intent of this chapter
    is to cover strategies and workarounds for the most likely scalability issues
    you will encounter when working with microservice clusters. In this chapter we
    will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当您扩展系统时，您使用的每个工具或框架都会达到一个破坏或不按预期运行的点。对于某些事物，这一点可能很高，对于某些事物，这一点可能很低，本章的目的是介绍在使用微服务集群时可能遇到的最常见的可扩展性问题的策略和解决方法。在本章中，我们将涵盖以下主题：
- en: Increasing service density and stability.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加服务密度和稳定性。
- en: Avoiding and mitigating common issues with large-scale deployments.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免和减轻大规模部署中的常见问题。
- en: Multi-service containers.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多服务容器。
- en: Best practices for zero-downtime deployments.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零停机部署的最佳实践。
- en: Limiting service resources
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制服务资源
- en: So far, we have not really spent any time talking about service isolation with
    regard to the resources available to the services, but it is a very important
    topic to cover. Without limiting resources, a malicious or misbehaving service
    could be liable to bring the whole cluster down, depending on the severity, so
    great care needs to be taken to specify exactly what allowance individual service
    tasks should use.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们并没有真正花时间讨论与服务可用资源相关的服务隔离，但这是一个非常重要的话题。如果不限制资源，恶意或行为不端的服务可能会导致整个集群崩溃，具体取决于严重程度，因此需要非常小心地指定个别服务任务应该使用的资源限额。
- en: 'The generally accepted strategy for handling cluster resources is the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 处理集群资源的通常接受的策略如下：
- en: Any resource that may cause errors or failures to other services if used beyond
    intended values is highly recommended to be limited on the service level. This
    is usually the RAM allocation, but may include CPU or others.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何资源如果超出预期值使用可能会导致其他服务出现错误或故障，强烈建议在服务级别上进行限制。这通常是RAM分配，但也可能包括CPU或其他资源。
- en: Any resources, specifically the hardware ones, for which you have an external
    limit should also be limited for Docker containers too (e.g. you are only allowed
    to use a specific portion of a 1-Gbps NAS connection).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何资源，特别是硬件资源，都应该在Docker容器中进行限制（例如，您只能使用1-Gbps NAS连接的特定部分）。
- en: Anything that needs to run on a specific device, machine, or host should be
    locked to those resources in the same fashion. This kind of setup is very common
    when only a certain number of machines have the right hardware for a service,
    such as in GPU computing clusters.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何需要在特定设备、机器或主机上运行的东西都应以相同的方式锁定到这些资源上。当只有一定数量的机器具有适合某项服务的正确硬件时，这种设置非常常见，比如在GPU计算集群中。
- en: Any resource that you would like specifically rationed within the cluster generally
    should have a limit applied. This includes things such as lowering the CPU time
    percentage for low-priority services.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常应该对希望在集群中特别配给的任何资源施加限制。这包括降低低优先级服务的CPU时间百分比等事项。
- en: In most cases, the rest of the resources should be fine using normal allocations
    of the available resources of the host.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，其余资源应该可以正常使用主机可用资源的正常分配。
- en: By applying these rules, we will ensure that our cluster is more stable and
    secure, with the exact division of resources that we want among the services.
    Also, if the exact resources required for a service are specified, the orchestration
    tool usually can make better decisions about where to schedule newly created tasks
    so that the service density per Engine is maximized.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用这些规则，我们将确保我们的集群更加稳定和安全，资源的分配也更加精确。此外，如果指定了服务所需的确切资源，编排工具通常可以更好地决定在哪里安排新创建的任务，以便最大化每个引擎的服务密度。
- en: RAM limits
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAM限制
- en: Strangely enough, even though CPU might be considered the most important computing
    resource, RAM allocation for clustered services is even more important due to
    the fact that RAM overuse can (and will) cause **Out of Memory** (**OOM**) process
    and task failures for anything running on the same host. With the prevalence of
    memory leaks in software, this usually is not a matter of "if" but "when", so
    setting limits for RAM allocation is generally very desirable, and in some orchestration
    configurations it is even mandatory. Suffering from this issue is usually indicated
    by seeing `SIGKILL`, `"Process killed"`, or `exit code -9` on your service.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的是，尽管CPU可能被认为是最重要的计算资源，但由于RAM的过度使用可能会导致内存不足（OOM）进程和任务失败，因此对集群服务的RAM分配甚至更为重要。由于软件中内存泄漏的普遍存在，这通常不是“是否”而是“何时”的问题，因此设置RAM分配限制通常是非常可取的，在某些编排配置中甚至是强制性的。遇到这个问题通常会看到`SIGKILL`，`"进程被杀死"`或`退出代码-9`。
- en: Keep in mind, though, that these signals could very well be caused by other
    things but the most common cause is OOM failures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些信号很可能是由其他原因引起的，但最常见的原因是OOM失败。
- en: By limiting the available RAM, instead of a random process on the host being
    killed by OOM manager, only the offending task's processes will be targeted for
    killing, so the identification of faulty code is much easier and faster because
    you can see the large number of failures from that service and your other services
    will stay operational, increasing the stability of the cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过限制可用的RAM，而不是由OOM管理器杀死主机上的随机进程，只有有问题的任务进程将被定位为目标，因此可以更容易和更快地识别出有问题的代码，因为您可以看到来自该服务的大量失败，而您的其他服务将保持运行，增加了集群的稳定性。
- en: OOM management is a huge topic and is much more broad than it would be wise
    to include in this section, but it is a very important thing to know if you spend
    a lot of time in the Linux kernel. If you are interested in this topic, I highly
    recommend that you visit [https://www.kernel.org/doc/gorman/html/understand/understand016.html](https://www.kernel.org/doc/gorman/html/understand/understand016.html)
    and read up on it.WARNING! On some of the most popular kernels, memory and/or
    swap cgroups are disabled due to their overhead. To enable memory and swap limiting
    on these kernels, your hosts kernel must be started with `cgroup_enable=memory`
    and `swapaccount=1` flags. If you are using GRUB for your bootloader, you can
    enable them by editing `/etc/default/grub` (or, on the latest systems, `/etc/default/grub.d/<name>`),
    setting `GRUB_CMDLINE_LINUX="cgroup_enable=memory swapaccount=1"`, running `sudo
    update-grub`, and then restarting your machine.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OOM管理是一个庞大的主题，比起在本节中包含它更明智，但如果您在Linux内核中花费大量时间，了解这一点非常重要。如果您对此主题感兴趣，我强烈建议您访问[https://www.kernel.org/doc/gorman/html/understand/understand016.html](https://www.kernel.org/doc/gorman/html/understand/understand016.html)并对其进行阅读。警告！在一些最流行的内核上，由于其开销，内存和/或交换cgroups被禁用。要在这些内核上启用内存和交换限制，您的主机内核必须以`cgroup_enable=memory`和`swapaccount=1`标志启动。如果您使用GRUB作为引导加载程序，您可以通过编辑`/etc/default/grub`（或者在最新系统上，`/etc/default/grub.d/<name>`），设置`GRUB_CMDLINE_LINUX="cgroup_enable=memory
    swapaccount=1"`，运行`sudo update-grub`，然后重新启动您的机器来启用它们。
- en: 'To use the RAM-limiting `cgroup` configuration, run the container with a combination
    of the following flags:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用限制RAM的`cgroup`配置，运行容器时使用以下标志的组合：
- en: '`-m` / `--memory`: A hard limit on the maximum amount of memory that a container
    can use. Allocations of new memory over this limit will fail, and the kernel will
    terminate a process in your container that will usually be the main one running
    the service.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -m / --内存：容器可以使用的最大内存量的硬限制。超过此限制的新内存分配将失败，并且内核将终止容器中通常运行服务的主要进程。
- en: '`--memory-swap`: The total amount of memory including swap that the container
    can use. This must be used with the previous option and be larger than it. By
    default, a container can use up to twice the amount of allowed memory maximum
    for a container. Setting this to `-1` allows the container to use as much swap
    as the host has.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --内存交换：容器可以使用的包括交换在内的内存总量。这必须与前一个选项一起使用，并且比它大。默认情况下，容器最多可以使用两倍于容器的允许内存的内存。将其设置为`-1`允许容器使用主机拥有的交换空间。
- en: '`--memory-swappiness`: How eager the system will be to move pages from physical
    memory to on-disk swap space. The value is between `0` and `100`, where `0` means
    that pages will try to stay in resident RAM as much as possible, and vice versa.
    On most machines this value is `80` and will be used as the default, but since
    swap space access is very slow compared to RAM, my recommendation is to set this
    number as close to `0` as you can afford.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --内存交换倾向：系统将页面从物理内存移动到磁盘交换空间的渴望程度。该值介于`0`和`100`之间，其中`0`表示页面将尽可能留在驻留RAM中，反之亦然。在大多数机器上，该值为`80`，将用作默认值，但由于与RAM相比，交换空间访问非常缓慢，我的建议是将此数字设置为尽可能接近`0`。
- en: '`--memory-reservation`: A soft limit for the RAM usage of a service, which
    is generally used only for the detection of resource contentions with the generally
    expected RAM usage so that the orchestration engine can schedule tasks for maximum
    usage density. This flag does not have any guarantees that it will keep the service''s
    RAM usage below this level.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --内存预留：服务的RAM使用的软限制，通常仅用于检测资源争用，以便编排引擎可以安排任务以实现最大使用密度。此标志不能保证将保持服务的RAM使用量低于此水平。
- en: There are a few more flags that can be used for memory limiting, but even the
    preceding list is a bit more verbose than you will probably ever need to worry
    about. For most deployments, big and small, you will probably only need to use
    `-m` and set a low value of `--memory-swappiness`, the latter usually being done
    on the host itself through the `sysctl.d` boot setting so that all services will
    utilize it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他标志可以用于内存限制，但即使前面的列表可能比你需要担心的要详细一些。对于大多数部署，无论大小，你可能只需要使用`-m`并设置一个较低的值`--memory-swappiness`，后者通常是通过`sysctl.d`引导设置在主机上完成的，以便所有服务都将利用它。
- en: 'You can check what your `swappiness` setting is by running `sysctl vm.swappiness`.
    If you would like to change this, and in most cluster deployments you will, you
    can set this value by running the following command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行`sysctl vm.swappiness`来检查你的`swappiness`设置是什么。如果你想改变这个值，在大多数集群部署中你会这样做，你可以通过运行以下命令来设置这个值：
- en: '`$ echo "vm.swappiness = 10" | sudo tee -a /etc/sysctl.d/60-swappiness.conf`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ echo "vm.swappiness = 10" | sudo tee -a /etc/sysctl.d/60-swappiness.conf`'
- en: 'To see this in action, we will first run one of the most resource-intensive
    frameworks (JBoss) with a limit of 30 MB of RAM and see what happens:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这一点的实际效果，我们首先将运行一个最资源密集的框架（JBoss），限制为30 MB的RAM，看看会发生什么：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As expected, the container used up too much RAM and was promptly killed by the
    kernel. Now, what if we try the same thing but give it 400 MB of RAM?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，容器使用了太多的RAM，并立即被内核杀死。现在，如果我们尝试相同的事情，但给它400 MB的RAM呢？
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our container can now start without any issues!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的容器现在可以无问题地启动了！
- en: 'If you have worked a lot with applications in bare metal environments, you
    might be asking yourselves why exactly the JBoss JVM didn''t know ahead of time
    that it wouldn''t be able to run within such a constrained environment and fail
    even sooner. The answer here lies in a really unfortunate quirk (though I think
    it might be considered a feature depending on your point of view) of `cgroups`
    that presents the host''s resources unaltered to the container even though the
    container itself is constrained. You can see this pretty easily if you run a memory-limited
    container and print out the available RAM limits:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在裸机环境中大量使用应用程序，你可能会问自己为什么JBoss JVM事先不知道它将无法在如此受限制的环境中运行并更早地失败。答案在于`cgroups`的一个非常不幸的怪癖（尽管我认为它可能被视为一个特性，取决于你的观点），它将主机的资源未经修改地呈现给容器，即使容器本身受到限制。如果你运行一个内存受限的容器并打印出可用的RAM限制，你很容易看到这一点：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can imagine, this causes all kinds of cascade issues with applications
    launched in a `cgroup` limited container such as this, the primary one being that
    the application does not know that there is a limit at all so it will just go
    and try to do its job assuming that it has full access to the available RAM. Once
    the application reaches the predefined limits, the app process will usually be
    killed and the container will die. This is a huge problem with apps and runtimes
    that can react to high memory pressures as they might be able to use less RAM
    in the container but because they cannot identify that they are running constrained,
    they tend to gobble up memory at a much higher rate than they should.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想象的，这会导致在这样一个`cgroup`受限制的容器中启动的应用程序出现各种级联问题，主要问题是应用程序根本不知道有限制，因此它会尝试做它的工作，假设它可以完全访问可用的RAM。一旦应用程序达到预定义的限制，应用程序进程通常会被杀死，容器也会死掉。这是一个巨大的问题，对于可以对高内存压力做出反应的应用程序和运行时来说，它们可能能够在容器中使用更少的RAM，但因为它们无法确定它们正在受到限制，它们倾向于以比应该更高的速率吞噬内存。
- en: Sadly, things are even worse on this front for containers. You must not only
    give the service a big enough RAM limit to start it, but also enough that it can
    handle any dynamically allocated memory during the full duration of the service.
    If you do not, the same situation will occur but at a much less predictable time.
    For example, if you ran an NGINX container with only a 4 MB of RAM limit, it will
    start just fine but after a few connections to it, the memory allocation will
    cross the threshold and the container will die. The service may then restart the
    task and unless you have a logging mechanism or your orchestration provides good
    tooling for it, you will just end up with a service that has a `running` state
    but, in actuality, it is unable to process any requests.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，对于容器来说，情况甚至更糟。你不仅必须给服务一个足够大的RAM限制来启动它，还必须足够大，以便它可以处理在服务的整个持续时间内动态分配的内存。如果不这样做，同样的情况将在一个不太可预测的时间发生。例如，如果你只给一个NGINX容器4MB的RAM限制，它会正常启动，但在连接到它的几次后，内存分配将超过阈值，容器将死机。然后服务可能会重新启动任务，除非你有日志记录机制或你的编排提供了良好的工具支持，否则你最终会得到一个状态为“运行”的服务，但实际上它无法处理任何请求。
- en: If that wasn't enough, you also really should not arbitrarily assign high limits
    either. This is due to the fact that one of the purposes of containers is to maximize
    service density for a given hardware configuration. By setting limits that are
    statistically nearly impossible to be reached by the running service, you are
    effectively wasting those resources because they can't be used by other services.
    In the long run, this increases both the cost of your infrastructure and the resources
    needed to maintain it, so there is a high incentive to keep the service limited
    by the minimum amount that can run it safely instead of using really high limits.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够，你也真的不应该随意地分配高限制。这是因为容器的一个目的是在给定的硬件配置下最大化服务密度。通过设置几乎不可能被运行服务达到的限制，你实际上在浪费这些资源，因为它们无法被其他服务使用。从长远来看，这会增加基础设施的成本和维护所需的资源，因此有很大的动力来保持服务受到最低限度的限制，以确保安全运行，而不是使用非常高的限制。
- en: Orchestration tooling generally prevents overcommiting resources, although there
    has been some progress to support this feature in both Docker Swarm and Kubernetes,
    where you can specify a soft limit (memory request) versus the true limit (memory
    limit). However, even with those parameters, tweaking the RAM setting is a really
    challenging task because you may get either under-utilization or constant rescheduling,
    so all the topics covered here are still very relevant. For more information on
    orchestration-specific handling of overcommiting, I suggest you read the latest
    documentation for your specific orchestration tool.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 编排工具通常可以防止资源过度分配，尽管Docker Swarm和Kubernetes都在支持这一特性方面取得了一些进展，你可以指定软限制（内存请求）与真实限制（内存限制）。然而，即使有了这些参数，调整RAM设置仍然是一个非常具有挑战性的任务，因为你可能会出现资源利用不足或持续重新调度的情况，因此这里涉及的所有主题仍然非常相关。关于编排特定处理资源过度分配的更多信息，我建议你阅读你特定编排工具的最新文档。
- en: So, when looking at all the things we must keep in mind, tweaking the limits
    is closer to an art form than anything else because it is almost like a variation
    of the famous bin-packing problem ([https://en.wikipedia.org/wiki/Bin_packing_problem](https://en.wikipedia.org/wiki/Bin_packing_problem)),
    but also adds the statistical component of the service on top of it, because you
    might need to figure out the optimum service availability compared to wasted resources
    due to loose limits.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当考虑所有必须牢记的事情时，调整限制更接近于一种艺术形式，而不是其他任何东西，因为它几乎就像著名的装箱问题的变体（[https://en.wikipedia.org/wiki/Bin_packing_problem](https://en.wikipedia.org/wiki/Bin_packing_problem)），但也增加了服务的统计组件，因为您可能需要找出最佳的服务可用性与由于宽松限制而浪费资源之间的平衡。
- en: 'Let''s say we have a service with the following distribution:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个以下分布的服务：
- en: Three physical hosts with 2 GB RAM each (yes, this is really low, but it is
    to demonstrate the issues on smaller scales)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个物理主机的RAM为2 GB（是的，这确实很低，但这是为了演示小规模问题）
- en: '**Service 1** (database) that has a memory limit of 1.5 GB, two tasks, and
    has a 1 percent chance of running over the hard limit'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务1**（数据库）的内存限制为1.5 GB，有两个任务，并且有1%的几率超过硬限制运行'
- en: '**Service 2** (application) that has a memory limit of 0.5 GB, three tasks,
    and has a 5 percent chance of running over the hard limit'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务2**（应用程序）的内存限制为0.5 GB，有三个任务，并且有5%的几率超过硬限制运行'
- en: '**Service 3** (data processing service) that has a memory limit of 0.5 GB,
    three tasks, and has a 5 percent chance of running over the hard limit'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务3**（数据处理服务）的内存限制为0.5 GB，有三个任务，并且有5%的几率超过硬限制运行'
- en: 'A scheduler may allocate the services in this manner:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 调度程序可以按以下方式分配服务：
- en: '![](assets/39e05686-46b7-42f2-bb8b-446d34e67613.png)WARNING! You should always
    have spare capacity on your clusters for rolling service updates, so having the
    configuration similar to the one shown in the diagram would not work well in the
    real world. Generally, this extra capacity is also a fuzzy value, just like RAM
    limits. Generally, my formula for it is the following, but feel free to tweak
    it as needed:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/39e05686-46b7-42f2-bb8b-446d34e67613.png)警告！您应该始终在集群上保留一定的容量以进行滚动服务更新，因此在实际情况下，配置与图表中所示的类似配置效果不佳。通常，这种额外的容量也是一个模糊值，就像RAM限制一样。通常，我的公式如下，但随时可以根据需要进行调整：'
- en: '`overcapacity = avg(service_sizes) * avg(service_counts) * avg(max_rolling_service_restarts)`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`过剩容量=平均（服务大小）*平均（服务计数）*平均（最大滚动服务重启）`'
- en: We will discuss this a bit more further in the text.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在文本中进一步讨论这一点。
- en: What if we take our last example and now say that we should just run with 1
    percent OOM failure rates across the board, increasing our **Service 2** and **Service
    3** memory limit from 0.5 GB to 0.75 GB, without taking into account that maybe
    having higher failure rates on the data processing service and application tasks
    might be acceptable (or even not noticeable if you are using messaging queues)
    to the end users?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们拿最后一个例子，现在说我们应该只在整体上以1%的OOM故障率运行，将我们的**服务2**和**服务3**的内存限制从0.5 GB增加到0.75
    GB，而不考虑也许在数据处理服务和应用程序任务上具有更高的故障率可能是可以接受的（甚至如果您使用消息队列，最终用户可能根本不会注意到）？
- en: 'The new service spread would now look like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 新的服务分布现在看起来是这样的：
- en: '![](assets/c864d401-c786-4c00-9c1d-b6b70a6b8160.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c864d401-c786-4c00-9c1d-b6b70a6b8160.png)'
- en: 'Our new configuration has a massive amount of pretty obvious issues:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新配置存在大量明显的问题：
- en: 25 percent reduction in service density. This number should be as high as possible
    to get all the benefits of using microservices.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务密度减少25%。这个数字应该尽可能高，以获得使用微服务的所有好处。
- en: 25 percent reduction in hardware utilization. Effectively, 1/4 of the available
    hardware resources are being wasted in this setup.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件利用率减少了25％。实际上，在这种设置中，可用硬件资源的四分之一被浪费。
- en: Node count has increased by 66 percent. Most cloud providers charge by the number
    of machines you have running assuming they are the same type. By making this change
    you have effectively raised your cloud costs by 66 percent and may need that much
    extra ops support to keep your cluster working.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点数量增加了66％。大多数云服务提供商按照运行的机器数量收费，假设它们是相同类型的。通过进行这种改变，您实际上增加了66％的云成本，并可能需要额外的运维支持来保持集群的运行。
- en: Even though this example has been intentionally rigged to cause the biggest
    impact when tweaked, it should be obvious that slight changes to these limits
    can have massive repercussions on your whole infrastructure. While in real-world
    scenarios this impact will be reduced because there will be larger host machines
    than in the example which will make them better able to stack smaller (relative
    to total capacity) services in the available space, *do not* underestimate the
    cascading effects of increasing service resource allocations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个例子是故意操纵的，以便在调整时产生最大的影响，但显而易见的是，对这些限制进行轻微的更改可能会对整个基础设施产生巨大的影响。在实际场景中，这种影响将会减少，因为主机机器会更大，这将使它们更能够在可用空间中堆叠较小（相对于总容量）的服务，*不要*低估增加服务资源分配的级联效应。
- en: CPU limits
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU限制
- en: 'Just like in our previous section about memory limits for services, `docker
    run` also supports a variety of CPU settings and parameters to tweak the computational
    needs of your services:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前关于服务内存限制的部分一样，`docker run`也支持各种CPU设置和参数，以调整您的服务的计算需求：
- en: '`-c`/`--cpu-shares`: On a high-load host, all tasks are weighted equally by
    default. Setting this on a task or service (from the default of `1024`) will increase
    or decrease the percentage of CPU utilization that the task can be scheduled for.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-c`/`--cpu-shares`：在高负载主机上，默认情况下所有任务的权重都是相等的。在任务或服务上设置此标志（从默认值`1024`）将增加或减少任务可以被调度的CPU利用率的百分比。'
- en: '`--cpu-quota`: This flag sets the number of microseconds that a task or service
    can use the CPU within a default block of time of 100 milliseconds (100,000 microseconds).
    For example, to only allow a maximum of 50% of a single CPU''s core usage to a
    task, you would set this flag to `50000`. For multiple cores, you would need to
    increase this value accordingly.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--cpu-quota`：此标志设置任务或服务在默认的100毫秒（100,000微秒）时间块内可以使用CPU的微秒数。例如，要仅允许任务最多使用单个CPU核心50％的使用率，您将把此标志设置为`50000`。对于多个核心，您需要相应地增加此值。'
- en: '`--cpu-period`: This changes the previous quota flag default interval in microseconds
    over which the `cpu-quota` is being evaluated (100 milliseconds/100,000 microseconds)
    and either reduces it or increases it to inversely affect the CPU resource allocation
    to a service.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--cpu-period`：这会更改以微秒为单位的先前配额标志默认间隔，用于评估`cpu-quota`（100毫秒/100,000微秒），并将其减少或增加以反向影响服务的CPU资源分配。'
- en: '`--cpus`: A float value that combines parts of both `cpu-quota` and `cpu-period`
    to limit the number of CPU core allocations to the task. For example, if you only
    want a task to use up to a quarter of a single CPU resource, you would set this
    to `0.25` and it would have the same effect as `--cpu-quota 25000 --cpu-period
    100000`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--cpus`：一个浮点值，结合了`cpu-quota`和`cpu-period`的部分，以限制任务对CPU核分配的数量。例如，如果您只希望任务最多使用四分之一的单个CPU资源，您可以将其设置为`0.25`，它将产生与`--cpu-quota
    25000 --cpu-period 100000`相同的效果。'
- en: '`--cpuset-cpus`: This array flag allows the service to only run on specified
    CPUs indexed from 0\. If you wanted a service to use only CPUs 0 and 3, you could
    use `--cpuset-cpus "0,3"`. This flag also supports entering values as a range
    (that is `1-3`).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--cpuset-cpus`：此数组标志允许服务仅在从0开始索引的指定CPU上运行。如果您希望服务仅使用CPU 0和3，您可以使用`--cpuset-cpus
    "0,3"`。此标志还支持将值输入为范围（即`1-3`）。'
- en: While it might seem like a lot of options to consider, in most cases you will
    only need to tweak the `--cpu-shares` and `--cpus` flags, but it is possible that
    you will need much more granular control over the resources that they provide.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能看起来有很多选项需要考虑，但在大多数情况下，您只需要调整`--cpu-shares`和`--cpus`标志，但有可能您需要更精细地控制它们提供的资源。
- en: How about we see what the `--cpu-shares` value can do for us? For this, we need
    to simulate resource contention and in the next example, we will try to do this
    by incrementing an integer variable as many times as we can within a period of
    60 seconds in as many containers as there are CPUs on the machine. The code is
    a bit gnarly, but most of it is to get the CPU to reach resource contention levels
    on all cores.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看`--cpu-shares`值对我们有什么作用？为此，我们需要模拟资源争用，在下一个示例中，我们将尝试通过在机器上的每个CPU上增加一个整数变量的次数来在60秒内尽可能多地模拟这一点。代码有点复杂，但其中大部分是为了使CPU在所有核心上达到资源争用水平。
- en: 'Add the following to a file called `cpu_shares.sh` (also available on [https://github.com/sgnn7/deploying_with_docker](https://github.com/sgnn7/deploying_with_docker)):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容添加到名为`cpu_shares.sh`的文件中（也可在[https://github.com/sgnn7/deploying_with_docker](https://github.com/sgnn7/deploying_with_docker)上找到）：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we will run this code and see the effects of our flag:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将运行此代码并查看我们标志的效果：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: While the container with the high `--cpu-share` value didn't get the full increase
    in count that might have been expected, if we ran the benchmark over a longer
    period of time with a tighter CPU-bound loop, the difference would be much more
    drastic. But even in our small example you can see that the last container had
    a distinct advantage over all the other running containers on the machine.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有较高`--cpu-share`值的容器没有得到预期的完全增加，但如果我们在更长的时间内使用更紧密的CPU绑定循环运行基准测试，差异将会更加明显。但即使在我们的小例子中，您也可以看到最后一个容器在机器上运行的所有其他容器中具有明显优势。
- en: 'To see how the `--cpus` flag compares, let''s take a look at what it can do
    on an uncontended system:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解`--cpus`标志的作用，让我们看看在一个没有争用的系统上它能做什么：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the `--cpus` flag is really good for ensuring that a task will
    not use any more CPU than the specified value even if there is no contention for
    resources on the machine.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`--cpus`标志非常适合确保任务不会使用超过指定值的CPU，即使在机器上没有资源争用的情况下。
- en: Keep in mind that there are a few more options for limiting resource usage for
    containers that are a bit outside of the scope of the general ones that we have
    covered already, but they are mainly for device-specific limitations (such as
    device IOPS). If you are interested in seeing all of the available ways to limit
    resources to a task or a service, you should be able to find them all at [https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，还有一些限制容器资源使用的选项，这些选项有些超出了我们已经涵盖的一般范围，但它们主要用于特定设备的限制（例如设备IOPS）。如果您有兴趣了解如何将资源限制到任务或服务的所有可用方式，您应该能够在[https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources)找到它们。
- en: Pitfall avoidance
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免陷阱
- en: In most small and medium deployments, you will never see the same problems that
    you will start seeing when you scale up beyond them, so this section is to show
    you the most common issues that you will encounter and how to work around them
    in the cleanest way possible. While this list should cover most of the glaring
    issues you will encounter, some of your own will need custom fixes. You shouldn't
    be scared to make those changes because almost all host OS installations are just
    not geared towards the configuration that a high-load multi-container would need.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数小型和中型部署中，您永远不会遇到与扩展超出它们时会开始遇到的相同问题，因此本节旨在向您展示您将遇到的最常见问题以及如何以最干净的方式解决它们。虽然这个列表应该涵盖您将遇到的大多数突出问题，但您自己的一些问题将需要自定义修复。您不应该害怕进行这些更改，因为几乎所有主机操作系统安装都不适合高负载多容器所需的配置。
- en: WARNING! Many of the values and tweaks in this section have been based on personal
    experiences with deploying Docker clusters in the cloud. Depending on your combination
    of cloud provider, OS distribution, and infrastructure-specific configurations,
    the values may not need changing from the defaults, and some may even be detrimental
    to your system if used verbatim without spending some time learning what they
    mean and how to modify them. If you continue reading this section, please use
    the examples only as examples on how to change the values and not as something
    to copy/paste directly into configuration management tooling.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 警告！本节中的许多值和调整都是基于在云中部署Docker集群的个人经验。根据您的云提供商、操作系统分发和基础设施特定配置的组合，这些值可能不需要从默认值更改，有些甚至可能对系统造成损害，如果直接使用而不花时间学习它们的含义和如何修改。如果您继续阅读本节，请将示例仅用作更改值的示例，而不是直接复制/粘贴到配置管理工具中。
- en: ulimits
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ulimits
- en: '`ulimit` settings are little-known settings to most Linux desktop users, but
    they are a really painful and often-encountered issue when working with servers.
    In a nutshell, `ulimit` settings control many aspects around a process'' resource
    usage just like our Docker resource tweaks we covered earlier and they are applied
    to every process and shell that has been started. These limits are almost always
    set on distributions to prevent a stray process from taking down your machine,
    but the numbers have usually been chosen with regular desktop usage in mind, so
    trying to run server-type code on unchanged systems is bound to hit at least the
    open file limit, and possibly some other limits.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`ulimit`设置对大多数Linux桌面用户来说是鲜为人知的，但在与服务器工作时，它们是一个非常痛苦且经常遇到的问题。简而言之，`ulimit`设置控制了进程资源使用的许多方面，就像我们之前介绍的Docker资源调整一样，并应用于已启动的每个进程和shell。这些限制几乎总是在发行版上设置的，以防止一个杂乱的进程使您的机器崩溃，但这些数字通常是根据常规桌面使用而选择的，因此尝试在未更改的系统上运行服务器类型的代码几乎肯定会至少触及打开文件限制，可能还会触及其他一些限制。'
- en: 'We can use `ulimit -a` to see what our current (also called **soft**) settings
    are:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`ulimit -a`来查看我们当前（也称为**软限制**）的设置：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, there are only a few things set here, but there is one that
    stands out: our "open files" limit (`1024`) is fine for general applications,
    but if we run many services that handle a large number of open files (such as
    a decent amount of Docker containers), this value must be changed or you will
    hit errors and your services will effectively die.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这里只设置了一些东西，但有一项突出：我们的“打开文件”限制（`1024`）对于一般应用程序来说是可以的，但如果我们运行许多处理大量打开文件的服务（例如相当数量的Docker容器），这个值必须更改，否则您将遇到错误，您的服务将有效地停止运行。
- en: 'You can change this value for your current shell with `ulimit -S <flag> <value>`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`ulimit -S <flag> <value>`来更改当前shell的值：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: But what if we try to set this to something really high?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们尝试将其设置为非常高的值会怎样呢？
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we have now encountered the hard limit imposed by the system. This limit
    is something that will need to be changed at the system level if we want to modify
    it beyond those values. We can check what these hard limits are with `ulimit -H
    -a`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们现在遇到了系统强加的硬限制。如果我们想要修改超出这些值，这个限制是需要在系统级别进行更改的。我们可以使用`ulimit -H -a`来检查这些硬限制是什么：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So, if we want to increase our open files number beyond `4096`, we really need
    to change the system-level settings. Also, even if the soft limit of `4086` is
    fine with us, the setting is only for our own shell and its child processes, so
    it won't affect any other service or process on the system.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们想要增加我们的打开文件数超过`4096`，我们确实需要更改系统级设置。此外，即使`4086`的软限制对我们来说没问题，该设置仅适用于我们自己的shell及其子进程，因此不会影响系统上的任何其他服务或进程。
- en: If you really wanted to, you actually can change the `ulimit` settings of an
    already-running process with `prlimit` from the `util-linux` package, but this
    method of adjusting the values is discouraged because the settings do not persist
    during process restarts and are thus pretty useless for that purpose. With that
    said, if you want to find out whether your `ulimit` settings have been applied
    to a service that is already running, this CLI tool is invaluable, so don't be
    afraid to use it in those cases.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想要，你实际上可以使用`util-linux`软件包中的`prlimit`更改已运行进程的`ulimit`设置，但不鼓励使用这种调整值的方法，因为这些设置在进程重新启动期间不会持续，因此对于这个目的而言是相当无用的。话虽如此，如果你想要找出你的`ulimit`设置是否已应用于已经运行的服务，这个CLI工具是非常宝贵的，所以在这些情况下不要害怕使用它。
- en: 'To change this setting, you need to do a combination of options that is dependent
    on your distribution:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改此设置，您需要根据您的发行版进行一系列选项的组合：
- en: 'Create a security limits configuration file. You can do this rather simply
    by adding a few lines to something like `/etc/security/limits.d/90-ulimit-open-files-increase.conf`.
    The following example sets the open file soft limit on `root` and then on all
    other accounts (`*` does not apply to the `root` account) to `65536`. You should
    find out what the appropriate value is for your system ahead of time:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个安全限制配置文件。你可以通过向`/etc/security/limits.d/90-ulimit-open-files-increase.conf`添加几行来简单地做到这一点。以下示例将`root`的打开文件软限制设置为`65536`，然后设置所有其他账户（`*`不适用于`root`账户）的限制。你应该提前找出你的系统的适当值是多少。
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Add the `pam_limits` module to **Pluggable Authentication Module** (**PAM**).
    This will, in turn, affect all user sessions with the previous ulimit change setting
    because some distributions do not have it included otherwise your changes might
    not persist. Add the following to `/etc/pam.d/common-session`:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`pam_limits`模块添加到**可插拔认证模块**（**PAM**）。这将影响所有用户会话以前的`ulimit`更改设置，因为一些发行版没有包含它，否则你的更改可能不会持续。将以下内容添加到`/etc/pam.d/common-session`：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Alternatively, on some distributions, you can directly add the setting to the
    affected service definition in `systemd` in an override file:'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，在一些发行版上，你可以直接在`systemd`中的受影响服务定义中添加设置到覆盖文件中：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Overriding `systemd` services is a somewhat lengthy and distracting topic for
    this section, but it is a very common strategy for tweaking third-party services
    running on cluster deployments with that init system, so it is a very valuable
    skill to have. If you would like to know more about this topic, you can find a
    condensed version of the process at [https://askubuntu.com/a/659268](https://askubuntu.com/a/659268),
    and if you want the detailed version the upstream documentation can be found at
    [https://www.freedesktop.org/software/systemd/man/systemd.service.html](https://www.freedesktop.org/software/systemd/man/systemd.service.html).CAUTION!
    In the first example, we used the `*` wildcard, which affects all accounts on
    the machine. Generally, you want to isolate this setting to only the affected
    service accounts, if possible, for security reasons. We also used `root` because
    root values are specifically set by name in some distributions, which overrides
    the `*` wildcard setting due to the higher specificity. If you want to learn more
    about limits, you can find more information on these settings at [https://linux.die.net/man/5/limits.conf](https://linux.die.net/man/5/limits.conf).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖`systemd`服务是本节中一个相当冗长和分散注意力的话题，但它是一个非常常见的策略，用于调整在具有该init系统的集群部署上运行的第三方服务，因此这是一个非常有价值的技能。如果您想了解更多关于这个话题的信息，您可以在[https://askubuntu.com/a/659268](https://askubuntu.com/a/659268)找到该过程的简化版本，如果您想要详细版本，可以在[https://www.freedesktop.org/software/systemd/man/systemd.service.html](https://www.freedesktop.org/software/systemd/man/systemd.service.html)找到上游文档。注意！在第一个例子中，我们使用了`*`通配符，它影响了机器上的所有账户。通常，出于安全原因，您希望将此设置隔离到仅受影响的服务账户，如果可能的话。我们还使用了`root`，因为在一些发行版中，根值是通过名称专门设置的，这会由于更高的特异性而覆盖`*`通配符设置。如果您想了解更多关于限制的信息，您可以在[https://linux.die.net/man/5/limits.conf](https://linux.die.net/man/5/limits.conf)找到更多信息。
- en: Max file descriptors
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大文件描述符
- en: In the same way that we have a maximum open file limit for sessions and processes,
    the kernel itself has a limit for the maximum open file descriptors across the
    whole system. If this limit is reached, no other files will be able to be opened,
    and thus this needs tweaking on machines that may have a large number of files
    open at any one time.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们对会话和进程有最大打开文件限制一样，内核本身对整个系统的最大打开文件描述符也有限制。如果达到了这个限制，就无法打开其他文件，因此在可能同时打开大量文件的机器上需要进行调整。
- en: 'This value is part of the kernel parameters and as such can be seen with `sysctl`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值是内核参数的一部分，因此可以使用`sysctl`命令查看：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: While on this machine the value seems reasonable, I have seen a few older distributions
    with a surprisingly low value that will get easily hit with errors if you are
    running a number of containers on the system.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在这台机器上这个值似乎是合理的，但我曾经看到一些旧版本的发行版具有令人惊讶的低值，如果您在系统上运行了大量容器，很容易出现错误。
- en: Most kernel configuration settings we discuss here and later in this chapter
    can be temporarily changed with `sysctl -w <key>="<value>"`. However, since those
    values are reset back to defaults on each reboot, they usually are of no long-term
    use for us and are not going to be covered here, but keep in mind that you can
    use such techniques if you need to debug a live system or apply a temporary time-sensitive
    fix.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里和本章后面讨论的大多数内核配置设置都可以使用`sysctl -w <key>="<value>"`进行临时更改。然而，由于这些值在每次重新启动时都会重置为默认值，它们通常对我们没有长期用途，因此这里不会涉及到它们，但请记住，如果您需要调试实时系统或应用临时的时间敏感的修复，您可以使用这些技术。
- en: 'To change this to a value that will persist across reboots, we will need to
    add the following to the `/etc/sysctl.d` folder (that is,  `/etc/sysctl.d/10-file-descriptors-increase.conf`):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改此值以使其在重新启动后保持不变，我们需要将以下内容添加到`/etc/sysctl.d`文件夹中（即`/etc/sysctl.d/10-file-descriptors-increase.conf`）：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After this change, reboot, and you should now be able to open up to 1 million
    file handles on the machine!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更改后，重新启动，您现在应该能够在机器上打开多达100万个文件句柄！
- en: Socket buffers
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 套接字缓冲区
- en: To increase performance, it is usually highly advantageous to increase the size
    of the socket buffers because they are no longer doing the work of a single machine
    but the work of as many Docker containers as you have running on top of regular
    machine connectivity. For this, there are a few settings that you should probably
    set to make sure that the socket buffers are not struggling to keep up with all
    the traffic passing through them. At the time of writing this book, most of these
    default buffer settings are generally pretty tiny when the machine starts (200
    KB in a few machines that I've checked) and they are supposed to be dynamically
    scaled, but you can force them to be much larger from the start.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，通常增加套接字缓冲区的大小非常有利，因为它们不再只是单台机器的工作，而是作为您在常规机器连接上运行的所有Docker容器的工作。为此，有一些设置您可能应该设置，以确保套接字缓冲区不会努力跟上所有通过它们传递的流量。在撰写本书时，大多数这些默认缓冲区设置在机器启动时通常非常小（在我检查过的一些机器上为200
    KB），它们应该是动态缩放的，但您可以强制从一开始就使它们变得更大。
- en: 'On an Ubuntu LTS 16.04 installation, the following are the default ones for
    the buffer settings (though yours may vary):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ubuntu LTS 16.04安装中，默认的缓冲区设置如下（尽管您的设置可能有所不同）：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will dial these values up to some sensible defaults by adding the following
    to `/etc/sysctl.d/10-socket-buffers.conf`, but be sure to use values that make
    sense in your environment:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将以下内容添加到`/etc/sysctl.d/10-socket-buffers.conf`中，将这些值调整为一些合理的默认值，但请确保在您的环境中使用合理的值：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: By increasing these values, our buffers start large and should be able to handle
    quite a bit of traffic with much better throughput, which is what we want in a
    clustering environment.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加这些值，我们的缓冲区变得更大，应该能够处理相当多的流量，并且具有更好的吞吐量，这是我们在集群环境中想要的。
- en: Ephemeral ports
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 临时端口
- en: If you aren't familiar with ephemeral ports, they are the port numbers that
    all outbound connections get assigned if the originating port is not explicitly
    specified on the connection, which is the vast majority of them. For example,
    if you do any kind of outbound HTTP request with almost every client library,
    you will most likely have one of these ephemeral ports assigned as the return
    communication port for your connection.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉临时端口，它们是所有出站连接分配的端口号，如果未在连接上明确指定起始端口，那就是绝大多数端口。例如，如果您使用几乎每个客户端库进行任何类型的出站HTTP请求，您很可能会发现其中一个临时端口被分配为连接的返回通信端口。
- en: 'To see some sample ephemeral port usage on your machine, you can use `netstat`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看您的机器上一些示例临时端口的使用情况，您可以使用`netstat`：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you develop systems with multiple services with numerous outbound connections
    (which is practically mandatory when working with Docker services), you may notice
    that there are limits on the number of ports you are allowed to use and are likely
    to find that these ports may overlap with the ranges that some of your internal
    Docker services are using, causing intermittent and often annoying connectivity
    issues. In order to fix these issues, changes need to be made to the ephemeral
    port range.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开发具有大量出站连接的多个服务的系统时（在使用Docker服务时几乎是强制性的），您可能会注意到您被允许使用的端口数量有限，并且可能会发现这些端口可能与一些内部Docker服务使用的范围重叠，导致间歇性且经常令人讨厌的连接问题。为了解决这些问题，需要对临时端口范围进行更改。
- en: 'Since these are also kernel settings, we can see what our current ranges are
    with `sysctl`, just like we did in a couple of earlier examples:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些也是内核设置，我们可以使用`sysctl`来查看我们当前的范围，就像我们在之前的几个示例中所做的那样：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can see that our range is in the upper half of the port allocations, but
    any service that may start listening within that range could be in trouble. It
    is also possible that we may need more than 28,000 ports.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们的范围在端口分配的上半部分，但在该范围内可能开始监听的任何服务都可能遇到麻烦。我们可能需要的端口数量也可能超过28,000个。
- en: You may be curious how you get or set the `ipv6` settings for this parameter,
    but luckily (at least for now) this same setting key is used for both `ipv4` and
    `ipv6` ephemeral port ranges. At some point, this setting name may change, but
    I think we are at least a couple of years away from that.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会好奇如何获取或设置此参数的`ipv6`设置，但幸运的是（至少目前是这样），这个相同的设置键用于`ipv4`和`ipv6`临时端口范围。在某个时候，这个设置名称可能会改变，但我认为至少还有几年的时间。
- en: 'To change this value, we can either use `sysctl -w` for a temporary change
    or `sysctl.d` for a permanent change:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改此值，我们可以使用`sysctl -w`进行临时更改，或者使用`sysctl.d`进行永久更改：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With this change, we have effectively increased the number of outbound connections
    we can support by over 30%, but we could have just as easily used the same setting
    to ensure that ephemeral ports do not collide with other running services.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个改变，我们有效地增加了我们可以支持的出站连接数量超过30％，但我们也可以使用相同的设置来确保临时端口不会与其他运行中的服务发生冲突。
- en: Netfilter tweaks
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Netfilter调整
- en: 'Sadly, the settings we have seen so far are not the only things that need tweaking
    with increased network connections to your server. As you increase the load on
    your server, you may also begin to see `nf_conntrack: table full` errors in your
    `dmesg` and/or kernel logs. For those unfamiliar with `netfilter`, it is a kernel
    module that tracks all **Network Address Translation** (**NAT**) sessions in a
    hashed table that adds any new connections to it and clears them after they are
    closed and a predefined timeout is reached, so as you increase the connection
    volume from and to a single machine, you will most likely find that the majority
    of these related settings are defaulted rather conservatively and are in need
    of tweaking (though your distribution may vary--make sure to verify yours!):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '很遗憾，到目前为止我们看到的设置并不是唯一需要调整的东西，随着对服务器的网络连接增加，您可能还会在`dmesg`和/或内核日志中看到`nf_conntrack:
    table full`错误。对于不熟悉`netfilter`的人来说，它是一个跟踪所有**网络地址转换**（**NAT**）会话的内核模块，它将任何新连接添加到哈希表中，并在关闭连接并达到预定义的超时后清除它们，因此随着对单台机器的连接数量增加，您很可能会发现大多数相关设置都是默认的保守设置，需要进行调整（尽管您的发行版可能有所不同-请确保验证您的设置！）：'
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Quite a few of these can be changed, but the usual suspects for errors that
    need tweaking are as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中有很多可以改变，但需要调整的错误通常是以下几种：
- en: '`net.netfilter.nf_conntrack_buckets`: Controls the size of the hash table for
    the connections. Increasing this is advisable, although it can be substituted
    with a more aggressive timeout. Note that this cannot be set with regular `sysctl.d`
    settings, but instead needs to be set with a kernel module parameter.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.netfilter.nf_conntrack_buckets`：控制连接的哈希表的大小。增加这个是明智的，尽管它可以用更激进的超时来替代。请注意，这不能使用常规的`sysctl.d`设置，而是需要使用内核模块参数进行设置。'
- en: '`net.netfilter.nf_conntrack_max`: The number of entries to hold. By default,
    this is four times the value of the previous entry.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.netfilter.nf_conntrack_max`：要保存的条目数。默认情况下，这是前一个条目值的四倍。'
- en: '`net.netfilter.nf_conntrack_tcp_timeout_established`: This keeps the mapping
    for an open connection for up to five days (!). This is generally almost mandatory
    to reduce in order to not overflow your connection tracking table, but don''t
    forget that it needs to be above the TCP `keepalive` timeout or you will get unexpected
    connection breaks.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`net.netfilter.nf_conntrack_tcp_timeout_established`: 这将保持开放连接的映射长达五天之久(!)。通常情况下，必须减少这个时间以避免连接跟踪表溢出，但不要忘记它需要大于TCP的`keepalive`超时时间，否则会出现意外的连接中断。'
- en: 'To apply the last two settings, you need to add the following to `/etc/sysctl.d/10-conntrack.conf`
    and adjust the values for your own infrastructure configuration:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用最后两个设置，您需要将以下内容添加到`/etc/sysctl.d/10-conntrack.conf`，并根据自己的基础架构配置调整值：
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: netfilter is a massively complex topic to cover in a small section, so reading
    up on its impacts and configuration settings is highly recommended before changing
    these numbers. To get an idea of each of the settings, you can visit [https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt)
    and read up about it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: netfilter是一个非常复杂的话题，在一个小节中涵盖不全，因此在更改这些数字之前，强烈建议阅读其影响和配置设置。要了解每个设置的情况，您可以访问[https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt)并阅读相关内容。
- en: 'For a bucket count, you need to directly change the `nf_conntrack` `hashsize`
    kernel module parameter:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于桶计数，您需要直接更改`nf_conntrack` `hashsize`内核模块参数：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, to ensure that the right order is followed when loading the netfilter
    module so these values persist correctly, you will probably also need to add the
    following to the end of `/etc/modules`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了确保在加载netfilter模块时遵循正确的顺序，以便这些值正确地持久化，您可能还需要将以下内容添加到`/etc/modules`的末尾：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If everything was done correctly, your next restart should have all of the netfilter
    settings we talked about set.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切都正确完成，下次重启应该会设置所有我们讨论过的netfilter设置。
- en: Multi-service containers
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多服务容器
- en: Multi-service containers are a particularly tricky topic to broach, as the whole
    notion and recommended use of Docker is that you are only running single-process
    services within the container. Because of that, there is quite a bit of implicit
    pressure not to cover this topic because it can easily be misused and abused by
    developers who do not understand the reasons why this practice is strongly discouraged.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 多服务容器是一个特别棘手的话题，因为Docker的整个概念和推荐的用法是您只在容器中运行单进程服务。因此，有相当多的隐含压力不要涉及这个话题，因为开发人员很容易滥用并误用它，而不理解为什么强烈不建议这种做法。
- en: However, with that said and out of the way, there will be times where you will
    need to run multiple processes in a tight logical grouping where a multi-container
    solution would not make sense or it would be overly kludgey, which is why this
    topic is still important to cover. Having said all that, I cannot stress enough
    that you should only use this type of service collocation as a last resort.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，话虽如此，有时您需要在一个紧密的逻辑分组中运行多个进程，而多容器解决方案可能没有意义，或者会过于笨拙，这就是为什么这个话题仍然很重要的原因。话虽如此，我再次强调，您应该将这种类型的服务共存作为最后的手段。
- en: Before we even write a single line of code, we must discuss an architectural
    issue with multiple processes running within the same container, which is called
    the `PID 1` problem. The crux of this issue is that Docker containers run in an
    isolated environment in which they do not get help from the host's `init` process
    in reaping orphaned child processes. Consider an example process `Parent Process`,
    that is a basic executable that starts another process called `Child Process`,
    but as some point after that, if the associated `Parent Process` exits or is killed
    you will be left with the zombie `Child Process` loitering around in your container
    since `Parent Process` is gone and there is no other orphan reaping process running
    within the container sandbox. If the container exits, then the zombie processes
    will get cleaned up because they are all wrapped in a namespace, but for long-running
    tasks this can present a serious problem for running multiple processes inside
    a single image.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们写下一行代码之前，我们必须讨论一个架构问题，即在同一个容器内运行多个进程的问题，这被称为`PID 1`问题。这个问题的关键在于Docker容器在一个隔离的环境中运行，它们无法从主机的`init`进程中获得帮助来清理孤儿子进程。考虑一个例子进程`父进程`，它是一个基本的可执行文件，启动另一个进程称为`子进程`，但在某个时刻，如果相关的`父进程`退出或被杀死，你将会留下在容器中游荡的僵尸`子进程`，因为`父进程`已经消失，容器沙盒中没有其他孤儿收割进程在运行。如果容器退出，那么僵尸进程将被清理，因为它们都被包裹在一个命名空间中，但对于长时间运行的任务来说，这可能会对在单个镜像内运行多个进程造成严重问题。
- en: Terminology here might be confusing, but what was meant in simple terms is that
    every process is supposed be removed (also known as `reaped`) from the process
    table after it exits, either by the parent process or some other designated process
    (usually `init`) in the hierarchy that will take ownership of of it in order to
    finalize it. A process that does not have a running parent process in this context
    is called an orphan process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的术语可能会令人困惑，但简单来说，每个进程在退出后都应该被从进程表中移除（也称为收割），要么是由父进程，要么是由层次结构中的其他指定进程（通常是`init`）来接管它以完成最终的清理。在这种情况下，没有运行父进程的进程被称为孤儿进程。
- en: Some tools have the ability to reap these zombie processes (such as Bash and
    a few other shells), but even they aren't good enough init processes for our containers
    because they do not pass signals such as `SIGKILL`, `SIGINT`, and others to child
    processes, so stopping the container or pressing things such as *Ctrl* + *C* in
    the Terminal are of no use and will not terminate the container. If you really
    want to run multiple processes inside the container, your launching process must
    do orphan reaping and signal passing to children. Since we don't want to use the
    full init system like `systemd` from the container, there are a couple of alternatives
    here, but in the recent versions of Docker we now have the `--init` flag, which
    can run our containers with a real init runner process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有些工具有能力收割这些僵尸进程（比如Bash和其他几个shell），但即使它们也不是我们容器的良好init进程，因为它们不会将信号（如SIGKILL、SIGINT等）传递给子进程，因此停止容器或在终端中按下Ctrl
    + C等操作是无效的，不会终止容器。如果你真的想在容器内运行多个进程，你的启动进程必须进行孤儿收割和信号传递给子进程。由于我们不想从容器中使用完整的init系统，比如`systemd`，这里有几种替代方案，但在最近的Docker版本中，我们现在有`--init`标志，它可以使用真正的init运行器进程来运行我们的容器。
- en: 'Let''s see this in action and try to exit a program where the starting process
    is `bash`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个过程，并尝试退出一个以`bash`为起始进程的程序：
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This time, we''ll run our container with the `--init` flag:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将使用`--init`标志运行我们的容器：
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, `--init` was able to take our signal and pass it to all the
    listening children processes, and it works well as an orphan process reaper, though
    the latter is really hard to show in a basic container. With this flag and its
    functionality, you should now be able to run multiple processes with either a
    shell such as Bash or upgrade to a full process management tool such as `supervisord`
    ([http://supervisord.org/](http://supervisord.org/)) without any issues.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，“--init”能够接收我们的信号并将其传递给所有正在监听的子进程，并且它作为一个孤儿进程收割者运行良好，尽管后者在基本容器中真的很难展示出来。有了这个标志及其功能，你现在应该能够使用诸如Bash之类的shell运行多个进程，或者升级到一个完整的进程管理工具，比如`supervisord`（[http://supervisord.org/](http://supervisord.org/)），而不会出现任何问题。
- en: Zero-downtime deployments
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署
- en: With every cluster deployment, you will at some point need to think about code
    redeployment while minimizing the impact on your users. With small deployments,
    it is feasible that you might have a maintenance period in which you turn off
    everything, rebuild the new images, and restart the services, but this style of
    deployment is really not the way that medium and large clusters should be managed
    because you want to minimize any and all direct work needed to maintain the cluster.
    In fact, even for small clusters, handling code and configuration upgrades in
    a seamless manner can be invaluable for increased productivity.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次集群部署时，您都会在某个时候需要考虑代码重新部署，同时最大程度地减少对用户的影响。对于小规模部署，有可能您会有一个维护期，在此期间您关闭所有内容，重建新的镜像，并重新启动服务，但这种部署方式实际上并不适合中等和大型集群的管理，因为您希望最小化维护集群所需的任何直接工作。事实上，即使对于小集群，以无缝的方式处理代码和配置升级对于提高生产率来说也是非常宝贵的。
- en: Rolling service restarts
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滚动服务重启
- en: 'If the new service code does not change the fundamental way that it interacts
    with other services (inputs and outputs), often the only thing that is needed
    is a rebuild (or replacement) of the container image that is then placed into
    the Docker registry, and then the service is restarted in an orderly and staggered
    way. By staggering the restarts, there is always at least one task that can handle
    the service request available, and from an external point of view, this changeover
    should be completely seamless. Most orchestration tooling will do this automatically
    for you if you change or update any settings for a service, but since they are
    very implementation-specific we will focus on Docker Swarm for our examples:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果新的服务代码没有改变它与其他服务交互的基本方式（输入和输出），通常唯一需要的就是重建（或替换）容器镜像，然后将其放入Docker注册表，然后以有序和交错的方式重新启动服务。通过交错重启，始终至少有一个任务可以处理服务请求，并且从外部观点来看，这种转换应该是完全无缝的。大多数编排工具会在您更改或更新服务的任何设置时自动为您执行此操作，但由于它们非常特定于实现，我们将专注于Docker
    Swarm作为我们的示例：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, it should be simple enough to do the same thing with your own
    code changes without any downtime!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，应该很容易做到在没有任何停机时间的情况下进行自己的代码更改！
- en: If you want to be able to restart multiple tasks instead of one at a time, Docker
    Swarm has an `--update-parallelism <count>` flag as well that can be set on a
    service. When using this flag, `--update-delay` is still observed but instead
    of a single task being restarted, they are done in batches of `<count>` size.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要能够一次重启多个任务而不是一个，Docker Swarm也有一个`--update-parallelism <count>`标志，可以设置在一个服务上。使用这个标志时，仍然会观察`--update-delay`，但是不是单个任务被重启，而是以`<count>`大小的批次进行。
- en: Blue-green deployments
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Rolling restarts are nice, but sometimes the changes that you need to apply
    are on the hosts themselves and will need to be done to every Docker Engine node
    in the cluster, for example, if you need to upgrade to a newer orchestration version
    or to upgrade the OS release version. In these cases, the generally accepted way
    of doing this without a large team for support is usually by something called
    **blue-green deployments**. It starts by deploying a secondary cluster in parallel
    to the currently running one, possibly tied to the same data store backend, and
    then at the most opportune time switching the entry routing to point to the new
    cluster. Once all the processing on the original cluster has died down it is deleted,
    and the new cluster becomes the main processing group. If done properly, the impact
    on the users should be imperceptible and the whole underlying infrastructure has
    been changed in the process.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动重启很好，但有时需要应用的更改是在主机上，需要对集群中的每个Docker Engine节点进行更改，例如，如果需要升级到更新的编排版本或升级操作系统版本。在这些情况下，通常接受的做法是使用一种称为**蓝绿部署**的方法来完成，而不需要大量的支持团队。它通过在当前运行的集群旁边部署一个次要集群开始，可能与相同的数据存储后端相关联，然后在最合适的时间将入口路由切换到新集群。一旦原始集群上的所有处理都完成后，它将被删除，新集群将成为主要处理组。如果操作正确，用户的影响应该是不可察觉的，并且整个基础设施在此过程中已经发生了变化。
- en: 'The process starts with the creation of the secondary cluster. At that point
    there is no effective change other than testing that the new cluster behaves as
    expected:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程始于次要集群的创建。在那时，除了测试新集群是否按预期运行外，没有实质性的变化：
- en: '![](assets/d809216d-0230-418c-8278-8645a5c6188d.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: （图片）
- en: 'After the secondary cluster is operational, the router swaps the endpoints
    and the processing continues on the new cluster:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 次要集群运行后，路由器交换端点，处理继续在新集群上进行：
- en: '![](assets/39ff5e17-667b-4762-acd3-0b732bfffa5d.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: （图片）
- en: 'With the swap made, after all the processing is done, the original cluster
    is decommissioned (or left as an emergency backup):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 交换完成后，所有处理完成后，原始集群被废弃（或作为紧急备份留下）：
- en: '![](assets/4da05ec7-9bda-4c89-b27e-13b53586ab68.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: （图片）
- en: But the application of this deployment pattern on full clusters is not the only
    use for it--in some cases, it is possible to do this at the service level within
    the same cluster, using the same pattern to swap in a newer component, but there
    is a better system for that, which we will cover next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在完整集群上应用这种部署模式并不是它的唯一用途——在某些情况下，可以在同一集群内的服务级别上使用相同的模式来替换更高版本的组件，但是有一个更好的系统可以做到这一点，我们接下来会介绍。
- en: Blue-turquoise-green deployments
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: With deployments of code, things get a bit trickier because changing APIs on
    either the input or output sides or the database schema can wreak havoc on a cluster
    with interspersed versions of code. To get around this problem, there is a modified
    blue-green deployment pattern called **blue-turquoise-green deployment** where
    the code is attempted to be kept compatible with all running versions until the
    new code is deployed, after which the service is again updated by removing the
    compat code.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码部署中，情况变得有些棘手，因为在输入或输出端或数据库架构上更改API可能会对具有交错代码版本的集群造成严重破坏。为了解决这个问题，有一种修改过的蓝绿部署模式称为**蓝绿松石绿部署**，其中尝试使代码与所有运行版本兼容，直到部署新代码后，然后通过删除兼容代码再次更新服务。
- en: 'The process here is pretty simple:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的过程非常简单：
- en: The service that uses API version `x` is replaced with a new version of the
    service that supports both API version `x` and API version `(x+1)` in a rolling
    fashion. This provides zero downtime from the user's perspective, but creates
    a new service that has the newer API support.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用API版本`x`的服务以滚动方式替换为支持API版本`x`和API版本`(x+1)`的新版本服务。这从用户的角度提供了零停机时间，但创建了一个具有更新的API支持的新服务。
- en: After everything is updated, the service that has the old API version `x` is
    removed from the codebase.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一切更新完成后，具有旧API版本`x`的服务将从代码库中删除。
- en: Another rolling restart is done on the service to remove traces of the deprecated
    API so only API version `(x+1)` support is left.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对服务进行另一次滚动重启，以删除废弃API的痕迹，只留下API版本`(x+1)`的支持。
- en: This approach is extremely valuable when the services you are using need to
    be persistently available, and in many cases you could easily replace the API
    version with the messaging queue format, if your cluster is based on queues. The
    transitions are smooth, but there is overhead in needing to twice modify the service
    compared to a single time with a hard-swap, but it is a decent trade-off. This
    approach is also extremely valuable when the services in use deal with a database
    that might need a migration, so you should probably use this approach when others
    are not good enough.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用的服务需要持续可用时，这种方法非常有价值，在许多情况下，您可以轻松地将API版本替换为消息队列格式，如果您的集群基于队列。过渡是平稳的，但与一次硬交换相比，需要两次修改服务，但这是一个不错的权衡。当使用的服务涉及可能需要迁移的数据库时，这种方法也非常有价值，因此当其他方法不够好时，您应该使用这种方法。
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered various tools and techniques that you will need
    as you increase your infrastructure scale beyond the simple prototypes. By now
    we should have learned how to limit service access to host's resources, handle
    the most common pitfalls with ease, run multiple services in a single container,
    and handle zero-downtime deployments and configuration changes.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了各种工具和技术，这些工具和技术将在您将基础架构规模扩大到简单原型之外时需要。到目前为止，我们应该已经学会了如何限制服务访问主机资源，轻松处理最常见的问题，运行多个服务在一个容器中，并处理零停机部署和配置更改。
- en: In the next chapter, we will spend time working on deploying our own mini version
    of **Platform-as-a-Service **(**PAAS**)  using many of the things we have learned
    so far.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将花时间部署我们自己的**平台即服务**（PAAS）的迷你版本，使用我们迄今为止学到的许多知识。
