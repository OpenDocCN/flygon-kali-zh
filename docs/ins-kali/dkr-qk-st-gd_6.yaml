- en: Docker Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker网络
- en: In this chapter, we will learn about Docker networking. We will dive deep into
    Docker networking, learning how containers can be isolated, how they can communicate
    with each other, and how they can communicate with the outside world. We will
    explore the local network drivers Docker provides in the out-of-the-box installation.
    Then, we will examine the use of remote network drivers with an example deployment
    of the Weave driver. After that, we will learn how to create Docker networks.
    We will round out the discussion with a look at the free services that we get
    with our Docker networks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习关于Docker网络的知识。我们将深入研究Docker网络，学习容器如何被隔离，它们如何相互通信，以及它们如何与外部世界通信。我们将探索Docker在开箱即用安装中提供的本地网络驱动程序。然后，我们将通过部署Weave驱动程序的示例来研究远程网络驱动程序的使用。之后，我们将学习如何创建Docker网络。我们将通过查看我们的Docker网络所获得的免费服务来结束讨论。
- en: '"Approximately 97% of all shipping containers are manufactured in China. It
    is far easier to produce the container close to the shipment than to re-position
    containers around the world."                                                 
                              – [https://www.billiebox.co.uk/](https://www.billiebox.co.uk/)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “大约97%的集装箱都是在中国制造的。在装运时，生产集装箱比在世界各地重新定位集装箱要容易得多。” - [https://www.billiebox.co.uk/](https://www.billiebox.co.uk/)
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What is a Docker network?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Docker网络？
- en: What built-in (also known as **local**) Docker networks are all about
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置（也称为**本地**）Docker网络的全部内容
- en: What about third-party (also known as **remote**) Docker networks?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方（也称为**远程**）Docker网络如何？
- en: How to create Docker networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建Docker网络
- en: The free service discovery and load balancing features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 免费的服务发现和负载平衡功能
- en: The right Docker network driver to use for your needs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合您需求的正确Docker网络驱动程序
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be pulling Docker images from Docker's public repo, and installing
    network drivers from Weave, so basic internet access is required to execute the
    examples within this chapter. Also, we will be using the `jq software` package,
    so if you haven't installed it yet, please see the instructions on how to do so—they
    can be found in *The container inspect command* section of [Chapter 2](e66034ed-dcc0-48a8-a2ec-9466669e6649.xhtml),
    *Learning Docker Commands*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您将从Docker的公共存储库中拉取Docker镜像，并从Weave安装网络驱动程序，因此在执行本章示例时需要基本的互联网访问。此外，我们将使用`jq软件`包，因此如果您尚未安装，请参阅如何执行此操作的说明-可以在[第2章](e66034ed-dcc0-48a8-a2ec-9466669e6649.xhtml)的*容器检查命令*部分找到，*学习Docker命令*。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Docker-Quick-Start-Guide/tree/master/Chapter06](https://github.com/PacktPublishing/Docker-Quick-Start-Guide/tree/master/Chapter06)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Docker-Quick-Start-Guide/tree/master/Chapter06](https://github.com/PacktPublishing/Docker-Quick-Start-Guide/tree/master/Chapter06)'
- en: 'Check out the following video to see the code in action: [http://bit.ly/2FJ2iBK](http://bit.ly/2FJ2iBK)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际操作：[http://bit.ly/2FJ2iBK](http://bit.ly/2FJ2iBK)
- en: What is a Docker network?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Docker网络？
- en: As you already know, a network is a linkage system that allows computers and
    other hardware devices to communicate. A Docker network is the same thing. It
    is a linkage system that allows Docker containers to communicate with each other
    on the same Docker host, or with containers, computers, and hardware outside of
    the container's host, including containers running on other Docker hosts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经知道的，网络是一个允许计算机和其他硬件设备进行通信的连接系统。Docker网络也是一样的。它是一个连接系统，允许Docker容器在同一台Docker主机上相互通信，或者与容器、计算机和容器主机之外的硬件进行通信，包括在其他Docker主机上运行的容器。
- en: If you are familiar with the cloud computing analogy of pets versus cattle,
    you understand the necessity of being able to manage resources at scale. Docker
    networks allow you to do just that. They abstract away most of the complexity
    of networking, delivering easy-to-understand, easy-to-document, and easy-to-use
    networks for your containerized apps. The Docker network is based on a standard,
    created by Docker, called the **Container Network Model** (**CNM**). There is
    a competing networking standard, created by CoreOS, called the **Container Network
    Interface** (**CNI**). The CNI standard has been adopted by several projects,
    most notably Kubernetes, and arguments can be made to support its use. However,
    in this chapter, we will focus our attention on the CNM standard from Docker.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉宠物与牛群的云计算类比，你就明白了能够在规模上管理资源的必要性。Docker网络允许你做到这一点。它们抽象了大部分网络的复杂性，为你的容器化应用程序提供了易于理解、易于文档化和易于使用的网络。Docker网络基于一个由Docker创建的标准，称为**容器网络模型**（**CNM**）。还有一个由CoreOS创建的竞争性网络标准，称为**容器网络接口**（**CNI**）。CNI标准已被一些项目采用，尤其是Kubernetes，可以提出支持其使用的论点。然而，在本章中，我们将把注意力集中在Docker的CNM标准上。
- en: 'The CNM has been implemented by the libnetwork project, and you can learn more
    about that project by following the link in the references for this section. The
    CNM implementation, written in Go, is made up of three constructs: the sandbox,
    the endpoint, and the network. The sandbox is a network namespace. Each container
    has its own sandbox. It holds the configuration of the container''s network stack.
    This includes its routing tables, interfaces, and DNS settings for IP and MAC
    addresses. The sandbox also contains the network endpoints for the container.
    Next, the endpoints are what join the sandbox to networks. Endpoints are essentially
    network interfaces, such as **eth0**. A container''s sandbox may have more than
    one endpoint, but each endpoint will connect to only a single network. Finally, a
    network is a collection of connected endpoints, which allow communication between
    connections. Every network has a name, an address space, an ID, and a network
    type.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CNM已经被libnetwork项目实现，你可以通过本节参考中的链接了解更多关于该项目的信息。用Go编写的CNM实现由三个构造组成：沙盒、端点和网络。沙盒是一个网络命名空间。每个容器都有自己的沙盒。它保存了容器的网络堆栈配置。这包括其路由表、接口和IP和MAC地址的DNS设置。沙盒还包含容器的网络端点。接下来，端点是连接沙盒到网络的东西。端点本质上是网络接口，比如**eth0**。一个容器的沙盒可能有多个端点，但每个端点只能连接到一个网络。最后，网络是一组连接的端点，允许连接之间进行通信。每个网络都有一个名称、地址空间、ID和网络类型。
- en: Libnetwork is a pluggable architecture that allows network drivers to implement
    the specifics for the components we just described. Each network type has its
    own network driver. Docker provides built-in drivers. These default, or local,
    drivers include the bridge driver and the overlay driver. In addition to the built-in
    drivers, libnetwork supports third-party-created drivers. These drivers are referred
    to as remote drivers. Some examples of remote drivers include Calico, Contiv,
    and Weave.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Libnetwork是一个可插拔的架构，允许网络驱动程序实现我们刚刚描述的组件的具体内容。每种网络类型都有自己的网络驱动程序。Docker提供了内置驱动程序。这些默认或本地驱动程序包括桥接驱动程序和覆盖驱动程序。除了内置驱动程序，libnetwork还支持第三方创建的驱动程序。这些驱动程序被称为远程驱动程序。一些远程驱动程序的例子包括Calico、Contiv和Weave。
- en: 'You now know a little about what a Docker network is, and after reading these
    details, you might be thinking, where''s the *easy* that he talked about? Hang
    in there. now we are going to start discussing how easy it is for you to create
    and use Docker networks. As with Docker volume, the network commands represent their own
    management category. As you would expect, the top-level management command for network
    is as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了Docker网络是什么，阅读了这些细节之后，你可能会想，他说的“简单”在哪里？坚持住。现在我们将开始讨论你如何轻松地创建和使用Docker网络。与Docker卷一样，网络命令代表它们自己的管理类别。正如你所期望的，网络的顶级管理命令如下：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The subcommands available in the network management group include the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 网络管理组中可用的子命令包括以下内容：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let's now take a look at the built-in or local network drivers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看内置或本地网络驱动程序。
- en: References
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: 'Check out the following links for more information:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以获取更多信息：
- en: Pets versus cattle talk slide-deck: [https://www.slideshare.net/randybias/architectures-for-open-and-scalable-clouds](https://www.slideshare.net/randybias/architectures-for-open-and-scalable-clouds)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宠物与牛的对话幻灯片：[https://www.slideshare.net/randybias/architectures-for-open-and-scalable-clouds](https://www.slideshare.net/randybias/architectures-for-open-and-scalable-clouds)
- en: Libnetwork project: [https://github.com/docker/libnetwork](https://github.com/docker/libnetwork)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libnetwork项目：[https://github.com/docker/libnetwork](https://github.com/docker/libnetwork)
- en: Libnetwork design: [https://github.com/docker/libnetwork/blob/master/docs/design.md](https://github.com/docker/libnetwork/blob/master/docs/design.md)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libnetwork设计：[https://github.com/docker/libnetwork/blob/master/docs/design.md](https://github.com/docker/libnetwork/blob/master/docs/design.md)
- en: Calico network driver: [https://www.projectcalico.org/](https://www.projectcalico.org/)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico网络驱动程序：[https://www.projectcalico.org/](https://www.projectcalico.org/)
- en: Contiv network driver: [http://contiv.github.io/](http://contiv.github.io/)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Contiv网络驱动程序：[http://contiv.github.io/](http://contiv.github.io/)
- en: Weave network driver: [https://www.weave.works/docs/net/latest/overview/](https://www.weave.works/docs/net/latest/overview/)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave网络驱动程序：[https://www.weave.works/docs/net/latest/overview/](https://www.weave.works/docs/net/latest/overview/)
- en: Built-in (local) Docker networks
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内置（本地）Docker网络
- en: 'The out-of-the-box install of Docker includes a few built-in network drivers.
    These are also known as local drivers. The two most commonly used drivers are
    the bridge network driver and the overlay network driver. Other built-in drivers
    include none, host, and MACVLAN. Also, without your creating networks, your fresh
    install will have a few networks pre-created and ready to use. Using the `network
    ls` command, we can easily see the list of pre-created networks available in the
    fresh installation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Docker的开箱即用安装包括一些内置网络驱动程序。这些也被称为本地驱动程序。最常用的两个驱动程序是桥接网络驱动程序和覆盖网络驱动程序。其他内置驱动程序包括none、host和MACVLAN。此外，没有创建网络的情况下，你的新安装将会有一些预先创建并准备好使用的网络。使用`network
    ls`命令，我们可以轻松地查看新安装中可用的预先创建的网络列表：
- en: '![](Images/75a654de-335d-4082-bf45-f0a8b6a60b6f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/75a654de-335d-4082-bf45-f0a8b6a60b6f.png)'
- en: In this list, you will notice that each network has its unique ID, a name, a
    driver used to create it (and that controls it), and a network scope. Don't confuse
    a scope of local with the category of driver, which is also local. The local category
    is used to differentiate the driver's origin from third-party drivers that have
    a category of remote. A scope value of local indicates that the limit of communication
    for the network is bound to within the local Docker host. To clarify, if two Docker
    hosts, H1 and H2, both contain a network that has the scope of local, containers
    on H1 will never be able to communicate directly with containers on H2, even if
    they use the same driver and the networks have the same name. The other scope
    value is swarm, which we'll talk more about shortly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中，您会注意到每个网络都有其独特的 ID、名称、用于创建它（并控制它）的驱动程序以及网络范围。不要将本地范围与驱动程序的类别混淆，驱动程序的类别也是本地。本地类别用于区分驱动程序的来源，而不是具有远程类别的第三方驱动程序。本地范围值表示网络的通信限制仅限于本地
    Docker 主机内。为了澄清，如果两个 Docker 主机 H1 和 H2 都包含具有本地范围的网络，即使它们使用相同的驱动程序并且网络具有相同的名称，H1
    上的容器也永远无法直接与 H2 上的容器通信。另一个范围值是 swarm，我们稍后会更多地谈论它。
- en: The pre-created networks that are found in all deployments of Docker are special
    in that they cannot be removed. It is not necessary to attach containers to any
    of them, but attempts to remove them with the `docker network rm` command will
    always result in an error.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有 Docker 部署中找到的预创建网络是特殊的，因为它们无法被移除。不需要将容器附加到其中任何一个，但是尝试使用 `docker network
    rm` 命令移除它们将始终导致错误。
- en: 'There are three built-in network drivers that have a scope of local: bridge,
    host, and none. The host network driver leverages the networking stack of the
    Docker host, essentially bypassing the networking of Docker. All containers on
    the host network are able to communicate with each other through the host''s interfaces.
    A significant limitation to using the host network driver is that each port can
    only be used by a single container. That is, for example, you cannot run two nginx
    containers that are both bound to port `80`. As you may have guessed because the
    host driver leverages the network of the host it is running on, each Docker host
    can only have one network using the host driver:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个内置的网络驱动程序，其范围为本地：桥接、主机和无。主机网络驱动程序利用 Docker 主机的网络堆栈，基本上绕过了 Docker 的网络。主机网络上的所有容器都能够通过主机的接口相互通信。使用主机网络驱动程序的一个重要限制是每个端口只能被单个容器使用。也就是说，例如，您不能运行两个绑定到端口
    `80` 的 nginx 容器。正如您可能已经猜到的那样，因为主机驱动程序利用了其所在主机的网络，每个 Docker 主机只能有一个使用主机驱动程序的网络：
- en: '![](Images/8878cc4b-db35-4ae2-9bd1-2bef6653af31.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/8878cc4b-db35-4ae2-9bd1-2bef6653af31.png)'
- en: 'Next up, is the null or none network. Using the null network driver creates
    a network that when a container is connected to it provides a full network stack
    but does not configure any interfaces within the container. This renders the container
    completely isolated. This driver is provided mainly for backward-compatibility
    purposes, and like the host driver, only one network of the null type can be created
    on a Docker host:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是空或无网络。使用空网络驱动程序创建一个网络，当容器连接到它时，会提供一个完整的网络堆栈，但不会在容器内配置任何接口。这使得容器完全隔离。这个驱动程序主要是为了向后兼容而提供的，就像主机驱动程序一样，Docker
    主机上只能创建一个空类型的网络：
- en: '![](Images/ae095a5c-2365-445b-84b0-f4930d6f947e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ae095a5c-2365-445b-84b0-f4930d6f947e.png)'
- en: 'The third network driver with a scope of local is the bridge driver. Bridge
    networks are the most common type. Any containers attached to the same bridge
    network are able to communicate with one another. A Docker host can have more
    than one network created with the bridge driver. However, containers attached
    to one bridge network are unable to communicate with containers on a different
    bridge network, even if the networks are on the same Docker host. Note that there
    are slight feature differences between the built-in bridge network and any user-created
    bridge networks. It is best practice to create your own bridge networks and utilize
    them instead of the using the built-in bridge network.  Here is an example of
    running a container using a bridge network:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个具有本地范围的网络驱动程序是桥接驱动程序。桥接网络是最常见的类型。连接到同一桥接网络的任何容器都能够彼此通信。Docker主机可以使用桥接驱动程序创建多个网络。但是，连接到一个桥接网络的容器无法与不同桥接网络上的容器通信，即使这些网络位于同一个Docker主机上。请注意，内置桥接网络和任何用户创建的桥接网络之间存在轻微的功能差异。最佳实践是创建自己的桥接网络并利用它们，而不是使用内置的桥接网络。以下是使用桥接网络运行容器的示例：
- en: '![](Images/e929dbdf-7012-41dc-8577-789de4f1c1ae.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/e929dbdf-7012-41dc-8577-789de4f1c1ae.png)'
- en: 'In addition to the drivers that create networks with local scope, there are
    built-in network drivers that create networks with swarm scope. Such networks
    will span all the hosts in a swarm and allow containers attached to them to communicate
    in spite of running on different Docker hosts. As you probably have surmised,
    use of networks that have swarm scope requires Docker swarm mode. In fact, when
    you initialize a Docker host into swarm mode, a special new network is created
    for you that has swarm scope. This swarm scope network is named *ingress* and
    is created using the built-in overlay driver. This network is vital to the load
    balancing feature of swarm mode that saw used in the *Accessing container applications
    in a swarm* section of [Chapter 5](f1681897-580b-44fb-9e43-4aed37e67529.xhtml),
    *Docker Swarm*. There''s also a new bridge network created in the `swarm init`,
    named docker_gwbridge. This network is used by swarm to communicate outward, kind
    of like a default gateway.  Here are the default built-in networks found in a
    new Docker swarm:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '除了创建具有本地范围的网络的驱动程序之外，还有内置网络驱动程序创建具有集群范围的网络。这些网络将跨越集群中的所有主机，并允许连接到它们的容器进行通信，尽管它们在不同的Docker主机上运行。您可能已经猜到，使用具有集群范围的网络需要Docker集群模式。实际上，当您将Docker主机初始化为集群模式时，将为您创建一个具有集群范围的特殊新网络。这个集群范围网络被命名为*ingress*，并使用内置的覆盖驱动程序创建。这个网络对于集群模式的负载平衡功能至关重要，该功能在[第5章](f1681897-580b-44fb-9e43-4aed37e67529.xhtml)的*访问集群中的容器应用*部分中使用了*Docker
    Swarm*。在`swarm init`中还创建了一个名为docker_gwbridge的新桥接网络。这个网络被集群用于向外通信，有点像默认网关。以下是在新的Docker集群中找到的默认内置网络： '
- en: '![](Images/73d4326f-1d8f-409a-906d-603de525d80c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/73d4326f-1d8f-409a-906d-603de525d80c.png)'
- en: Using the overlay driver allows you to create networks that span Docker hosts.
    These are layer 2 networks. There is a lot of network plumbing that gets laid
    down behind the scenes when you create an overlay network. Each host in the swarm
    gets a network sandbox with a network stack. Within that sandbox, a bridge is
    created and named br0\. Then, a VXLAN tunnel endpoint is created and attached
    to bridge br0\. Once all of the swarm hosts have the tunnel endpoint created,
    a VXLAN tunnel is created that connects all of the endpoints together. This tunnel
    is actually what we see as the overlay network. When containers are attached to
    the overlay network, they get an IP address assigned from the overlay's subnet,
    and all communications between containers on that network are carried out via
    the overlay. Of course, behind the scenes that communication traffic is passing
    through the VXLAN endpoints, going across the Docker hosts network, and any routers
    connecting the host to the networks of the other Docker hosts. But, you never
    have to worry about all the behind-the-scenes stuff. Just create an overlay network,
    attach your containers to it, and you're golden.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用覆盖驱动程序允许您创建跨Docker主机的网络。这些是第2层网络。在创建覆盖网络时，幕后会铺设大量网络管道。集群中的每个主机都会获得一个带有网络堆栈的网络沙盒。在该沙盒中，会创建一个名为br0的桥接。然后，会创建一个VXLAN隧道端点并将其附加到桥接br0上。一旦所有集群主机都创建了隧道端点，就会创建一个连接所有端点的VXLAN隧道。实际上，这个隧道就是我们看到的覆盖网络。当容器连接到覆盖网络时，它们会从覆盖子网中分配一个IP地址，并且该网络上的容器之间的所有通信都通过覆盖网络进行。当然，在幕后，通信流量通过VXLAN端点传递，穿过Docker主机网络，并且通过连接主机与其他Docker主机网络的任何路由器。但是，您永远不必担心所有幕后的事情。只需创建一个覆盖网络，将您的容器连接到它，您就大功告成了。
- en: The next local network driver that we're going to discuss is called MACVLAN.
    This driver creates networks that allow containers to each have their own IP and
    MAC addresses, and to be attached to a non-Docker network. What that means is
    that in addition to the container-to-container communication you get with bridge
    and overlay networks, with MACVLAN networks you also are able to connect with
    VLANs, VMs, and other physical servers. Said another way, the MACVLAN driver allows
    you to get your containers onto existing networks and VLANs. A MACVLAN network
    has to be created on each Docker host where you will run containers that need
    to connect to your existing networks. What's more, you will need a different MACVLAN
    network created for each VLAN you want containers to connect to. While using MACVLAN
    networks sounds like the way to go, there are two important challenges to using
    it. First, you have to be very careful about the subnet ranges you assign to the
    MACVLAN network. Containers will be assigned IPs from your range without any consideration
    of the IPs in use elsewhere. If you have a DHCP system handing out IPs that overlap
    with the range you gave to the MACVLAN driver, it can easily cause duplicate IP
    scenarios. The second challenge is that MACVLAN networks require your network
    cards to be configured in promiscuous mode. This is usually frowned upon in on-premise
    networks but is pretty much forbidden in cloud-provider networks such as AWS and
    Azure, so the MACVLAN driver will have very limited use cases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的下一个本地网络驱动程序称为MACVLAN。该驱动程序创建的网络允许每个容器都有自己的IP和MAC地址，并连接到非Docker网络。这意味着除了使用桥接和覆盖网络进行容器间通信外，使用MACVLAN网络还可以连接到VLAN、虚拟机和其他物理服务器。换句话说，MACVLAN驱动程序允许您将容器连接到现有网络和VLAN。必须在每个要运行需要连接到现有网络的容器的Docker主机上创建MACVLAN网络。而且，您需要为要连接的每个VLAN创建一个不同的MACVLAN网络。虽然使用MACVLAN网络听起来是一个好方法，但使用它有两个重要的挑战。首先，您必须非常小心地分配给MACVLAN网络的子网范围。容器将从您的范围中分配IP，而不考虑其他地方使用的IP。如果您有一个分配IP的DHCP系统与您给MACVLAN驱动程序的范围重叠，很容易导致重复的IP情况。第二个挑战是MACVLAN网络需要将您的网络卡配置为混杂模式。这在企业网络中通常是不被赞成的，但在云提供商网络中几乎是被禁止的，例如AWS和Azure，因此MACVLAN驱动程序的使用情况非常有限。
- en: There is a lot of information covered in this section on local or built-in network
    drivers. Don't despair! They are much easier to create and use than this wealth
    of information seems to indicate. We will go into creating and using info shortly
    in the *Creating Docker networks* section, but next, let's have a quick discussion
    about remote (also known as third-party) network drivers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了大量关于本地或内置网络驱动程序的信息。不要绝望！它们比这些丰富的信息所表明的要容易得多。我们将在*创建Docker网络*部分很快讨论创建和使用信息，但接下来，让我们快速讨论一下远程（也称为第三方）网络驱动程序。
- en: References
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Check out these links for more information:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以获取更多信息：
- en: Excellent, in-depth Docker article for Docker networking: [https://success.docker.com/article/networking](https://success.docker.com/article/networking)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优秀的、深入的Docker网络文章：[https://success.docker.com/article/networking](https://success.docker.com/article/networking)
- en: Networking with Overlay Networks: [https://docs.docker.com/network/network-tutorial-overlay/](https://docs.docker.com/network/network-tutorial-overlay/)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用覆盖网络进行网络连接：[https://docs.docker.com/network/network-tutorial-overlay/](https://docs.docker.com/network/network-tutorial-overlay/)
- en: Using MACVLAN networks: [https://docs.docker.com/v17.12/network/macvlan/](https://docs.docker.com/v17.12/network/macvlan/)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MACVLAN网络：[https://docs.docker.com/v17.12/network/macvlan/](https://docs.docker.com/v17.12/network/macvlan/)
- en: Third-party (remote) network drivers
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三方（远程）网络驱动程序
- en: 'As mentioned previously in the *What is a Docker network*? section, in addition
    to the built-in, or local, network drivers provided by Docker, the CNM supports
    community- and vendor-created network drivers. Some examples of these third-party
    drivers include Contiv, Weave, Kuryr, and Calico. One of the benefits of using
    one of these third-party drivers is that they fully support deployment in cloud-hosted
    environments, such as AWS. In order to use these drivers, they need to be installed
    in a separate installation step for each of your Docker hosts. Each of the third-party
    network drivers brings their own set of features to the table. Here is the summary
    description of these drivers as shared by Docker in the reference architecture
    document:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在*什么是Docker网络*？部分中，除了Docker提供的内置或本地网络驱动程序外，CNM还支持社区和供应商创建的网络驱动程序。其中一些第三方驱动程序的例子包括Contiv、Weave、Kuryr和Calico。使用这些第三方驱动程序的好处之一是它们完全支持在云托管环境中部署，例如AWS。为了使用这些驱动程序，它们需要在每个Docker主机的单独安装步骤中安装。每个第三方网络驱动程序都带来了自己的一套功能。以下是Docker在参考架构文档中分享的这些驱动程序的摘要描述：
- en: '![](Images/4b8d5ba1-8d15-41fe-9a21-fe4577e95705.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/4b8d5ba1-8d15-41fe-9a21-fe4577e95705.png)'
- en: 'Although each of these third-party drivers has its own unique installation,
    setup, and execution methods, the general steps are similar. First, you download
    the driver, then you handle any configuration setup, and finally you run the driver.
    These remote drivers typically do not require swarm mode and can be used with
    or without it. As an example, let''s take a deep-dive into using the weave driver.
    To install the weave network driver, issue the following commands on each Docker
    host:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些第三方驱动程序各自具有独特的安装、设置和执行方法，但一般步骤是相似的。首先，您下载驱动程序，然后处理任何配置设置，最后运行驱动程序。这些远程驱动程序通常不需要群集模式，并且可以在有或没有群集模式的情况下使用。例如，让我们深入了解如何使用织物驱动程序。要安装织物网络驱动程序，请在每个Docker主机上发出以下命令：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding steps need to be completed on each Docker host that will be used
    to run containers that will communicate with each other over the weave network.
    The launch command can provide the hostname or IP address of the first Docker
    host, which was set up and already running the weave network, to peer with it
    so that their containers can communicate. For example, if you have set up `node01`
    with the weave network when you start up weave on `node02`, you would use the
    following command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤需要在将用于在织物网络上相互通信的容器的每个Docker主机上完成。启动命令可以提供第一个Docker主机的主机名或IP地址，该主机已设置并已运行织物网络，以便与其对等，以便它们的容器可以通信。例如，如果您已经在`node01`上设置了织物网络，当您在`node02`上启动织物时，您将使用以下命令：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Alternatively, you can connect new (Docker host) peers using the connect command,
    executing it from the first host configured. To add `node02` (after it has weave
    installed and running), use the following command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用连接命令连接新的（Docker主机）对等体，从已配置的第一个主机执行。要添加`node02`（在安装和运行织物后），请使用以下命令：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can utilize the weave network driver without enabling swarm mode on your
    hosts. Once weave has been installed and started, and the peers (other Docker
    hosts) have been connected, your containers will automatically utilize the weave
    network and be able to communicate with each other regardless of whether they
    are on the same Docker host or different ones.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在主机上不启用群集模式的情况下使用织物网络驱动程序。一旦织物被安装和启动，并且对等体（其他Docker主机）已连接，您的容器将自动利用织物网络，并能够相互通信，无论它们是在同一台Docker主机上还是在不同的主机上。
- en: 'The weave network shows up in your network list just like any of your other
    networks:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 织物网络显示在您的网络列表中，就像您的其他任何网络一样：
- en: '![](Images/77ef436f-4d57-4313-9ec0-9a109434c4f8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/77ef436f-4d57-4313-9ec0-9a109434c4f8.png)'
- en: 'Let''s test out our shiny new network. First, make sure that you have installed
    the weave driver on all the hosts you want to be connected by following the steps
    described previously. Make sure that you either use the launch command with `node01`
    as a parameter, or from `node01` you use the connect command for each of the additional
    nodes you are configuring. For this example, my lab servers are named ubuntu-node01
    and ubuntu-`node02`. Let''s start with `node02`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下我们闪亮的新网络。首先确保你已经按照之前描述的步骤在所有你想要连接的主机上安装了weave驱动。确保你要么使用`node01`作为参数启动命令，要么从`node01`开始为你配置的每个额外节点使用connect命令。在这个例子中，我的实验服务器名为ubuntu-node01和ubuntu-`node02`。让我们从`node02`开始：
- en: 'Note the following, on `ubuntu-node01`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在`ubuntu-node01`上：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And, note the following, on `ubuntu-node02`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，请注意，在`ubuntu-node02`上：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, back on `ubuntu-node01`, note the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到`ubuntu-node01`，请注意以下内容：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](Images/c4bd83df-3b2a-4931-9dbd-bcd67e7bb982.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/c4bd83df-3b2a-4931-9dbd-bcd67e7bb982.png)'
- en: 'Now, let''s launch a container on each node. Make sure we name them for easy
    identification, starting with `ubuntu-node01`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在每个节点上启动一个容器。确保给它们命名以便易于识别，从`ubuntu-node01`开始：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](Images/3a1ca418-01e8-4774-96ac-cf34e7d948f3.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/3a1ca418-01e8-4774-96ac-cf34e7d948f3.png)'
- en: 'Now, launch a container on `ubuntu-node02`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在`ubuntu-node02`上启动一个容器：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](Images/6e796094-aece-4402-96d3-fcb4854893d2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/6e796094-aece-4402-96d3-fcb4854893d2.png)'
- en: 'Excellent. Now, we have containers running on both nodes. Let''s see whether
    they can communicate. Since we are on `node02`, we will check there first:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。现在，我们在两个节点上都有容器在运行。让我们看看它们是否可以通信。因为我们在`node02`上，我们首先检查那里：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](Images/69fb11da-ad53-4ec5-8b6a-f4ba031e70e4.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/69fb11da-ad53-4ec5-8b6a-f4ba031e70e4.png)'
- en: 'Yeah! That worked. Let''s try going the other way:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！成功了。让我们试试反过来：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](Images/5ed6a092-d5db-46d9-bc71-2157d4bb58e1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/5ed6a092-d5db-46d9-bc71-2157d4bb58e1.png)'
- en: Perfect! We have bi-directional communication. Did you notice anything else?
    We have name resolution for our app containers (we didn't have to ping by IP only).
    Pretty nice, right?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们有双向通信。你注意到了什么其他的吗？我们的应用容器有名称解析（我们不仅仅需要通过IP来ping）。非常好，对吧？
- en: References
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Check out these links for more information:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这些链接以获取更多信息：
- en: Installing and using the weave network driver: [https://www.weave.works/docs/net/latest/overview/](https://www.weave.works/docs/net/latest/overview/)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用weave网络驱动：[https://www.weave.works/docs/net/latest/overview/](https://www.weave.works/docs/net/latest/overview/)
- en: Weaveworks weave github repo: [https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weaveworks weave github仓库：[https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)
- en: Creating Docker networks
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Docker网络
- en: 'OK, you now know a lot about both the local and the remote network drivers,
    and you have seen how several of them are created for you when you install Docker
    and/or initialize swarm mode (or install a remote driver). But, what if you want
    to create your own networks using some of these drivers? It is really pretty simple.
    Let''s take a look. The built-in help for the `network create` command looks like
    this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在你已经对本地和远程网络驱动有了很多了解，你已经看到了在安装Docker和/或初始化swarm模式（或安装远程驱动）时，有几个驱动是为你创建的。但是，如果你想使用其中一些驱动创建自己的网络怎么办？这其实非常简单。让我们来看看。`network
    create`命令的内置帮助如下：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Examining this, we see there are essentially two parts of this command we need
    to handle, the OPTIONS followed by the NETWORK name to make the network we wish
    to create. What are our options? Well, there are quite a lot, but let's pick out
    a few to get you going quickly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这个，我们看到这个命令基本上有两个部分需要处理，OPTIONS后面跟着我们想要创建的网络的NETWORK名称。我们有哪些选项？嗯，有相当多，但让我们挑选一些让你快速上手的。
- en: 'Probably the most important option is the `--driver` option. This is how we
    tell Docker which of the pluggable network drivers to use when creating this network.
    As you have seen, the choice of driver determines the network characteristics.
    The value you supply to the driver option will be like the ones shown in the DRIVER
    column of the output from the `docker network ls` command. Some of the possible
    values are bridge, overlay, and macvlan. Remember that you cannot create additional
    host or null networks as they are limited to one per Docker host. So far, what
    might this look like? Here is an example of creating a new overlay network, using
    mostly defaults for options:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最重要的选项是`--driver`选项。这是我们告诉Docker在创建此网络时要使用哪个可插拔网络驱动程序的方式。正如您所见，驱动程序的选择决定了网络的特性。您提供给驱动程序选项的值将类似于从`docker
    network ls`命令的输出中显示的DRIVER列中显示的值。一些可能的值是bridge、overlay和macvlan。请记住，您不能创建额外的主机或空网络，因为它们限制为每个Docker主机一个。到目前为止，这可能是什么样子？以下是使用大部分默认选项创建新覆盖网络的示例：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'That works just fine. You can run new services and attach them to your new
    network. But what else might we want to control in our network? Well, how about
    the IP space? Yep, and Docker provides options for controlling the IP settings
    for our networks. This is done using the `--subnet`, `--gateway`, and `--ip-range`
    optional parameters. So, let''s take a look at creating a new network using this
    options. See [Chapter 2](e66034ed-dcc0-48a8-a2ec-9466669e6649.xhtml), *Learning
    Docker Commands*, for how to install jq if you have not done so already:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好。您可以运行新服务并将它们附加到您的新网络。但是我们可能还想控制网络中的其他内容吗？嗯，IP空间怎么样？是的，Docker提供了控制网络IP设置的选项。这是使用`--subnet`、`--gateway`和`--ip-range`可选参数来完成的。所以，让我们看看如何使用这些选项创建一个新网络。如果您还没有安装jq，请参阅[第2章](e66034ed-dcc0-48a8-a2ec-9466669e6649.xhtml)，*学习Docker命令*，了解如何安装它：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Executing the preceding code in my lab looks like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的实验室中执行上述代码看起来是这样的：
- en: '![](Images/bbd502b4-374d-406d-915c-b2af371914b3.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bbd502b4-374d-406d-915c-b2af371914b3.png)'
- en: Looking over this example, we see that we created a new overlay network using
    specific IP parameters for the subnet, the IP range, and the gateway. Then, we
    validated that the network was created with the requested options. Next, we created
    a service using our new network. Then, we found the container ID for a container
    belonging to the service and used it to inspect the network settings for the container.
    We can see that the container was run using an IP address (in this case, `172.30.0.7`)
    from the IP range we configured our network with. Looks like we made it!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看这个例子，我们看到我们使用特定的IP参数为子网、IP范围和网关创建了一个新的覆盖网络。然后，我们验证了网络是否使用了请求的选项进行创建。接下来，我们使用我们的新网络创建了一个服务。然后，我们找到了属于该服务的容器的容器ID，并用它来检查容器的网络设置。我们可以看到，容器是使用我们配置网络的IP范围中的IP地址（在这种情况下是`172.30.0.7`）运行的。看起来我们成功了！
- en: As mentioned, there are many other options available when creating Docker networks,
    and I will leave it as an exercise for you to discover them with the `docker network
    create --help` command, and to try some of them out to see what they do.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在创建Docker网络时还有许多其他选项可用，我将把它作为一个练习留给您，让您使用`docker network create --help`命令来发现它们，并尝试一些选项以查看它们的功能。
- en: References
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: You can find the documentation for the `network create` command at [https://docs.docker.com/engine/reference/commandline/network_create/](https://docs.docker.com/engine/reference/commandline/network_create/).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://docs.docker.com/engine/reference/commandline/network_create/](https://docs.docker.com/engine/reference/commandline/network_create/)找到`network
    create`命令的文档。
- en: Free networking features
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 免费网络功能
- en: There are two networking features or services that you get for free with your
    Docker swarm networks. The first is Service Discovery, and the second is load
    balancing. When you create Docker services, you get these features automatically. We
    experienced these features in this chapter and in [Chapter 5](f1681897-580b-44fb-9e43-4aed37e67529.xhtml),
    *Docker Swarm*, but didn't really refer to them by name. So, let's call them out
    here.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个网络功能或服务是您在Docker群集网络中免费获得的。第一个是服务发现，第二个是负载均衡。当您创建Docker服务时，您会自动获得这些功能。我们在本章和第5章《Docker
    Swarm》中体验了这些功能，但并没有真正以名称的方式提到它们。所以，在这里我们来具体提一下。
- en: 'First up is Service Discovery. When you create a service, it gets a unique
    name. That name gets registered with the swarm DNS. And, every service uses the
    swarm DNS for name resolution. Here is an example for you. We are going to leverage
    the `specifics-over` overlay network we created earlier in the creating Docker
    networks section. We''ll create two services (`tester1` and `tester2`) attached
    to that network, then we will connect to a container in the `tester1` services
    and ping the `tester2` service by name. Check it out:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是服务发现。当您创建一个服务时，它会得到一个唯一的名称。该名称会在群集DNS中注册。而且，每个服务都使用群集DNS进行名称解析。这里有一个例子。我们将利用之前在创建Docker网络部分创建的`specifics-over`叠加网络。我们将创建两个服务（`tester1`和`tester2`）并连接到该网络，然后我们将连接到`tester1`服务中的一个容器，并通过名称ping`tester2`服务。看一下：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is what the preceding commands look like when executed:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行前述命令时的样子：
- en: '![](Images/d89ec999-415d-4163-832d-414748894ff4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/d89ec999-415d-4163-832d-414748894ff4.png)'
- en: Note that I typed the first part of the service name (`tester1`) and used command-line
    completion by hitting *Tab* to fill in the container name for the exec command.
    But, as you can see, I was able to reference the `tester2` service by name from
    within a `tester1` container.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我输入了服务名称的第一部分（`tester1`）并使用命令行补全，通过按下*Tab*键来填写exec命令的容器名称。但是，正如您所看到的，我能够在`tester1`容器内通过名称引用`tester2`服务。
- en: For free!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 免费！
- en: The second free feature we get is Load balancing. This powerful feature is pretty
    easy to understand. It allows traffic intended for a service to be sent to any
    host in a swarm regardless of whether that host is running a replica of the service.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的第二个免费功能是负载均衡。这个强大的功能非常容易理解。它允许将发送到服务的流量发送到群集中的任何主机，而不管该主机是否正在运行服务的副本。
- en: Imagine a scenario where you have a six-node swarm cluster, and a service that
    has only one replica deployed. You can send traffic to that service via any host
    in the swarm and know that it will arrive at the service's one container no matter
    which host the container is actually running on. In fact, you can direct traffic
    to all hosts in the swarm using a load balancer, say in a round-robin model, and
    each time traffic is sent to the load balancer, that traffic will get delivered
    to the app container without fail.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这样的情景：您有一个六节点的群集集群，以及一个只部署了一个副本的服务。您可以通过群集中的任何主机发送流量到该服务，并知道无论容器实际在哪个主机上运行，流量都会到达服务的一个容器。事实上，您可以使用负载均衡器将流量发送到群集中的所有主机，比如采用轮询模式，每次将流量发送到负载均衡器时，该流量都会无误地传递到应用程序容器。
- en: Pretty handy, right? Again, for free!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 相当方便，对吧？再次强调，这是免费的！
- en: References
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: Want to have a go at service discovery? Then check out [https://training.play-with-docker.com/swarm-service-discovery/](https://training.play-with-docker.com/swarm-service-discovery/).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 想要尝试服务发现吗？那就查看[https://training.play-with-docker.com/swarm-service-discovery/](https://training.play-with-docker.com/swarm-service-discovery/)。
- en: You can read about swarm service load balancing at [https://docs.docker.com/engine/swarm/key-concepts/#load-balancing](https://docs.docker.com/engine/swarm/key-concepts/#load-balancing).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://docs.docker.com/engine/swarm/key-concepts/#load-balancing](https://docs.docker.com/engine/swarm/key-concepts/#load-balancing)阅读有关swarm服务负载平衡的信息。
- en: Which Docker network driver should I use?
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我应该使用哪个Docker网络驱动程序？
- en: The short answer to that question is the right one for the job. That means there
    is no single network driver that is the right fit for every situation. If you're
    doing work on your laptop, running with swarm inactive, and you just need your
    containers to be able to communicate with each other, the simple bridge mode driver
    is ideal.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题的简短答案就是适合工作的正确驱动程序。这意味着没有单一的网络驱动程序适合每种情况。如果你在笔记本电脑上工作，swarm处于非活动状态，并且只需要容器之间能够通信，那么简单的桥接模式驱动程序是理想的。
- en: If you have multiple nodes and just need container-to-container traffic, the
    overlay driver is the right one to use. This one works well in AWS, if you are
    within the container-to-container realm. If you need container-to-VM or container-to-physical-server
    communication (and can tolerate promiscuous mode), the MACVLAN driver is the way
    to go. Or, if you have a more complex requirement, one of the many remote drivers
    might be just what the doctor ordered.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个节点，只需要容器对容器的流量，那么覆盖驱动程序是正确的选择。如果你需要容器对VM或容器对物理服务器的通信（并且可以容忍混杂模式），那么MACVLAN驱动程序是最佳选择。或者，如果你有更复杂的需求，许多远程驱动程序可能正是你需要的。
- en: I've found that for most multi-host scenarios, the overlay driver will get the
    job done, so I would recommend that you enable swarm mode, and give the overlay
    driver a try before you ramp up to any of the other multi-host options.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现对于大多数多主机场景，覆盖驱动程序可以胜任，所以我建议你启用swarm模式，并在升级到其他多主机选项之前尝试覆盖驱动程序。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'How do you feel about Docker networking now? Docker has taken a complex technology,
    networking, and made it easy to understand and use. Most of the crazy, difficult
    setup stuff is literally handled with a single `swarm init` command. Let''s review:
    you learned about the network design that Docker created, called the container
    network model, or CNM. Then,  you learned how the libnetwork project turned that
    model into a pluggable architecture. After that, you found out that Docker created
    a powerful set of drivers to plug into the libnetwork architecture to enable a
    variety of network options for most of your container communication needs. Since
    the architecture is so pluggable, others have created even more network drivers
    that solve any edge cases that the Docker drivers don''t handle. Docker networking
    has really come into its own.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在对Docker网络有什么感觉？Docker已经将复杂的技术网络变得易于理解和使用。大部分疯狂、困难的设置都可以通过一个`swarm init`命令来处理。让我们回顾一下：你了解了Docker创建的网络设计，称为容器网络模型或CNM。然后，你了解了libnetwork项目如何将该模型转化为可插拔架构。之后，你发现Docker创建了一组强大的驱动程序，可以插入libnetwork架构，以满足大部分容器通信需求的各种网络选项。由于架构是可插拔的，其他人已经创建了更多的网络驱动程序，解决了Docker驱动程序无法处理的任何边缘情况。Docker网络真的已经成熟了。
- en: I hope you are ready for more, because in [Chapter 7](1a206f3a-faf8-43cb-9413-d1e451bd2a35.xhtml),
    *Docker Stacks*, we are going to dive into Docker stacks. This is where all of
    the information you have learned so far really comes together into a symphony
    of brilliance. Take a deep breath and turn the page!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你已经做好准备，因为在[第7章](1a206f3a-faf8-43cb-9413-d1e451bd2a35.xhtml)中，*Docker Stacks*，我们将深入探讨Docker堆栈。这是你迄今为止学到的所有信息真正汇聚成一种辉煌的交响乐。深呼吸，翻开下一页吧！
