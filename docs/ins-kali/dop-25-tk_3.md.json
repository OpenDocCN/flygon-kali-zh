["```\n 1  cd k8s-specs\n 2\n 3  git pull\n```", "```\n 1  cat mon/prom-values-bare.yml\n```", "```\nserver:\n  ingress:\n    enabled: true\n    annotations:\n      ingress.kubernetes.io/ssl-redirect: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n  resources:\n    limits:\n      cpu: 100m\n      memory: 1000Mi\n    requests:\n      cpu: 10m\n      memory: 500Mi\nalertmanager:\n  ingress:\n    enabled: true\n    annotations:\n      ingress.kubernetes.io/ssl-redirect: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\nkubeStateMetrics:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 50Mi\n    requests:\n      cpu: 5m\n      memory: 25Mi\nnodeExporter:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n    requests:\n      cpu: 5m\n      memory: 10Mi\npushgateway:\n  resources:\n    limits:\n      cpu: 10m\n      memory: 20Mi\n        requests:\n      cpu: 5m\n      memory: 10Mi\n```", "```\n 1  PROM_ADDR=mon.$LB_IP.nip.io\n 2\n 3  AM_ADDR=alertmanager.$LB_IP.nip.io\n```", "```\n 1  helm install stable/prometheus \\\n 2      --name prometheus \\\n 3      --namespace metrics \\\n 4      --version 7.1.3 \\\n 5      --set server.ingress.hosts={$PROM_ADDR} \\\n 6      --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7      -f mon/prom-values-bare.yml\n```", "```\n...\nRESOURCES:\n==> v1beta1/DaemonSet\nNAME                     DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\nprometheus-node-exporter 3       3       0     3          0         <none>        3s \n==> v1beta1/Deployment\nNAME                          DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\nprometheus-alertmanager       1       1       1          0         3s\nprometheus-kube-state-metrics 1       1       1          0         3s\nprometheus-pushgateway        1       1       1          0         3s\nprometheus-server             1       1       1          0         3s\n...\n```", "```\n 1  kubectl -n metrics \\\n 2      rollout status \\\n 3      deploy prometheus-server\n```", "```\n 1  kubectl -n metrics \\\n 2      describe deployment \\\n 3      prometheus-server\n```", "```\n  Containers:\n   prometheus-server-configmap-reload:\n    Image: jimmidyson/configmap-reload:v0.2.2\n    ...\n   prometheus-server:\n    Image: prom/prometheus:v2.4.2\n    ...\n```", "```\n 1  kubectl -n metrics \\\n 2      describe cm prometheus-server\n```", "```\n...\nData\n====\nalerts:\n----\n{} \nprometheus.yml:\n----\nglobal:\n  evaluation_interval: 1m\n  scrape_interval: 1m\n  scrape_timeout: 10s \nrule_files:\n- /etc/config/rules\n- /etc/config/alerts\nscrape_configs:\n- job_name: prometheus\n  static_configs:\n  - targets:\n    - localhost:9090\n- bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  job_name: kubernetes-apiservers\n  kubernetes_sd_configs:\n  - role: endpoints\n  relabel_configs:\n  - action: keep\n    regex: default;kubernetes;https\n    source_labels:\n    - __meta_kubernetes_namespace\n    - __meta_kubernetes_service_name\n    - __meta_kubernetes_endpoint_port_name\n  scheme: https\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    insecure_skip_verify: true\n...\n```", "```\n 1  open \"http://$PROM_ADDR/config\"\n```", "```\n 1  open \"http://$PROM_ADDR/targets\"\n```", "```\n 1  kubectl -n metrics get svc\n```", "```\nNAME                          TYPE      CLUSTER-IP    EXTERNAL-IP PORT(S)  AGE\nprometheus-alertmanager       ClusterIP 10.23.245.165 <none>      80/TCP   41d\nprometheus-kube-state-metrics ClusterIP None          <none>      80/TCP   41d\nprometheus-node-exporter      ClusterIP None          <none>      9100/TCP 41d\nprometheus-pushgateway        ClusterIP 10.23.244.47  <none>      9091/TCP 41d\nprometheus-server             ClusterIP 10.23.241.182 <none>      80/TCP   41d\n```", "```\n 1  kubectl -n metrics run -it test \\\n 2      --image=appropriate/curl \\\n 3      --restart=Never \\\n 4      --rm \\\n 5      -- prometheus-node-exporter:9100/metrics\n```", "```\n 1  # HELP node_memory_MemTotal_bytes Memory information field\n    MemTotal_bytes.\n 2  # TYPE node_memory_MemTotal_bytes gauge\n 3  node_memory_MemTotal_bytes 3.878477824e+09\n```", "```\n 1  kubectl -n metrics run -it test \\\n 2      --image=appropriate/curl \\\n 3      --restart=Never \\\n 4      --rm \\\n 5      -- prometheus-kube-state-metrics:8080/metrics\n```", "```\n 1  kube_deployment_created{deployment=\"prometheus-\n    server\",namespace=\"metrics\"} 1.535566512e+09\n```", "```\n 1  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  kubectl -n metrics run -it test \\\n 2      --image=appropriate/curl \\\n 3      --restart=Never \\\n 4      --rm \\\n 5      -- prometheus-kube-state-metrics:8080/metrics \\\n 6      | grep \"kube_node_info\"\n```", "```\n 1  # HELP kube_node_info Information about a cluster node.\n 2  # TYPE kube_node_info gauge\n 3  ...\n```", "```\n 1  kube_node_info\n```", "```\n 1  count(kube_node_info)\n```", "```\n 1  diff mon/prom-values-bare.yml \\\n 2      mon/prom-values-nodes.yml\n```", "```\n> serverFiles:\n>   alerts:\n>     groups:\n>     - name: nodes\n>       rules:\n>       - alert: TooManyNodes\n>         expr: count(kube_node_info) > 3\n>         for: 15m\n>         labels:\n>           severity: notify\n>         annotations:\n>           summary: Cluster increased\n>           description: The number of the nodes in the cluster increased\n>       - alert: TooFewNodes\n>         expr: count(kube_node_info) < 1\n>         for: 15m\n>         labels:\n>           severity: notify\n>         annotations:\n>           summary: Cluster decreased\n>           description: The number of the nodes in the cluster decreased\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-nodes.yml\n```", "```\n 1  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  diff mon/prom-values-nodes.yml \\\n 2      mon/prom-values-nodes-0.yml\n```", "```\n57,58c57,58\n< expr: count(kube_node_info) > 3\n< for: 15m\n---\n> expr: count(kube_node_info) > 0\n> for: 1m\n66c66\n< for: 15m\n---\n> for: 1m\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-nodes-0.yml\n```", "```\n 1  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  open \"http://$AM_ADDR\"\n```", "```\n 1  diff mon/prom-values-nodes-0.yml \\\n 2      mon/prom-values-nodes-am.yml\n```", "```\n71a72,93\n> alertmanagerFiles:\n>   alertmanager.yml:\n>     global: {}\n>     route:\n>       group_wait: 10s\n>       group_interval: 5m\n>       receiver: slack\n>       repeat_interval: 3h\n>       routes:\n>       - receiver: slack\n>         repeat_interval: 5d\n>         match:\n>           severity: notify\n>           frequency: low\n>     receivers:\n>     - name: slack\n>       slack_configs:\n>       - api_url: \"https://hooks.slack.com/services/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\"\n>         send_resolved: true\n>         title: \"{{ .CommonAnnotations.summary }}\"\n>         text: \"{{ .CommonAnnotations.description }}\"\n>         title_link: http://my-prometheus.com/alerts\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-nodes-am.yml\n```", "```\n 1  open \"https://devops20.slack.com/messages/CD8QJA8DS/\"\n```", "```\n 1  GD5_ADDR=go-demo-5.$LB_IP.nip.io\n 2\n 3  helm install \\\n 4      https://github.com/vfarcic/go-demo-5/releases/download/\n    0.0.1/go-demo-5-0.0.1.tgz \\\n 5      --name go-demo-5 \\\n 6      --namespace go-demo-5 \\\n 7      --set ingress.host=$GD5_ADDR\n```", "```\n 1  kubectl -n go-demo-5 \\\n 2      rollout status \\\n 3      deployment go-demo-5\n```", "```\n 1  curl \"http://$GD5_ADDR/demo/hello\"\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  nginx_ingress_controller_request_duration_seconds_bucket\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_count[5m]\n 3  )) \n 4  by (ingress)\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_bucket{\n 3      le=\"0.25\"\n 4    }[5m]\n 5  )) \n 6  by (ingress)\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_bucket{\n 3      le=\"0.25\"\n 4    }[5m]\n 5  )) \n 6  by (ingress) / \n 7  sum(rate(\n 8    nginx_ingress_controller_request_duration_seconds_count[5m]\n 9  )) \n10  by (ingress)\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_bucket{\n 3      le=\"0.25\", \n 4      ingress=\"go-demo-5\"\n 5    }[5m]\n 6  )) \n 7  by (ingress) / \n 8  sum(rate(\n 9    nginx_ingress_controller_request_duration_seconds_count{\n10      ingress=\"go-demo-5\"\n11    }[5m]\n12  )) \n13  by (ingress)\n```", "```\n 1  for i in {1..30}; do\n 2    DELAY=$[ $RANDOM % 1000 ]\n 3    curl \"http://$GD5_ADDR/demo/hello?delay=$DELAY\"\n 4  done\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_bucket{\n 3      le=\"0.25\", \n 4      ingress=\"go-demo-5\"\n 5    }[5m]\n 6  )) \n 7  by (ingress) / \n 8  sum(rate(\n 9    nginx_ingress_controller_request_duration_seconds_count{\n10      ingress=\"go-demo-5\"\n11    }[5m]\n12  )) \n13  by (ingress)\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_bucket{\n 3      le=\"0.25\"\n 4    }[5m]\n 5  ))\n 6  by (ingress) /\n 7  sum(rate(\n 8    nginx_ingress_controller_request_duration_seconds_count[5m]\n 9  ))\n10  by (ingress) < 0.95\n```", "```\n 1  diff mon/prom-values-nodes-am.yml \\\n 2      mon/prom-values-latency.yml\n```", "```\n53a54,62\n> - name: latency\n>   rules:\n>   - alert: AppTooSlow\n>     expr: sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le= \"0.25\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress) < 0.95\n>     labels:\n>       severity: notify\n>     annotations:\n>       summary: Application is too slow\n>       description: More then 5% of requests are slower than 0.25s\n57c66\n<     expr: count(kube_node_info) > 0\n---\n>     expr: count(kube_node_info) > 3\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-latency.yml\n 8\n 9  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  for i in {1..30}; do\n 2    DELAY=$[ $RANDOM % 10000 ]\n 3    curl \"http://$GD5_ADDR/demo/hello?delay=$DELAY\"\n 4  done\n```", "```\n 1  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  open \"https://devops20.slack.com/messages/CD8QJA8DS/\"\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_bucket{\n 3      le=\"0.25\", \n 4      ingress!~\"prometheus-server|jenkins\"\n 5    }[5m]\n 6  )) \n 7  by (ingress) / \n 8  sum(rate(\n 9    nginx_ingress_controller_request_duration_seconds_count{\n10      ingress!~\"prometheus-server|jenkins\"\n11    }[5m]\n12  )) \n13  by (ingress)\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_request_duration_seconds_bucket{\n 3      le=\"0.5\",\n 4      ingress=~\"prometheus-server|jenkins\"\n 5    }[5m]\n 6  )) \n 7  by (ingress) /\n 8  sum(rate(\n 9    nginx_ingress_controller_request_duration_seconds_count{\n10      ingress=~\"prometheus-server|jenkins\"\n11    }[5m]\n12  ))\n13  by (ingress)\n```", "```\n 1  for i in {1..100}; do\n 2      curl \"http://$GD5_ADDR/demo/hello\"\n 3  done\n 4\n 5  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_requests[5m]\n 3  ))\n 4  by (ingress)\n```", "```\n 1  kube_deployment_status_replicas\n```", "```\n 1  label_join(\n 2    kube_deployment_status_replicas,\n 3    \"ingress\", \n 4    \",\", \n 5    \"deployment\"\n 6  )\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_requests[5m]\n 3  ))\n 4  by (ingress) /\n 5  sum(label_join(\n 6    kube_deployment_status_replicas,\n 7    \"ingress\",\n 8    \",\",\n 9    \"deployment\"\n10  ))\n11  by (ingress)\n```", "```\n 1  diff mon/prom-values-latency.yml \\\n 2      mon/prom-values-latency2.yml\n```", "```\n62a63,69\n> - alert: TooManyRequests\n>   expr: sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) / sum(label_join(kube_deployment_status_replicas, \"ingress\", \",\", \"deployment\")) by (ingress) > 0.1\n>   labels:\n>     severity: notify\n>   annotations:\n>     summary: Too many requests\n>     description: There is more than average of 1 requests per second per replica for at least one application\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-latency2.yml\n 8\n 9  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  for i in {1..200}; do\n 2      curl \"http://$GD5_ADDR/demo/hello\"\n 3  done\n 4\n 5  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  open \"https://devops20.slack.com/messages/CD8QJA8DS/\"\n```", "```\n 1  for i in {1..100}; do\n 2      curl \"http://$GD5_ADDR/demo/hello\"\n 3  done\n 4\n 5  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  nginx_ingress_controller_requests\n```", "```\n 1  for i in {1..100}; do\n 2    curl \"http://$GD5_ADDR/demo/random-error\"\n 3  done\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  sum(rate(\n 2    nginx_ingress_controller_requests{\n 3      status=~\"5..\"\n 4    }[5m]\n 5  ))\n 6  by (ingress) /\n 7  sum(rate(\n 8    nginx_ingress_controller_requests[5m]\n 9  ))\n10  by (ingress)\n```", "```\n 1  diff mon/prom-values-cpu-memory.yml \\\n 2      mon/prom-values-errors.yml\n```", "```\n127a128,136\n> - name: errors\n>   rules:\n>   - alert: TooManyErrors\n>     expr: sum(rate(nginx_ingress_controller_requests{status=~\"5..\"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) > 0.025\n>     labels:\n>       severity: error\n>     annotations:\n>       summary: Too many errors\n>       description: At least one application produced more then 5% of error responses\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-errors.yml\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  sum(rate(\n 2    node_cpu_seconds_total{\n 3      mode!=\"idle\", \n 4      mode!=\"iowait\", \n 5      mode!~\"^(?:guest.*)$\"\n 6   }[5m]\n 7  ))\n 8  by (instance)\n```", "```\n 1  count(\n 2    node_cpu_seconds_total{\n 3      mode=\"system\"\n 4    }\n 5  )\n```", "```\n 1  sum(rate(\n 2    node_cpu_seconds_total{\n 3      mode!=\"idle\", \n 4      mode!=\"iowait\",\n 5      mode!~\"^(?:guest.*)$\"\n 6    }[5m]\n 7  )) /\n 8  count(\n 9    node_cpu_seconds_total{\n10      mode=\"system\"\n11    }\n12  )\n```", "```\n 1  kube_node_status_allocatable_cpu_cores\n```", "```\n 1  sum(\n 2    kube_node_status_allocatable_cpu_cores\n 3  )\n```", "```\n 1  kube_pod_container_resource_requests_cpu_cores\n```", "```\n 1  sum(\n 2    kube_pod_container_resource_requests_cpu_cores\n 3  )\n```", "```\n 1  sum(\n 2    kube_pod_container_resource_requests_cpu_cores\n 3  ) /\n 4  sum(\n 5    kube_node_status_allocatable_cpu_cores\n 6  )\n```", "```\n 1  diff mon/prom-values-latency2.yml \\\n 2      mon/prom-values-cpu.yml\n```", "```\n64c64\n<   expr: sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) / sum(label_join(kube_deployment_status_replicas, \"ingress\", \",\", \"deployment\")) by (ingress) > 0.1\n---\n>   expr: sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) / sum(label_join(kube_deployment_status_replicas, \"ingress\", \",\", \"deployment\")) by (ingress) > 1\n87a88,103\n> - alert: NotEnoughCPU\n>   expr: sum(rate(node_cpu_seconds_total{mode!=\"idle\", mode!=\"iowait\", mode!~\"^(?:guest.*)$\"}[5m])) / count(node_cpu_seconds_total{mode=\"system\"}) > 0.9\n```", "```\n>   for: 30m\n>   labels:\n>     severity: notify\n>   annotations:\n>     summary: There's not enough CPU\n>     description: CPU usage of the cluster is above 90%\n> - alert: TooMuchCPURequested\n>   expr: sum(kube_pod_container_resource_requests_cpu_cores) / sum(kube_node_status_allocatable_cpu_cores) > 0.9\n>   for: 30m\n>   labels:\n>     severity: notify\n>   annotations:\n>     summary: There's not enough allocatable CPU\n>     description: More than 90% of allocatable CPU is requested\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-cpu.yml\n 8\n 9  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  node_memory_MemTotal_bytes\n```", "```\n 1  node_memory_MemAvailable_bytes\n```", "```\n 1  1 -\n 2  sum(\n 3    node_memory_MemAvailable_bytes\n 4  ) /\n 5  sum(\n 6    node_memory_MemTotal_bytes\n 7  )\n```", "```\n 1  kube_node_status_allocatable_memory_bytes\n```", "```\n 1  kube_pod_container_resource_requests_memory_bytes\n```", "```\n 1  sum(\n 2    kube_pod_container_resource_requests_memory_bytes\n 3  )\n```", "```\n 1  sum(\n 2    kube_pod_container_resource_requests_memory_bytes\n 3  ) / \n 4  sum(\n 5    kube_node_status_allocatable_memory_bytes\n 6  )\n```", "```\n 1  diff mon/prom-values-cpu.yml \\\n 2      mon/prom-values-memory.yml\n```", "```\n103a104,119\n> - alert: NotEnoughMemory\n>   expr: 1 - sum(node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes) > 0.9\n>   for: 30m\n>   labels:\n>     severity: notify\n>   annotations:\n>     summary: There's not enough memory\n>     description: Memory usage of the cluster is above 90%\n> - alert: TooMuchMemoryRequested\n>   expr: sum(kube_pod_container_resource_requests_memory_bytes) / sum(kube_node_status_allocatable_memory_bytes) > 0.9\n>   for: 30m\n>   labels:\n>     severity: notify\n>   annotations:\n>     summary: There's not enough allocatable memory\n>     description: More than 90% of allocatable memory is requested\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-memory.yml\n 8\n 9  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  sum(rate(\n 2    node_cpu_seconds_total{\n 3      mode!=\"idle\",\n 4      mode!=\"iowait\",\n 5      mode!~\"^(?:guest.*)$\"\n 6    }[5m]\n 7  ))\n 8  by (instance) /\n 9  count(\n10    node_cpu_seconds_total{\n11      mode=\"system\"\n12    }\n13  )\n14  by (instance)\n15\n16  1 -\n17  sum(\n18    node_memory_MemAvailable_bytes\n19  ) \n20  by (instance) /\n21  sum(\n22    node_memory_MemTotal_bytes\n23  )\n24  by (instance)\n```", "```\n 1  diff mon/prom-values-memory.yml \\\n 2      mon/prom-values-cpu-memory.yml\n```", "```\n119a120,127\n> - alert: TooMuchCPUAndMemory\n>   expr: (sum(rate(node_cpu_seconds_total{mode!=\"idle\", mode!=\"iowait\", mode!~\"^(?:guest.*)$\"}[5m])) by (instance) / count(node_cpu_seconds_total{mode=\"system\"}) by (instance)) < 0.5 and (1 - sum(node_memory_MemAvailable_bytes) by (instance) / sum(node_memory_MemTotal_bytes) by (instance)) < 0.5\n>   for: 30m\n>   labels:\n>     severity: notify\n>   annotations:\n>     summary: Too much unused CPU and memory\n>     description: Less than 50% of CPU and 50% of memory is used on at least one node\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-cpu-memory.yml\n 8\n 9  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  kube_pod_status_phase\n```", "```\n 1  sum(\n 2    kube_pod_status_phase\n 3  ) \n 4  by (phase)\n```", "```\n 1  sum(\n 2    kube_pod_status_phase{\n 3      phase=~\"Failed|Unknown|Pending\"\n 4    }\n 5  ) \n 6  by (phase)\n```", "```\n 1  kubectl run problem \\\n 2      --image i-do-not-exist \\\n 3      --restart=Never\n```", "```\n 1  kubectl get pods\n```", "```\nNAME    READY STATUS       RESTARTS AGE\nproblem 0/1   ErrImagePull 0        27s\n```", "```\n 1  kubectl describe pod problem\n```", "```\n...\nEvents:\n...  Message\n...  -------\n...  Successfully assigned default/problem to aks-nodepool1-29770171-2\n...  Back-off pulling image \"i-do-not-exist\"\n...  Error: ImagePullBackOff\n...  pulling image \"i-do-not-exist\"\n...  Failed to pull image \"i-do-not-exist\": rpc error: code = Unknown desc = Error response from daemon: repository i-do-not-exist not found: does not exist or no pull access\n Warning  Failed     8s (x3 over 46s)   kubelet, aks-nodepool1-29770171-2  Error: ErrImagePull\n```", "```\n 1  diff mon/prom-values-errors.yml \\\n 2      mon/prom-values-phase.yml\n```", "```\n136a137,146\n> - name: pods\n>   rules:\n>   - alert: ProblematicPods\n>     expr: sum(kube_pod_status_phase{phase=~\"Failed|Unknown|Pending\"}) by (phase) > 0\n>     for: 1m\n>     labels:\n>       severity: notify\n>     annotations:\n>       summary: At least one Pod could not run\n>       description: At least one Pod is in a problematic phase\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-phase.yml\n```", "```\n 1  open \"https://devops20.slack.com/messages/CD8QJA8DS/\"\n```", "```\n 1  kubectl delete pod problem\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  kube_pod_start_time\n```", "```\n 1  time()\n```", "```\n 1  time() -\n 2  kube_pod_start_time\n```", "```\n 1  (\n 2    time() -\n 3    kube_pod_start_time{\n 4      namespace!=\"kube-system\"\n 5    }\n 6  ) > 60\n```", "```\n 1  (\n 2    time() -\n 3    kube_pod_start_time{\n 4      namespace!=\"kube-system\"\n 5    }\n 6  ) >\n 7  (60 * 60 * 24 * 90)\n```", "```\n 1  diff mon/prom-values-phase.yml \\\n 2      mon/prom-values-old-pods.yml\n```", "```\n146a147,154\n> - alert: OldPods\n>   expr: (time() - kube_pod_start_time{namespace!=\"kube-system\"}) > 60\n>   labels:\n>     severity: notify\n>     frequency: low\n>   annotations:\n>     summary: Old Pods\n>     description: At least one Pod has not been updated to more than 90 days\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-old-pods.yml\n 8\n 9  open \"https://devops20.slack.com/messages/CD8QJA8DS/\"\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  container_memory_usage_bytes\n```", "```\n 1  container_memory_usage_bytes{\n 2    container_name!=\"\"\n 3  }\n```", "```\n 1  container_memory_usage_bytes{\n 2    container_name=\"prometheus-server\"\n 3  }\n```", "```\n 1  sum(rate(\n 2    container_cpu_usage_seconds_total{\n 3      container_name=\"prometheus-server\"\n 4    }[5m]\n 5  ))\n 6  by (pod_name)\n```", "```\n 1 open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  kube_pod_container_resource_requests_memory_bytes{\n 2    container=\"prometheus-server\"\n 3  }\n```", "```\n 1  sum(label_join(\n 2    container_memory_usage_bytes{\n 3      container_name=\"prometheus-server\"\n 4    },\n 5    \"pod\",\n 6    \",\",\n 7    \"pod_name\"\n 8  ))\n 9  by (pod)\n```", "```\n 1  sum(label_join(\n 2    container_memory_usage_bytes{\n 3      container_name=\"prometheus-server\"\n 4    },\n 5    \"pod\",\n 6    \",\",\n 7    \"pod_name\"\n 8  ))\n 9  by (pod) /\n10  sum(\n11    kube_pod_container_resource_requests_memory_bytes{\n12      container=\"prometheus-server\"\n13    }\n14  )\n15  by (pod)\n```", "```\n 1  sum(label_join(\n 2    container_memory_usage_bytes{\n 3      namespace!=\"kube-system\"\n 4    },\n 5    \"pod\",\n 6    \",\",\n 7    \"pod_name\"\n 8  ))\n 9  by (pod) /\n10  sum(\n11    kube_pod_container_resource_requests_memory_bytes{\n12      namespace!=\"kube-system\"\n13    }\n14  )\n15  by (pod)\n```", "```\n 1  diff mon/prom-values-old-pods.yml \\\n 2      mon/prom-values-req-mem.yml\n```", "```\n148c148\n<   expr: (time() - kube_pod_start_time{namespace!=\"kube-system\"}) > 60\n---\n>   expr: (time() - kube_pod_start_time{namespace!=\"kube-system\"}) > (60 * 60 * 24 * 90)\n154a155,172\n> - alert: ReservedMemTooLow\n>   expr: sum(label_join(container_memory_usage_bytes{namespace!=\"kube-system\", namespace!=\"ingress-nginx\"}, \"pod\", \",\", \"pod_name\")) by (pod) /\n sum(kube_pod_container_resource_requests_memory_bytes{namespace!=\"kube-system\"}) by (pod) > 1.5\n>   for: 1m\n>   labels:\n>     severity: notify\n>     frequency: low\n>   annotations:\n>     summary: Reserved memory is too low\n>     description: At least one Pod uses much more memory than it reserved\n> - alert: ReservedMemTooHigh\n>   expr: sum(label_join(container_memory_usage_bytes{namespace!=\"kube-system\", namespace!=\"ingress-nginx\"}, \"pod\", \",\", \"pod_name\")) by (pod) / sum(kube_pod_container_resource_requests_memory_bytes{namespace!=\"kube-system\"}) by (pod) < 0.5 and sum(kube_pod_container_resource_requests_memory_bytes{namespace!=\"kube-system\"}) by (pod) > 5.25e+06\n>   for: 6m\n>   labels:\n>     severity: notify\n>     frequency: low\n>   annotations:\n>     summary: Reserved memory is too high\n>     description: At least one Pod uses much less memory than it reserved\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-req-mem.yml\n```", "```\n 1  diff mon/prom-values-req-mem.yml \\\n 2      mon/prom-values-req-cpu.yml\n```", "```\n157c157\n<   for: 1m\n---\n>   for: 1h\n166c166\n<   for: 6m\n---\n>   for: 6h\n172a173,190\n> - alert: ReservedCPUTooLow\n>   expr: sum(label_join(rate(container_cpu_usage_seconds_total{namespace!=\"kube-system\", namespace!=\"ingress-nginx\", pod_name!=\"\"}[5m]), \"pod\", \",\", \"pod_name\")) by (pod) / sum(kube_pod_container_resource_requests_cpu_cores{namespace!=\"kube-system\"}) by (pod) > 1.5\n>   for: 1m\n>   labels:\n>     severity: notify\n>     frequency: low\n>   annotations:\n>     summary: Reserved CPU is too low\n>     description: At least one Pod uses much more CPU than it reserved\n> - alert: ReservedCPUTooHigh\n>   expr: sum(label_join(rate(container_cpu_usage_seconds_total{namespace!=\"kube-system\", pod_name!=\"\"}[5m]), \"pod\", \",\", \"pod_name\")) by (pod) / sum(kube_pod_container_resource_requests_cpu_cores{namespace!=\"kube-system\"}) by (pod) < 0.5 and \nsum(kube_pod_container_resource_requests_cpu_cores{namespace!=\"kube-system\"}) by (pod) > 0.005\n>   for: 6m\n>   labels:\n>     severity: notify\n>     frequency: low\n>   annotations:\n>     summary: Reserved CPU is too high\n>     description: At least one Pod uses much less CPU than it reserved\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-req-cpu.yml\n```", "```\n 1  open \"http://$PROM_ADDR/graph\"\n```", "```\n 1  sum(label_join(\n 2    container_memory_usage_bytes{\n 3      namespace!=\"kube-system\"\n 4    }, \n 5    \"pod\", \n 6    \",\", \n 7    \"pod_name\"\n 8  ))\n 9  by (pod) /\n10  sum(\n11    kube_pod_container_resource_limits_memory_bytes{\n12      namespace!=\"kube-system\"\n13    }\n14  )\n15  by (pod)\n```", "```\n 1  diff mon/prom-values-req-cpu.yml \\\n 2      mon/prom-values-limit-mem.yml\n```", "```\n175c175\n<   for: 1m\n---\n>   for: 1h\n184c184\n<   for: 6m\n---\n>   for: 6h\n190a191,199\n> - alert: MemoryAtTheLimit\n>   expr: sum(label_join(container_memory_usage_bytes{namespace!=\"kube-system\"}, \"pod\", \",\", \"pod_name\")) by (pod) / sum(kube_pod_container_resource_limits_memory_bytes{namespace!=\"kube-system\"}) by (pod) > 0.8\n>   for: 1h\n>   labels:\n>     severity: notify\n>     frequency: low\n>   annotations:\n>     summary: Memory usage is almost at the limit\n>     description: At least one Pod uses memory that is close it its limit\n```", "```\n 1  helm upgrade -i prometheus \\\n 2    stable/prometheus \\\n 3    --namespace metrics \\\n 4    --version 7.1.3 \\\n 5    --set server.ingress.hosts={$PROM_ADDR} \\\n 6    --set alertmanager.ingress.hosts={$AM_ADDR} \\\n 7    -f mon/prom-values-limit-mem.yml\n```", "```\n 1  open \"http://$PROM_ADDR/alerts\"\n```", "```\n 1  helm delete prometheus --purge\n 2\n 3  helm delete go-demo-5 --purge\n 4\n 5  kubectl delete ns go-demo-5 metrics\n```"]