- en: '12: Docker overlay networking'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12：Docker叠加网络
- en: Overlay networks are at the beating heart of most things we do with container-related
    networking. In this chapter we’ll cover the fundamentals of native Docker overlay
    networking, as implemented in a Docker Swarm cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 叠加网络是我们在容器相关网络中所做的大部分事情的核心。在本章中，我们将介绍本机Docker叠加网络的基础知识，这是在Docker Swarm集群中实现的。
- en: Docker overlay networking on Windows has feature parity with Linux. This means
    the examples we’ll use in this chapter will all work on Linux and Windows.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Windows上的Docker叠加网络具有与Linux相同的功能对等性。这意味着我们在本章中使用的示例将在Linux和Windows上都可以工作。
- en: 'We’ll split this chapter into the usual three parts:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把本章分为通常的三个部分：
- en: The TLDR
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简而言之
- en: The deep dive
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入挖掘
- en: The commands
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令
- en: Let’s do some networking magic!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一些网络魔术！
- en: Docker overlay networking - The TLDR
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker叠加网络 - 简而言之
- en: In the real world, it’s vital that containers can communicate with each other
    reliably and securely, even when they’re on different hosts that are on different
    networks. This is where overlay networking comes in to play. It allows you to
    create a flat, secure, layer-2 network, spanning multiple hosts. Containers connect
    to this and can communicate directly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，容器之间能够可靠且安全地通信是至关重要的，即使它们位于不同的网络上的不同主机上。这就是叠加网络发挥作用的地方。它允许您创建一个扁平、安全的第二层网络，跨越多个主机。容器连接到这个网络并可以直接通信。
- en: Docker offers native overlay networking that is simple to configure and secure
    by default.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Docker提供了本机叠加网络，简单配置且默认安全。
- en: Behind the scenes, it’s built on top of `libnetwork` and drivers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，它是建立在`libnetwork`和驱动程序之上的。
- en: '`libnetwork`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`libnetwork`'
- en: '`drivers`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`驱动程序`'
- en: Libnetwork is the canonical implementation of the Container Network Model (CNM),
    and drivers are pluggable components that implement different networking technologies
    and topologies. Docker offers native drivers such as the `overlay` driver, and
    third parties also offer drivers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Libnetwork是容器网络模型（CNM）的规范实现，驱动程序是实现不同网络技术和拓扑的可插拔组件。Docker提供了本机驱动程序，如`overlay`驱动程序，第三方也提供了驱动程序。
- en: Docker overlay networking - The deep dive
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker叠加网络 - 深入挖掘
- en: In March 2015, Docker, Inc. acquired a container networking startup called *Socket
    Plane*. Two of the reasons behind the acquisition were to bring *real networking*
    to Docker, and to make container networking simple enough that even developers
    could do it :-P
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年3月，Docker公司收购了一个名为*Socket Plane*的容器网络初创公司。收购背后的两个原因是为Docker带来*真正的网络*，并使容器网络简单到连开发人员都能做到
    :-P
- en: They’ve made immense progress on both fronts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在这两个方面取得了巨大的进展。
- en: However, hiding behind the simple networking commands are a lot of moving parts.
    The kind of stuff you need understand before doing production deployments and
    attempting to troubleshoot issues!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在简单的网络命令背后隐藏着许多复杂的部分。这些是你在进行生产部署和尝试解决问题之前需要了解的内容！
- en: 'The rest of this chapter will be broken into two parts:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将分为两个部分：
- en: 'Part 1: we’ll build and test a Docker overlay network in Swarm mode'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1部分：我们将在Swarm模式下构建和测试Docker叠加网络
- en: 'Part 2: We’ll explain the theory behind how it works.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2部分：我们将解释它是如何工作的理论。
- en: Build and test a Docker overlay network in Swarm mode
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Swarm模式下构建和测试Docker叠加网络
- en: For the following examples, we’ll use two Docker hosts, on two separate Layer
    2 networks, connected by a router. See Figure 12.1, and note the different networks
    that each node is on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下示例，我们将使用两个Docker主机，位于两个单独的第二层网络上，通过路由器连接。请参见图12.1，并注意每个节点所在的不同网络。
- en: '![Figure 12.1](images/figure12-1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1](images/figure12-1.png)'
- en: Figure 12.1
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1
- en: You can follow along with either Linux or Windows Docker hosts. Linux should
    have at least a 4.4 Linux kernel (newer is always better) and Windows should be
    Windows Server 2016 with the latest hotfixes installed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Linux或Windows Docker主机上跟着做。Linux应至少具有4.4 Linux内核（更新的总是更好），Windows应为安装了最新热修复的Windows
    Server 2016。
- en: Build a Swarm
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 构建Swarm
- en: The first thing we’ll do is configure the two hosts into a two-node Swarm. We’ll
    run the `docker swarm init` command on **node1** to make it a *manager*, and then
    we’ll run the `docker swarm join` command on **node2** to make it a *worker*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是将两个主机配置为两节点Swarm。我们将在**node1**上运行`docker swarm init`命令，使其成为*管理者*，然后在**node2**上运行`docker
    swarm join`命令，使其成为*工作节点*。
- en: '**Warning:** If you are following along in your own lab, you’ll need to swap
    the IP addresses, container IDs, tokens etc. with the correct values for your
    environment.'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**警告：**如果您在自己的实验室中跟着做，您需要将IP地址、容器ID、令牌等与您环境中的正确值进行交换。'
- en: Run the following command on **node1**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在**node1**上运行以下命令。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Run the next command on **node2**. For this to work on Windows Server, you
    may need to modify your Windows firewall rules to allow ports `2377/tcp`, `7946/tcp`
    and `7946/udp`.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在**node2**上运行下一个命令。在Windows Server上，您可能需要修改Windows防火墙规则以允许端口`2377/tcp`、`7946/tcp`和`7946/udp`。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`We now have a two-node Swarm with **node1** as a manager and **node2** as
    a worker.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个两节点的Swarm，**node1**作为管理者，**node2**作为工作节点。
- en: Create a new overlay network
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建新的覆盖网络
- en: Now let’s create a new *overlay network* called **uber-net**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个名为**uber-net**的新的*覆盖网络*。
- en: Run the following command from **node1** (manager). For this to work on Windows
    you may need to add a rule for port `4789/udp` on your Windows Docker nodes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在**node1**（管理者）上运行以下命令。在Windows上，您可能需要为Windows Docker节点上的端口`4789/udp`添加规则才能使其工作。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`That’s it! You’ve just created a brand-new overlay network that is available
    to all hosts in the Swarm and has its control plane encrypted with TLS! If you
    want to encrypt the data plane, you just add the `-o encrypted` flag to the command.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！您刚刚创建了一个全新的覆盖网络，该网络对Swarm中的所有主机都可用，并且其控制平面已使用TLS加密！如果您想加密数据平面，只需在命令中添加`-o
    encrypted`标志。
- en: You can list all networks on each node with the `docker network ls` command.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`docker network ls`命令在每个节点上列出所有网络。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`The output will look more like this on a Windows server:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows服务器上，输出将更像这样：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`The network we created is at the bottom of the list called **uber-net**. The
    other networks were automatically created when Docker was installed and when we
    initialized the Swarm.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的网络位于名为**uber-net**的列表底部。其他网络是在安装Docker和初始化Swarm时自动创建的。
- en: If you run the `docker network ls` command on **node2**, you’ll notice that
    it can’t see the **uber-net** network. This is because new overlay networks are
    only made available to worker nodes that are running containers attached to them.
    This lazy approach improves network scalability by reducing the amount of network
    gossip.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在**node2**上运行`docker network ls`命令，您会注意到它看不到**uber-net**网络。这是因为新的覆盖网络只对运行附加到它们的容器的工作节点可用。这种懒惰的方法通过减少网络八卦的数量来提高网络可扩展性。
- en: Attach a service to the overlay network
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将服务附加到覆盖网络
- en: Now that we have an overlay network, let’s create a new *Docker service* and
    attach it to it. We’ll create the service with two replicas (containers) so that
    one runs on **node1** and the other runs on **node2**. This will automatically
    extend the **uber-net** overlay to **node2**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们有了一个覆盖网络，让我们创建一个新的*Docker服务*并将其附加到它。我们将创建一个具有两个副本（容器）的服务，以便一个在**node1**上运行，另一个在**node2**上运行。这将自动将**uber-net**覆盖扩展到**node2**。
- en: Run the following commands from **node1**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从**node1**运行以下命令。
- en: 'Linux example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Linux示例：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Windows example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Windows示例：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`> **Note:** The Windows example uses the backtick character to split parameters
    over multiple lines to make the command more readable. The backtick is how PowerShell
    escapes line feeds.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`> **注意：** Windows示例使用反引号字符来分割参数，使命令更易读。反引号是PowerShell转义换行的方式。'
- en: The command creates a new service called **test**, attaches it to the **uber-net**
    overlay network, and creates two replicas (containers) based on the image provided.
    In both examples, we issued a sleep command to the containers to keep them running
    and stop them from exiting.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令创建了一个名为**test**的新服务，将其附加到**uber-net**叠加网络，并基于提供的镜像创建了两个副本（容器）。在这两个示例中，我们向容器发出了一个休眠命令，以使它们保持运行状态并阻止它们退出。
- en: Because we’re running two replicas (containers), and the Swarm has two nodes,
    one replica will be scheduled on each node.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们运行了两个副本（容器），而Swarm有两个节点，一个副本将被调度到每个节点上。
- en: Verify the operation with a `docker service ps` command.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`docker service ps`命令验证操作。
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`When Swarm starts a container on an overlay network, it automatically extends
    that network to the node the container is running on. This means that the **uber-net**
    network is now visible on **node2**.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`当Swarm在叠加网络上启动容器时，它会自动将该网络扩展到容器所在的节点。这意味着**uber-net**网络现在在**node2**上可见。'
- en: Congratulations! You’ve created a new overlay network spanning two nodes on
    separate physical underlay networks. You’ve also attached two containers to it.
    How simple was that!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经创建了一个跨越两个位于不同物理底层网络的节点的新叠加网络。您还将两个容器连接到了它。这是多么简单！
- en: Test the overlay network
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试叠加网络
- en: Now let’s test the overlay network with the ping command.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用ping命令测试叠加网络。
- en: As shown in Figure 12.2, we’ve got two Docker hosts on separate networks, with
    a single overlay plumbed into both. We’ve got one container connected to the overlay
    network on each node. Let’s see if they can ping each other.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如图12.2所示，我们在不同网络上有两个Docker主机，都连接了一个叠加网络。我们在每个节点上都有一个容器连接到叠加网络。让我们看看它们是否可以相互ping通。
- en: '![Figure 12.2](images/figure12-2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2](images/figure12-2.png)'
- en: Figure 12.2
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2
- en: To perform the test, we’ll need the IP address of each container (for the purposes
    of this test, we’re ignoring the fact that containers on the same overlay can
    ping each other by name).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行测试，我们需要每个容器的IP地址（在本次测试中，我们忽略了相同叠加上的容器可以通过名称相互ping通的事实）。
- en: Run a `docker network inspect` to see the **Subnet** assigned to the overlay.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`docker network inspect`来查看分配给叠加网络的**子网**。
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`The output above shows that **uber-net**’s subnet is `10.0.0.0/24`. Note that
    this does not match either of the physical underlay networks (`172.31.1.0/24`
    and `192.168.1.0/24`).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`上面的输出显示**uber-net**的子网为`10.0.0.0/24`。请注意，这与任何一个物理底层网络（`172.31.1.0/24`和`192.168.1.0/24`）都不匹配。'
- en: Run the following two commands on **node1** and **node2**. These will get the
    container’s ID’s and IP addresses. Be sure to use the container ID’s from your
    own lab in the second command.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在**node1**和**node2**上运行以下两个命令。这些命令将获取容器的ID和IP地址。确保在第二个命令中使用您自己实验室中的容器ID。
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Make sure you run these commands on both nodes to get the IP addresses of
    both containers.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`确保您在两个节点上运行这些命令，以获取两个容器的IP地址。'
- en: Figure 12.3 shows the configuration so far. Subnet and IP addresses may be different
    in your lab.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3显示了到目前为止的配置。在您的实验室中，子网和IP地址可能会有所不同。
- en: '![Figure 12.3](images/Figure12-3.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3](images/Figure12-3.png)'
- en: Figure 12.3
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3
- en: As we can see, there is a Layer 2 overlay network spanning both hosts, and each
    container has an IP address on this overlay network. This means that the container
    on **node1** will be able to ping the container on **node2** using its `10.0.0.4`
    address from the overlay network. This works despite the fact that both *nodes*
    are on different Layer 2 underlay networks. Let’s prove it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，有一个跨越两个主机的第2层叠加网络，并且每个容器在这个叠加网络上都有一个IP地址。这意味着**node1**上的容器将能够使用其来自叠加网络的`10.0.0.4`地址来ping
    **node2**上的容器。尽管两个*节点*位于不同的第2层底层网络上，这也能够实现。让我们来证明一下。
- en: Log on to the container on **node1** and ping the remote container.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到**node1**上的容器并ping远程容器。
- en: To do this on the Linux Ubuntu container you will need to install the `ping`
    utility. If you’re following along with the Windows PowerShell example the `ping`
    utility is already installed.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Linux Ubuntu容器上执行此操作，您需要安装`ping`实用程序。如果您正在使用Windows PowerShell示例，`ping`实用程序已经安装。
- en: Remember that the container IDs will be different in your environment.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您的环境中容器的ID将是不同的。
- en: 'Linux example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Linux示例：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`Windows example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Windows示例：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Congratulations. The container on **node1** can ping the container on **node2**
    using the overlay network.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`恭喜。**node1**上的容器可以使用叠加网络ping **node2**上的容器。'
- en: You can also trace the route of the ping command from within the container.
    This will report a single hop, proving that the containers are communicating directly
    over the overlay network — blissfully unaware of any underlay networks that are
    being traversed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在容器内跟踪ping命令的路由。这将报告一个单一的跳跃，证明容器正在通过叠加网络直接通信，对正在穿越的任何底层网络毫不知情。
- en: '**Note:** For the `traceroute` to work on the Linux example, you will need
    to install the `traceroute` package.'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**对于Linux示例中的`traceroute`工作，您需要安装`traceroute`软件包。'
- en: 'Linux example:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Linux示例：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Windows example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Windows示例：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`So far, we’ve created an overlay network with a single command. We then added
    containers to it. The containers were scheduled on two hosts that were on two
    different Layer 2 underlay networks. Once we worked out the container’s IP addresses,
    we proved that they could talk directly over the overlay network.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经用一个命令创建了一个叠加网络。然后我们将容器添加到其中。这些容器被安排在两个位于两个不同第2层底层网络上的主机上。一旦我们确定了容器的IP地址，我们证明它们可以直接通过叠加网络进行通信。
- en: The theory of how it all works
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的理论
- en: Now that we’ve seen how to build and use a container overlay network, let’s
    find out how it’s all put together behind the scenes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到如何构建和使用容器叠加网络，让我们找出它在幕后是如何组合在一起的。
- en: Some of the detail in this section will be specific to Linux. However, the same
    overall principles apply to Windows.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的一些细节将是特定于Linux的。然而，相同的总体原则也适用于Windows。
- en: VXLAN primer
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: VXLAN入门
- en: First and foremost, Docker overlay networking uses VXLAN tunnels to create virtual
    Layer 2 overlay networks. So, before we go any further, let’s do a quick VXLAN
    primer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Docker叠加网络使用VXLAN隧道来创建虚拟的第2层叠加网络。因此，在我们进一步之前，让我们快速了解一下VXLAN。
- en: At the highest level, VXLANs let you create a virtual Layer 2 network on top
    of an existing Layer 3 infrastructure. The example we used earlier created a new
    10.0.0.0/24 Layer 2 network on top of a Layer 3 IP network comprising two Layer
    2 networks — 172.31.1.0/24 and 192.168.1.0/24\. This is shown in Figure 12.4.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，VXLAN允许您在现有的第3层基础设施上创建一个虚拟的第2层网络。我们之前使用的示例在两个第2层网络 — 172.31.1.0/24和192.168.1.0/24的基础上创建了一个新的10.0.0.0/24第2层网络。如图12.4所示。
- en: '![Figure 12.4](images/figure12-4.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4](images/figure12-4.png)'
- en: Figure 12.4
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4
- en: The beauty of VXLAN is that it’s an encapsulation technology that existing routers
    and network infrastructure just see as regular IP/UDP packets and handle without
    issue.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN的美妙之处在于，它是一种封装技术，现有的路由器和网络基础设施只会将其视为常规的IP/UDP数据包，并且可以处理而无需问题。
- en: To create the virtual Layer 2 overlay network, a VXLAN *tunnel* is created through
    the underlying Layer 3 IP infrastructure. You might hear the term *underlay network*
    used to refer to the underlying Layer 3 infrastructure.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建虚拟二层叠加网络，通过底层三层IP基础设施创建了一个VXLAN*隧道*。您可能会听到术语*底层网络*用于指代底层三层基础设施。
- en: Each end of the VXLAN tunnel is terminated by a VXLAN Tunnel Endpoint (VTEP).
    It’s this VTEP that performs the encapsulation/de-encapsulation and other magic
    required to make all of this work. See Figure 12.5.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN隧道的每一端都由VXLAN隧道端点（VTEP）终止。正是这个VTEP执行了封装/解封装和其他使所有这些工作的魔术。参见图12.5。
- en: '![Figure 12.5](images/figure12-5.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5](images/figure12-5.png)'
- en: Figure 12.5
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5
- en: Walk through our two-container example
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 步骤通过我们的两个容器示例
- en: In the example we built earlier, we had two hosts connected via an IP network.
    Each host ran a single container, and we created a single VXLAN overlay network
    for the containers to connect to.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前构建的示例中，我们有两台主机通过IP网络连接。每台主机运行一个容器，并为容器创建了一个VXLAN叠加网络以进行连接。
- en: To accomplish this, a new *sandbox* (network namespace) was created on each
    host. As mentioned in the previous chapter, a *sandbox* is like a container, but
    instead of running an application, it runs an isolated network stack — one that’s
    sandboxed from the network stack of the host itself.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，在每台主机上创建了一个新的*沙盒*（网络命名空间）。正如在上一章中提到的，*沙盒*类似于一个容器，但它不是运行应用程序，而是运行一个与主机本身的网络堆栈隔离的网络堆栈。
- en: A virtual switch (a.k.a. virtual bridge) called **Br0** is created inside the
    sandbox. A VTEP is also created with one end plumbed into the **Br0** virtual
    switch, and the other end plumbed into the host network stack (VTEP). The end
    in the host network stack gets an IP address on the underlay network the host
    is connected to, and is bound to a UDP socket on port 4789\. The two VTEPs on
    each host create the overlay via a VXLAN tunnel as seen in Figure 12.6.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在沙盒内创建了一个名为**Br0**的虚拟交换机（又名虚拟桥）。还创建了一个VTEP，其中一端连接到**Br0**虚拟交换机，另一端连接到主机网络堆栈（VTEP）。主机网络堆栈的一端在主机连接的底层网络上获取IP地址，并绑定到端口4789上的UDP套接字。每台主机上的两个VTEP通过VXLAN隧道创建叠加网络，如图12.6所示。
- en: '![Figure 12.6](images/figure12-6.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6](images/figure12-6.png)'
- en: Figure 12.6
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6
- en: This is essentially the VXLAN overlay network created and ready for use.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是创建并准备好供使用的VXLAN叠加网络。
- en: Each container then gets its own virtual Ethernet (`veth`) adapter that is also
    plumbed into the local **Br0** virtual switch. The topology now looks like Figure
    12.7, and it should be getting easier to see how the two containers can communicate
    over the VXLAN overlay network despite their hosts being on two separate networks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每个容器都会获得自己的虚拟以太网（`veth`）适配器，也连接到本地的**Br0**虚拟交换机。拓扑现在看起来像图12.7，应该更容易看出这两个容器如何在VXLAN叠加网络上进行通信，尽管它们的主机位于两个独立的网络上。
- en: '![Figure 12.7](images/figure12-7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图12.7](images/figure12-7.png)'
- en: Figure 12.7
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7
- en: Communication example
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通信示例
- en: Now that we’ve seen the main plumbing elements, let’s see how the two containers
    communicate.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了主要的管道元素，让我们看看这两个容器是如何通信的。
- en: For this example, we’ll call the container on node1 “**C1**” and the container
    on node2 “**C2**”. And let’s assume **C1** wants to ping **C2** like we did in
    the practical example earlier in the chapter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将node1上的容器称为“**C1**”，将node2上的容器称为“**C2**”。假设**C1**想要像我们在本章早些时候的实际示例中那样ping
    **C2**。
- en: '![Figure 12.8](images/figure12-8.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8](images/figure12-8.png)'
- en: Figure 12.8
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8
- en: '**C1** creates the ping requests and sets the destination IP address to be
    the `10.0.0.4` address of **C2**. It sends the traffic over its `veth` interface
    which is connected to the **Br0** virtual switch. The virtual switch doesn’t know
    where to send the packet as it doesn’t have an entry in its MAC address table
    (ARP table) that corresponds to the destination IP address. As a result, it floods
    the packet to all ports. The VTEP interface connected to **Br0** knows how to
    forward the frame, so responds with its own MAC address. This is a *proxy ARP*
    reply and results in the **Br0** switch *learning* how to forward the packet.
    So it updates its ARP table, mapping 10.0.0.4 to the MAC address of the local
    VTEP.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**C1**创建ping请求，并将目标IP地址设置为**C2**的`10.0.0.4`地址。它通过连接到**Br0**虚拟交换机的`veth`接口发送流量。虚拟交换机不知道该如何发送数据包，因为它的MAC地址表（ARP表）中没有对应于目标IP地址的条目。因此，它将数据包洪泛到所有端口。连接到**Br0**的VTEP接口知道如何转发帧，因此用自己的MAC地址进行回复。这是一个*代理ARP*回复，并导致**Br0**交换机*学习*如何转发数据包。因此，它更新了ARP表，将10.0.0.4映射到本地VTEP的MAC地址。'
- en: Now that the **Br0** switch has *learned* how to forward traffic to **C2**,
    all future packets for **C2** will be transmitted directly to the VTEP interface.
    The VTEP interface knows about **C2** because all newly started containers have
    their network details propagated to other nodes in the Swarm using the network’s
    built-in gossip protocol.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在**Br0**交换机已经*学会*了如何将流量转发到**C2**，所有未来发往**C2**的数据包都将直接传输到VTEP接口。VTEP接口知道**C2**，因为所有新启动的容器都使用网络内置的八卦协议将它们的网络细节传播到Swarm中的其他节点。
- en: The switch then sends the packet to the VTEP interface, which encapsulates the
    frames so they can be sent over the underlay transport infrastructure. At a fairly
    high level, this encapsulation includes adding a VXLAN header to the Ethernet
    frame. The VXLAN header contains the VXLAN network ID (VNID) which is used to
    map frames from VLANs to VXLANs and vice versa. Each VLAN gets mapped to VNID
    so that the packet can be de-encapsulated on the receiving end and forwarded to
    the correct VLAN. This obviously maintains network isolation. The encapsulation
    also wraps the frame in a UDP packet with the IP address of the remote VTEP on
    node2 in the *destination IP field*, and the UDP port 4789 socket information.
    This encapsulation allows the data to be sent across the underlying networks without
    the underlying networks having to know anything about VXLAN.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 交换机然后将数据包发送到VTEP接口，VTEP接口封装帧，以便可以通过底层传输基础设施发送。在相当高的层面上，这种封装包括向以太网帧添加VXLAN头。VXLAN头包含VXLAN网络ID（VNID），用于将帧从VLAN映射到VXLAN，反之亦然。每个VLAN都映射到VNID，以便可以在接收端解封数据包并转发到正确的VLAN。这显然保持了网络隔离。封装还将帧包装在具有node2上远程VTEP的IP地址的UDP数据包中的*目标IP字段*，以及UDP端口4789套接字信息。这种封装允许数据在底层网络上发送，而底层网络无需了解VXLAN。
- en: When the packet arrives at node2, the kernel sees that it’s addressed to UDP
    port 4789\. The kernel also knows that it has a VTEP interface bound to this socket.
    As a result, it sends the packet to the VTEP, which reads the VNID, de-encapsulates
    the packet, and sends it on to its own local **Br0** switch on the VLAN that corresponds
    the VNID. From there it is delivered to container C2.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据包到达node2时，内核会看到它的目的地是UDP端口4789。内核还知道它有一个绑定到此套接字的VTEP接口。因此，它将数据包发送到VTEP，VTEP读取VNID，解封数据包，并将其发送到对应VNID的本地**Br0**交换机上的VLAN。然后将其传递给容器C2。
- en: That’s the basics of how VXLAN technology is leveraged by native Docker overlay
    networking.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是VXLAN技术如何被原生Docker覆盖网络利用的基础知识。
- en: We’re only scratching the surface here, but it should be enough for you to be
    able to start the ball rolling with any potential production Docker deployments.
    It should also give you the knowledge required to talk to your networking team
    about the networking aspects of your Docker infrastructure.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里只是浅尝辄止，但这应该足够让您能够开始进行任何潜在的生产 Docker 部署。这也应该让您具备与网络团队讨论 Docker 基础设施的网络方面所需的知识。
- en: One final thing. Docker also supports Layer 3 routing within the same overlay
    network. For example, you can create an overlay network with two subnets, and
    Docker will take care of routing between them. The command to create a network
    like this could be `docker network create --subnet=10.1.1.0/24 --subnet=11.1.1.0/24
    -d overlay prod-net`. This would result in two virtual switches, **Br0** and **Br1**,
    being created inside the *sandbox*, and routing happens by default.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一件事。Docker 还支持同一覆盖网络内的三层路由。例如，您可以创建一个具有两个子网的覆盖网络，Docker 将负责在它们之间进行路由。创建这样一个网络的命令可能是
    `docker network create --subnet=10.1.1.0/24 --subnet=11.1.1.0/24 -d overlay prod-net`。这将导致在
    *沙盒* 内创建两个虚拟交换机 **Br0** 和 **Br1**，并且默认情况下会进行路由。
- en: Docker overlay networking - The commands
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker 覆盖网络 - 命令
- en: '`docker network create` is the command that we use to create a new container
    network. The `-d` flag lets you specify the driver to use, and the most common
    driver is the `overlay` driver. You can also specify *remote* drivers from 3rd
    parties. For overlay networks, the control plane is encrypted by default. Just
    add the `-o encrypted` flag to encrypt the data plane (performance overheads may
    be incurred).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network create` 是我们用来创建新容器网络的命令。`-d` 标志允许您指定要使用的驱动程序，最常见的驱动程序是 `overlay`
    驱动程序。您还可以指定来自第三方的 *远程* 驱动程序。对于覆盖网络，默认情况下控制平面是加密的。只需添加 `-o encrypted` 标志即可加密数据平面（可能会产生性能开销）。'
- en: '`docker network ls` lists all of the container networks visible to a Docker
    host. Docker hosts running in *Swarm mode* only see overlay networks if they are
    hosting containers running on that particular network. This keeps network-related
    gossip to a minimum.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network ls` 列出 Docker 主机可见的所有容器网络。运行在 *Swarm 模式* 中的 Docker 主机只会看到托管在特定网络上运行的容器的覆盖网络。这可以将与网络相关的传闻最小化。'
- en: '`docker network inspect` shows you detailed information about a particular
    container network. This includes *scope*, *driver*, *IPv6*, *subnet configuration*,
    *VXLAN network ID*, and *encryption state*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network inspect` 显示有关特定容器网络的详细信息。这包括 *范围*、*驱动程序*、*IPv6*、*子网配置*、*VXLAN
    网络 ID* 和 *加密状态*。'
- en: '`docker network rm` deletes a network'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network rm` 删除一个网络'
- en: Chapter Summary
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 章节总结
- en: In this chapter, we saw how easy it is to create new Docker overlay networks
    with the `docker network create` command. We then learned how they are put together
    behind the scenes using VXLAN technology.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到使用 `docker network create` 命令创建新的 Docker 覆盖网络是多么容易。然后我们学习了它们如何在幕后使用
    VXLAN 技术组合在一起。
- en: We’ve only scratched the surface of what can be done with Docker overlay networking.[PRE14]`
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是触及了 Docker 覆盖网络可以做的一小部分。[PRE14]`
