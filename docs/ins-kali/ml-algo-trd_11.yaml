- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Random Forests – A Long-Short Strategy for Japanese Stocks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林-日本股票的多头策略
- en: 'In this chapter, we will learn how to use two new classes of machine learning
    models for trading: **decision trees** and **random forests**. We will see how
    decision trees learn rules from data that encode nonlinear relationships between
    the input and the output variables. We will illustrate how to train a decision
    tree and use it for prediction with regression and classification problems, visualize
    and interpret the rules learned by the model, and tune the model''s hyperparameters
    to optimize the bias-variance trade-off and prevent overfitting.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何为交易使用两种新的机器学习模型：**决策树**和**随机森林**。我们将看到决策树如何从数据中学习规则，编码输入和输出变量之间的非线性关系。我们将说明如何训练决策树并将其用于回归和分类问题的预测，可视化和解释模型学习的规则，并调整模型的超参数以优化偏差-方差权衡并防止过拟合。
- en: Decision trees are not only important standalone models but are also frequently
    used as components in other models. In the second part of this chapter, we will
    introduce ensemble models that combine multiple individual models to produce a
    single aggregate prediction with lower prediction-error variance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅是重要的独立模型，而且经常被用作其他模型的组成部分。在本章的第二部分中，我们将介绍集成模型，它将多个单独模型组合成一个具有更低预测错误方差的单一聚合预测。
- en: We will illustrate **bootstrap aggregation**, often called *bagging*, as one
    of several methods to randomize the construction of individual models and reduce
    the correlation of the prediction errors made by an ensemble's components. We
    will illustrate how bagging effectively reduces the variance and learn how to
    configure, train, and tune random forests. We will see how random forests, as
    an ensemble of a (potentially large) number of decision trees, can dramatically
    reduce prediction errors, at the expense of some loss in interpretation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将说明**自助聚合**，通常称为*装袋*，作为多种方法之一，用于随机化单个模型的构建并减少集成组件所做的预测错误的相关性。我们将说明装袋如何有效地减少方差，并学习如何配置、训练和调整随机森林。我们将看到随机森林作为（可能大量的）决策树集成，可以显著减少预测错误，但会牺牲一些解释性。
- en: Then, we will proceed and build a long-short trading strategy that uses a random
    forest to generate profitable signals for large-cap Japanese equities over the
    last 3 years. We will source and prepare the stock price data, tune the hyperparameters
    of a random forest model, and backtest trading rules based on the model's signals.
    The resulting long-short strategy uses machine learning rather than the cointegration
    relationship we saw in *Chapter 9*, *Time-Series Models for Volatility Forecasts
    and Statistical Arbitrage*, to identify and trade baskets of securities whose
    prices will likely move in opposite directions over a given investment horizon.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将继续构建一个长短交易策略，该策略使用随机森林为过去3年的日本大型股票生成盈利信号。我们将获取并准备股价数据，调整随机森林模型的超参数，并根据模型的信号进行交易规则的回测。由此产生的多头策略使用机器学习而不是我们在第9章《用于波动率预测和统计套利的时间序列模型》中看到的协整关系，来识别和交易价格在给定投资期限内可能朝相反方向移动的证券篮子。
- en: 'In short, after reading this chapter, you will be able to:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在阅读本章后，您将能够：
- en: Use decision trees for regression and classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行回归和分类
- en: Gain insights from decision trees and visualize the rules learned from the data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从决策树中获得见解，并可视化从数据中学习的规则
- en: Understand why ensemble models tend to deliver superior results
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解为什么集成模型往往能够产生更优越的结果
- en: Use bootstrap aggregation to address the overfitting challenges of decision
    trees
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自助聚合来解决决策树的过拟合挑战
- en: Train, tune, and interpret random forests
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、调整和解释随机森林
- en: Employ a random forest to design and evaluate a profitable trading strategy
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林设计和评估盈利交易策略
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: Decision trees – learning rules from data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树-从数据中学习规则
- en: A decision tree is a machine learning algorithm that predicts the value of a
    target variable based on **decision rules learned from data**. The algorithm can
    be applied to both regression and classification problems by changing the objective
    function that governs how the tree learns the rules.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种机器学习算法，根据从数据中学习的决策规则预测目标变量的值。该算法可以通过改变控制树学习规则的目标函数来应用于回归和分类问题。
- en: We will discuss how decision trees use rules to make predictions, how to train
    them to predict (continuous) returns as well as (categorical) directions of price
    movements, and how to interpret, visualize, and tune them effectively. See Rokach
    and Maimon (2008) and Hastie, Tibshirani, and Friedman (2009) for additional details
    and further background information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论决策树如何使用规则进行预测，如何训练它们来预测（连续）收益以及（分类）价格走势的方向，以及如何有效地解释、可视化和调整它们。有关更多细节和背景信息，请参阅Rokach和Maimon（2008）以及Hastie，Tibshirani和Friedman（2009）。
- en: How trees learn and apply decision rules
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树是如何学习和应用决策规则的
- en: The **linear models** we studied in *Chapter 7*, *Linear Models – From Risk
    Factors to Return Forecasts*, and *Chapter 9*, *Time-Series Models for Volatility
    Forecasts and Statistical Arbitrage*, learn a set of parameters to predict the
    outcome using a linear combination of the input variables, possibly after being
    transformed by an S-shaped link function, in the case of logistic regression.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第7章《线性模型-从风险因素到收益预测》和第9章《用于波动率预测和统计套利的时间序列模型》中学习的**线性模型**通过学习一组参数来预测结果，可能是通过S形链接函数进行转换后的输入变量的线性组合。
- en: 'Decision trees take a different approach: they learn and sequentially apply
    a set of rules that split data points into subsets and then make one prediction
    for each subset. The predictions are based on the outcome values for the subset
    of training samples that result from the application of a given sequence of rules.
    **Classification trees** predict a probability estimated from the relative class
    frequencies or the value of the majority class directly, whereas **regression
    trees** compute prediction from the mean of the outcome values for the available
    data points.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树采取了一种不同的方法：它们学习并依次应用一组规则，将数据点分割成子集，然后为每个子集做出预测。这些预测是基于应用给定规则序列后产生的训练样本子集的结果值。**分类树**预测的是从相对类频率或直接从多数类的值估计出的概率，而**回归树**则计算可用数据点的结果值的平均值来进行预测。
- en: 'Each of these rules relies on one particular feature and uses a threshold to
    split the samples into two groups, with values either below or above the threshold
    for this feature. A **binary tree** naturally represents the logic of the model:
    the root is the starting point for all samples, nodes represent the application
    of the decision rules, and the data moves along the edges as it is split into
    smaller subsets until it arrives at a leaf node, where the model makes a prediction.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规则都依赖于一个特定的特征，并使用阈值将样本分成两组，其值要么低于要么高于该特征的阈值。**二叉树**自然地表示了模型的逻辑：根是所有样本的起点，节点表示决策规则的应用，数据沿着边移动，直到它被分割成更小的子集，最终到达叶节点，模型在那里做出预测。
- en: For a linear model, the parameter values allow an interpretation of the impact
    of the input variables on the output and the model's prediction. In contrast,
    for a decision tree, the various possible paths from the root to the leaves determine
    how the features and their values lead to specific decisions by the model. As
    a consequence, decision trees are **capable of capturing interdependence** among
    features that linear models cannot capture "out of the box."
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着递归分裂向树添加新节点，训练样本的数量继续减少。如果规则均匀地分割样本，导致树的每个节点具有相等数量的子节点，那么在第*n*级将会有2^n个节点，每个节点包含总观察数的相应部分。实际上，这是不太可能的，因此沿着某些分支的样本数量可能会迅速减少，并且树倾向于沿着不同路径增长到不同深度的水平。
- en: 'The following diagram highlights how the model learns a rule. During training,
    the algorithm scans the features and, for each feature, seeks to find a cutoff
    that splits the data to minimize the loss that results from predictions made.
    It does so using the subsets that would result from the split, weighted by the
    number of samples in each subset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：决策树如何从数据中学习规则
- en: '![](img/B15439_11_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: 对于线性模型，参数值允许解释输入变量对输出和模型预测的影响。相比之下，对于决策树，从根到叶子的各种可能路径决定了特征及其值如何导致模型做出具体决策。因此，决策树能够捕捉线性模型无法“开箱即用”捕捉的特征之间的相互依赖关系。
- en: 'Figure 11.1: How a decision tree learns rules from data'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B15439_11_01.png)'
- en: To build an entire tree during training, the learning algorithm repeats this
    process of dividing the feature space, that is, the set of possible values for
    the *p* input variables, *X*[1], *X*[2], ..., *X*[p], into mutually-exclusive
    and collectively exhaustive regions, each represented by a leaf node. Unfortunately,
    the algorithm will not be able to evaluate every possible partition of the feature
    space, given the explosive number of possible combinations of sequences of features
    and thresholds. Tree-based learning takes a **top-down**, **greedy approach**,
    known as **recursive binary splitting**, to overcome this computational limitation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间构建整个树，学习算法重复这个过程，将特征空间（即*p*个输入变量*X*[1]、*X*[2]、...、*X*[p]的可能值集合）划分为互斥且完全穷尽的区域，每个区域由一个叶节点表示。不幸的是，由于特征空间的可能组合序列和阈值的爆炸性数量级，算法将无法评估特征空间的每种可能分区。基于树的学习采用了一种称为**自上而下**、**贪婪**的方法，即**递归二元分裂**，以克服这种计算上的限制。
- en: This process is recursive because it uses subsets of data resulting from prior
    splits. It is top-down because it begins at the root node of the tree, where all
    observations still belong to a single region, and then successively creates two
    new branches of the tree by adding one more split to the predictor space. It is
    greedy because the algorithm picks the best rule in the form of a feature-threshold
    combination based on the immediate impact on the objective function, rather than
    looking ahead and evaluating the loss several steps ahead. We will return to the
    splitting logic in the more specific context of regression and classification
    trees because this represents the major difference between them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是递归的，因为它使用了先前分裂产生的数据子集。它是自上而下的，因为它从树的根节点开始，所有观察结果仍然属于单个区域，然后通过向预测空间添加一个分裂来逐步创建树的两个新分支。它是贪婪的，因为算法选择了最佳规则，即基于对目标函数的直接影响的特征-阈值组合，而不是向前看并评估几步后的损失。我们将在更具体的回归和分类树的上下文中返回到分裂逻辑，因为这代表了它们之间的主要区别。
- en: The number of training samples continues to shrink as recursive splits add new nodes
    to the tree. If rules split the samples evenly, resulting in a perfectly balanced
    tree with an equal number of children for every node, then there would be 2^n nodes
    at level *n*, each containing a corresponding fraction of the total number of
    observations. In practice, this is unlikely, so the number of samples along some
    branches may diminish rapidly, and trees tend to grow to different levels of depth
    along different paths.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表突出了模型如何从数据中学习规则。在训练期间，算法扫描特征，并针对每个特征寻找一个分割数据的截断点，以最小化由预测产生的损失。它使用将由分割产生的子集，加权每个子集中的样本数来实现这一点：
- en: Recursive splitting would continue until each leaf node contains only a single
    sample and the training error has been reduced to zero. We will introduce several
    methods to limit splits and prevent this natural tendency of decision trees to
    produce extreme overfitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 递归分割将持续进行，直到每个叶节点只包含单个样本，并且训练误差已经降低到零。我们将介绍几种方法来限制分割并防止决策树产生极端过拟合的自然倾向。
- en: To arrive at a **prediction** for a new observation, the model uses the rules
    that it inferred during training to decide which leaf node the data point should
    be assigned to, and then uses the mean (for regression) or the mode (for classification)
    of the training observations in the corresponding region of the feature space.
    A smaller number of training samples in a given region of the feature space, that
    is, in a given leaf node, reduces the confidence in the prediction and may reflect
    overfitting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得出对新观察结果的**预测**，模型使用在训练期间推断出的规则来决定数据点应分配到哪个叶节点，然后使用该特征空间相应区域的训练观察结果的平均值（用于回归）或众数（用于分类）。在特征空间的给定区域，也就是给定叶节点中的训练样本数量较少，会降低对预测的信心，并可能反映出过拟合。
- en: Decision trees in practice
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际中的决策树
- en: In this section, we will illustrate how to use tree-based models to gain insight
    and make predictions. To demonstrate regression trees, we predict returns, and
    for the classification case, we return to the example of positive and negative
    asset price moves. The code examples for this section are in the notebook `decision_trees`,
    unless stated otherwise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将说明如何使用基于树的模型来获得见解和进行预测。为了演示回归树，我们预测回报，对于分类情况，我们回到正面和负面资产价格变动的例子。本节的代码示例在笔记本`decision_trees`中，除非另有说明。
- en: The data – monthly stock returns and features
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据-月度股票回报和特征
- en: 'We will select a subset of the Quandl US equity dataset covering the period
    2006-2017 and follow a process similar to our first feature engineering example
    in *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*.
    We will compute monthly returns and 25 (hopefully) predictive features for the
    500 most-traded stocks based on the 5-year moving average of their dollar volume,
    yielding 56,756 observations. The features include:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择Quandl美国股票数据集的子集，涵盖2006年至2017年的时间段，并按照第4章“金融特征工程-如何研究Alpha因子”中的第一个特征工程示例进行类似的处理。我们将计算月度回报和500支交易量最大的股票的25个（希望）预测特征，这些特征是基于它们的美元成交量的5年移动平均值得出的，共有56,756个观察结果。这些特征包括：
- en: '**Historical returns** for the past 1, 3, 6, and 12 months.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去1、3、6和12个月的**历史回报**。
- en: '**Momentum indicators** that relate the most recent 1- or 3-month returns to
    those for longer horizons.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动量指标**将最近1个或3个月的回报与较长期的回报进行比较。'
- en: '**Technical indicators** designed to capture volatility like the (normalized)
    average true range (NATR and ATR) and momentum like the **relative strength index**
    (**RSI**).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术指标旨在捕捉波动性，如（标准化的）平均真实范围（NATR和ATR）和相对强弱指数（RSI）等动量指标。
- en: '**Factor loadings** for the five Fama-French factors based on rolling OLS regressions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于滚动OLS回归的Fama-French五因子的**因子载荷**。
- en: '**Categorical variables** for year and month, as well as sector.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**年份和月份**的分类变量，以及行业。 '
- en: '*Figure 11.2* displays the mutual information between these features and the
    monthly returns we use for regression (left panel) and their binarized classification
    counterpart, which represents positive or negative price moves for the same period.
    It shows that, on a univariate basis, there appear to be substantial differences
    in the signal content regarding both outcomes across the features.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.2*显示了这些特征与我们用于回归的月度回报之间的互信息（左侧面板），以及它们的二值化分类对应物，代表了同一时期的正面或负面价格变动。它显示，从单变量的角度来看，各个特征在两种结果方面的信号内容似乎存在实质性差异。'
- en: 'More details can be found in the `data_prep` notebook in the GitHub repository
    for this chapter. The decision tree models in this chapter are not equipped to
    handle missing or categorical variables, so we will drop the former and apply
    dummy encoding (see *Chapter 4*, *Financial Feature Engineering – How to Research
    Alpha Factors* and *Chapter 6*, *The Machine Learning Process*) to the categorical
    sector variable:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节可以在本章的GitHub存储库中的`data_prep`笔记本中找到。本章的决策树模型无法处理缺失或分类变量，因此我们将放弃前者，并对分类的行业变量应用虚拟编码（参见第4章“金融特征工程-如何研究Alpha因子”和第6章“机器学习过程”）：
- en: '![](img/B15439_11_02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_02.png)'
- en: 'Figure 11.2: Mutual information for features and returns or price move direction'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：特征与回报或价格变动方向的互信息
- en: Building a regression tree with time-series data
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用时间序列数据构建回归树
- en: Regression trees make predictions based on the mean outcome value for the training
    samples assigned to a given node, and typically rely on the mean-squared error
    to select optimal rules during recursive binary splitting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树根据分配给给定节点的训练样本的平均结果值进行预测，并通常依赖于均方误差来选择递归二元分割期间的最佳规则。
- en: Given a training set, the algorithm iterates over the *p* predictors, *X*[1],
    *X*[2], ..., *X*[p], and *n* possible cutpoints, *s*[1], *s*[2], ..., *s*[n],
    to find an optimal combination. The optimal rule splits the feature space into
    two regions, {*X*|*X*[i] < *s*[j]} and {*X*|*X*[i] > *s*[j]}, with values for
    the *X*[i] feature either below or above the *s*[j] threshold, so that predictions
    based on the training subsets maximize the reduction of the squared residuals
    relative to the current node.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练集，算法在*p*个预测变量*X*[1]，*X*[2]，...，*X*[p]和*n*个可能的切点*s*[1]，*s*[2]，...，*s*[n]上进行迭代，以找到最佳组合。最佳规则将特征空间分割为两个区域，{*X*|*X*[i]
    < *s*[j]}和{*X*|*X*[i] > *s*[j]}，其中特征*X*[i]的值要么低于要么高于阈值*s*[j]，以便基于训练子集的预测最大化相对于当前节点的平方残差的减少。
- en: 'Let''s start with a simplified example to facilitate visualization and also
    demonstrate how we can use time-series data with a decision tree. We will only
    use 2 months of lagged returns to predict the following month, in the vein of
    an AR(2) model from the previous chapter:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简化的例子开始，以便进行可视化，并演示如何使用时间序列数据与决策树。我们将只使用2个月的滞后回报来预测接下来的一个月，类似于上一章的AR(2)模型：
- en: '![](img/B15439_11_001.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_001.png)'
- en: 'Using scikit-learn, configuring and training a regression tree is very straightforward:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn，配置和训练回归树非常简单：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The OLS summary and a visualization of the first two levels of the decision
    tree reveal the striking differences between the models (see *Figure 11.3*). The
    OLS model provides three parameters for the intercepts and the two features in
    line with the linear assumption this model makes about the function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: OLS摘要和决策树的前两个层级的可视化显示了模型之间的显著差异（见*图11.3*）。OLS模型为截距和两个特征提供了三个参数，符合该模型对函数的线性假设。
- en: 'In contrast, the regression tree chart displays, for each node of the first
    two levels, the feature and threshold used to split the data (note that features
    can be used repeatedly), as well as the current value of the **mean-squared error**
    (**MSE**), the number of samples, and the predicted value based on these training
    samples. Also, note that training the decision tree takes 58 milliseconds compared
    to 66 microseconds for the linear regression. While both models run fast with
    only two features, the difference is a factor of 1,000:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，回归树图表显示了前两个层级的每个节点使用的特征和阈值来分割数据（注意特征可以重复使用），以及当前**均方误差**（**MSE**）的值，样本数量和基于这些训练样本的预测值。还要注意，训练决策树需要58毫秒，而线性回归需要66微秒。虽然两种模型在只有两个特征时运行速度很快，但差异是1000倍：
- en: '![](img/B15439_11_03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_03.png)'
- en: 'Figure 11.3: OLS results and regression tree'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：OLS结果和回归树
- en: The tree chart also highlights the uneven distribution of samples across the
    nodes as the numbers vary between 545 and 55,000 samples after the first splits.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图还突出显示了样本在节点之间的不均匀分布，因为第一次分割后样本数量在545和55,000之间变化。
- en: 'To further illustrate the different assumptions about the functional form of
    the relationships between the input variables and the output, we can visualize
    the current return predictions as a function of the feature space, that is, as
    a function of the range of values for the lagged returns. The following image
    shows the current monthly return as a function of returns one and two periods
    ago for linear regression (left panel) and the regression tree:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明关于输入变量和输出之间关系的不同假设，我们可以将当前的回报预测可视化为特征空间的函数，即滞后回报的值范围的函数。以下图片显示了线性回归（左侧面板）和回归树的前两个周期的回报的函数：
- en: '![](img/B15439_11_04.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_04.png)'
- en: 'Figure 11.4: Decision surfaces for linear regression and the regression tree'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：线性回归和回归树的决策面
- en: The linear regression model result on the left-hand side underlines the linearity
    of the relationship between lagged and current returns, whereas the regression
    tree chart on the right illustrates the nonlinear relationship encoded in the
    recursive partitioning of the feature space.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的线性回归模型结果强调了滞后和当前回报之间关系的线性，而右侧的回归树图表说明了特征空间的递归分区中编码的非线性关系。
- en: Building a classification tree
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建分类树
- en: A classification tree works just like the regression version, except that the
    categorical nature of the outcome requires a different approach to making predictions
    and measuring the loss. While a regression tree predicts the response for an observation
    assigned to a leaf node using the mean outcome of the associated training samples,
    a classification tree uses the mode, that is, the most common class among the
    training samples in the relevant region. A classification tree can also generate
    probabilistic predictions based on relative class frequencies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树的工作方式与回归版本相同，只是结果的分类性质需要不同的方法来进行预测和测量损失。回归树预测分配给叶节点的观测的响应，使用相关训练样本的平均结果，而分类树使用模式，即相关区域中训练样本中最常见的类。分类树还可以基于相对类频率生成概率预测。
- en: How to optimize for node purity
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何优化节点纯度
- en: When growing a classification tree, we also use recursive binary splitting,
    but instead of evaluating the quality of a decision rule using the reduction of
    the mean-squared error, we can use the **classification error rate**, which is
    simply the fraction of the training samples in a given (leaf) node that do not
    belong to the most common class.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建分类树时，我们也使用递归二元分割，但是不是使用减少均方误差来评估决策规则的质量，而是可以使用**分类错误率**，它简单地是给定（叶）节点中训练样本的分数，不属于最常见类的分数。
- en: However, the alternative measures, either **Gini impurity** or **cross-entropy**,
    are preferred because they are more sensitive to node purity than the classification
    error rate, as you can see in *Figure 11.5*. **Node purity** refers to the extent
    of the preponderance of a single class in a node. A node that only contains samples
    with outcomes belonging to a single class is pure and implies successful classification
    for this particular region of the feature space.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更倾向于使用**基尼不纯度**或**交叉熵**等替代度量，因为它们对节点纯度的敏感性比分类错误率更高，如*图11.5*所示。**节点纯度**指的是节点中单一类别的优势程度。只包含属于单一类别结果的样本的节点是纯的，并且意味着对特征空间的这个特定区域进行了成功分类。
- en: 'Let''s see how to compute these measures for a classification outcome with
    *K* categories 0,1,…, *K*-1 (with *K*=2, in the binary case). For a given node
    *m*, let *p*[mk] be the proportion of samples from the *k*^(th) class:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何计算具有*K*类别0,1,…,*K*-1（在二进制情况下*K*=2）的分类结果的这些度量。对于给定节点*m*，让*p*[mk]是来自第*k*类的样本的比例：
- en: '![](img/B15439_11_002.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_002.png)'
- en: 'The following plot shows that both the Gini impurity and cross-entropy measures
    are maximized over the [0, 1] interval when the class proportions are even, or
    0.5 in the binary case. Both measures decline when the class proportions approach
    zero or one and the child nodes tend toward purity as a result of a split. At
    the same time, they imply a higher penalty for node impurity than the classification
    error rate:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了当类别比例均匀时，基尼不纯度和交叉熵测量在[0, 1]区间内达到最大值，或者在二元情况下为0.5。当类别比例接近零或一时，这两个测量都会下降，并且由于分割的结果，子节点趋向于纯度。与此同时，它们对节点不纯度的惩罚要高于分类错误率：
- en: '![](img/B15439_11_05.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_05.png)'
- en: 'Figure 11.5: Classification loss functions'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：分类损失函数
- en: Note that cross-entropy takes almost 20 times as long to compute as the Gini
    measure (see the notebook for details).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与基尼测量相比，交叉熵的计算时间几乎要长20倍（有关详细信息，请参阅笔记本）。
- en: How to train a classification tree
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何训练分类树
- en: We will now train, visualize, and evaluate a classification tree with up to
    five consecutive splits using 80 percent of the samples for training to predict
    the remaining 20 percent. We will take a shortcut here to simplify the illustration
    and use the built-in `train_test_split`, which does not protect against lookahead
    bias, as the custom `MultipleTimeSeriesCV` iterator we introduced in *Chapter
    6*, *The Machine Learning Process* and will use later in this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用80%的样本进行训练，可视化和评估一个具有最多五个连续分割的分类树，以预测剩余的20%。为了简化说明，我们将采用一种捷径，并使用内置的`train_test_split`，它不会保护免受前瞻性偏见，就像我们在*第6章*，*机器学习过程*中介绍的自定义`MultipleTimeSeriesCV`迭代器，并且稍后在本章中使用。
- en: 'The tree configuration implies up to 2⁵=32 leaf nodes that, on average, in
    the balanced case, would contain over 1,400 of the training samples. Take a look
    at the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 树的配置意味着最多有2⁵=32个叶节点，在平衡的情况下，平均会包含超过1400个训练样本。看一下下面的代码：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output after training the model displays all the `DecisionTreeClassifier`
    parameters. We will address these in more detail in the *Hyperparameter tuning*
    section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后的输出显示所有`DecisionTreeClassifier`参数。我们将在*超参数调整*部分更详细地讨论这些参数。
- en: Visualizing a decision tree
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化决策树
- en: 'You can visualize the tree using the Graphviz library (see GitHub for installation
    instructions) because scikit-learn can output a description of the tree using
    the DOT language used by that library. You can configure the output to include
    feature and class labels and limit the number of levels to keep the chart readable,
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Graphviz库可视化树（请参阅GitHub获取安装说明），因为scikit-learn可以输出该库使用的DOT语言描述树。您可以配置输出以包括特征和类标签，并限制级别的数量以保持图表的可读性，如下所示：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following diagram shows how the model uses different features and indicates
    the split rules for both continuous and categorical (dummy) variables. Under the
    label value for each node, the chart shows the number of samples from each class
    and, under the label class, the most common class (there were more up months during
    the sample period):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了模型如何使用不同特征，并指示连续和分类（虚拟）变量的分割规则。在每个节点的标签值下，图表显示了每个类别的样本数量，在类别标签下显示了最常见的类别（在样本期间有更多的上升月份）：
- en: '![](img/B15439_11_06.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_06.png)'
- en: 'Figure 11.6: Visualization of a classification tree'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：分类树的可视化
- en: Evaluating decision tree predictions
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估决策树预测
- en: 'To evaluate the predictive accuracy of our first classification tree, we will
    use our test set to generate predicted class probabilities, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们第一个分类树的预测准确性，我们将使用测试集生成预测的类别概率，如下所示：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `.predict_proba()` method produces one probability for each class. In the
    binary class, these probabilities are complementary and sum to 1, so we only need
    the value for the positive class. To evaluate the generalization error, we will
    use the area under the curve based on the receiver-operating characteristic, which
    we introduced in *Chapter 6*, *The Machine Learning Process*. The result indicates
    a significant improvement above and beyond the baseline value of 0.5 for a random
    prediction (but keep in mind that the cross-validation method here does not respect
    the time-series nature of the data):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`.predict_proba()`方法为每个类别生成一个概率。在二元类别中，这些概率是互补的，并且总和为1，因此我们只需要正类的值。为了评估泛化误差，我们将使用基于接收器操作特征的曲线下面积，这是我们在*第6章*，*机器学习过程*中介绍的。结果表明，相对于随机预测的基线值0.5，有显著的改进（但请记住，这里的交叉验证方法并不尊重数据的时间序列性）：'
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Overfitting and regularization
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和正则化
- en: Decision trees have a strong tendency to overfit, especially when a dataset
    has a large number of features relative to the number of samples. As discussed
    in previous chapters, overfitting increases the prediction error because the model
    does not only learn the signal contained in the training data, but also the noise.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有过拟合的倾向，特别是当数据集的特征数量相对于样本数量较多时。正如在之前的章节中讨论的，过拟合会增加预测误差，因为模型不仅学习了训练数据中包含的信号，还学习了噪音。
- en: 'There are multiple ways to **address the risk of overfitting**, including:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法来**解决过拟合的风险**，包括：
- en: '**Dimensionality reduction** (see *Chapter 13*, *Data-Driven Risk Factors and
    Asset Allocation with Unsupervised Learning*) improves the feature-to-sample ratio
    by representing the existing features with fewer, more informative, and less noisy
    features.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**（请参阅*第13章*，*使用无监督学习改进数据驱动风险因素和资产配置*）通过用更少、更具信息性和更少噪声的特征表示现有特征来改善特征与样本的比率。'
- en: '**Ensemble models**, such as random forests, combine multiple trees while randomizing
    the tree construction, as we will see in the second part of this chapter.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成模型**，如随机森林，结合了多个树，同时随机化树的构建，我们将在本章的第二部分中看到。'
- en: Decision trees provide several regularization hyperparameters to limit the growth
    of a tree and the associated complexity. While every split increases the number
    of nodes, it also reduces the number of samples available per node to support
    a prediction. For each additional level, twice the number of samples is needed
    to populate the new nodes with the same sample density.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树提供了几个正则化超参数来限制树的生长和相关复杂性。每次分割增加节点数量，同时减少可用于支持预测的每个节点的样本数量。对于每个额外级别，需要两倍的样本数来以相同的样本密度填充新节点。
- en: '**Tree pruning** is an additional tool to reduce the complexity of a tree.
    It does so by eliminating nodes or entire parts of a tree that add little value
    but increase the model''s variance. Cost-complexity-pruning, for instance, starts
    with a large tree and recursively reduces its size by replacing nodes with leaves,
    essentially running the tree construction in reverse. The various steps produce
    a sequence of trees that can then be compared using cross-validation to select
    the ideal size.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**树修剪** 是减少树复杂性的另一个工具。它通过消除增加模型方差但增加很少价值的节点或整个树的部分来实现。例如，成本复杂度修剪从一个大树开始，通过用叶子替换节点递归地减小其大小，本质上是反向运行树构造。各个步骤产生一系列树，然后可以使用交叉验证来选择理想的大小。'
- en: How to regularize a decision tree
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何正则化决策树
- en: 'The following table lists the key parameters available for this purpose in
    the scikit-learn decision tree implementation. After introducing the most important
    parameters, we will illustrate how to use cross-validation to optimize the hyperparameter
    settings with respect to the bias-variance trade-off and lower prediction errors:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了 scikit-learn 决策树实现中可用于此目的的关键参数。介绍了最重要的参数后，我们将说明如何使用交叉验证来优化超参数设置，以达到偏差-方差权衡和降低预测误差的目的：
- en: '| Parameter | Description | Default | Options |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 | 默认值 | 选项 |'
- en: '| `max_depth` | The maximum number of levels: split the nodes until `max_depth`
    has been reached. All leaves are pure or contain fewer samples than `min_samples_split`.
    | None | int |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `max_depth` | 最大级别数：分割节点直到达到 `max_depth`。所有叶子节点都是纯的，或者包含的样本少于 `min_samples_split`。
    | 无 | int |'
- en: '| `max_features` | Number of features to consider for a split. | None | None:
    all features `int`: # features`float`: fraction`auto`, `sqrt`: sqrt(`n_features`)`log2`:
    log2(`n_features`) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `max_features` | 考虑用于分割的特征数量。 | 无 | 无：所有特征 `int`：# 特征 `float`：分数 `auto`，`sqrt`：sqrt(`n_features`)
    `log2`：log2(`n_features`) |'
- en: '| `max_leaf_nodes` | Split nodes until creating this many leaves. | None |
    None: unlimited `int` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '`max_leaf_nodes` | 分割节点，直到创建这么多叶子节点。 | 无 | 无：无限制 `int` |'
- en: '| `min_impurity_decrease` | Split node if impurity decreases by at least this
    value. | 0 | `float` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `min_impurity_decrease` | 如果不纯度至少减少了这个值，就分割节点。 | 0 | `float` |'
- en: '| `min_samples_leaf` | A split will only be considered if there are at least
    `min_samples_leaf` training samples in each of the left and right branches. |
    1 | `int`;`float` (as a percent of *N*) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `min_samples_leaf` | 只有在左右分支的每个中至少有 `min_samples_leaf` 训练样本时，才会考虑分割。 | 1
    | `int`;`float`（作为 *N* 的百分比） |'
- en: '| `min_samples_split` | The minimum number of samples required to split an
    internal node. | 2 | `int`; `float` (percent of *N*) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `min_samples_split` | 分割内部节点所需的最小样本数。 | 2 | `int`; `float`（*N*的百分比） |'
- en: '| `min_weight_fraction_leaf` | The minimum weighted fraction of the sum total
    of all sample weights needed at a leaf node. Samples have equal weight unless
    `sample_weight` is provided in the fit method. | 0 |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `min_weight_fraction_leaf` | 叶子节点需要的样本权重总和的最小加权分数。除非在拟合方法中提供了 `sample_weight`，否则样本权重相等。
    | 0 |  |'
- en: The `max_depth` parameter imposes a hard limit on the number of consecutive
    splits and represents the most straightforward way to cap the growth of a tree.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth` 参数对连续分割的数量施加了硬性限制，是限制树生长的最直接方式。'
- en: The `min_samples_split` and `min_samples_leaf` parameters are alternative, data-driven
    ways to limit the growth of a tree. Rather than imposing a hard limit on the number
    of consecutive splits, these parameters control the minimum number of samples
    required to further split the data. The latter guarantees a certain number of
    samples per leaf, while the former can create very small leaves if a split results
    in a very uneven distribution. Small parameter values facilitate overfitting,
    while a high number may prevent the tree from learning the signal in the data.
    The default values are often quite low, and you should use cross-validation to
    explore a range of potential values. You can also use a float to indicate a percentage,
    as opposed to an absolute number.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_split` 和 `min_samples_leaf` 参数是限制树生长的替代、数据驱动的方法。这些参数不是对连续分割的数量施加硬性限制，而是控制进一步分割数据所需的最小样本数。后者保证每个叶子节点有一定数量的样本，而前者如果分割导致非常不均匀的分布，可能会创建非常小的叶子。较小的参数值有利于过度拟合，而较高的值可能会阻止树学习数据中的信号。默认值通常相当低，您应该使用交叉验证来探索一系列潜在值。您还可以使用浮点数表示百分比，而不是绝对数值。'
- en: The scikit-learn documentation contains additional details about how to use
    the various parameters for different use cases; see the resources linked on GitHub
    for more information.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 文档包含有关如何在不同用例中使用各种参数的其他详细信息；有关更多信息，请参见 GitHub 上链接的资源。
- en: Decision tree pruning
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树修剪
- en: Recursive binary-splitting will likely produce good predictions on the training
    set but tends to overfit the data and produce poor generalization performance.
    This is because it leads to overly complex trees, which are reflected in a large
    number of leaf nodes, or partitioning of the feature space. Fewer splits and leaf
    nodes imply an overall smaller tree and often lead to better predictive performance,
    as well as interpretability.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 递归二进制分割可能会在训练集上产生良好的预测，但往往会过度拟合数据并产生较差的泛化性能。这是因为它导致了过于复杂的树，这反映在大量叶子节点或特征空间的分区中。较少的分割和叶子节点意味着整体较小的树，通常会导致更好的预测性能，以及可解释性。
- en: One approach to limit the number of leaf nodes is to avoid further splits unless
    they yield significant improvements in the objective metric. The downside of this
    strategy, however, is that sometimes, splits that result in small improvements
    enable more valuable splits later as the composition of the samples keeps changing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 限制叶节点数量的一种方法是除非它们在目标指标上产生显著改进，否则避免进一步分裂。然而，这种策略的缺点是有时产生小幅改进的分裂使得后来的样本组成不断变化的更有价值的分裂。
- en: Tree pruning, in contrast, starts by growing a very large tree before removing
    or pruning nodes to reduce the large tree to a less complex and overfit subtree.
    Cost-complexity-pruning generates a sequence of subtrees by adding a penalty for
    adding leaf nodes to the tree model and a regularization parameter, similar to
    the lasso and ridge linear-regression models, that modulates the impact of the
    penalty. Applied to the large tree, an increasing penalty will automatically produce
    a sequence of subtrees. Cross-validation of the regularization parameter can be
    used to identify the optimal, pruned subtree.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，树修剪是通过先生长一个非常大的树，然后删除或修剪节点，以将大树减少为一个不太复杂且过拟合的子树。成本复杂度修剪通过为向树模型添加叶节点添加惩罚和一个正则化参数，类似于套索和岭线性回归模型，调节惩罚的影响，生成一系列子树。对大树进行交叉验证正则化参数可以用来识别最佳的修剪子树。
- en: This method was introduced in scikit-learn version 0.22; see Esposito et al.
    (1997) for a survey of how various methods work and perform.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法是在scikit-learn版本0.22中引入的；有关各种方法的工作和性能的调查，请参见Esposito等人（1997）。
- en: Hyperparameter tuning
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Decision trees offer an array of hyperparameters to control and tune the training
    result. Cross-validation is the most important tool to obtain an unbiased estimate
    of the generalization error, which, in turn, permits an informed choice among
    the various configuration options. scikit-learn offers several tools to facilitate
    the process of cross-validating numerous parameter settings, namely the `GridSearchCV`
    convenience class, which we will illustrate in the next section. Learning curves
    also allow diagnostics that evaluate potential benefits of collecting additional
    data to reduce the generalization error.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树提供了一系列超参数来控制和调整训练结果。交叉验证是获得无偏估计的泛化误差的最重要工具，从而可以在各种配置选项中做出明智的选择。scikit-learn提供了几种工具来简化交叉验证多个参数设置的过程，即`GridSearchCV`便利类，我们将在下一节中进行说明。学习曲线还允许诊断，评估收集额外数据以减少泛化误差的潜在好处。
- en: Using GridsearchCV with a custom metric
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义指标的GridsearchCV
- en: As highlighted in *Chapter 6*, *The Machine Learning Process*, scikit-learn
    provides a method to define ranges of values for multiple hyperparameters. It
    automates the process of cross-validating the various combinations of these parameter
    values to identify the optimal configuration. Let's walk through the process of
    automatically tuning your model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*第6章*，*机器学习过程*中所强调的，scikit-learn提供了一种定义多个超参数值范围的方法。它自动化了交叉验证这些参数值的各种组合的过程，以确定最佳配置。让我们一起走过自动调整模型的过程。
- en: 'The first step is to instantiate a model object and define a dictionary where
    the keywords name the hyperparameters, and the values list the parameter settings
    to be tested:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是实例化一个模型对象，并定义一个字典，其中关键字命名超参数，值列出要测试的参数设置：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, instantiate the `GridSearchCV` object, providing the estimator object
    and parameter grid, as well as a scoring method and cross-validation choice, to
    the initialization method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，实例化`GridSearchCV`对象，提供估计器对象和参数网格，以及评分方法和交叉验证选择，传递给初始化方法。
- en: 'We set our custom `MultipleTimeSeriesSplit` class to train the model for 60
    months, or 5 years, of data and to validate performance using the subsequent 6
    months, repeating the process over 10 folds to cover an out-of-sample period of
    5 years:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了自定义的`MultipleTimeSeriesSplit`类来训练模型60个月，或5年的数据，并使用随后的6个月验证性能，重复这个过程10次，以覆盖5年的样本外期：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We use the `roc_auc` metric to score the classifier, and define a custom information
    coefficient (IC) metric using scikit-learn''s `make_scorer` function for the regression
    model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`roc_auc`指标对分类器进行评分，并使用scikit-learn的`make_scorer`函数为回归模型定义自定义信息系数（IC）指标：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can parallelize the search using the `n_jobs` parameter and automatically
    obtain a trained model that uses the optimal hyperparameters by setting `refit=True`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`n_jobs`参数并设置`refit=True`来并行化搜索，并自动获得使用最佳超参数的训练模型。
- en: 'With all the settings in place, we can fit `GridSearchCV` just like any other
    model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有设置就绪后，我们可以像任何其他模型一样拟合`GridSearchCV`：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The training process produces some new attributes for our `GridSearchCV` object,
    most importantly the information about the optimal settings and the best cross-validation
    score (now using the proper setup, which avoids lookahead bias).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程为我们的`GridSearchCV`对象产生了一些新属性，最重要的是关于最佳设置和最佳交叉验证分数的信息（现在使用适当的设置，避免了前瞻性偏差）。
- en: 'The following table lists the parameters and scores for the best regression
    and classification model, respectively. With a shallower tree and more regularized
    leaf nodes, the regression tree achieves an IC of 0.083, while the classifier''s
    AUC score is 0.525:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了最佳回归和分类模型的参数和分数。通过使用更浅的树和更规则的叶节点，回归树实现了0.083的IC，而分类器的AUC分数为0.525：
- en: '| Parameter | Regression | Classification |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: 参数 | 回归 | 分类 |
- en: '| **max_depth** | 6 | 12 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **max_depth** | 6 | 12 |'
- en: '| **max_features** | sqrt | sqrt |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **max_features** | sqrt | sqrt |'
- en: '| **min_samples_leaf** | 50 | 5 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **min_samples_leaf** | 50 | 5 |'
- en: '| **Score** | 0.0829 | 0.5250 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **Score** | 0.0829 | 0.5250 |'
- en: The automation is quite convenient, but we also would like to inspect how the
    performance evolves for different parameter values. Upon completion of this process,
    the `GridSearchCV` object makes detailed cross-validation results available so
    that we can gain more insights.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化非常方便，但我们也想检查不同参数值的性能如何演变。完成此过程后，`GridSearchCV`对象提供了详细的交叉验证结果，以便我们可以获得更多见解。
- en: How to inspect the tree structure
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何检查树结构
- en: 'The notebook also illustrates how to run cross-validation more manually to
    obtain custom tree attributes, such as the total number of nodes or leaf nodes
    associated with certain hyperparameter settings. The following function accesses
    the internal `.tree_ attribute` to retrieve information about the total node count,
    as well as how many of these nodes are leaf nodes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本还说明了如何手动运行交叉验证以获取自定义树属性，例如与某些超参数设置相关的总节点数或叶节点数。以下函数访问内部的`.tree_属性`，以检索有关总节点数以及这些节点中有多少是叶节点的信息：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can combine this information with the train and test scores to gain detailed
    knowledge about the model behavior throughout the cross-validation process, as
    follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些信息与训练和测试分数结合起来，以获得有关模型在整个交叉验证过程中行为的详细知识，如下所示：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following plot displays how the number of leaf nodes increases with the
    depth of the tree. Due to the sample size of each cross-validation fold containing
    60 months with around 500 data points each, the number of leaf nodes is limited
    to around 3,000 when limiting the number of `min_samples_leaf` to 10 samples:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了叶节点数量随树深度增加而增加的情况。由于每个交叉验证折叠的样本大小包含大约500个数据点的60个月，当将`min_samples_leaf`的数量限制为10个样本时，叶节点的数量限制在大约3,000个左右：
- en: '![](img/B15439_11_07.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_07.png)'
- en: 'Figure 11.7: Visualization of a classification tree'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：分类树的可视化
- en: Comparing regression and classification performance
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较回归和分类性能
- en: To take a closer look at the performance of the models, we will show the cross-validation
    performance for various levels of depth, while maintaining the other parameter
    settings that produced the best grid search results. *Figure 11.8* displays the
    train and the validation scores and highlights the degree of overfitting for deeper
    trees. This is because the training scores steadily increase, whereas validation
    performance remains flat or decreases.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地观察模型的性能，我们将展示不同深度水平的交叉验证性能，同时保持产生最佳网格搜索结果的其他参数设置。*图11.8*显示了训练和验证分数，并突出了更深树的过拟合程度。这是因为训练分数稳步增加，而验证性能保持不变或下降。
- en: 'Note that, for the classification tree, the grid search suggested 12 levels
    for the best predictive accuracy. However, the plot shows similar AUC scores for
    less complex trees, with three or seven levels. We would prefer a shallower tree
    that promises comparable generalization performance while reducing the risk of
    overfitting:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于分类树，网格搜索建议最佳预测准确度的12个级别。然而，图表显示较简单的树，如三级或七级，具有类似的AUC分数。我们更倾向于选择一个更浅的树，它承诺具有可比较的泛化性能，同时减少过拟合的风险：
- en: '![](img/B15439_11_08.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_08.png)'
- en: 'Figure 11.8: Train and validation scores for both models'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：两个模型的训练和验证分数
- en: Diagnosing training set size with learning curves
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用学习曲线诊断训练集大小
- en: A **learning curve** is a useful tool that displays how the validation and training
    scores evolve as the number of training samples increases.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习曲线**是一种有用的工具，显示验证和训练分数随着训练样本数量的增加而发展的方式。'
- en: The purpose of the learning curve is to find out whether and how much the model
    would benefit from using more data during training. It also helps to diagnose
    whether the model's generalization error is more likely driven by bias or variance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线的目的是找出模型在训练过程中是否以及如何受益于使用更多数据。它还有助于诊断模型的泛化误差更可能是由偏差还是方差驱动。
- en: If the training score meets performance expectations and the validation score
    exhibits significant improvement as the training sample grows, training for a
    longer lookback period or obtaining more data might add value. If, on the other
    hand, both the validation and the training score converge to a similarly poor
    value, despite an increasing training set size, the error is more likely due to
    bias, and additional training data is unlikely to help.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练分数符合性能预期，并且随着训练样本的增加，验证分数显着提高，那么进行更长的回溯期或获取更多数据可能会增加价值。另一方面，如果验证和训练分数都收敛到类似的较差值，尽管训练集大小增加，错误更可能是由于偏差，额外的训练数据不太可能有所帮助。
- en: 'The following image depicts the learning curves for the best regression and
    classification models:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片描述了最佳回归和分类模型的学习曲线：
- en: '![](img/B15439_11_09.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_09.png)'
- en: 'Figure 11.9: Learning curves for the best version of each model'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：每个模型的最佳版本的学习曲线
- en: Especially for the regression model, the validation performance improves with
    a larger training set. This suggests that a longer training period may yield better
    results. Try it yourself to see if it works!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于回归模型，随着训练集的增加，验证性能会提高。这表明更长的训练期可能会产生更好的结果。自己尝试看看是否有效！
- en: Gaining insight from feature importance
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从特征重要性中获得见解
- en: Decision trees can not only be visualized to inspect the decision path for a
    given feature, but can also summarize the contribution of each feature to the
    rules learned by the model to fit the training data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅可以可视化以检查给定特征的决策路径，还可以总结每个特征对模型学习的规则对训练数据的贡献。
- en: Feature importance captures how much the splits produced by each feature help
    optimize the model's metric used to evaluate the split quality, which in our case
    is the Gini impurity. A feature's importance is computed as the (normalized) total
    reduction of this metric and takes into account the number of samples affected
    by a split. Hence, features used earlier in the tree where the nodes tend to contain
    more samples are typically considered of higher importance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性捕捉了每个特征产生的分裂如何帮助优化用于评估分裂质量的模型指标，我们的情况下是基尼不纯度。特征的重要性计算为该指标的（归一化）总减少量，并考虑到受分裂影响的样本数量。因此，在树的早期使用的特征，节点往往包含更多的样本，通常被认为是更重要的。
- en: '*Figure 11.10* shows the plots for feature importance for the top 15 features
    of each model. Note how the order of features differs from the univariate evaluation
    based on the mutual information scores given at the beginning of this section.
    Clearly, the ability of decision trees to capture interdependencies, such as between
    time periods and other features, can alter the value of each feature:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.10*显示了每个模型的前15个特征的特征重要性图。请注意，特征的顺序与本节开头给出的互信息评分的单变量评估不同。显然，决策树捕捉时间段和其他特征之间的相互依赖关系的能力可以改变每个特征的价值：'
- en: '![](img/B15439_11_10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_10.png)'
- en: 'Figure 11.10: Feature importance for the best regression and classification
    models'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：最佳回归和分类模型的特征重要性
- en: Strengths and weaknesses of decision trees
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树的优势和劣势
- en: 'Regression and classification trees approach making predictions very differently
    from the linear models we have explored in the previous chapters. How do you **decide
    which model is more suitable** for the problem at hand? Consider the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 回归和分类树在预测方面的方法与我们在前几章中探讨过的线性模型非常不同。你如何**决定哪个模型更适合**手头的问题？考虑以下内容：
- en: If the relationship between the outcome and the features is approximately linear
    (or can be transformed accordingly), then linear regression will likely outperform
    a more complex method, such as a decision tree that does not exploit this linear
    structure.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果结果和特征之间的关系大致是线性的（或者可以相应地进行转换），那么线性回归很可能会胜过更复杂的方法，比如不利用这种线性结构的决策树。
- en: If the relationship appears highly nonlinear and more complex, decision trees
    will likely outperform the classical models. Keep in mind that the complexity
    of the relationship needs to be systematic or "real," rather than driven by noise,
    which leads more complex models to overfit.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果关系看起来非常非线性和更复杂，决策树很可能会胜过经典模型。请记住，关系的复杂性需要是系统的或“真实的”，而不是由噪音驱动的，这会导致更复杂的模型过拟合。
- en: 'Several **advantages** have made decision trees very popular:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的几个**优点**使其非常受欢迎：
- en: They are fairly straightforward to understand and to interpret, not least because
    they can be easily visualized and are thus more accessible to a non-technical
    audience. Decision trees are also referred to as white-box models, given the high
    degree of transparency about how they arrive at a prediction. Black-box models,
    such as ensembles and neural networks, may deliver better prediction accuracy,
    but the decision logic is often much more challenging to understand and interpret.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们相当容易理解和解释，也可以很容易地可视化，因此更容易被非技术人员理解。决策树也被称为白盒模型，因为它们在预测方面的透明度很高。黑盒模型，比如集成和神经网络，可能会提供更好的预测准确性，但决策逻辑往往更难理解和解释。
- en: Decision trees require less data preparation than models that make stronger
    assumptions about the data or are more sensitive to outliers and require data
    standardization (such as regularized regression).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树需要比那些对数据做出更强的假设或对离群值更敏感的模型更少的数据准备（比如正则化回归）。
- en: Some decision tree implementations handle categorical input, do not require
    the creation of dummy variables (improving memory efficiency), and can work with
    missing values, as we will see in *Chapter 12*, *Boosting Your Trading Strategy*,
    but this is not the case for scikit-learn.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些决策树实现处理分类输入，不需要创建虚拟变量（提高内存效率），并且可以处理缺失值，但这不适用于scikit-learn。
- en: Prediction is fast because it is logarithmic in the number of leaf nodes (unless
    the tree becomes extremely unbalanced).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测速度快，因为它是基于叶节点数量的对数（除非树变得极度不平衡）。
- en: It is possible to validate the model using statistical tests and account for
    its reliability (see the references for more details).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用统计测试来验证模型，并考虑其可靠性（更多细节请参考参考文献）。
- en: 'Decision trees also have several key **disadvantages**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也有一些关键的**缺点**：
- en: Decision trees have a built-in tendency to overfit to the training set and produce
    a high generalization error. The key steps to address this weakness are pruning
    and regularization using the early-stopping criteria that limits tree growth,
    as outlined in this section.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树内置了对训练集过拟合的倾向，并产生高泛化误差。解决这个弱点的关键步骤是修剪和正则化，使用限制树生长的早停止标准，正如本节所述。
- en: Decision trees are also sensitive to unbalanced class weights and may produce
    biased trees. One option is to oversample the underrepresented classes or undersample
    the more frequent class. It is typically better, though, to use class weights
    and directly adjust the objective function.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树也对不平衡的类权重敏感，可能会产生有偏的树。一种选择是过采样代表性不足的类别或者欠采样更频繁的类别。然而，更好的选择是使用类权重并直接调整目标函数。
- en: The high variance of decision trees is tied to their ability to closely adapt
    to a training set. As a result, minor variations in the data can produce wide
    swings in the tree's structure and, consequently, the model's predictions. A key
    prevention mechanism is the use of an ensemble of randomized decision trees that
    have low bias and produce uncorrelated prediction errors.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的高方差与其密切适应训练集的能力有关。因此，数据的轻微变化可能会导致树结构和模型预测的巨大波动。关键的预防机制是使用低偏差的随机决策树集成，产生不相关的预测误差。
- en: The greedy approach to decision-tree learning optimizes local criteria that
    reduce the prediction error at the current node and do not guarantee a globally
    optimal outcome. Again, ensembles consisting of randomized trees help to mitigate
    this problem.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树学习的贪婪方法优化了减少当前节点的预测误差的局部标准，并不能保证全局最优结果。同样，由随机树组成的集成有助于减轻这个问题。
- en: We will now turn to the ensemble method of mitigating the risk of overfitting
    that's inherent when using decision trees.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将转向集成方法，以减轻在使用决策树时固有的过拟合风险。
- en: Random forests – making trees more reliable
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林 - 使树更可靠
- en: Decision trees are not only useful for their transparency and interpretability.
    They are also fundamental building blocks for more powerful ensemble models that
    combine many individual trees, while randomly varying their design to address
    the overfitting problems we just discussed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅因其透明度和可解释性而有用。它们也是更强大的集成模型的基本构建模块，这些模型结合了许多个体树，同时随机变化它们的设计以解决我们刚刚讨论过的过拟合问题。
- en: Why ensemble models perform better
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么集成模型表现更好
- en: Ensemble learning involves combining several machine learning models into a
    single new model that aims to make better predictions than any individual model.
    More specifically, an ensemble integrates the predictions of several base estimators,
    trained using one or more learning algorithms, to reduce the generalization error
    that these models produce on their own.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习涉及将几个机器学习模型组合成一个新的模型，旨在比任何单个模型做出更好的预测。更具体地说，集成整合了几个基本估计器的预测，这些估计器使用一个或多个学习算法进行训练，以减少这些模型单独产生的泛化误差。
- en: 'For ensemble learning to achieve this goal, **the individual models must be**:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，**个体模型必须是**：
- en: '**Accurate**: Outperform a naive baseline (such as the sample mean or class
    proportions)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确**: 胜过天真的基准（如样本均值或类别比例）'
- en: '**Independent**: Predictions are generated differently to produce different
    errors'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立**: 预测是通过不同方式生成的，产生不同的错误'
- en: Ensemble methods are among the most successful machine learning algorithms,
    in particular for standard numerical data. Large ensembles are very successful
    in machine learning competitions and may consist of many distinct individual models
    that have been combined by hand or using another machine learning algorithm.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是最成功的机器学习算法之一，特别是对于标准的数值数据。大型集成在机器学习竞赛中非常成功，可能由许多不同的个体模型组成，这些模型已经通过手工或使用其他机器学习算法进行了组合。
- en: There are several disadvantages to combining predictions made by different models.
    These include reduced interpretability and higher complexity and cost of training,
    prediction, and model maintenance. As a result, in practice (outside of competitions),
    the small gains in accuracy from large-scale ensembling may not be worth the added
    costs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同模型的预测组合起来有几个缺点。这些包括降低可解释性，以及训练、预测和模型维护的复杂性和成本增加。因此，在实践中（除了竞赛之外），大规模集成带来的准确性微小增益可能不值得额外的成本。
- en: 'There are two groups of ensemble methods that are typically distinguished between,
    depending on how they optimize the constituent models and then integrate the results
    for a single ensemble prediction:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通常区分两组集成方法，具体取决于它们如何优化组成模型，然后将结果整合为单个集成预测：
- en: '**Averaging methods** train several base estimators independently and then
    average their predictions. If the base models are not biased and make different
    prediction errors that are not highly correlated, then the combined prediction
    may have lower variance and can be more reliable. This resembles the construction
    of a portfolio from assets with uncorrelated returns to reduce the volatility
    without sacrificing the return.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均方法**独立训练多个基本估计器，然后平均它们的预测。如果基本模型没有偏差，并且产生不高度相关的不同预测误差，那么组合预测可能具有较低的方差，并且可能更可靠。这类似于从具有不相关回报的资产构建投资组合，以减少波动而不牺牲回报。'
- en: '**Boosting methods**, in contrast, train base estimators sequentially with
    the specific goal of reducing the bias of the combined estimator. The motivation
    is to combine several weak models into a powerful ensemble.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升方法**相反，顺序训练基本估计器，其特定目标是减少组合估计器的偏差。动机是将几个弱模型组合成一个强大的集成。'
- en: We will focus on automatic averaging methods in the remainder of this chapter
    and boosting methods in *Chapter 12*, *Boosting Your Trading Strategy*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将专注于自动平均方法，并在*第12章*“提升您的交易策略”中讨论提升方法。
- en: Bootstrap aggregation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bootstrap聚合
- en: We saw that decision trees are likely to make poor predictions due to high variance,
    which implies that the tree structure is quite sensitive to the available training
    sample. We have also seen that a model with low variance, such as linear regression,
    produces similar estimates, despite different training samples, as long as there
    are sufficient samples given the number of features.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，由于高方差，决策树很可能会做出糟糕的预测，这意味着树结构对可用训练样本非常敏感。我们还看到，低方差的模型，如线性回归，产生类似的估计，尽管训练样本不同，只要给定特征的样本足够。
- en: For a given a set of independent observations, each with a variance of ![](img/B15439_11_003.png),
    the standard error of the sample mean is given by ![](img/B15439_11_004.png).
    In other words, averaging over a larger set of observations reduces the variance.
    A natural way to reduce the variance of a model and its generalization error would,
    thus, be to collect many training sets from the population, train a different
    model on each dataset, and average the resulting predictions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组独立观察值，每个观察值的方差为![](img/B15439_11_003.png)，样本均值的标准误差为![](img/B15439_11_004.png)。换句话说，对更大的观察集进行平均会减少方差。因此，减少模型和其泛化误差的方差的一种自然方法是从总体中收集许多训练集，在每个数据集上训练不同的模型，然后对产生的预测进行平均。
- en: In practice, we do not typically have the luxury of many different training
    sets. This is where **bagging**, short for **bootstrap aggregation**, comes in.
    Bagging is a general-purpose method that's used to reduce the variance of a machine
    learning model, which is particularly useful and popular when applied to decision
    trees.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们通常没有很多不同的训练集的奢侈。这就是**装袋**的用武之地，它是**自助聚合**的缩写。装袋是一种通用方法，用于减少机器学习模型的方差，特别适用于决策树时尤其有用和受欢迎。
- en: We will first explain how this technique mitigates overfitting and then show
    how to apply it to decision trees.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先解释这种技术如何减轻过拟合，然后展示如何将其应用于决策树。
- en: How bagging lowers model variance
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 装袋如何降低模型方差
- en: Bagging refers to the aggregation of bootstrap samples, which are random samples
    with replacement. Such a random sample has the same number of observations as
    the original dataset but may contain duplicates due to replacement.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋指的是对自助样本的聚合，这些样本是带有替换的随机样本。这样的随机样本具有与原始数据集相同数量的观察值，但由于替换可能包含重复项。
- en: Bagging increases predictive accuracy but decreases model interpretability because
    it's no longer possible to visualize the tree to understand the importance of
    each feature. As an ensemble algorithm, bagging methods train a given number of
    base estimators on these bootstrapped samples and then aggregate their predictions
    into a final ensemble prediction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋增加了预测准确性，但降低了模型的可解释性，因为不再可能可视化树来理解每个特征的重要性。作为一个集成算法，装袋方法在这些自助样本上训练给定数量的基本估计器，然后将它们的预测聚合成最终的集成预测。
- en: 'Bagging reduces the variance of the base estimators to reduce their generalization
    error by:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋通过以下方式减少基本估计器的方差，从而减少它们的泛化误差：
- en: Randomizing how each tree is grown
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每棵树的生长方式随机化
- en: Averaging their predictions
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平均它们的预测
- en: It is often a straightforward approach to improve on a given model without the
    need to change the underlying algorithm. This technique works best with **complex
    models that have low bias and high variance**, such as deep decision trees, because
    its goal is to limit overfitting. Boosting methods, in contrast, work best with
    weak models, such as shallow decision trees.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是一种简单的方法来改进给定模型，而无需更改基础算法。这种技术最适用于**具有低偏差和高方差**的复杂模型，例如深度决策树，因为其目标是限制过拟合。相比之下，增强方法最适用于弱模型，例如浅决策树。
- en: 'There are several bagging methods that differ by the random sampling process
    they apply to the training set:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种装袋方法，它们通过对训练集应用的随机抽样过程而有所不同：
- en: '**Pasting** draws random samples from the training data without replacement,
    whereas bagging samples with replacement.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**粘贴**从训练数据中无放回地随机抽取样本，而装袋则是有放回地抽取样本。'
- en: '**Random subspaces** randomly sample from the features (that is, the columns)
    without replacement.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机子空间**无放回地从特征（即列）中随机抽取。'
- en: '**Random patches** train base estimators by randomly sampling both observations
    and features.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机补丁**通过随机抽取观察值和特征来训练基本估计器。'
- en: Bagged decision trees
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 装袋决策树
- en: To apply bagging to decision trees, we create bootstrap samples from our training
    data by repeatedly sampling with replacement. Then, we train one decision tree
    on each of these samples and create an ensemble prediction by averaging over the
    predictions of the different trees. You can find the code for this example in
    the notebook `bagged_decision_trees`, unless otherwise noted.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要将装袋应用于决策树，我们通过重复抽样来创建自助样本，然后在每个样本上训练一棵决策树，并通过对不同树的预测进行平均来创建一个集成预测。您可以在笔记本`bagged_decision_trees`中找到此示例的代码，除非另有说明。
- en: Bagged decision trees are usually grown large, that is, they have many levels
    and leaf nodes and are not pruned so that each tree has a low bias but high variance.
    The effect of averaging their predictions then aims to reduce their variance.
    Bagging has been shown to substantially improve predictive performance by constructing
    ensembles that combine hundreds or even thousands of trees trained on bootstrap
    samples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋决策树通常生长得很大，即它们有许多层和叶节点，并且不进行修剪，因此每棵树的偏差很低，但方差很高。然后，平均它们的预测的效果是减少它们的方差。已经证明，通过构建组合了数百甚至数千棵树的集成来显著提高预测性能。
- en: 'To illustrate the effect of bagging on the variance of a regression tree, we
    can use the `BaggingRegressor` meta-estimator provided by scikit-learn. It trains
    a user-defined base estimator based on parameters that specify the sampling strategy:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明装袋对回归树方差的影响，我们可以使用scikit-learn提供的`BaggingRegressor`元估计器。它根据指定抽样策略训练用户定义的基本估计器：
- en: '`max_samples` and `max_features` control the size of the subsets drawn from
    the rows and the columns, respectively.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_samples`和`max_features`分别控制从行和列中抽取的子集的大小。'
- en: '`bootstrap` and `bootstrap_features` determine whether each of these samples
    is drawn with or without replacement.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap`和`bootstrap_features`确定每个样本是有放回还是无放回地抽取。'
- en: The following example uses an exponential function to generate training samples
    for a single `DecisionTreeRegressor` and a `BaggingRegressor` ensemble that consists
    of 10 trees, each grown 10 levels deep. Both models are trained on the random
    samples and predict outcomes for the actual function with added noise.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用指数函数生成单个`DecisionTreeRegressor`和由10棵树组成的`BaggingRegressor`集成的训练样本，每棵树都生长到10层深。这两个模型都是在随机样本上训练的，并对带有添加噪声的实际函数的结果进行预测。
- en: 'Since we know the true function, we can decompose the mean-squared error into
    bias, variance, and noise, and compare the relative size of these components for
    both models according to the following breakdown:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道真实函数，我们可以将均方误差分解为偏差、方差和噪声，并根据以下分解比较这两个模型的这些组成部分的相对大小：
- en: '![](img/B15439_11_005.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_005.png)'
- en: 'We will draw 100 random samples of 250 training and 500 test observations each
    to train each model and collect the predictions:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将抽取100个250个训练和500个测试观察值的随机样本来训练每个模型并收集预测：
- en: '[PRE11]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For each model, the plots in *Figure 11.11* show:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，*图11.11*中的图表显示：
- en: The mean prediction and a band of two standard deviations around the mean (upper
    panel)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值预测和均值周围两个标准差的带（上部面板）
- en: The bias-variance-noise breakdown based on the values for the true function
    (bottom panel)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于真实函数值的偏差-方差-噪声分解（底部面板）
- en: 'We find that the variance of the predictions of the individual decision tree
    (left side) is almost twice as high as that for the small ensemble of 10 bagged
    trees, based on bootstrapped samples:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，单个决策树的预测方差（左侧）几乎是10棵装袋树的两倍，基于自助采样：
- en: '![](img/B15439_11_11.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_11.png)'
- en: 'Figure 11.11: Bias-variance breakdown for individual and bagged decision trees'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11：单个和装袋决策树的偏差-方差分解
- en: See the notebook `bagged_decision_trees` for implementation details.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见笔记本`bagged_decision_trees`以获取实现细节。
- en: How to build a random forest
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何构建随机森林
- en: The random forest algorithm builds on the randomization introduced by bagging
    to further reduce variance and improve predictive performance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法建立在装袋引入的随机化基础上，以进一步减少方差并提高预测性能。
- en: In addition to training each ensemble member on bootstrapped training data,
    random forests also randomly sample from the features used in the model (without
    replacement). Depending on the implementation, the random samples can be drawn
    for each tree or each split. As a result, the algorithm faces different options
    when learning new rules, either at the level of a tree or for each split.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在自助训练数据上训练每个集成成员之外，随机森林还会从模型中使用的特征中进行随机抽样（不重复）。根据实现方式的不同，随机样本可以为每棵树或每个分裂绘制。因此，该算法在学习新规则时面临不同的选择，无论是在树的层面还是在每个分裂的层面。
- en: 'The **sample size for the features** differs between regression and classification
    trees:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征的样本大小**在回归树和分类树之间有所不同：'
- en: For **classification**, the sample size is typically the square root of the
    number of features.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于**分类**，样本大小通常是特征数量的平方根。
- en: For **regression**, it can be anywhere from one-third to all features and should
    be selected based on cross-validation.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于**回归**，可以是从三分之一到所有特征，应基于交叉验证进行选择。
- en: 'The following diagram illustrates how random forests randomize the training
    of individual trees and then aggregate their predictions into an ensemble prediction:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了随机森林如何随机化单个树的训练，然后将它们的预测聚合成一个集成预测：
- en: '![](img/B15439_11_12.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_12.png)'
- en: 'Figure 11.12: How a random forest grows individual trees'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：随机森林如何生长单个树
- en: The goal of randomizing the features in addition to the training observations
    is to further **decorrelate the prediction errors** of the individual trees. All
    features are not created equal, and a small number of highly relevant features
    will be selected much more frequently and earlier in the tree-construction process,
    making decision trees more alike across the ensemble. However, the less the generalization
    errors of individual trees correlate, the more the overall variance will be reduced.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 随机化特征以及训练观察的目标是进一步**使个体树的预测误差不相关**。所有特征并非一视同仁，一小部分高度相关的特征会在树构建过程中更频繁地被选择，并且更早地被选中，使得决策树在整个集成中更加相似。然而，个体树的泛化误差相关性越低，整体方差就会降低得越多。
- en: How to train and tune a random forest
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何训练和调整随机森林
- en: 'The key configuration parameters include the various hyperparameters for the
    individual decision trees introduced in the *How to tune the hyperparameters*
    section. The following table lists additional options for the two `RandomForest`
    classes:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 关键配置参数包括*如何调整超参数*部分介绍的个体决策树的各种超参数。以下表格列出了两个`RandomForest`类的其他选项：
- en: '| Keyword | Default | Description |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 关键词 | 默认值 | 描述 |'
- en: '| `bootstrap` | `TRUE` | Bootstrap samples during training |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `bootstrap` | `TRUE` | 训练期间使用自助采样 |'
- en: '| `n_estimators` | `10` | Number of trees in the forest |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `n_estimators` | `10` | 森林中的树的数量 |'
- en: '| `oob_score` | `FALSE` | Uses out-of-bag samples to estimate the R2 on unseen
    data |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `oob_score` | `FALSE` | 使用袋外样本来估计未见数据上的R2 |'
- en: The `bootstrap` parameter activates the bagging algorithm just described. Bagging,
    in turn, enables the computation of the out-of-bag score (`oob_score`), which
    estimates the generalization accuracy from samples not included in the bootstrap
    sample used to train a given tree (see the *Out-of-bag testing* section).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap`参数激活了刚才描述的装袋算法。装袋又使得可以计算出袋外得分(`oob_score`)，它估计了未包含在用于训练给定树的自助采样中的样本的泛化准确性（参见*袋外测试*部分）。'
- en: The parameter `n_estimators` defines the number of trees to be grown as part
    of the forest. Larger forests perform better, but also take more time to build.
    It is important to monitor the cross-validation error as the number of base learners
    grows. The goal is to identify when the rising cost of training an additional
    tree outweighs the benefit of reducing the validation error, or when the latter
    starts to increase again.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`n_estimators`定义了森林中要生长的树的数量。更大的森林表现更好，但建造时间也更长。重要的是要监控交叉验证误差，因为基本学习者的数量增加。目标是确定训练额外树木的成本上升超过减少验证误差的好处，或者后者再次开始增加的时候。
- en: The `max_features` parameter controls the size of the randomly selected feature
    subsets available when learning a new decision rule and to split a node. A lower
    value reduces the correlation of the trees and, thus, the ensemble's variance,
    but may also increase the bias. As pointed out at the beginning of this section,
    good starting values are the number of training features for regression problems
    and the square root of this number for classification problems, but will depend
    on the relationships among features and should be optimized using cross-validation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features`参数控制在学习新的决策规则并分割节点时可用的随机选择特征子集的大小。较低的值会减少树之间的相关性，从而减少集成的方差，但也可能增加偏差。正如本节开头所指出的，对于回归问题，良好的起始值是训练特征的数量，对于分类问题，良好的起始值是这个数量的平方根，但将取决于特征之间的关系，并应使用交叉验证进行优化。'
- en: Random forests are designed to contain deep fully-grown trees, which can be
    created using `max_depth=None` and `min_samples_split=2`. However, these values
    are not necessarily optimal, especially for high-dimensional data with many samples
    and, consequently, potentially very deep trees that can become very computationally,
    and memory, intensive.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林旨在包含深度完全生长的树，可以使用`max_depth=None`和`min_samples_split=2`来创建。然而，这些值不一定是最佳的，特别是对于具有许多样本的高维数据，因此可能会变得非常计算和内存密集。
- en: The `RandomForest` class provided by scikit-learn supports parallel training
    and prediction by setting the `n_jobs` parameter to the *k* number of jobs to
    run on different cores. The `-1` value uses all available cores. The overhead
    of interprocess communication may limit the speedup from being linear so that
    *k* jobs may take more than 1/*k* the time of a single job. Nonetheless, the speedup
    is often quite significant for large forests or deep individual trees that may
    take a meaningful amount of time to train when the data is large, and split evaluation
    becomes costly.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供的`RandomForest`类通过将`n_jobs`参数设置为要在不同核心上运行的*k*个作业来支持并行训练和预测。值`-1`使用所有可用核心。进程间通信的开销可能限制速度提升为线性，因此*k*个作业可能需要超过单个作业的1/*k*的时间。尽管如此，对于大型森林或深度个体树，在数据较大且分割评估变得昂贵时，速度提升通常相当显著。
- en: As always, the best parameter configuration should be identified using cross-validation.
    The following steps illustrate the process. The code for this example is in the
    notebook `random_forest_tuning`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，应使用交叉验证来确定最佳的参数配置。以下步骤说明了这个过程。此示例的代码在笔记本`random_forest_tuning`中。
- en: 'We will use `GridSearchCV` to identify an optimal set of parameters for an
    ensemble of classification trees:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`GridSearchCV`来确定一组最佳参数，用于分类树的集成：
- en: '[PRE12]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We use the same 10-fold custom cross-validation as in the decision tree example
    previously and populate the parameter grid with values for the key configuration
    settings:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与之前决策树示例中相同的10折自定义交叉验证，并使用关键配置设置的值填充参数网格：
- en: '[PRE13]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Configure `GridSearchCV` using the preceding as input:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述内容配置`GridSearchCV`：
- en: '[PRE14]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We run our grid search as before and find the following result for the best-performing
    regression and classification models. A random forest regression model does better
    with shallower trees compared to the classifier but otherwise uses the same settings:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样运行网格搜索，并找到最佳表现的回归和分类模型的以下结果。与分类器相比，随机森林回归模型在树较浅时表现更好，但其他设置相同：
- en: '| Parameter | Regression | Classification |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 回归 | 分类 |'
- en: '| max_depth | 5 | 15 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| max_depth | 5 | 15 |'
- en: '| min_samples_leaf | 5 | 5 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| min_samples_leaf | 5 | 5 |'
- en: '| n_estimators | 100 | 100 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| n_estimators | 100 | 100 |'
- en: '| Score | 0.0435 | 0.5205 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 得分 | 0.0435 | 0.5205 |'
- en: However, both models underperform their individual decision tree counterparts,
    highlighting that more complex models do not necessarily outperform simpler approaches,
    especially when the data is noisy and the risk of overfitting is high.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两个模型的表现都不如它们各自的决策树对应模型，突出表明更复杂的模型不一定优于更简单的方法，特别是当数据嘈杂且过拟合的风险很高时。
- en: Feature importance for random forests
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林的特征重要性
- en: A random forest ensemble may contain hundreds of individual trees, but it is
    still possible to obtain an overall summary measure of feature importance from
    bagged models.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林集成可能包含数百棵单独的树，但仍然可以从装袋模型中获得特征重要性的总体摘要度量。
- en: For a given feature, the **importance score** is the total reduction in the
    objective function's value due to splits on this feature and is averaged over
    all trees. Since the objective function takes into account how many features are
    affected by a split, features used near the top of a tree will get higher scores
    due to the larger number of observations contained in the smaller number of available
    nodes. By averaging over many trees grown in a randomized fashion, the feature
    importance estimate loses some variance and becomes more accurate.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的特征，**重要性分数**是由于对该特征进行分割而导致目标函数值的总减少，并且在所有树上进行了平均。由于目标函数考虑了分割影响的特征数量，因此在树的顶部附近使用的特征将因包含在较少可用节点中的较大数量的观察而获得更高的分数。通过在随机方式下生长的许多树上进行平均，特征重要性估计失去了一些方差，并变得更准确。
- en: The score is measured in terms of the mean-squared error for regression trees
    and the Gini impurity or entropy for classification trees. scikit-learn further
    normalizes feature importance so that it sums up to 1\. Feature importance thus
    computed is also popular for feature selection as an alternative to the mutual
    information measures we saw in *Chapter 6*, *The Machine Learning Process* (see
    `SelectFromModel` in the `sklearn.feature_selection` module).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 得分是以回归树的均方误差和分类树的基尼不纯度或熵来衡量的。scikit-learn进一步对特征重要性进行归一化，使其总和为1。因此，计算出的特征重要性也很受欢迎，作为特征选择的替代方法，而不是我们在*第6章*，*机器学习过程*中看到的互信息测量（请参阅`sklearn.feature_selection`模块中的`SelectFromModel`）。
- en: '*Figure 11.13* shows the values for the top 15 features for both models. The
    regression model relies much more on time periods than the better-performing decision
    tree:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.13*显示了两种模型的前15个特征的数值。回归模型更多地依赖于时间段，而表现更好的决策树则不是。'
- en: '![](img/B15439_11_13.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_13.png)'
- en: 'Figure 11.13: Random forest feature importance'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13：随机森林特征重要性
- en: Out-of-bag testing
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋外测试
- en: Random forests offer the benefit of built-in cross-validation because individual
    trees are trained on bootstrapped versions of the training data. As a result,
    each tree uses, on average, only two-thirds of the available observations. To
    see why, consider that a bootstrap sample has the same size, *n*, as the original
    sample, and each observation has the same probability, 1/*n*, to be drawn. Hence,
    the probability of not entering a bootstrap sample at all is (1-1/*n*)*n*, which
    converges (quickly) to 1/*e*, or roughly one third.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林提供了内置的交叉验证的好处，因为单独的树是在训练数据的自助版本上训练的。因此，每棵树平均只使用了可用观测的三分之二。要了解原因，考虑自助样本的大小与原始样本的大小相同，每个观测被抽取的概率也相同，为1/*n*。因此，不进入自助样本的概率是(1-1/*n*)*n*，这收敛（迅速）到1/*e*，或者大约三分之一。
- en: This remaining one-third of the observations that are not included in the training
    set is used to grow a bagged tree called **out-of-bag** (**OOB**) observations,
    and can serve as a validation set. Just as with cross-validation, we predict the
    response for an OOB sample for each tree built without this observation, and then
    average the predicted responses (if regression is the goal) or take a majority
    vote or predicted probability (if classification is the goal) for a single ensemble
    prediction for each OOB sample. These predictions produce an unbiased estimate
    of the generalization error, which is conveniently computed during training.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 未包括在训练集中的剩余三分之一观测数据被用来生成一个袋装树，称为**袋外**（**OOB**）观测，并可以作为验证集。就像交叉验证一样，我们对每棵树建立的不包括这个观测的OOB样本进行响应预测，然后对预测的响应进行平均（如果目标是回归）或进行多数投票或预测概率（如果目标是分类），得到每个OOB样本的单一集成预测。这些预测产生了对泛化误差的无偏估计，方便地在训练期间计算。
- en: The resulting OOB error is a valid estimate of the generalization error for
    this observation. This is because the prediction is produced using decision rules
    learned in the absence of this observation. Once the random forest is sufficiently
    large, the OOB error closely approximates the leave-one-out cross-validation error.
    The OOB approach to estimate the test error is very efficient for large datasets
    where cross-validation can be computationally costly.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预测是在没有这个观测的情况下学习的决策规则产生的，因此得到的OOB误差是该观测的泛化误差的有效估计。一旦随机森林足够大，OOB误差就会紧密逼近留一次交叉验证误差。对于大型数据集，OOB方法估计测试误差非常高效，而交叉验证可能会产生计算成本。
- en: 'However, the same caveats apply as for cross-validation: you need to take care
    to avoid a lookahead bias that would ensue if OOB observations could be selected
    *out-of-order*. In practice, this makes it very difficult to use OOB testing with
    time-series data, where the validation set needs to be selected subject to the
    sequential nature of the data.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与交叉验证一样，同样的警告也适用：您需要小心避免前瞻性偏差，如果OOB观测可以被*无序*选择，则会产生。在实践中，这使得在时间序列数据中使用OOB测试非常困难，因为验证集需要根据数据的顺序性选择。
- en: Pros and cons of random forests
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林的优缺点
- en: Bagged ensemble models have both advantages and disadvantages.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型有其优点和缺点。
- en: 'The **advantages** of random forests include:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的**优点**包括：
- en: Depending on the use case, a random forest can perform on par with the best
    supervised learning algorithms.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据使用情况，随机森林可以与最佳监督学习算法表现相当。
- en: Random forests provide a reliable feature importance estimate.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林提供了可靠的特征重要性估计。
- en: They offer efficient estimates of the test error without incurring the cost
    of repeated model training associated with cross-validation.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们能够有效地估计测试误差，而不需要承担与交叉验证相关的重复模型训练的成本。
- en: 'On the other hand, the **disadvantages** of random forests include:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，随机森林的**缺点**包括：
- en: An ensemble model is inherently less interpretable than an individual decision
    tree.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成模型本质上比单个决策树不太可解释。
- en: Training a large number of deep trees can have high computational costs (but
    can be parallelized) and use a lot of memory.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练大量深树可能会产生高计算成本（但可以并行化）并使用大量内存。
- en: Predictions are slower, which may create challenges for applications that require
    low latency.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测速度较慢，这可能对需要低延迟的应用程序造成挑战。
- en: Let's now take a look at how we can use a random forest for a trading strategy.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将随机森林用于交易策略。
- en: Long-short signals for Japanese stocks
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日本股票的多空信号
- en: In *Chapter 9*, *Time-Series Models for Volatility Forecasts and Statistical
    Arbitrage*, we used cointegration tests to identify pairs of stocks with a long-term
    equilibrium relationship in the form of a common trend to which their prices revert.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第9章*，*波动率预测和统计套利的时间序列模型*中，我们使用协整检验来识别具有长期均衡关系的股票对，其价格具有共同趋势，价格会回归到该趋势。
- en: In this chapter, we will use the predictions of a machine learning model to
    identify assets that are likely to go up or down so we can enter market-neutral
    long and short positions, accordingly. The approach is similar to our initial
    trading strategy that used linear regression in *Chapter 7*, *Linear Models –
    From Risk Factors to Return Forecasts*, and *Chapter 8*, *The ML4T Workflow –
    From Model to Strategy Backtesting*.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用机器学习模型的预测来识别可能上涨或下跌的资产，以便我们可以相应地进入市场中性的多头和空头头寸。这种方法类似于我们最初在*第7章*，*线性模型-从风险因素到收益预测*和*第8章*，*ML4T工作流程-从模型到策略回测*中使用线性回归的交易策略。
- en: Instead of the scikit-learn random forest implementation, we will use the LightGBM
    package, which has been primarily designed for gradient boosting. One of several
    advantages is LightGBM's ability to efficiently encode categorical variables as
    numeric features rather than using one-hot dummy encoding (Fisher 1958). We'll
    provide a more detailed introduction in the next chapter, but the code samples
    should be easy to follow as the logic is similar to the scikit-learn version.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用LightGBM软件包，而不是scikit-learn随机森林实现，LightGBM主要设计用于梯度提升。LightGBM的几个优势之一是其能够高效地将分类变量编码为数值特征，而不是使用独热编码（Fisher
    1958）。我们将在下一章中提供更详细的介绍，但代码示例应该很容易理解，因为逻辑与scikit-learn版本类似。
- en: The data – Japanese equities
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据-日本股票
- en: We are going to design a strategy for a universe of Japanese stocks, using data
    provided by Stooq, a Polish data provider that currently offers interesting datasets
    for various asset classes, markets, and frequencies, which we also relied upon
    in *Chapter 9*, *Time-Series Models for Volatility Forecasts and Statistical Arbitrage*.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设计一个针对日本股票的策略，使用由Stooq提供的数据，Stooq是一家波兰数据提供商，目前提供各种资产类别、市场和频率的有趣数据集，我们在*第9章*，*用于波动率预测和统计套利的时间序列模型*中也依赖于这些数据。
- en: While there is little transparency regarding the sourcing and quality of the
    data, it has the powerful advantage of currently being free of charge. In other
    words, we get to experiment with data on stocks, bonds, commodities, and FX at
    daily, hourly, and 5-minute frequencies, but should take the results with a large
    grain of salt.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关于数据的来源和质量几乎没有透明度，但它目前的强大优势是免费提供。换句话说，我们可以在股票、债券、大宗商品和外汇的日常、小时和5分钟频率上进行数据实验，但应该对结果持怀疑态度。
- en: The `create_datasets` notebook in the data directory of this book's GitHub repository
    contains instructions for downloading the data and storing them in HDF5 format.
    For this example, we are using price data on some 3,000 Japanese stocks for the
    2010-2019 period. The last 2 years will serve as the out-of-sample test period,
    while the prior years will serve as our cross-validation sample for model selection.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的GitHub存储库中的数据目录中的`create_datasets`笔记本包含了下载数据并以HDF5格式存储数据的说明。在本例中，我们使用了2010年至2019年期间约3000支日本股票的价格数据。最后的2年将作为样本外测试期，而之前的年份将作为我们进行模型选择的交叉验证样本。
- en: Please refer to the notebook `japanese_equity_features` for the code samples
    in this section. We remove tickers with more than five consecutive missing values
    and only keep the 1,000 most-traded stocks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅`japanese_equity_features`笔记本中本节的代码示例。我们删除了连续缺失值超过五个的股票，并只保留了交易量最高的1000支股票。
- en: The features – lagged returns and technical indicators
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征-滞后收益和技术指标
- en: We'll keep it relatively simple and combine historical returns for 1, 5, 10,
    21, and 63 trading days with several technical indicators provided by TA-Lib (see
    *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持相对简单，将1、5、10、21和63个交易日的历史收益与TA-Lib提供的几个技术指标相结合（详见*第4章*，*金融特征工程-如何研究Alpha因子*）。
- en: 'More specifically, we compute for each stock:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，我们为每支股票计算：
- en: '**Percentage price oscillator** (**PPO**): A normalized version of the **moving
    average convergence/divergence** (**MACD**) indicator that measures the difference
    between the 14-day and the 26-day exponential moving average to capture differences
    in momentum across assets.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**百分比价格振荡器**（**PPO**）：**移动平均收敛/发散**（**MACD**）指标的标准化版本，用于捕捉各种资产之间的动量差异。'
- en: '**Normalized average true range** (**NATR**): Measures price volatility in
    a way that can be compared across assets.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化平均真实范围**（**NATR**）：以一种可比较各种资产的方式衡量价格波动性。'
- en: '**Relative strength index** (**RSI**): Another popular momentum indicator (see
    *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors* for
    details).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相对强度指数**（**RSI**）：另一个流行的动量指标（详见*第4章*，*金融特征工程-如何研究Alpha因子*）。'
- en: '**Bollinger Bands**: Ratios of the moving average to the moving standard deviations
    used to identify opportunities for mean reversion.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**布林带**：移动平均与移动标准差的比率，用于识别均值回归的机会。'
- en: We will also include markers for the time periods year, month, and weekday,
    and rank stocks on a scale from 1 to 20 with respect to their latest return for
    each of the six intervals on each trading day.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将包括年、月和工作日的时间段标记，并根据每个交易日的六个间隔的最新收益对股票进行1到20的排名。
- en: The outcomes – forward returns for different horizons
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果-不同时间段的前瞻性收益
- en: To test the predictive ability of a random forest given these features, we generate
    forward returns for the same intervals up to 21 trading days (1 month).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试随机森林在给定这些特征的情况下的预测能力，我们生成了相同时间段的前瞻性收益，最多达到21个交易日（1个月）。
- en: The leads and lags implied by the historical and forward returns cause some
    loss of data that increases with the investment horizon. We end up with 2.3 million
    observations on 18 features and 4 outcomes for 941 stocks.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 由历史和前瞻性收益暗示的领先和滞后导致了一些数据损失，这种损失随着投资期限的增加而增加。我们最终得到了941支股票的18个特征和4个结果的230万观察结果。
- en: The ML4T workflow with LightGBM
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LightGBM的ML4T工作流程
- en: We will now embark on selecting a random forest model that produces tradeable
    signals. Several studies have done so successfully; see, for instance, Krauss,
    Do, and Huck (2017) and Rasekhschaffe and Jones (2019) and the resources referenced
    there.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将着手选择一个能产生可交易信号的随机森林模型。一些研究已经成功地做到了这一点；例如，参见Krauss，Do和Huck（2017）以及Rasekhschaffe和Jones（2019）以及那里引用的资源。
- en: We will use the fast and memory-efficient LightGBM implementation that's open
    sourced by Microsoft and most popular for gradient boosting, which is the topic
    of the next chapter, where we will take a closer look at the various LightGBM
    features.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由微软开源的快速和内存高效的LightGBM实现，它是梯度提升最受欢迎的工具，这也是下一章的主题，我们将更详细地了解各种LightGBM特性。
- en: We will begin by discussing key experimental design decisions, then build and
    evaluate a predictive model whose signals will drive the trading strategy that
    we will design and evaluate in the final step. Please refer to the notebook `random_forest_return_signals`
    for the code samples in this section, unless otherwise stated.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论关键的实验设计决策，然后构建和评估一个预测模型，其信号将驱动我们将在最后一步设计和评估的交易策略。除非另有说明，请参考笔记本`random_forest_return_signals`中的代码示例。
- en: From universe selection to hyperparameter tuning
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从宇宙选择到超参数调优
- en: 'To develop a trading strategy that uses a machine learning model, we need to
    make several decisions on the scope and design of the model, including:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一个使用机器学习模型的交易策略，我们需要在模型的范围和设计上做出几项决定，包括：
- en: '**Lookback period**: How many historical trading days to use for training'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回溯期**：用于训练的历史交易日数量'
- en: '**Lookahead period**: How many days into the future to predict returns'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**展望期**：预测未来收益的天数'
- en: '**Test period**: For how many consecutive days to make predictions with the
    same model'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试期**：使用相同模型进行连续预测的天数'
- en: '**Hyperparameters**: Which parameters and configurations to evaluate'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数**：要评估哪些参数和配置'
- en: '**Ensembling**: Whether to rely on a single model or some combination of multiple models'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：是依赖于单一模型还是一些组合的多个模型'
- en: To evaluate the options of interest, we also need to select a **universe** and
    **time period** for cross-validation, as well as an out-of-sample test period
    and universe. More specifically, we cross-validate several options for the period
    up to 2017 on a subset of our sample of Japanese stocks.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估感兴趣的选项，我们还需要选择一个**宇宙**和**时间段**进行交叉验证，以及一个样本外测试期和宇宙。更具体地说，我们将在我们的日本股票样本的子集上交叉验证2017年之前的几个选项。
- en: Once we've settled on a model, we'll define trading rules and backtest the strategy
    that uses the signals of our model **out-of-sample** over the last 2 years on
    the complete universe to validate its performance.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了一个模型，我们将定义交易规则，并对使用我们模型的信号在过去2年的完整市场中进行**样本外**验证其表现的策略进行回测。
- en: 'For the time-series cross-validation, we''ll rely on the `MultipleTimeSeriesCV`
    that we developed in *Chapter 7*, *Linear Models – From Risk Factors to Return
    Forecasts*, to parameterize the length of the training and test period while avoiding
    lookahead bias. This custom CV class permits us to:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间序列交叉验证，我们将依赖于我们在*第7章*，*线性模型-从风险因素到收益预测*中开发的`MultipleTimeSeriesCV`，以参数化训练和测试期的长度，同时避免展望偏差。这个自定义的CV类允许我们：
- en: Train the model on a consecutive sample containing `train_length` days for each
    ticker.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每只股票在一个连续的样本上进行`train_length`天的训练。
- en: Validate its performance during a subsequent period containing `test_length`
    days and `lookahead` number of days, apart from the training period, to avoid
    data leakage.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在包含`test_length`天和`lookahead`天数的后续期间验证其表现，除了训练期，以避免数据泄漏。
- en: Repeat for a given number of `n_splits` while rolling the train and validation
    periods forward for `test_length` number of days each time.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在滚动训练和验证期间向前推进`test_length`天的情况下，重复给定数量的`n_splits`。
- en: We'll work on the model selection step in this section and on strategy backtesting
    in the following one.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中进行模型选择步骤，并在下一节中进行策略回测。
- en: Sampling tickers to speed up cross-validation
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抽样股票以加快交叉验证速度
- en: Training a random forest takes quite a bit longer than linear regression and
    depends on the configuration, where the number of trees and their depth are the
    main drivers.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 训练随机森林所需的时间比线性回归长得多，并且取决于配置，树的数量和深度是主要驱动因素。
- en: 'To keep our experiments manageable, we''ll select the 250 most-traded stocks
    over the 2010-17 period to evaluate the performance of different outcomes and
    model configurations, as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的实验可管理，我们将选择2010-17年间交易量最大的250只股票，以评估不同结果和模型配置的表现，如下所示：
- en: '[PRE15]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Defining lookback, lookahead, and roll-forward periods
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义回溯、展望和滚动期
- en: Running our strategy requires training models on a rolling basis, using a certain
    number of trading days (the lookback period) from our universe to learn the model
    parameters and predict the outcome for a certain number of future days. In our
    example, we'll consider 63, 126, 252, 756, and 1,260 trading days for training
    while rolling forward and predicting for 5, 21, or 63 days during each iteration.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 运行我们的策略需要在滚动基础上训练模型，使用一定数量的交易日（回溯期）从我们的宇宙中学习模型参数，并预测未来一定数量的交易日的结果。在我们的示例中，我们将考虑在每次迭代中进行63、126、252、756和1,260个交易日的训练，同时滚动前进并预测5、21或63天。
- en: 'We will pair the parameters in a list for easy iteration and optional sampling
    and/or shuffling, as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把参数配对在一个列表中，以便进行简单的迭代和可选的抽样和/或洗牌，如下所示：
- en: '[PRE16]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Hyperparameter tuning with LightGBM
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用LightGBM进行超参数调优
- en: 'The LightGBM model accepts a large number of parameters, as the documentation
    explains in detail (see [https://lightgbm.readthedocs.io/](https://lightgbm.readthedocs.io/)
    and the next chapter). For our purposes, we just need to enable the random forest
    algorithm by defining `boosting_type`, setting `bagging_freq` to a positive number,
    and setting `objective` to `regression`:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM模型接受大量参数，如文档中详细解释（参见[https://lightgbm.readthedocs.io/](https://lightgbm.readthedocs.io/)和下一章）。对于我们的目的，我们只需要通过定义`boosting_type`启用随机森林算法，将`bagging_freq`设置为正数，并将`objective`设置为`regression`：
- en: '[PRE17]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we select the hyperparameters most likely to affect the predictive accuracy,
    namely:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择最有可能影响预测准确性的超参数，即：
- en: The number of trees to grow for the model (`num_boost_round`)
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型生长的树的数量（`num_boost_round`）
- en: The share of rows (`bagging_fraction`) and columns (`feature_fraction`) used
    for bagging
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于装袋的行数（`bagging_fraction`）和列数（`feature_fraction`）的份额。
- en: The minimum number of samples required in a leaf (`min_data_in_leaf`) to control
    for overfitting
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶子节点中所需的最小样本数（`min_data_in_leaf`）以控制过拟合
- en: Another benefit of LightGBM is that we can evaluate a trained model for a subset
    of its trees (or continue training after a certain number of evaluations), which
    allows us to test multiple `num_iteration` values during a single training session.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM的另一个好处是，我们可以评估训练模型的子树（或在一定数量的评估后继续训练），这使我们能够在单个训练会话中测试多个`num_iteration`值。
- en: Alternatively, you can enable `early_stopping` to interrupt training when the
    loss metric for a validation set no longer improves. However, the cross-validation
    performance estimates will be biased upward as the model uses information on the
    outcome that will not be available under realistic circumstances.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以启用`early_stopping`来在验证集的损失指标不再改善时中断训练。然而，由于模型使用了在现实情况下不可用的结果信息，交叉验证性能估计将被偏向上方。
- en: 'We''ll use the following values for the hyperparameters, which control the
    bagging method and tree growth:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下超参数值来控制装袋方法和树生长：
- en: '[PRE18]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Cross-validating signals over various horizons
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在各种时间跨度上交叉验证信号
- en: To evaluate a model for a given set of hyperparameters, we will generate predictions
    using the lookback, lookahead, and roll-forward periods.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估给定一组超参数的模型，我们将使用回溯、前瞻和滚动前期生成预测。
- en: 'First, we will identify categorical variables because LightGBM does not require
    one-hot encoding; instead, it sorts the categories according to the outcome, which
    delivers better results for regression trees, according to Fisher (1958). We''ll
    create variables to identify different periods:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将识别分类变量，因为LightGBM不需要独热编码；相反，它根据结果对类别进行排序，这对于回归树会产生更好的结果，根据Fisher（1958）的说法。我们将创建变量来识别不同的时期：
- en: '[PRE19]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To this end, we will create the binary LightGBM Dataset and configure `MultipleTimeSeriesCV`
    using the given `train_length` and `test_length`, which determine the number of
    splits for our 2-year validation period:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将创建二进制LightGBM数据集，并使用给定的`train_length`和`test_length`配置`MultipleTimeSeriesCV`，这将决定我们2年验证期间的拆分数量：
- en: '[PRE20]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we take the following steps:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将采取以下步骤：
- en: Select the hyperparameters for this iteration.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择此迭代的超参数。
- en: Slice the binary LightGM Dataset we just created into train and test sets.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们刚刚创建的二进制LightGM数据集切片成训练集和测试集。
- en: Train the model.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: 'Generate predictions for the validation set for a range of `num_iteration`
    settings:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为一系列`num_iteration`设置生成验证集的预测：
- en: '[PRE21]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To evaluate the validation performance, we compute the IC for the complete
    set of predictions, as well as on a daily basis, for a range of numbers of iterations:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估验证性能，我们计算了完整预测集的IC，以及每日基础上的IC，针对一系列迭代次数：
- en: '[PRE22]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now, we need to assess the signal content of the predictions to select a model
    for our trading strategy.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要评估预测的信号内容，以选择我们交易策略的模型。
- en: Analyzing cross-validation performance
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析交叉验证性能
- en: First, we'll take a look at the distribution of the IC for the various train
    and test windows, as well as prediction horizons across all hyperparameter settings.
    Then, we'll take a closer look at the impact of the hyperparameter settings on
    the model's predictive accuracy.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看在所有超参数设置中，各种训练和测试窗口以及预测时间跨度的IC分布。然后，我们将更仔细地研究超参数设置对模型预测准确性的影响。
- en: IC for different lookback, roll-forward, and lookahead periods
  id: totrans-355
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同回溯、滚动前进和前瞻期间的IC
- en: 'The following image illustrates the distribution and quantiles of the daily
    mean IC for four prediction horizons and five training windows, as well as the
    best-performing 21-day test window. Unfortunately, it does not yield conclusive
    insights into whether shorter or longer windows do better, but rather illustrates
    the degree of noise in the data due to the range of model configurations we tested
    and the resulting lack of consistency in outcomes:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片说明了四个预测时间跨度和五个训练窗口的每日平均IC的分布和分位数，以及表现最佳的21天测试窗口。不幸的是，它并没有提供关于较短还是较长窗口表现更好的明确见解，而是说明了由于我们测试的模型配置范围和结果的不一致性而导致的数据噪音程度：
- en: '![](img/B15439_11_14.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_14.png)'
- en: 'Figure 11.14: Distribution of the daily mean information coefficient for various
    model configurations'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14：各种模型配置的每日平均信息系数分布
- en: OLS regression of random forest configuration parameters
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林配置参数的OLS回归
- en: To understand in more detail how the parameters of our experiment affect the
    outcome, we can run an OLS regression of these parameters on the daily mean IC.
    *Figure 11.15* shows the coefficients and confidence intervals for the 1- and
    5-day lookahead periods.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地了解我们实验的参数如何影响结果，我们可以对这些参数在每日平均IC上进行OLS回归。*图11.15*显示了1天和5天前瞻期间的系数和置信区间。
- en: 'All variables are one-hot encoded and can be interpreted relative to the smallest
    category of each that is captured by the constant. The results differ across the
    horizons; the longest training period works best for the 1-day prediction but
    yields the worst performance for 5 days, with no clear patterns. Longer training
    appears to improve the 1-day model up to a certain point, but this is less clear
    for the 5-day model. The only somewhat consistent result seems to suggest a lower
    bagging fraction and higher minimum sample settings:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 所有变量都是独热编码的，并且可以相对于每个变量的最小类别进行解释，该类别由常数捕获。结果在不同的时间段有所不同；最长的训练期对于1天预测效果最好，但对于5天效果最差，没有明显的模式。更长的训练似乎可以改进1天模型，但对于5天模型来说不太明显。唯一比较一致的结果似乎表明较低的装袋分数和较高的最小样本设置：
- en: '![](img/B15439_11_15.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_15.png)'
- en: 'Figure 11.15: OLS coefficients and confidence intervals for the various random
    forest configuration parameters'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：各种随机森林配置参数的OLS系数和置信区间
- en: Ensembling forecasts – signal analysis using Alphalens
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成预测-使用Alphalens进行信号分析
- en: Ultimately, we care about the signal content of the model predictions regarding
    our investment universe and holding period. To this end, we'll evaluate the return
    spread produced by equal-weighted portfolios invested in different quantiles of
    the predicted returns using Alphalens.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们关心模型预测在投资领域和持有期方面的信号内容。为此，我们将使用Alphalens评估在不同预测回报的等权重投资组合中产生的回报差异。
- en: As discussed in *Chapter 4*, *Financial Feature Engineering – How to Research
    Alpha Factors*, Alphalens computes and visualizes various metrics that summarize
    the predictive performance of an Alpha Factor. The notebook `alphalens_signals_quality`
    illustrates how to combine the model predictions with price data in the appropriate
    format using the utility function `get_clean_factor_and_forward_returns`.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第4章*中所讨论的*金融特征工程-如何研究Alpha因子*，Alphalens计算和可视化了各种指标，总结了Alpha因子的预测性能。笔记本`alphalens_signals_quality`说明了如何使用实用程序函数`get_clean_factor_and_forward_returns`将模型预测与价格数据以适当的格式相结合。
- en: To address some of the noise inherent in the CV predictions, we select the top
    three 1-day models according to their mean daily IC and average their results.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决CV预测中固有的一些噪音，我们根据它们的平均每日IC选择了前三个1天模型，并平均了它们的结果。
- en: 'When we provide the resulting signal to Alphalens, we find the following for
    a 1-day holding period:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将结果信号提供给Alphalens时，我们发现以下内容适用于1天持有期：
- en: Annualized alpha of 0.081 and beta of 0.083
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年化α为0.081，β为0.083
- en: A mean period-wise spread between top and bottom quintile returns of 5.16 basis points
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前五分位数回报之间的平均期间差距为5.16个基点
- en: 'The following image visualizes the mean period-wise returns by factor quintile
    and the cumulative daily forward returns associated with the stocks in each quintile:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像可视化了按因子五分位数计算的平均期间回报和与每个五分位数中的股票相关的累积每日前瞻回报：
- en: '![](img/B15439_11_16.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_16.png)'
- en: 'Figure 11.16: Alphalens factor signal evaluation'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16：Alphalens因子信号评估
- en: The preceding image shows that the 1-day ahead predictions appear to contain
    useful trading signals over a short horizon based on the return spread of the
    top and bottom quintiles. We'll now move on and develop and backtest a strategy
    that uses predictions generated by the top ten 1-day lookahead models that produced
    the results shown here for the validation period.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像显示，1天后的预测似乎包含了短期内有用的交易信号，基于前五分位数的回报差异。我们现在将继续开发和回测一个策略，该策略使用了前十个1天前瞻模型生成的预测结果，用于验证期间的结果。
- en: The strategy – backtest with Zipline
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略-使用Zipline进行回测
- en: To design and backtest a trading strategy using Zipline, we need to generate
    predictions for our universe for the test period, ingest the Japanese equity data
    and load the signal into Zipline, set up a pipeline, and define rebalancing rules
    to trigger trades accordingly.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 要设计和回测使用Zipline的交易策略，我们需要为测试期间的我们的投资领域生成预测，将日本股票数据引入，并将信号加载到Zipline中，设置管道，并定义重新平衡规则以相应地触发交易。
- en: Ingesting Japanese Equities into Zipline
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将日本股票引入Zipline
- en: We follow the process described in *Chapter 8*, *The ML4T Workflow – From Model
    to Strategy Backtesting*, to convert our Stooq equity OHLCV data into a Zipline
    bundle. The directory `custom_bundle` contains the preprocessing module that creates
    the asset IDs and metadata, defines an ingest function that does the heavy lifting,
    and registers the bundle with an extension.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循*第8章*中描述的过程，将我们的Stooq股票OHLCV数据转换为Zipline bundle。目录`custom_bundle`包含了创建资产ID和元数据的预处理模块，定义了一个执行繁重工作的摄取函数，并使用扩展注册了bundle。
- en: The folder contains a `README` with additional instructions.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件夹包含了一个带有额外说明的`README`。
- en: Running an in- and out-of-sample strategy backtest
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行样本内和样本外策略回测
- en: The notebook `random_forest_return_signals` shows how to select the hyperparameters
    that produced the best validation IC performance and generate forecasts accordingly.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`random_forest_return_signals`显示了如何选择产生最佳验证IC性能的超参数，并相应地生成预测。
- en: 'We will use our 1-day model predictions and apply some simple logic: we will
    enter long and short positions for the 25 assets with the highest positive and
    lowest negative predicted returns. We will trade every day, as long as there are
    at least 15 candidates on either side, and close out all positions that are not
    among the current top forecasts.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的1天模型预测并应用一些简单的逻辑：我们将对预测回报最高的25个资产和预测回报最低的25个资产进行多头和空头头寸。我们将每天交易，只要每边至少有15个候选者，并清算所有不在当前前景中的头寸。
- en: This time, we will also include a small trading commission of $0.05 per share
    but will not use slippage since we are trading the most liquid Japanese stocks
    with a relatively modest capital base.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们还将包括每股0.05美元的小交易佣金，但不会使用滑点，因为我们正在交易日本最流动的股票，资本基础相对较小。
- en: The results – evaluation with pyfolio
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果-使用pyfolio进行评估
- en: The left panel shown in *Figure 11.17* shows the in-sample (2016-17) and out-of-sample
    (2018-19) performance of the strategy relative to the Nikkei 225, which was mostly
    flat throughout the period.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧面板显示了策略相对于日经225指数的样本内（2016-17）和样本外（2018-19）表现，而该指数在整个时期基本持平。
- en: The strategy earns 10.4 percent for in-sample and 5.5 percent for out-of-sample
    on an annualized basis.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略在样本内年化基础上获得了10.4%的收益，在样本外获得了5.5%的收益。
- en: 'The right panel shows the 3-month rolling Sharpe ratio, which reaches 0.96
    in-sample and 0.61 out-of-sample:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧面板显示了3个月滚动夏普比率，在样本内达到0.96，在样本外达到0.61：
- en: '![](img/B15439_11_17.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_11_17.png)'
- en: 'Figure 11.17: Pyfolio strategy evaluation'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17：Pyfolio策略评估
- en: 'The overall performance statistics highlight cumulative returns of 36.6 percent
    after the (low) transaction costs of $0.05 cent per share, implying an out-of-sample
    alpha of 0.06 and a beta of 0.08 (relative to the NIKKEI 225). The maximum drawdown
    was 11.0 percent in-sample and 8.7 percent out-of-sample:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 总体表现统计数据突显了36.6%的累积回报，扣除（低）每股0.05美分的交易成本后，意味着样本外阿尔法为0.06，相对于日经225的贝塔为0.08。样本内最大回撤为11.0%，样本外为8.7%：
- en: '|  | All | In-sample | Out-of-sample |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| |全部|样本内|样本外|'
- en: '| # Months | 48 | 25 | 23 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|#月份|48|25|23|'
- en: '| Annual return | 8.00% | 10.40% | 5.50% |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|年回报|8.00%|10.40%|5.50%|'
- en: '| Cumulative returns | 36.60% | 22.80% | 11.20% |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|累积回报|36.60%|22.80%|11.20%|'
- en: '| Annual volatility | 10.20% | 10.90% | 9.60% |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|年波动率|10.20%|10.90%|9.60%|'
- en: '| Sharpe ratio | 0.8 | 0.96 | 0.61 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|夏普比率|0.8|0.96|0.61|'
- en: '| Calmar ratio | 0.72 | 0.94 | 0.63 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|Calmar比率|0.72|0.94|0.63|'
- en: '| Stability | 0.82 | 0.82 | 0.64 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|稳定性|0.82|0.82|0.64|'
- en: '| Max drawdown | -11.00% | -11.00% | -8.70% |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|最大回撤|-11.00%|-11.00%|-8.70%|'
- en: '| Sortino ratio | 1.26 | 1.53 | 0.95 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|Sortino比率|1.26|1.53|0.95|'
- en: '| Daily value at risk | -1.30% | -1.30% | -1.20% |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|每日风险价值|-1.30%|-1.30%|-1.20%|'
- en: '| Alpha | 0.08 | 0.11 | 0.06 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|阿尔法|0.08|0.11|0.06|'
- en: '| Beta | 0.06 | 0.04 | 0.08 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|贝塔|0.06|0.04|0.08|'
- en: The pyfolio tearsheets contain lots of additional details regarding exposure,
    risk profile, and other aspects.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: pyfolio的tearsheets包含有关敞口、风险配置和其他方面的大量额外细节。
- en: Summary
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about a new class of model capable of capturing
    a non-linear relationship, in contrast to the classical linear models we had explored
    so far. We saw how decision trees learn rules to partition the feature space into
    regions that yield predictions, and thus segment the input data into specific
    regions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了一种能够捕捉非线性关系的新模型类别，与我们迄今为止探索过的经典线性模型形成对比。我们看到决策树如何学习规则，将特征空间划分为产生预测的区域，并因此将输入数据分割成特定区域。
- en: Decision trees are very useful because they provide unique insights into the
    relationships between features and target variables, and we saw how to visualize
    the sequence of decision rules encoded in the tree structure.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树非常有用，因为它们提供了关于特征和目标变量之间关系的独特见解，我们看到了如何可视化树结构中编码的决策规则序列。
- en: Unfortunately, a decision tree is prone to overfitting. We learned that ensemble
    models and the bootstrap aggregation method manage to overcome some of the shortcomings
    of decision trees and render them useful as components of much more powerful composite
    models.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，决策树容易过拟合。我们了解到集成模型和自助聚合方法设法克服了决策树的一些缺点，并使它们成为更强大的复合模型的组成部分。
- en: In the next chapter, we will explore another ensemble model, boosting, which
    has come to be considered one of the most important machine learning algorithms.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一个集成模型boosting，这已被认为是最重要的机器学习算法之一。
