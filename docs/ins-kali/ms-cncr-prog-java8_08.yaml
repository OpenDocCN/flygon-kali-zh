- en: Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。使用并行流处理大型数据集-映射和减少模型
- en: 'Undoubtedly, the most important innovations introduced in Java 8 are **lambda**
    expressions and **stream** API. A stream is a sequence of elements that can be
    processed in a sequential or parallel way. We can transform the stream applying
    the intermediate operations and then perform a final computation to get the desired
    result (a list, an array, a number, and so on). In this chapter, we will cover
    the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，Java 8引入的最重要的创新是lambda表达式和stream API。流是可以按顺序或并行方式处理的元素序列。我们可以应用中间操作来转换流，然后执行最终计算以获得所需的结果（列表、数组、数字等）。在本章中，我们将涵盖以下主题：
- en: An introduction to streams
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流的介绍
- en: The first example – a numerical summarization application
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个例子-数字摘要应用程序
- en: The second example – an information retrieval search tool
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个例子-信息检索搜索工具
- en: An introduction to streams
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流的介绍
- en: 'A stream is a sequence of data (it is not a data structure) that allows you
    to apply a sequence of operations in a sequential or concurrent way to filter,
    convert, sort, reduce, or organize those elements to obtain a final object. For
    example, if you have a stream with the data of your employees, you can use a stream
    to:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 流是一系列数据（不是数据结构），允许您以顺序或并行方式应用一系列操作来过滤、转换、排序、减少或组织这些元素以获得最终对象。例如，如果您有一个包含员工数据的流，您可以使用流来：
- en: Count the total number of employees
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算员工的总数
- en: Calculate the average salary of all employees who live in a particular place
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算居住在特定地方的所有员工的平均工资
- en: Obtain a list of the employees who haven't met their objectives
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取未达到目标的员工列表
- en: Any operation that implies work with all or part of the employees
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何涉及所有或部分员工的操作
- en: Streams are greatly influenced by functional programming (the **Scala** programming
    language provides a very similar mechanism), and they were thought to work with
    lambda expressions. A stream API resembles **LINQ** (short for **Language-Integrated
    Query**) queries available in C# language and, to some extent, could be compared
    with SQL queries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 流受到函数式编程的极大影响（Scala编程语言提供了一个非常类似的机制），并且它们被设计用于使用lambda表达式。流API类似于C#语言中可用的LINQ（Language-Integrated
    Query）查询，在某种程度上可以与SQL查询进行比较。
- en: In the following sections, we will explain the basic characteristics of streams
    and the parts you will find in a stream.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解释流的基本特性以及您将在流中找到的部分。
- en: Basic characteristics of streams
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流的基本特性
- en: 'The main characteristics of a stream are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 流的主要特点是：
- en: A stream does not store its elements. The stream takes the elements from its
    source and sends them across all the operations that form the pipeline.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流不存储它的元素。流从其源获取元素，并将它们发送到形成管道的所有操作中。
- en: You can work with streams in parallel without any extra work. When you create
    a stream, you can use the `stream()` method to create a sequential stream or `parallelStream()`
    to create a concurrent one. The `BaseStream` interface defines the `sequential()`
    methods to obtain a sequential version of the stream and `parallel()` to obtain
    a concurrent version of the stream. You can convert a sequential stream to parallel
    and a parallel to sequential as many times as you want. Take into account that
    when the terminal stream operation is performed, all the stream operations will
    be processed according to the last setting. You cannot instruct a stream to perform
    some operations sequentially and other operations concurrently. Internally, parallel
    streams in Oracle JDK 8 and Open JDK 8 use an implementation of the Fork/Join
    framework to execute the concurrent operations.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在并行中使用流而无需额外工作。创建流时，您可以使用`stream()`方法创建顺序流，或使用`parallelStream()`创建并发流。`BaseStream`接口定义了`sequential()`方法以获取流的顺序版本，以及`parallel()`以获取流的并发版本。您可以将顺序流转换为并行流，将并行流转换为顺序流，反复多次。请注意，当执行终端流操作时，所有流操作将根据最后的设置进行处理。您不能指示流按顺序执行某些操作，同时按并发方式执行其他操作。在Oracle
    JDK 8和Open JDK 8中，内部使用Fork/Join框架的实现来执行并发操作。
- en: Streams are greatly influenced by the functional programming and the Scala programming
    language. You can use the new lambda expressions as a way to define the algorithm
    to be executed in an operation over a stream.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流受到函数式编程和Scala编程语言的极大影响。您可以使用新的lambda表达式来定义在流操作中执行的算法。
- en: Streams can't be reusable. When you obtain a stream, for example, from a list
    of values, you can use that stream only once. If you want to perform another operation
    on the same data, you have to create one more stream.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流不能重复使用。例如，当您从值列表中获取流时，您只能使用该流一次。如果您想对相同的数据执行另一个操作，您必须创建一个新的流。
- en: Streams make a lazy processing of data. They don't obtain the data until it's
    necessary. As you will learn later, a stream has an origin, some intermediate
    operations, and a terminal operation. The data isn't processed until the terminal
    operation needs it, so the stream processing doesn't begin until the terminal
    operation is executed.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流对数据进行延迟处理。直到必要时才获取数据。正如您将在后面学到的，流有一个起源、一些中间操作和一个终端操作。直到终端操作需要它，数据才会被处理，因此流处理直到执行终端操作才开始。
- en: You can't access the elements of the stream in a different way. When you have
    a data structure, you can access one determined element stored in it, for example,
    indicating its position or its key. Stream operations usually process the elements
    uniformly, so the only thing you have is the element itself. You don't know the
    position of the element in the stream and the neighbor elements. In the case of
    parallel streams, the elements can be processed in any order.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您无法以不同的方式访问流的元素。当您有一个数据结构时，您可以访问其中存储的一个确定的元素，例如指定其位置或其键。流操作通常统一处理元素，因此您唯一拥有的就是元素本身。您不知道元素在流中的位置和相邻元素。在并行流的情况下，元素可以以任何顺序进行处理。
- en: Stream operations don't allow you to modify the stream source. For example,
    if you use a list as the stream source, you can store the processing result into
    the new list, but you cannot add, remove, or replace the elements of the original
    list. Although this sounds restrictive, it's a very useful feature as you can
    return the stream created from your internal collection without a fear that the
    list will be modified by the caller.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流操作不允许您修改流源。例如，如果您将列表用作流源，可以将处理结果存储到新列表中，但不能添加，删除或替换原始列表的元素。尽管听起来很受限制，但这是一个非常有用的功能，因为您可以返回从内部集合创建的流，而不必担心列表将被调用者修改。
- en: Sections of a stream
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流的部分
- en: 'A stream has three different sections:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 流有三个不同的部分：
- en: A **source** that generates the data consumed by the stream.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**源**，生成流所消耗的数据。
- en: Zero or more **intermediate operations**, which generate another stream as an
    output.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零个或多个**中间操作**，生成另一个流作为输出。
- en: One **terminal operation** that generates an object, which can be a simple object
    or a collection as an array, a list, or a hash table. There can be also terminal
    operations that don't produce any explicit result.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**终端操作**，生成一个对象，可以是一个简单对象或一个集合，如数组，列表或哈希表。还可以有不产生任何显式结果的终端操作。
- en: Sources of a stream
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流的源
- en: The source of the stream generates the data that will be processed by the `Stream`
    object. You can create a stream from different sources. For example, the `Collection`
    interface has included the `stream()` methods in Java 8 to generate a sequential
    stream and `parallelStream()` to generate a parallel one. This allows you to generate
    a stream to process all the data from almost all the data structures implemented
    in Java as lists (`ArrayList`, `LinkedList`, and so on), sets (`HashSet`, `EnumSet`),
    or concurrent data structures (`LinkedBlockingDeque`, `PriorityBlockingQueue`,
    and so on). Another data structure that can generate streams is an array. The
    `Array` class includes four versions of the `stream()` method to generate a stream
    from the array. If you pass an array of `int` numbers to the method, it will generate
    `IntStream`. This is a special kind of stream implemented to work with integer
    numbers (you can still use `Stream<Integer>` instead of `IntStream`, but performance
    might be significantly worse). Similarly, you can create `LongStream` or `DoubleStream`
    from the `long[]` or `double[]` array.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 流的源生成将由`Stream`对象处理的数据。您可以从不同的源创建流。例如，`Collection`接口在Java 8中包含了`stream()`方法来生成顺序流，`parallelStream()`来生成并行流。这使您可以生成一个流来处理几乎所有Java中实现的数据结构的数据，如列表（`ArrayList`，`LinkedList`等），集合（`HashSet`，`EnumSet`）或并发数据结构（`LinkedBlockingDeque`，`PriorityBlockingQueue`等）。另一个可以生成流的数据结构是数组。`Array`类包括`stream()`方法的四个版本，用于从数组生成流。如果您将`int`数组传递给该方法，它将生成`IntStream`。这是一种专门用于处理整数的流（您仍然可以使用`Stream<Integer>`而不是`IntStream`，但性能可能会显着下降）。类似地，您可以从`long[]`或`double[]`数组创建`LongStream`或`DoubleStream`。
- en: Of course, if you pass an array of object to the `stream()` method, you will
    obtain a generic stream of the same type. In this case, there is no `parallelStream()`
    method, but once you have obtained the stream, you can call the `parallel()` method
    defined in the `BaseStream` interface to convert the sequential stream into a
    concurrent one.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您将对象数组传递给`stream()`方法，您将获得相同类型的通用流。在这种情况下，没有`parallelStream()`方法，但是一旦您获得了流，您可以调用`BaseStream`接口中定义的`parallel()`方法，将顺序流转换为并发流。
- en: Other interesting functionality provided by the `Stream` API is that you can
    generate and stream to process the content of a directory or a file. The `Files`
    class provides different methods to work with files using streams. For example,
    the `find()` method returns a stream with the `Path` objects of the files in a
    file tree that meet certain conditions. The `list()` method returns a stream of
    the `Path` objects with the contents of a directory. The `walk()` method returns
    a stream of the `Path` objects processing all the objects in a directory tree
    using a depth-first algorithm. But the most interesting method is the `lines()`
    method that creates a stream of `String` objects with the lines of a file, so
    you can process its content using a stream. Unfortunately, all of the methods
    mentioned here parallelize badly unless you have many thousands of elements (files
    or lines).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`Stream` API提供的另一个有趣的功能是，您可以生成并流来处理目录或文件的内容。`Files`类提供了使用流处理文件的不同方法。例如，`find()`方法返回一个流，其中包含满足某些条件的文件树中的`Path`对象。`list()`方法返回一个包含目录内容的`Path`对象的流。`walk()`方法返回一个使用深度优先算法处理目录树中所有对象的`Path`对象流。但最有趣的方法是`lines()`方法，它创建一个包含文件行的`String`对象流，因此您可以使用流来处理其内容。不幸的是，除非您有成千上万的元素（文件或行），这里提到的所有方法都无法很好地并行化。'
- en: 'Also, you can create a stream using two methods provided by the `Stream` interface:
    the `generate()` and `iterate()` methods. The `generate()` method receives a `Supplier`
    parameterized with an object type as a parameter and generates an infinite sequential
    stream of objects of that type. The `Supplier` interface has the `get()` method.
    Every time the stream needs a new object it will call this method to obtain the
    next value of the stream. As we mentioned earlier, streams process the data in
    a lazy way, so there is no problem with the infinite nature of the stream. You
    will use other methods that will convert that stream in finite way. The `iterate()`
    method is similar, but in this case, the method receives a seed and a `UnaryOperator`.
    The first value is the result of apply the `UnaryOperator` to the seed; the second
    value is the result of apply the `UnaryOperator` to the first result, and so on.
    This method should be avoided as much as possible in concurrent applications because
    of their performance.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以使用`Stream`接口提供的两种方法来创建流：`generate()`和`iterate()`方法。`generate()`方法接收一个参数化为对象类型的`Supplier`作为参数，并生成该类型的对象的无限顺序流。`Supplier`接口具有`get()`方法。每当流需要一个新对象时，它将调用此方法来获取流的下一个值。正如我们之前提到的，流以一种懒惰的方式处理数据，因此流的无限性质并不成问题。您将使用其他方法将该流转换为有限方式。`iterate()`方法类似，但在这种情况下，该方法接收一个种子和一个`UnaryOperator`。第一个值是将`UnaryOperator`应用于种子的结果；第二个值是将`UnaryOperator`应用于第一个结果的结果，依此类推。在并发应用程序中应尽量避免使用此方法，因为它们的性能问题。
- en: 'There''s also more stream sources as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多的流来源如下：
- en: '`String.chars()`: This returns a `IntStream` with the `char` values of the
    `String`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`String.chars()`: 返回一个`IntStream`，其中包含`String`的`char`值。'
- en: '`Random.ints()`, `Random.doubles()`, or `Random.longs()`: This returns `IntStream`,
    `DoubleStream`, and `LongStream`, respectively with pseudorandom values. You can
    specify the range of numbers between the random numbers or the number or random
    values that you want to obtain. For example, you can generate pseudorandom numbers
    between 10 and 20 using `new Random.ints(10,20)`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Random.ints()`、`Random.doubles()`或`Random.longs()`: 分别返回`IntStream`、`DoubleStream`和`LongStream`，具有伪随机值。您可以指定随机数之间的范围，或者您想要获取的随机值的数量。例如，您可以使用`new
    Random.ints(10,20)`生成10到20之间的伪随机数。'
- en: 'The `SplittableRandom` class: This class provides the same methods as the `Random`
    class to generate pseudorandom `int`, `double`, and `long` values, but is more
    suitable for parallel processing. You can check the Java API documentation to
    get the details of this class.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SplittableRandom`类：这个类提供了与`Random`类相同的方法，用于生成伪随机的`int`、`double`和`long`值，但更适合并行处理。您可以查看Java
    API文档以获取该类的详细信息。'
- en: 'The `Stream.concat()` method: This receives two streams as parameters and creates
    a new stream with the elements of the first stream followed by the elements of
    the second stream.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Stream.concat()`方法：这个方法接收两个流作为参数，并创建一个新的流，其中包含第一个流的元素，后跟第二个流的元素。'
- en: You can generate streams from other sources, but we think they are not significant.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从其他来源生成流，但我们认为它们不重要。
- en: Intermediate operations
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中间操作
- en: 'The most important characteristic of an intermediate operation is that they
    return another stream as their result. The objects of the input and output stream
    can be of a different type, but an intermediate operation always will generate
    a new stream. You can have zero or more intermediate operations in a stream. The
    most important intermediate operations provided by the `Stream` interface are:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 中间操作的最重要特征是它们将另一个流作为它们的结果返回。输入流和输出流的对象可以是不同类型的，但中间操作总是会生成一个新的流。在流中可以有零个或多个中间操作。`Stream`接口提供的最重要的中间操作是：
- en: '`distinct()`: This method returns a stream with unique values. All the repeated
    elements will be eliminated'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distinct()`: 这个方法返回一个具有唯一值的流。所有重复的元素将被消除'
- en: '`filter()`: This method returns a stream with the elements that meet certain
    criteria'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter()`: 这个方法返回一个满足特定条件的元素的流'
- en: '`flatMap()`: This method is used to convert a stream of streams (for example,
    a stream of list, sets, and so on) in a single stream'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap()`: 这个方法用于将流的流（例如，列表流，集合流等）转换为单个流'
- en: '`limit()`: This method returns a stream that contains at the most the specified
    number of the original elements in the encounter order starting from the first
    element'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limit()`: 这个方法返回一个包含最多指定数量的原始元素的流，按照首个元素的顺序开始'
- en: '`map()`: This method is used to transform the elements of a stream for one
    type to another'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map()`: 这个方法用于将流的元素从一种类型转换为另一种类型'
- en: '`peek()`: This method returns the same stream, but it executes some code; normally,
    it is used to write log messages'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peek()`: 这个方法返回相同的流，但它执行一些代码；通常用于编写日志消息'
- en: '`skip()`: This method ignores the first elements (the concrete number is passed
    as parameter) of the stream'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip()`: 这个方法忽略流的前几个元素（具体数字作为参数传递）'
- en: '`sorted()`: This method sorts the elements of the stream'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sorted()`: 这个方法对流的元素进行排序'
- en: Terminal operations
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 终端操作
- en: 'A terminal operation returns an object as a result. It never returns a stream.
    In general, all streams will end with a terminal operation that returns the final
    result of all the sequence of operations. The most important terminal operations
    are:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 终端操作返回一个对象作为结果。它永远不会返回一个流。一般来说，所有流都将以一个终端操作结束，该操作返回所有操作序列的最终结果。最重要的终端操作是：
- en: '`collect()`: This method provides a way to reduce the number of elements of
    the source stream organizing the elements of the stream in a data structure. For
    example, you want to group the elements of your stream by any criterion.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect()`: 这个方法提供了一种方法来减少源流的元素数量，将流的元素组织成数据结构。例如，您想按任何标准对流的元素进行分组。'
- en: '`count()`: This returns the number of elements of the stream.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count()`: 返回流的元素数量。'
- en: '`max()`: This returns the maximum element of the stream.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max()`: 返回流的最大元素。'
- en: '`min()`: This returns the minimum element of the stream.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min()`: 这返回流的最小元素。'
- en: '`reduce()`: This method transforms the elements of the stream in a unique object
    that represents the stream.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce()`: 这种方法将流的元素转换为表示流的唯一对象。'
- en: '`forEach()`/`forEachOrdered()`: This methods apply an action to every element
    in the stream. The second method uses the order of the elements of the stream
    if the stream has a defined order.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEach()`/`forEachOrdered()`: 这些方法对流中的每个元素应用操作。如果流有定义的顺序，第二种方法使用流的元素顺序。'
- en: '`findFirst()`/`findAny()`: This returns `1` or the first element of the stream,
    respectively, if they exist.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`findFirst()`/`findAny()`: 如果存在，分别返回`1`或流的第一个元素。'
- en: '`anyMatch()`/`allMatch()`/`noneMatch()`: They receive a predicate as a parameter
    and return a Boolean value to indicate if any, all, or none elements of the stream
    match the predicate.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`anyMatch()`/`allMatch()`/`noneMatch()`: 它们接收一个谓词作为参数，并返回一个布尔值，指示流的任何、所有或没有元素是否与谓词匹配。'
- en: '`toArray()`: This method returns an array with the elements of the stream.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`toArray()`: 这种方法返回流的元素数组。'
- en: MapReduce versus MapCollect
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce与MapCollect
- en: 'MapReduce is a programming model to process very large datasets in distributed
    environments with a lot of machines working in a cluster. It has two steps generally
    implemented by two methods:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是一种编程模型，用于在具有大量机器的集群中处理非常大的数据集。通常由两种方法实现两个步骤：
- en: '**Map**: This filters and transforms the data'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Map**: 这过滤和转换数据。'
- en: '**Reduce**: This applies a summary operation in the data'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reduce**: 这对数据应用汇总操作'
- en: To make this operation in a distributed environment, we have to split the data
    and then distribute over the machines of the cluster. This programming model has
    been used for a long time in the functional programming world. Google recently
    developed a framework based on this principle, and in the **Apache Foundation**,
    the **Hadoop** project is very popular as an open source implementation of this
    model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要在分布式环境中执行此操作，我们必须拆分数据，然后分发到集群的机器上。这种编程模型在函数式编程世界中已经使用了很长时间。谷歌最近基于这一原则开发了一个框架，在**Apache基金会**中，**Hadoop**项目作为这一模型的开源实现非常受欢迎。
- en: Java 8 with streams allows programmers to implement something very similar to
    this. The `Stream` interface defines intermediate operations (`map()`, `filter()`,
    `sorted()`, `skip()`, and so on) that can be considered as map functions, and
    it provides the `reduce()` method as a terminal operation whose main objective
    is to make a reduction of the elements of the stream as the reduction of the MapReduce
    model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Java 8与流允许程序员实现与此非常相似的东西。`Stream`接口定义了中间操作(`map()`, `filter()`, `sorted()`,
    `skip()`等)，可以被视为映射函数，并且它提供了`reduce()`方法作为终端操作，其主要目的是对流的元素进行减少，就像MapReduce模型的减少一样。
- en: The main idea of the `reduce` operation is to create a new intermediate result
    based on a previous intermediate result and a stream element. The alternative
    way of reduction (also named mutable reduction) is to incorporate the new resulting
    item into the mutable container (for example, adding it into `ArrayList`). This
    kind of reduction is performed by the `collect()` operation, and we will name
    it as a **MapCollect** model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce`操作的主要思想是基于先前的中间结果和流元素创建新的中间结果。另一种减少的方式(也称为可变减少)是将新的结果项合并到可变容器中(例如，将其添加到`ArrayList`中)。这种减少是通过`collect()`操作执行的，我们将其称为**MapCollect**模型。'
- en: We will see how to work with the MapReduce model in this chapter and how to
    work with the MapCollect model in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将看到如何使用MapReduce模型，以及如何在[第8章](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "第8章。使用并行流处理大规模数据集-Map和Collect模型")中使用MapCollect模型。*使用并行流处理大规模数据集-Map和Collect模型*。
- en: The first example – a numerical summarization application
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个示例-数值汇总应用程序
- en: One of the most common needs when you have a big set of data is to process its
    elements to measure certain characteristics. For example, if you have a set with
    the products purchased in a shop, you can count the number of products you have
    sold, the number of units per product you have sold, or the average amount that
    each customer spent on it. We have named that process **numerical summarization**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当您拥有大量数据集时，最常见的需求之一是处理其元素以测量某些特征。例如，如果您有一个商店中购买的产品集合，您可以计算您销售的产品数量，每种产品的销售单位数，或者每位客户在其上花费的平均金额。我们称这个过程为**数值汇总**。
- en: In this chapter, we are going to use streams to obtain some measures of the
    **Bank Marketing** dataset of the **UCI Machine Learning Repository** that you
    can download from [http://archive.ics.uci.edu/ml/datasets/Bank+Marketing](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing).
    Specifically, we have used the `bank-additional-full.csv` file. This dataset stores
    information about marketing campaigns of a Portuguese banking institution.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用流来获取**UCI机器学习库**的**银行营销**数据集的一些度量，您可以从[http://archive.ics.uci.edu/ml/datasets/Bank+Marketing](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing)下载。具体来说，我们使用了`bank-additional-full.csv`文件。该数据集存储了葡萄牙银行机构营销活动的信息。
- en: Unlike other chapters, in this case, we explain the concurrent version using
    streams and then how to implement a serial equivalent version to verify that concurrency
    improves performance with streams too. Take into account that concurrency is transparent
    for the programmer, as we mentioned in the introduction of the chapter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他章节不同的是，在这种情况下，我们首先解释使用流的并发版本，然后说明如何实现串行等效版本，以验证并发对流的性能也有所改进。请注意，并发对程序员来说是透明的，正如我们在本章的介绍中提到的那样。
- en: The concurrent version
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发版本
- en: 'Our numerical summarization application is very simple. It has the following
    components:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数值汇总应用程序非常简单。它具有以下组件：
- en: '`Record`: This class defines the internal structure of every record of the
    file. It defines the 21 attributes of every record and the corresponding `get()`
    and `set()` method to establish their values. Its code is very simple, so it won''t
    be included in the book.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Record`：这个类定义了文件中每条记录的内部结构。它定义了每条记录的21个属性和相应的`get()`和`set()`方法来建立它们的值。它的代码非常简单，所以不会包含在书中。'
- en: '`ConcurrentDataLoader`: This class will load the `bank-additional-full.csv`
    file with the data and convert it to a list of `Record` objects. We will use streams
    to load the data and make the conversion.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConcurrentDataLoader`：这个类将加载`bank-additional-full.csv`文件中的数据，并将其转换为`Record`对象的列表。我们将使用流来加载数据并进行转换。'
- en: '`ConcurrentStatistics`: This class implements the operations that we will use
    to make the calculations over the data.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConcurrentStatistics`：这个类实现了我们将用来对数据进行计算的操作。'
- en: '`ConcurrentMain`: This class implements the `main()` method to call the operations
    of the `ConcurrentStatistics` class and measure its execution time.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConcurrentMain`：这个类实现了`main()`方法，调用`ConcurrentStatistics`类的操作并测量其执行时间。'
- en: Let's describe the last three classes in detail.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细描述最后三个类。
- en: The ConcurrentDataLoader class
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`ConcurrentDataLoader`类'
- en: 'The `ConcurrentDataLoader` class implements the `load()` method that loads
    the file with the Bank Marketing dataset and converts it to a list of `Record`
    objects. First, we use the method `readAllLines()` of the `Files` method to load
    the file and convert its contents in a list of `String` objects. Every line of
    the file will be converted in an element of the list:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConcurrentDataLoader`类实现了`load()`方法，加载银行营销数据集的文件并将其转换为`Record`对象的列表。首先，我们使用`Files`方法的`readAllLines()`方法加载文件并将其内容转换为`String`对象的列表。文件的每一行将被转换为列表的一个元素：'
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we apply the necessary operations to the stream to get the list of `Record`
    objects:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对流应用必要的操作来获取`Record`对象的列表：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The operations we use are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的操作有：
- en: '`parallelStream()`: We create a parallel stream to process all the lines of
    the file.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallelStream()`：我们创建一个并行流来处理文件的所有行。'
- en: '`skip(1)`: We ignore the first item of the stream; in this case, the first
    line of the file, which contains the headers of the file.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip(1)`：我们忽略流的第一个项目；在这种情况下，文件的第一行，其中包含文件的标题。'
- en: '`map (l → l.split(";"))`: We convert each string in a `String[]` array dividing
    the line by the `;` character. We use a lambda expression where `l` represents
    the input parameter and `l.split()` will generate the array of strings. We call
    this method in a stream of strings, and it will generate a stream of `String[]`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map (l → l.split(";"))`：我们将每个字符串转换为`String[]`数组，通过`；`字符分割行。我们使用lambda表达式，其中`l`表示输入参数，`l.split()`将生成字符串数组。我们在字符串流中调用此方法，它将生成`String[]`流。'
- en: '`map(t → new Record(t))`: We convert each array of string in a `Record` object
    using the constructor of the `Record` class. We use a lambda expression where
    `t` represents the array of strings. We call this method in a stream of `String[]`,
    and we generate a stream of `Record` objects.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map(t → new Record(t))`：我们使用`Record`类的构造函数将每个字符串数组转换为`Record`对象。我们使用lambda表达式，其中`t`表示字符串数组。我们在`String[]`流中调用此方法，并生成`Record`对象流。'
- en: '`collect(Collectors.toList())`: This method converts the stream into a list.
    We will work with the `collect` method in more detail in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect(Collectors.toList())`：这个方法将流转换为列表。我们将在[第8章](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "第8章。使用并行流处理大型数据集-映射和收集模型")中更详细地讨论`collect`方法，*使用并行流处理大型数据集-映射和收集模型*。'
- en: 'As you can see, we have made the transformation in a compact, elegant, and
    concurrent way without the utilization of any thread, task, or framework. Finally,
    we return the list of `Record` objects, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们以一种紧凑、优雅和并发的方式进行了转换，而没有使用任何线程、任务或框架。最后，我们返回`Record`对象的列表，如下所示：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The ConcurrentStatistics class
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`ConcurrentStatistics`类'
- en: The `ConcurrentStatistics` class implements the methods that make the calculus
    over the data. We have seven different operations to obtain information about
    the dataset. Let's describe each of them.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConcurrentStatistics`类实现了对数据进行计算的方法。我们有七种不同的操作来获取关于数据集的信息。让我们描述每一个。'
- en: Job information from subscribers
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 订阅者的工作信息
- en: The main objective of this method is to obtain the number of people per job
    type (field job) for those people who have subscribed a bank deposit (field subscribe
    equals to `yes`).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的主要目标是获取订阅了银行存款（字段subscribe等于`yes`）的人员职业类型（字段job）的人数。
- en: 'This is the source code of this method:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个方法的源代码：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The method receives the list of `Record` objects as input parameters. First,
    we use a stream to obtain a `ConcurrentMap<String, List<Record>>` object where
    there are different job types and the list includes the records of each job type.
    This stream starts with the `parallelStream()`method to create a parallel stream.
    Then, we use the `filter()` method to select those `Record` objects with the subscribe
    attribute to `yes`. Finally, we use the `collect()` method passing the `Collectors.groupingByConcurrent()`method
    to group the actual elements of the stream by the values of the job attribute.
    Take into account that the `groupingByConcurrent()` method is an unordered collector.
    The records collected into the list can be in arbitrary order, not in the original
    order (unlike the simple `groupingBy()` collector).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`Record`对象的列表作为输入参数。首先，我们使用流来获取一个`ConcurrentMap<String, List<Record>>`对象，其中包含不同的工作类型和每种工作类型的记录列表。该流以`parallelStream()`方法开始，创建一个并行流。然后，我们使用`filter()`方法选择那些`subscribe`属性为`yes`的`Record`对象。最后，我们使用`collect()`方法传递`Collectors.groupingByConcurrent()`方法，将流的实际元素按照工作属性的值进行分组。请注意，`groupingByConcurrent()`方法是一个无序收集器。收集到列表中的记录可能是任意顺序的，而不是原始顺序（不像简单的`groupingBy()`收集器）。
- en: Once we have the `ConcurrentMap` object, we use the `forEach()` method to write
    the information on the screen.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`ConcurrentMap`对象，我们使用`forEach()`方法将信息写入屏幕。
- en: Age data from subscribers
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 订阅者的年龄数据
- en: The main objective of this method is to obtain statistical information (maximum,
    minimum, and average values) from the age of the subscribers of a bank deposit
    (field subscribe equals to `yes`).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要目标是从银行存款的订阅者的年龄（字段subscribe等于`yes`）中获取统计信息（最大值、最小值和平均值）。
- en: 'This is the source code of the method:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该方法的源代码：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This method receives the list of `Record` objects as input parameters and uses
    a stream to get a `DoubleSummaryStatistics` object with the statistical information.
    First, we use the `parallelStream()` method to get a parallel stream. Then, we
    use the `filter()` method to obtain the subscribers of a bank deposit. Finally,
    we use the `collect()` method with the `Collectors.summarizingDouble()` parameter
    to obtain the `DoubleSummaryStatistics` object. This class implements the `DoubleConsumer`
    interface and collects statistical data of the values it receives in the `accept()`
    method. This `accept()` method is called internally by the `collect()` method
    of the stream. Java provides the `IntSummaryStatistics` and `LongSummaryStatistics`
    classes also to obtain statistical data from the `int` and `long` values. In this
    case, we use the `max()`, `min()`, and `average()`methods to obtain the maximum,
    minimum, and average values, respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`Record`对象的列表作为输入参数，并使用流来获取带有统计信息的`DoubleSummaryStatistics`对象。首先，我们使用`parallelStream()`方法获取并行流。然后，我们使用`filter()`方法获取银行存款的订阅者。最后，我们使用带有`Collectors.summarizingDouble()`参数的`collect()`方法来获取`DoubleSummaryStatistics`对象。该类实现了`DoubleConsumer`接口，并在`accept()`方法中收集接收到的值的统计数据。`accept()`方法由流的`collect()`方法在内部调用。Java还提供了`IntSummaryStatistics`和`LongSummaryStatistics`类，用于从`int`和`long`值获取统计数据。在这种情况下，我们使用`max()`、`min()`和`average()`方法分别获取最大值、最小值和平均值。
- en: Marital data from subscribers
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 订阅者的婚姻数据
- en: The main objective of this method is to obtain the different marital status
    (field marital) of the subscribers of a bank deposit (field subscribe equals to
    `yes`).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要目标是获取银行存款订阅者的不同婚姻状况（字段婚姻）。
- en: 'This is the source code of the method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该方法的源代码：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The method receives the list of `Record` objects as input parameters and uses
    the `parallelStream()` method to get a parallel stream. Then, we use the `filter()`
    method to only get the subscribers of a bank deposit. Then, we use the `map()`
    method to obtain a stream of `String` objects with the marital status of all the
    subscribers. With the `distinct()` method, we take only the unique values, and
    with the `sorted()` method, we sort those values alphabetically. Finally, we are
    using `forEachOrdered()` to print the result. Be careful not to use `forEach()`
    here as it will print the results in no particular order, which would make the
    `sorted()` step useless. The `forEach()` operation is useful when the elements
    order is not important and may work much faster for parallel streams than `forEachOrdered()`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`Record`对象的列表作为输入参数，并使用`parallelStream()`方法获取并行流。然后，我们使用`filter()`方法仅获取银行存款的订阅者。接下来，我们使用`map()`方法获取所有订阅者的婚姻状况的`String`对象流。使用`distinct()`方法，我们只取唯一的值，并使用`sorted()`方法按字母顺序排序这些值。最后，我们使用`forEachOrdered()`打印结果。请注意，不要在这里使用`forEach()`，因为它会以无特定顺序打印结果，这将使`sorted()`步骤变得无用。当元素顺序不重要且可能比`forEachOrdered()`更快时，`forEach()`操作对于并行流非常有用。
- en: Campaign data from nonsubscribers
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非订阅者的联系人数据
- en: One of the most common mistakes we have when we use streams is to try to reuse
    a stream. We will show you the consequences of this mistake with this method,
    the main objective of which is to obtain the maximum number of contacts (attribute
    campaign).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用流时，最常见的错误之一是尝试重用流。我们将通过这个方法展示这个错误的后果，该方法的主要目标是获取最大联系人数（属性campaign）。
- en: 'The first version of the method is to try to reuse a stream. This is its source
    code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的第一个版本是尝试重用流。以下是其源代码：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The method receives the list of `Record` objects as input parameters. First,
    we create an `IntStream` object using that list. With the `parallelStream()` method,
    we create a parallel stream. Then, we use the `filter()` method to get the nonsubscribers
    and the `mapToInt()` method to convert a stream of `Record` objects in an `IntStream`
    object replacing each object by the value of the `getCampaign()` method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`Record`对象的列表作为输入参数。首先，我们使用该列表创建一个`IntStream`对象。使用`parallelStream()`方法创建并行流。然后，我们使用`filter()`方法获取非订阅者，并使用`mapToInt()`方法将`Record`对象流转换为`IntStream`对象，将每个对象替换为`getCampaign()`方法的值。
- en: We try to use that stream to get the maximum value, with the `max()` method,
    and the minimum value, with the `min()` method. If we execute this method, we
    will obtain `IllegalStateException` in the second call with the message **the
    stream has already been operated upon or closed**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试使用该流获取最大值（使用`max()`方法）和最小值（使用`min()`方法）。如果执行此方法，我们将在第二次调用中获得`IllegalStateException`，并显示消息**流已经被操作或关闭**。
- en: 'We can solve this problem creating two different streams, one to obtain the
    maximum and the other to obtain the minimum. This is the source code of this option:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建两个不同的流来解决这个问题，一个用于获取最大值，另一个用于获取最小值。这是此选项的源代码：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Another option is to use the `summaryStatistics()` method to obtain an `IntSummaryStatistics`
    object, as we showed in a previous method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是使用`summaryStatistics()`方法获取一个`IntSummaryStatistics`对象，就像我们在之前的方法中展示的那样。
- en: Multiple data filter
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多数据过滤
- en: 'The main objective of this method is to obtain the number of records that meet
    at most one of the following conditions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要目标是获取满足以下条件之一的记录数量：
- en: The `defaultCredit` attribute takes the value `true`
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`defaultCredit`属性取值为`true`'
- en: The `housing` attribute takes the value `false`
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`housing`属性取值为`false`'
- en: The `loan` attribute takes the value `false`
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loan`属性取值为`false`'
- en: 'One solution to implement this method is to implement a filter that checks
    whether the elements meet one of these conditions. You can implement other solutions
    with the `concat()` method provided by the `Stream` interface. This is the source
    code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此方法的一种解决方案是实现一个过滤器，检查元素是否满足这些条件之一。您还可以使用`Stream`接口提供的`concat()`方法实现其他解决方案。这是源代码：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method receives the list of `Record` objects as input parameters. First,
    we create three streams with the elements that meet each of the conditions and
    then we use the `concat()` method to generate a single stream. The `concat()`
    method only creates a stream with the elements of the first stream followed by
    the elements of the second stream. For this reason, with the final stream, we
    use the `parallel()` method to convert the final stream in a parallel once, the
    `unordered()` method to get an unordered stream that will give us better performance
    in the `distinct()` method with parallel streams and the `distinct()` method to
    get only unique values, and the `count()` method to obtain the number of elements
    in the stream.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`Record`对象列表作为输入参数。首先，我们创建三个满足每个条件的元素流，然后使用`concat()`方法生成单个流。`concat()`方法只创建一个流，其中包含第一个流的元素，然后是第二个流的元素。因此，对于最终流，我们使用`parallel()`方法将最终流转换为并行流，`unordered()`方法获取无序流，这将在使用并行流的`distinct()`方法中提供更好的性能，`distinct()`方法获取唯一值，以及`count()`方法获取流中的元素数量。
- en: This is not the most optimal solution. We have used it to show you how the `concat()`
    and `distinct()` methods work. You can implement the same in a more optimal way
    using the following code
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是最优的解决方案。我们使用它来向您展示`concat()`和`distinct()`方法的工作原理。您可以使用以下代码以更优化的方式实现相同的功能
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We create a stream of three predicates and reduce them via the `Predicate::or`
    operation to create the compound predicate, which is `true` when either of the
    input predicates is `true`. You can also use the `Predicate::and` reduction operation
    to create a predicate, which is `true` when all the input predicates are `true`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了三个谓词的流，并通过`Predicate::or`操作将它们减少为一个复合谓词，当输入谓词之一为`true`时，该复合谓词为`true`。您还可以使用`Predicate::and`减少操作来创建一个谓词，当所有输入谓词都为`true`时，该谓词为`true`。
- en: Duration data from nonsubscribers
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非订阅者的持续时间数据
- en: The main objective of this method is to obtain the 10 longest phone calls (duration
    attribute) to people that finally didn't subscribe a bank deposit (field subscribe
    equals to `no`).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要目标是获取最长的10次电话通话（持续时间属性），这些通话最终没有订阅银行存款（字段subscribe等于`no`）。
- en: 'This is the source code of this method:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这是此方法的源代码：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The method receives the list of `Record` objects as input parameters and uses
    the `parallelStream()`method to get a parallel stream. We use the `filter()` method
    to get the nonsubscribers. Then, we use the `sorted()` method and we pass a comparator.
    The comparator is created using the `Comparator.comparingInt()` static method.
    As we need to sort in reverse order (biggest duration first), we simply add the
    `reversed()` method to the created comparator. The `sorted()` method uses that
    comparator to compare and sort the elements of the stream, so we can obtain the
    elements sorted as we want.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`Record`对象列表作为输入参数，并使用`parallelStream()`方法获取并行流。我们使用`filter()`方法获取非订阅者。然后，我们使用`sorted()`方法并传递一个比较器。比较器是使用`Comparator.comparingInt()`静态方法创建的。由于我们需要按照相反的顺序排序（最长持续时间优先），我们只需将`reversed()`方法添加到创建的比较器中。`sorted()`方法使用该比较器来比较和排序流的元素，因此我们可以按照我们想要的方式获取排序后的元素。
- en: Once the elements have been sorted, we get the first 10 results using the `limit()`
    method and print the result using the `forEachOrdered()` method.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 元素排序后，我们使用`limit()`方法获取前10个结果，并使用`forEachOrdered()`方法打印结果。
- en: People aged between 25 and 50
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 年龄在25到50岁之间的人
- en: The main objective of this method is to obtain the number of people in the file
    with an age between 25 and 50.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的主要目标是获取文件中年龄在25到50岁之间的人数。
- en: 'This is the source code of this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是此方法的源代码：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The method receives the list of `Record` objects as input parameters and uses
    the `parallelStream()`method to get a parallel stream. Then, we use the `map()`
    method to transform the stream of `Record` objects into a stream of `int` values
    replacing each object by the value of its age attribute. Then, we use the `filter()`
    method to select only the people with an age of 25 and 50 and the `map()` method
    again to convert each value into `1`. Finally, we use the `reduce()` method to
    sum all those 1s and obtain the total number of people between 25 and 50\. The
    first parameter of the `reduce()` method is the identity value, and the second
    parameter is the operation that is used to obtain a single value from all the
    elements of the stream. In this case, we use the `Integer::sum` operation. The
    first sum is between the initial and the first value of the stream, the second
    sum between the result of the first sum and the second value of the stream, and
    so on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`Record`对象的列表作为输入参数，并使用`parallelStream()`方法获取并行流。然后，我们使用`map()`方法将`Record`对象流转换为`int`值流，将每个对象替换为其年龄属性的值。然后，我们使用`filter()`方法仅选择年龄在25到50岁之间的人，并再次使用`map()`方法将每个值转换为`1`。最后，我们使用`reduce()`方法对所有这些`1`进行求和，得到25到50岁之间的人的总数。`reduce()`方法的第一个参数是身份值，第二个参数是用于从流的所有元素中获得单个值的操作。在这种情况下，我们使用`Integer::sum`操作。第一次求和是在流的初始值和第一个值之间进行的，第二次求和是在第一次求和的结果和流的第二个值之间进行的，依此类推。
- en: The ConcurrentMain class
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`ConcurrentMain`类'
- en: 'The `ConcurrentMain` class implements the `main()` method to test the `ConcurrentStatistic`
    class. First, we implement the `measure()` method that measures the execution
    time of a task:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConcurrentMain`类实现了`main()`方法来测试`ConcurrentStatistic`类。首先，我们实现了`measure()`方法，用于测量任务的执行时间：'
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We use a map to store all the execution times of every method. We are going
    to execute each method 10 times to see how the execution time decreases after
    the first execution. Then, we include the `main()` method code. It uses the `measure()`
    method to measure the execution time of every method and repeats this process
    10 times:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个映射来存储每个方法的所有执行时间。我们将执行每个方法10次，以查看第一次执行后执行时间的减少。然后，我们包括`main()`方法的代码。它使用`measure()`方法来测量每个方法的执行时间，并重复这个过程10次：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we write in the console all the execution times and the average execution
    time, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在控制台中写入所有执行时间和平均执行时间，如下所示：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The serial version
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 串行版本
- en: In this case, the serial version is almost equal to the concurrent one. We only
    replace all the calls to the `parallelStream()` method by calls to the `stream()`
    method to obtain a sequential stream instead of a parallel stream. We also have
    to delete the call to the `parallel()` method we use in one of the samples and
    change the call to the `groupingByConcurrent()` method to `groupingBy()`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，串行版本几乎等于并行版本。我们只需将所有对`parallelStream()`方法的调用替换为对`stream()`方法的调用，以获得顺序流而不是并行流。我们还必须删除我们在其中一个示例中使用的`parallel()`方法的调用，并将对`groupingByConcurrent()`方法的调用更改为`groupingBy()`。
- en: Comparing the two versions
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较两个版本
- en: 'We have executed both versions of the operations to test if the use of parallel
    streams provides us with a better performance. We have executed them using the
    JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods such as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. Take into
    account that we have implemented a special class to execute the JMH tests. You
    can find these classes in the `com.javferna.packtpub.mastering.numericalSummarization.benchmark`
    package of the source code. These are the results in milliseconds:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经执行了操作的两个版本，以测试并行流的使用是否提供更好的性能。我们使用了JMH框架（[http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/)）来执行它们，该框架允许您在Java中实现微基准测试。使用基准测试框架比简单地使用`currentTimeMillis()`或`nanoTime()`等方法来测量时间更好。我们在一个四核处理器的计算机上执行了它们10次，并计算了这10次的平均执行时间。请注意，我们已经实现了一个特殊的类来执行JMH测试。您可以在源代码的`com.javferna.packtpub.mastering.numericalSummarization.benchmark`包中找到这些类。以下是以毫秒为单位的结果：
- en: '| Operation | Sequential streams | Parallel streams |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 顺序流 | 并行流 |'
- en: '| --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Job info | 13.704 | 9.550 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 作业信息 | 13.704 | 9.550 |'
- en: '| Age info | 7.218 | 5.512 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 年龄信息 | 7.218 | 5.512 |'
- en: '| Marital info | 8.551 | 6.783 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 婚姻信息 | 8.551 | 6.783 |'
- en: '| Multiple filter | 27.002 | 23.668 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 多重过滤 | 27.002 | 23.668 |'
- en: '| Multiple filter with predicate | 9.413 | 6.963 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 具有谓词的多重过滤 | 9.413 | 6.963 |'
- en: '| Duration data | 41.762 | 23.641 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 数据持续时间 | 41.762 | 23.641 |'
- en: '| Number of contacts | 22.148 | 13.059 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 联系人数 | 22.148 | 13.059 |'
- en: '| People between 25 and 50 | 9.102 | 6.014 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 年龄在25到50岁之间的人 | 9.102 | 6.014 |'
- en: 'We can see how parallel streams always get a better performance than the serial
    streams. This is the speed-up for all the examples:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，并行流始终比串行流获得更好的性能。这是所有示例的加速比：
- en: '| Operation | Speed-up |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 加速比 |'
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Job info | 1.30 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 作业信息 | 1.30 |'
- en: '| Age info | 1.25 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 年龄信息 | 1.25 |'
- en: '| Marital info | 1.16 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 婚姻信息 | 1.16 |'
- en: '| Multiple filter | 1.08 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 多重过滤 | 1.08 |'
- en: '| Duration data | 1.51 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 数据持续时间 | 1.51 |'
- en: '| Number of contacts | 1.64 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 联系人数 | 1.64 |'
- en: '| People between 25 and 50 | 1.37 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 年龄在25到50岁之间的人 | 1.37 |'
- en: The second example – an information retrieval search tool
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二个示例 - 信息检索搜索工具
- en: 'According to Wikipedia ([https://en.wikipedia.org/wiki/Information_retrieval](https://en.wikipedia.org/wiki/Information_retrieval)),
    **information retrieval** is:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 根据维基百科（[https://en.wikipedia.org/wiki/Information_retrieval](https://en.wikipedia.org/wiki/Information_retrieval)），**信息检索**是：
- en: '"The activity obtaining information resources relevant to an information need
    from a collection of information resources."'
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “从信息资源集合中获取与信息需求相关的信息资源。”
- en: 'Usually, the information resources are a collection of documents and the information
    need is a set of words, which summarizes our need. To make a fast search over
    the document collection, we use a data structure named **inverted index**. It
    stores all the words of the document collection, and for each word, a list of
    the documents that contains that word. In [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, you constructed
    an inverted index of a document collection constructed with the Wikipedia pages
    with information about movies to construct a set of 100,673 documents. We have
    converted each Wikipedia page in a text file. This inverted index is stored in
    a text file where each line contains the word, its document frequency, and all
    the documents in which the word appears with the `tfxidf` attribute of the word
    in the document. The documents are sorted by the value of the `tfxidf` attribute.
    For example, a line of the file looks like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，信息资源是一组文档，信息需求是一组单词，这总结了我们的需求。为了在文档集合上进行快速搜索，我们使用了一种名为**倒排索引**的数据结构。它存储了文档集合中的所有单词，对于每个单词，都有一个包含该单词的文档列表。在[第4章](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "第4章。从任务中获取数据 - Callable和Future接口")中，*从任务中获取数据 - Callable和Future接口*，您构建了一个由维基百科页面构成的文档集合的倒排索引，其中包含有关电影的信息，构成了一组100,673个文档。我们已经将每个维基百科页面转换为一个文本文件。这个倒排索引存储在一个文本文件中，每一行包含单词、它的文档频率，以及单词在文档中出现的所有文档，以及单词在文档中的`tfxidf`属性的值。文档按照`tfxidf`属性的值进行排序。例如，文件的一行看起来像这样：
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This line contains the `velankanni` word with a DF of `4`. It appears in the
    `18005302.txt` document with a `tfxidf` value of `10.13`, in the `20681361.txt`
    document with a `tfxidf` value of `10.13`, in the document `45672176.txt` with
    a `tfxidf` value of `10.13`, and in the `6592085.txt` document with a `tfxidf`
    value of `10.13`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行包含了`velankanni`一词，DF为`4`。它出现在`18005302.txt`文档中，`tfxidf`值为`10.13`，在`20681361.txt`文档中，`tfxidf`值为`10.13`，在`45672176.txt`文档中，`tfxidf`值为`10.13`，在`6592085.txt`文档中，`tfxidf`值为`10.13`。
- en: In this chapter, we will use the stream API to implement different versions
    of our search tool and obtain information about the inverted index.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用流API来实现我们的搜索工具的不同版本，并获取有关倒排索引的信息。
- en: An introduction to the reduction operation
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少操作的介绍
- en: As we mentioned earlier in this chapter, the `reduce` operation applies a summary
    operation to the elements of a stream to generate a single summary result. This
    single result can be of the same type as the elements of the stream of an other
    type. A simple example of the `reduce` operation is calculating the sum of a stream
    of numbers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章前面提到的，`reduce`操作将一个摘要操作应用于流的元素，生成一个单一的摘要结果。这个单一的结果可以与流的元素相同类型，也可以是其他类型。`reduce`操作的一个简单例子是计算一系列数字的总和。
- en: 'The stream API provides the `reduce()` method to implement reduction operations.
    This method has the following three different versions:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 流API提供了`reduce()`方法来实现减少操作。这个方法有以下三个不同的版本：
- en: '`reduce(accumulator)`: This version applies the `accumulator` function to all
    the elements of the stream. There is no initial value in this case. It returns
    an `Optional` object with the final result of the `accumulator` function or an
    empty `Optional` object if the stream is empty. This `accumulator` function must
    be an `associative` function. It implements the `BinaryOperator` interface. Both
    parameters could be either the stream elements or the partial results returned
    by previous accumulator calls.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce(accumulator)`: 此版本将`accumulator`函数应用于流的所有元素。在这种情况下没有初始值。它返回一个`Optional`对象，其中包含`accumulator`函数的最终结果，如果流为空，则返回一个空的`Optional`对象。这个`accumulator`函数必须是一个`associative`函数，它实现了`BinaryOperator`接口。两个参数可以是流元素，也可以是之前累加器调用返回的部分结果。'
- en: '`reduce(identity, accumulator)`: This version must be used when the final result
    and the elements of the stream has the same type. The identity value must be an
    identity value for the `accumulator` function. That is to say, if you apply the
    `accumulator` function to the identity value and any value `V`, it must return
    the same value `V: accumulator(identity,V)=V`. That identity value is used as
    the first result for the accumulator function and is the returned value if the
    stream has no elements. As in the other version, the accumulator must be an `associative`
    function that implements the `BinaryOperator` interface.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce(identity, accumulator)`: 当最终结果和流的元素具有相同类型时，必须使用此版本。身份值必须是`accumulator`函数的身份值。也就是说，如果你将`accumulator`函数应用于身份值和任何值`V`，它必须返回相同的值`V:
    accumulator(identity,V)=V`。该身份值用作累加器函数的第一个结果，并且如果流没有元素，则作为返回值。与另一个版本一样，累加器必须是一个实现`BinaryOperator`接口的`associative`函数。'
- en: '`reduce(identity, accumulator, combiner)`: This version must be used when the
    final result has a different type than the elements of the stream. The identity
    value must be an identity for the `combiner` function, that is to say, `combiner(identity,v)=v`.
    A `combiner` function must be compatible with the `accumulator` function, that
    is to say, `combiner(u,accumulator(identity,v))=accumulator(u,v)`. The `accumulator`
    function takes a partial result and the next element of the stream to generate
    a partial result, and the combiner takes two partial results to generate another
    partial result. Both functions must be associative, but in this case, the `accumulator`
    function is an implementation of the `BiFunction` interface and the `combiner`
    function is an implementation of the `BinaryOperator` interface.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce(identity, accumulator, combiner)`: 当最终结果的类型与流的元素不同时，必须使用此版本。identity值必须是`combiner`函数的标识，也就是说，`combiner(identity,v)=v`。`combiner`函数必须与`accumulator`函数兼容，也就是说，`combiner(u,accumulator(identity,v))=accumulator(u,v)`。`accumulator`函数接受部分结果和流的下一个元素以生成部分结果，combiner接受两个部分结果以生成另一个部分结果。这两个函数必须是可结合的，但在这种情况下，`accumulator`函数是`BiFunction`接口的实现，`combiner`函数是`BinaryOperator`接口的实现。'
- en: The `reduce()` method has a limitation. As we mentioned before, it must return
    a single value. You shouldn't use the `reduce()` method to generate a collection
    or a complex object. The first problem is performance. As the documentation of
    the stream API specifies, the `accumulator` function returns a new value every
    time it processes an element. If your `accumulator` function works with collections,
    every time it processes an element it creates a new collection, which is very
    inefficient. Another problem is that, if you work with parallel streams, all the
    threads will share the identity value. If this value is a mutable object, for
    example, a collection, all the threads will be working over the same collection.
    This does not comply with the philosophy of the `reduce()` operation. In addition,
    the `combiner()` method will always receive two identical collections (all the
    threads are working over only one collection), that also doesn't comply the philosophy
    of the `reduce()` operation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce()`方法有一个限制。正如我们之前提到的，它必须返回一个单一的值。你不应该使用`reduce()`方法来生成一个集合或复杂对象。第一个问题是性能。正如流API的文档所指定的，`accumulator`函数在处理一个元素时每次都会返回一个新值。如果你的`accumulator`函数处理集合，每次处理一个元素时都会创建一个新的集合，这是非常低效的。另一个问题是，如果你使用并行流，所有线程将共享identity值。如果这个值是一个可变对象，例如一个集合，所有线程将在同一个集合上工作。这与`reduce()`操作的理念不符。此外，`combiner()`方法将始终接收两个相同的集合（所有线程都在同一个集合上工作），这也不符合`reduce()`操作的理念。'
- en: 'If you want to make a reduction that generates a collection or a complex object,
    you have the following two options:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进行生成集合或复杂对象的减少，你有以下两个选项：
- en: Apply a mutable reduction with the `collect()` method. [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*, explains in detail how to use this method in different situations.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`collect()`方法进行可变减少。[第8章](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "第8章。使用并行流处理大型数据集 - 映射和收集模型")，“使用并行流处理大型数据集 - 映射和收集模型”详细解释了如何在不同情况下使用这种方法。
- en: Create the collection and use the `forEach()` method to fill the collection
    with the required values.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建集合并使用`forEach()`方法填充集合所需的值。
- en: In this example, we will use the `reduce()` method to obtain information about
    the inverted index and the `forEach()` method to reduce the index to the list
    of relevant documents for a query.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用`reduce()`方法获取倒排索引的信息，并使用`forEach()`方法将索引减少到查询的相关文档列表。
- en: The first approach – full document query
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一种方法 - 完整文档查询
- en: 'In our first approach, we will use all the documents associated with a word.
    The steps of this implementation of our search process are:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一种方法中，我们将使用与一个单词相关联的所有文档。我们搜索过程的实现步骤如下：
- en: We select in the inverted index the lines corresponding with the words of the
    query.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在倒排索引中选择与查询词对应的行。
- en: We group all the document lists in a single list. If a document appears associated
    with two or more different words, we sum the `tfxidf` value of those words in
    the document to obtain the final `tfxidf` value of the document. If a document
    is only associated with one word, the `tfxidf` value of that word will be the
    final `tfxidf` value for that document.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将所有文档列表分组成一个单一列表。如果一个文档与两个或更多不同的单词相关联，我们将这些单词在文档中的`tfxidf`值相加，以获得文档的最终`tfxidf`值。如果一个文档只与一个单词相关联，那么该单词的`tfxidf`值将成为该文档的最终`tfxidf`值。
- en: We sort the documents using its `tfxidf` value, from high to low.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按照`tfxidf`值对文档进行排序，从高到低。
- en: We show to the user the 100 documents with a higher value of `tfxidf`.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们向用户展示具有更高`tfxidf`值的100个文档。
- en: 'We have implemented this version in the `basicSearch()`method of the `ConcurrentSearch`
    class. This is the source code of the method:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`ConcurrentSearch`类的`basicSearch()`方法中实现了这个版本。这是该方法的源代码：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We receive an array of string objects with the words of the query. First, we
    transform that array in a set. Then, we use a *try-with-resources* stream with
    the lines of the `invertedIndex.txt` file, which is the file that contains the
    inverted index. We use a *try-with-resources* so we don''t have to worry about
    opening or closing the file. The aggregate operations of the stream will generate
    a `QueryResult` object with the relevant documents. We use the following methods
    to obtain that list:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接收一个包含查询词的字符串对象数组。首先，我们将该数组转换为一个集合。然后，我们使用`invertedIndex.txt`文件的行进行*try-with-resources*流处理，该文件包含倒排索引。我们使用*try-with-resources*，这样我们就不必担心打开或关闭文件。流的聚合操作将生成一个具有相关文档的`QueryResult`对象。我们使用以下方法来获取该列表：
- en: '`parallel()`: First, we obtain a parallel stream to improve the performance
    of the search process.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallel()`: 首先，我们获取并行流以提高搜索过程的性能。'
- en: '`filter()`: We select the lines that associated the word in the set with the
    words in the query. The `Utils.getWord()`method obtains the word of the line.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter()`: 我们选择将单词与查询中的单词相关联的行。`Utils.getWord()`方法获取行的单词。'
- en: '`flatMap()`: We convert the stream of strings where each string is a line of
    the inverted index in a stream of `Token` objects. Each token contains the `tfxidf`
    value of a word in a file. Of every line, we will generate as many tokens as files
    containing that word.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap()`: 我们将包含倒排索引每一行的字符串流转换为`Token`对象流。每个标记包含文件中单词的`tfxidf`值。对于每一行，我们将生成与包含该单词的文件数量相同的标记。'
- en: '`forEach()`: We generate the `QueryResult` object adding every token with the
    `add()` method of that class.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEach()`: 我们使用该类的`add()`方法生成`QueryResult`对象。'
- en: 'Once we have created the `QueryResult` object, we create other streams to obtain
    the final list of results using the following methods:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了`QueryResult`对象，我们使用以下方法创建其他流来获取最终结果列表：
- en: '`getAsList()`: The `QueryResult` object returns a list with the relevant documents'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getAsList()`: `QueryResult`对象返回一个包含相关文档的列表'
- en: '`stream()`: To create a stream to process the list'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream()`: 创建一个流来处理列表'
- en: '`sorted()`: To sort the list of documents by its `tfxidf` value'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sorted()`: 按其`tfxidf`值对文档列表进行排序'
- en: '`limit()`: To get the first 100 results'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limit()`: 获取前100个结果'
- en: '`forEach()`: To process the 100 results and write the information in the screen'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEach()`: 处理100个结果并将信息写入屏幕'
- en: Let's describe the auxiliary classes and methods used in the example.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下示例中使用的辅助类和方法。
- en: The basicMapper() method
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: basicMapper()方法
- en: 'This method converts a stream of strings into a stream of `Token` objects.
    As we will describe later in detail, a token stores the `tfxidf` value of a word
    in a document. This method receives a string with a line of the inverted index.
    It splits the line into the tokens and generates as many `Token` objects as documents
    containing the word. This method is implemented in the `ConcurrentSearch` class.
    This is the source code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将字符串流转换为`Token`对象流。正如我们将在后面详细描述的那样，标记存储文档中单词的`tfxidf`值。该方法接收一个包含倒排索引行的字符串。它将行拆分为标记，并生成包含包含该单词的文档数量的`Token`对象。该方法在`ConcurrentSearch`类中实现。以下是源代码：
- en: '[PRE17]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: First, we create a `ConcurrentLinkedDeque` object to store the `Token` objects.
    Then, we split the string using the `split()` method and use the `stream()` method
    of the `Arrays` class to generate a stream. Skip the first element (which contains
    the information of the word) and process the rest of the tokens in parallel. For
    each element, we create a new `Token` object (we pass to the constructor the word
    and the token that has the `file:tfxidf` format) and add it to the stream. Finally,
    we return a stream using the `stream()` method of the `ConcurrenLinkedDeque` object.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`ConcurrentLinkedDeque`对象来存储`Token`对象。然后，我们使用`split()`方法拆分字符串，并使用`Arrays`类的`stream()`方法生成一个流。跳过第一个元素（包含单词的信息），并并行处理其余的标记。对于每个元素，我们创建一个新的`Token`对象（将单词和具有`file:tfxidf`格式的标记传递给构造函数），并将其添加到流中。最后，我们使用`ConcurrenLinkedDeque`对象的`stream()`方法返回一个流。
- en: The Token class
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Token类
- en: 'As we mentioned earlier, this class stores the `tfxidf` value of a word in
    a document. So, it has three attributes to store this information, as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，这个类存储文档中单词的`tfxidf`值。因此，它有三个属性来存储这些信息，如下所示：
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The constructor receives two strings. The first one contains the word, and
    the second one contains the file and the `tfxidf` attribute in the `file:tfxidf`
    format, so we have to process it as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接收两个字符串。第一个包含单词，第二个包含文件和`file:tfxidf`格式中的`tfxidf`属性，因此我们必须按以下方式处理它：
- en: '[PRE19]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we have added methods to obtain (not to set) the values of the three
    attributes and to convert an object to a string, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加了一些方法来获取（而不是设置）三个属性的值，并将对象转换为字符串，如下所示：
- en: '[PRE20]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The QueryResult class
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QueryResult类
- en: 'This class stores the list of documents relevant to a query. Internally, it
    uses a map to store the information of the relevant documents. The key is the
    name of the file that stores the document, and the value is a `Document` object
    that also contains the name of the file and the total `tfxidf` value of that document
    to the query, as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类存储与查询相关的文档列表。在内部，它使用一个映射来存储相关文档的信息。键是存储文档的文件的名称，值是一个`Document`对象，它还包含文件的名称和该文档对查询的总`tfxidf`值，如下所示：
- en: '[PRE21]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We use the constructor of the class to indicate the concrete implementation
    of the `Map` interface we will use. We use a `ConcurrentHashMap` to the concurrent
    version and a `HashMap` in the serial version:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用类的构造函数来指示我们将使用的`Map`接口的具体实现。我们在并发版本中使用`ConcurrentHashMap`，在串行版本中使用`HashMap`：
- en: '[PRE22]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The class includes the `append` method that inserts a token in the map, as
    follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 该类包括`append`方法，用于将标记插入映射，如下所示：
- en: '[PRE23]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We use the `computeIfAbsent()` method to create a new `Document` object if there
    is no `Document` object associated with the file or to obtain the corresponding
    one if already exists and add the `tfxidf` value of the token to the total `tfxidf`
    value of the document using the `addTfxidf()` method.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`computeIfAbsent()`方法来创建一个新的`Document`对象，如果没有与文件关联的`Document`对象，或者如果已经存在，则获取相应的对象，并使用`addTfxidf()`方法将标记的`tfxidf`值添加到文档的总`tfxidf`值中。
- en: 'Finally, we have included a method to obtain the map as a list, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们包含了一个将映射作为列表获取的方法，如下所示：
- en: '[PRE24]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `Document` class stores the name of the file as a string and the total `tfxidf`
    value as `DoubleAdder`. This class is a new feature of Java 8 and allows us to
    sum values to the variable from different threads without worrying about synchronization.
    It implements the `Comparable` interface to sort the documents by its `tfxidf`
    value, so the documents with biggest `tfxidf` will be first. Its source code is
    very simple, so it is not included.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`Document`类将文件名存储为字符串，并将总`tfxidf`值存储为`DoubleAdder`。这个类是Java 8的一个新特性，允许我们在不担心同步的情况下从不同的线程对变量进行求和。它实现了`Comparable`接口，以按其`tfxidf`值对文档进行排序，因此具有最大`tfxidf`值的文档将排在前面。它的源代码非常简单，所以没有包含在内。'
- en: The second approach – reduced document query
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二种方法 - 减少文档查询
- en: The first approach creates a new `Token` object per word and file. We have noted
    that common words, for example, `the`, take a lot of documents associated and
    a lot of them have low values of `tfxidf`. We have changed our mapper method to
    take into account only 100 files per word, so the number of `Token` objects generated
    will be smaller.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法为每个单词和文件创建一个新的`Token`对象。我们注意到常见单词，例如`the`，有很多相关联的文档，而且很多文档的`tfxidf`值很低。我们已经改变了我们的映射方法，只考虑每个单词的前100个文件，因此生成的`Token`对象数量将更少。
- en: 'We have implemented this version in the `reducedSearch()` method of the `ConcurrentSearch`
    class. This method is very similar to the `basicSearch()` method. It only changes
    the stream operations that generate the `QueryResult` object, as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`ConcurrentSearch`类的`reducedSearch()`方法中实现了这个版本。这个方法与`basicSearch()`方法非常相似。它只改变了生成`QueryResult`对象的流操作，如下所示：
- en: '[PRE25]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, we use the `limitedMapper()` method as function in the `flatMap()` method.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将`limitedMapper()`方法作为`flatMap()`方法中的函数使用。
- en: The limitedMapper() method
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`limitedMapper()`方法'
- en: 'This method is similar to the `basicMapper()` method, but, as we mentioned
    earlier, we only take into account the first 100 documents associated with every
    word. As the documents are sorted by its `tfxidf` value, we are using the 100
    documents in which the word is more important, as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法类似于`basicMapper()`方法，但是，正如我们之前提到的，我们只考虑与每个单词相关联的前100个文档。由于文档按其`tfxidf`值排序，我们使用了单词更重要的100个文档，如下所示：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The only difference with the `basicMapper()` method is the `limit(100)` call
    that takes the first 100 elements of the stream.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与`basicMapper()`方法的唯一区别是`limit(100)`调用，它获取流的前100个元素。
- en: The third approach – generating an HTML file with the results
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三种方法 - 生成包含结果的HTML文件
- en: While working with a search tool using a web search engine (for example, Google),
    when you make a search, it returns you the results of your search (the 10 most
    important) and for every result the title of the document and a fragment of the
    document where the words you have searched for appears.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用网络搜索引擎（例如Google）的搜索工具时，当您进行搜索时，它会返回您的搜索结果（最重要的10个），并且对于每个结果，它会显示文档的标题和包含您搜索的单词的片段。
- en: Our third approach to the search tool is based on the second approach but by
    adding a third stream to generate an HTML file with the results of the search.
    For every result, we will show the title of the document and three lines where
    one of the words introduced in the query appears. To implement this, you need
    access to the files that appears in the inverted index. We have stored them in
    a folder named `docs`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对搜索工具的第三种方法是基于第二种方法，但是通过添加第三个流来生成包含搜索结果的HTML文件。对于每个结果，我们将显示文档的标题和其中出现查询词的三行。为了实现这一点，您需要访问倒排索引中出现的文件。我们已经将它们存储在一个名为`docs`的文件夹中。
- en: 'This third approach is implemented in the `htmlSearch()`method of the `ConcurrentSearch`
    class. The first part of the method to construct the `QueryResult` object with
    the 100 results is equal to the `reducedSearch()` method, as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这第三种方法是在`ConcurrentSearch`类的`htmlSearch()`方法中实现的。构造`QueryResult`对象的方法的第一部分与`reducedSearch()`方法相同，如下所示：
- en: '[PRE27]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we create the file to write the output and the HTML headers in it:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建文件以写入输出和其中的HTML标头：
- en: '[PRE28]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, we include the stream that generates the results in the HTML file:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们包括生成HTML文件中结果的流：
- en: '[PRE29]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We have used the following methods:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了以下方法：
- en: '`getAsList()` to get the list of relevant documents for the query.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getAsList()`获取与查询相关的文档列表。'
- en: '`stream()` to generate a sequential stream. We can''t parallelize this stream.
    If we try to do it, the results in the final file won''t be sorted by the `tfxidf`
    value of the documents.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream()`生成一个顺序流。我们不能并行化这个流。如果我们尝试这样做，最终文件中的结果将不会按文档的`tfxidf`值排序。'
- en: '`sorted()` to sort the results by its `tfxidf` attribute.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sorted()`按其`tfxidf`属性对结果进行排序。'
- en: '`map()` to convert a `Result` object into a string with the HTML code for each
    result using the `ContentMapper` class. We will explain the details of this class
    later.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map()`使用`ContentMapper`类将`Result`对象转换为每个结果的HTML代码字符串。我们稍后会解释这个类的细节。'
- en: '`forEach()` to write the `String` objects returned by the `map()` method in
    the file. The methods of the `Stream` object can''t throw a checked exception,
    so we have to include the try-catch block that will throw.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEach()`将`map()`方法返回的`String`对象写入文件。`Stream`对象的方法不能抛出已检查的异常，所以我们必须包含将抛出异常的try-catch块。'
- en: Let's see the details of the `ContentMapper` class.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`ContentMapper`类的细节。
- en: The ContentMapper class
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`ContentMapper`类'
- en: The `ContentMapper` class is an implementation of the `Function` interface that
    converts a `Result` object in an HTML block with the title of the document and
    three lines that include one or more words of the query.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`ContentMapper`类是`Function`接口的实现，它将`Result`对象转换为包含文档标题和三行文本的HTML块，其中包括一个或多个查询词。'
- en: 'The class uses an internal attribute to store the query and implements a constructor
    to initialize that attribute, as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 该类使用内部属性存储查询，并实现构造函数来初始化该属性，如下所示：
- en: '[PRE30]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The title of the document is stored in the first line of the file. We use a
    try-with-resources instruction and the `lines()` method of the `Files` class to
    create and stream `String` objects with the lines of the file and take the first
    one with the `findFirst()` to obtain the line as a string:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的标题存储在文件的第一行。我们使用try-with-resources指令和Files类的lines()方法来创建和流式传输文件行的String对象，并使用findFirst()获取第一行作为字符串：
- en: '[PRE31]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we use a similar structure, but in this case, we use the `filter()` method
    to get only the lines that contain one or more words of the query, the `limit()`
    method to take three of those lines. Then, we use the `map()` method to add the
    HTML tags for a paragraph (`<p>`) and the `reduce()` method to complete the HTML
    code with the selected lines:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用类似的结构，但在这种情况下，我们使用filter()方法仅获取包含一个或多个查询单词的行，使用limit()方法获取其中三行。然后，我们使用map()方法为段落添加HTML标签（<p>），并使用reduce()方法完成所选行的HTML代码：
- en: '[PRE32]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The fourth approach – preloading the inverted index
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四种方法-预加载倒排索引
- en: The three previous solutions have a problem when they are executed in parallel.
    As we mentioned earlier, parallel streams are executed using the common Fork/Join
    pool provided by the Java concurrency API. In [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*, you learned
    that you shouldn't use I/O operations as read or write data in a file inside the
    tasks. This is because when a thread has blocked reading or writing data from
    or to a file, the framework doesn't use the work-stealing algorithm. As we use
    a file as the source of our streams, we are penalizing our concurrent solution.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 前三种解决方案在并行执行时存在问题。正如我们之前提到的，使用常见的Java并发API提供的Fork/Join池执行并行流。在[第6章](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "第6章。优化分治解决方案-分支/加入框架")中，*优化分治解决方案-分支/加入框架*，您学到了不应在任务内部使用I/O操作，如从文件中读取或写入数据。这是因为当线程阻塞读取或写入文件中的数据时，框架不使用工作窃取算法。由于我们使用文件作为流的源，因此我们正在惩罚我们的并发解决方案。
- en: One solution to this problem is to read the data to a data structure and then
    create our streams from that data structure. Obviously, the execution time of
    this approach will be smaller when we compare it with the other approaches, but
    we want to compare the serial and concurrent versions to see (as we expect) that
    the concurrent version gives us better performance than the serial version. The
    bad part of this approach is that you need to have your data structure in memory,
    so you will need a big amount of memory.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个方法是将数据读取到数据结构中，然后从该数据结构创建流。显然，与其他方法相比，这种方法的执行时间会更短，但我们希望比较串行和并行版本，以查看（正如我们所期望的那样）并行版本是否比串行版本具有更好的性能。这种方法的不好之处在于你需要将数据结构保存在内存中，因此你需要大量的内存。
- en: 'This fourth approach is implemented in the `preloadSearch()` method of the
    `ConcurrentSearch` class. This method receives the query as an `Array` of `String`
    and an object of the `ConcurrentInvertedIndex` class (we will see the details
    of this class later) with the data of the inverted index as parameters. This is
    the source code of this version:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这第四种方法是在ConcurrentSearch类的preloadSearch()方法中实现的。该方法接收查询作为String的Array和ConcurrentInvertedIndex类的对象（稍后我们将看到该类的详细信息）作为参数。这是此版本的源代码：
- en: '[PRE33]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `ConcurrentInvertedIndex` class has `List<Token>` to store all the `Token`
    objects read from the file. It has two methods, `get()` and `set()` for this list
    of elements.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ConcurrentInvertedIndex类具有List<Token>来存储从文件中读取的所有Token对象。它有两个方法，get()和set()用于这个元素列表。
- en: 'As in other approaches, we use two streams: the first one to get a `ConcurrentLinkedDeque`
    of `Result` objects with the whole list of results and the second one to write
    the results in the console. The second one doesn''t change over other versions,
    but the first one changes. We use the following methods in this stream:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法一样，我们使用两个流：第一个流获取Result对象的ConcurrentLinkedDeque，其中包含整个结果列表，第二个流将结果写入控制台。第二个流与其他版本相同，但第一个流不同。我们在这个流中使用以下方法：
- en: '`getIndex()`: First, we obtain the list of `Token` objects'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getIndex()`: 首先，我们获取Token对象的列表'
- en: '`parallelStream()`: Then, we create a parallel stream to process all the elements
    of the list'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallelStream()`: 然后，我们创建一个并行流来处理列表的所有元素'
- en: '`filter()`: We select the token associated with the words in the query'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter()`: 我们选择与查询中的单词相关联的标记'
- en: '`forEach()`: We process the list of tokens adding them to the `QueryResult`
    object using the `append()` method'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEach()`: 我们处理标记列表，使用append()方法将它们添加到QueryResult对象中'
- en: The ConcurrentFileLoader class
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ConcurrentFileLoader类
- en: 'The `ConcurrentFileLoader` class loads into memory the contents of the `invertedIndex.txt`
    file with the information of the inverted index. It provides a static method named
    `load()` that receives a path with the route of the file where the inverted index
    is stored and returns an `ConcurrentInvertedIndex` object. We have the following
    code:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ConcurrentFileLoader类将invertedIndex.txt文件的内容加载到内存中，其中包含倒排索引的信息。它提供了一个名为load()的静态方法，该方法接收存储倒排索引的文件路径，并返回一个ConcurrentInvertedIndex对象。我们有以下代码：
- en: '[PRE34]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We open the file using a *try-with-resources* structure and create a stream
    to process all the lines:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用try-with-resources结构打开文件并创建一个流来处理所有行：
- en: '[PRE35]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We use the following methods in the stream:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在流中使用以下方法：
- en: '`parallel()`: We convert the stream into a parallel one'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallel()`: 我们将流转换为并行流'
- en: '`flatMap()`: We convert the line into a stream of `Token` objects using the
    `limitedMapper()` method of the `ConcurrentSearch` class'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap()`: 我们使用ConcurrentSearch类的limitedMapper()方法将行转换为Token对象的流'
- en: '`forEach()`: We process the list of `Token` objects adding them to a `ConcurrentLinkedDeque`
    object using the `add()` method'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forEach()`: 我们处理Token对象的列表，使用add()方法将它们添加到ConcurrentLinkedDeque对象中'
- en: Finally, we convert the `ConcurrentLinkedDeque` object into `ArrayList` and
    set it in the `InvertedIndex` object using the `setIndex()` method.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将`ConcurrentLinkedDeque`对象转换为`ArrayList`，并使用`setIndex()`方法将其设置在`InvertedIndex`对象中。
- en: The fifth approach – using our own executor
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五种方法-使用我们自己的执行器
- en: 'To go further with this example, we''re going to test another concurrent version.
    As we mentioned in the introduction of this chapter, parallel streams use the
    common Fork/Join pool introduced in Java 8\. However, we can use a trick to use
    our own pool. If we execute our method as a task of the Fork/Join pool, all the
    operations of the stream will be executed in the same Fork/Join pool. To test
    this functionality, we have added the `executorSearch()` method to the `ConcurrentSearch`
    class. This method receives the query as an array of `String` objects as a parameter,
    the `InvertedIndex` object, and a `ForkJoinPool` object. This is the source code
    of this method:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明这个例子，我们将测试另一个并发版本。正如我们在本章的介绍中提到的，并行流使用了Java 8中引入的常见Fork/Join池。然而，我们可以使用一个技巧来使用我们自己的池。如果我们将我们的方法作为Fork/Join池的任务执行，流的所有操作将在同一个Fork/Join池中执行。为了测试这个功能，我们已经在`ConcurrentSearch`类中添加了`executorSearch()`方法。该方法接收查询作为`String`对象数组的参数，`InvertedIndex`对象和`ForkJoinPool`对象。这是该方法的源代码：
- en: '[PRE36]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We execute the content of the method, with its two streams, as a task in the
    Fork/Join pool using the `submit()` method, and wait for its finalization using
    the `join()` method.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`submit()`方法将该方法的内容及其两个流作为Fork/Join池中的任务执行，并使用`join()`方法等待其完成。
- en: Getting data from the inverted index – the ConcurrentData class
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从倒排索引获取数据-`ConcurrentData`类
- en: We have implemented some methods to get information about the inverted index
    using the `reduce()` method in the `ConcurrentData` class.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了一些方法，使用`ConcurrentData`类中的`reduce()`方法获取有关倒排索引的信息。
- en: Getting the number of words in a file
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取文件中的单词数
- en: 'The first method calculates the number of words in a file. As we mentioned
    earlier in this chapter, the inverted index stores the files in which a word appears.
    If we want to know the words that appear in a file, we have to process all the
    inverted index. We have implemented two versions of this method. The first one
    is implemented in `getWordsInFile1()`. It receives the name of the file and the
    `InvertedIndex` object as parameters, as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法计算文件中的单词数。正如我们在本章前面提到的，倒排索引存储了单词出现的文件。如果我们想知道出现在文件中的单词，我们必须处理整个倒排索引。我们已经实现了这个方法的两个版本。第一个版本实现在`getWordsInFile1()`中。它接收文件的名称和`InvertedIndex`对象作为参数，如下所示：
- en: '[PRE37]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In this case, we get the list of `Token` objects using the `getIndex()` method
    and create a parallel stream using the `parallelStream()` method. Then, we filter
    the tokens associated with the file using the `filter()` method, and finally,
    we count the number of words associated with that file using the `count()` method.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用`getIndex()`方法获取`Token`对象的列表，并使用`parallelStream()`方法创建并行流。然后，我们使用`filter()`方法过滤与文件相关的令牌，最后，我们使用`count()`方法计算与该文件相关的单词数。
- en: 'We have implemented another version of this method using the `reduce()` method
    instead of the `count()` method. It''s the `getWordsInFile2()` method:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了该方法的另一个版本，使用`reduce()`方法而不是`count()`方法。这是`getWordsInFile2()`方法：
- en: '[PRE38]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The start of the sequence of operations is the same as the previous one. When
    we have obtained the stream of `Token` objects with the words of the file, we
    use the `mapToInt()` method to convert that stream into a stream of `1` and then
    the `reduce()` method to sum all the `1` numbers.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 操作序列的开始与前一个相同。当我们获得了文件中单词的`Token`对象流时，我们使用`mapToInt()`方法将该流转换为`1`的流，然后使用`reduce()`方法来求和所有`1`的数字。
- en: Getting the average tfxidf value in a file
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取文件中的平均tfxidf值
- en: 'We have implemented the `getAverageTfxidf()` method that calculates the average
    `tfxidf` value of the words of a file in the collection. We have used here the
    `reduce()` method to show how it works. You can use other methods here with better
    performance:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了`getAverageTfxidf()`方法，它计算集合中文件的单词的平均`tfxidf`值。我们在这里使用了`reduce()`方法来展示它的工作原理。您可以在这里使用其他方法来获得更好的性能：
- en: '[PRE39]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We use two streams. The first one calculates the number of words in a file
    and has the same source code as the `getWordsInFile2()` method. The second one
    calculates the total `tfxidf` value of all the words in the file. We use the same
    methods to get the stream of `Token` objects with the words in the file and then
    we use the `reduce` method to sum the `tfxidf` value of all the words. We pass
    the following three parameters to the `reduce()` method:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两个流。第一个计算文件中的单词数，其源代码与`getWordsInFile2()`方法相同。第二个计算文件中所有单词的总`tfxidf`值。我们使用相同的方法来获取文件中单词的`Token`对象流，然后我们使用`reduce`方法来计算所有单词的`tfxidf`值的总和。我们将以下三个参数传递给`reduce()`方法：
- en: '`O`: This is passed as the identity value.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`O`: 这作为标识值传递。'
- en: '`(n,t) -> n+t.getTfxidf()`: This is passed as the `accumulator` function. It
    receives a `double` number and a `Token` object and calculates the sum of the
    number and the `tfxidf` attribute of the token.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(n,t) -> n+t.getTfxidf()`: 这作为`accumulator`函数传递。它接收一个`double`数字和一个`Token`对象，并计算数字和令牌的`tfxidf`属性的总和。'
- en: '`(n1,n2) -> n1+n2`: This is passed as the `combiner` function. It receives
    two numbers and calculates their sum.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(n1,n2) -> n1+n2`: 这作为`combiner`函数传递。它接收两个数字并计算它们的总和。'
- en: Getting the maximum and minimum tfxidf values in the index
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取索引中的最大和最小tfxidf值
- en: 'We have also used the `reduce()` method to calculate the maximum and minimum
    `tfxidf` values of the inverted index in the `maxTfxidf()` and `minTfxidf()` methods:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用`reduce()`方法在`maxTfxidf()`和`minTfxidf()`方法中计算倒排索引的最大和最小`tfxidf`值：
- en: '[PRE40]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The method receives the `ConcurrentInvertedIndex` as a parameter. We use the
    `getIndex()` to obtain the list of `Token` objects. Then, we use the `parallelStream()`
    method to create a parallel stream over the list and the `reduce()` method to
    obtain the `Token` with the biggest `tfxidf`. In this case, we use the `reduce()`
    method with two parameters: an identity value and an `accumulator` function. The
    identity value is a `Token` object. We don''t care about the word and the file
    name, but we initialize its `tfxidf` attribute with the value `0`. Then, the `accumulator`
    function receives two `Token` objects as parameters. We compare the `tfxidf` attribute
    of both objects and return the one with greater value.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收`ConcurrentInvertedIndex`作为参数。我们使用`getIndex()`来获取`Token`对象的列表。然后，我们使用`parallelStream()`方法在列表上创建并行流，使用`reduce()`方法来获取具有最大`tfxidf`的`Token`。在这种情况下，我们使用两个参数的`reduce()`方法：一个身份值和一个`accumulator`函数。身份值是一个`Token`对象。我们不关心单词和文件名，但是我们将其`tfxidf`属性初始化为值`0`。然后，`accumulator`函数接收两个`Token`对象作为参数。我们比较两个对象的`tfxidf`属性，并返回具有更大值的对象。
- en: 'The `minTfxidf()` method is very similar, as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`minTfxidf()`方法非常相似，如下所示：'
- en: '[PRE41]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The main difference is that in this case, the identity value is initialized
    with a very high value for the `tfxidf` attribute.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于，在这种情况下，身份值用非常高的值初始化了`tfxidf`属性。
- en: The ConcurrentMain class
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConcurrentMain类
- en: 'To test all the methods explained in the previous sections, we have implemented
    the `ConcurrentMain` class that implements the `main()` method to launch our tests.
    In these tests, we have used the following three queries:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试前面部分中解释的所有方法，我们实现了`ConcurrentMain`类，该类实现了`main()`方法来启动我们的测试。在这些测试中，我们使用了以下三个查询：
- en: '`query1`, with the words `james` and `bond`'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query1`，使用单词`james`和`bond`'
- en: '`query2`, with the words `gone`, `with`, `the`, and `wind`'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query2`，使用单词`gone`，`with`，`the`和`wind`'
- en: '`query3`, with the words `rocky`'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query3`，使用单词`rocky`'
- en: 'We have tested the three queries with the three versions of our search process
    measuring the execution time of each test. All the tests have a code similar to
    this:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用三个版本的搜索过程测试了三个查询，测量了每个测试的执行时间。所有测试都类似于这样的代码：
- en: '[PRE42]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To load the inverted index from a file to an `InvertedIndex` object, you can
    use the following code:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 要将倒排索引从文件加载到`InvertedIndex`对象中，您可以使用以下代码：
- en: '[PRE43]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To create the `Executor` to use in the `executorSearch()` method, you can use
    the following code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建用于`executorSearch()`方法的`Executor`，您可以使用以下代码：
- en: '[PRE44]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The serial version
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 串行版本
- en: 'We have implemented a serial version of this example with the `SerialSearch`,
    `SerialData`, `SerialInvertendIndex`, `SerialFileLoader`, and `SerialMain` classes.
    To implement that version, we have made the following changes:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了这个示例的串行版本，使用了`SerialSearch`，`SerialData`，`SerialInvertendIndex`，`SerialFileLoader`和`SerialMain`类。为了实现该版本，我们进行了以下更改：
- en: Use sequential streams instead of parallel ones. You have to delete the use
    of the `parallel()` method to convert the streams in parallel or replace the method
    `parallelStream()` to create a parallel stream for the `stream()` method to create
    a sequential one.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用顺序流而不是并行流。您必须删除使用`parallel()`方法将流转换为并行流的用法，或者将`parallelStream()`方法替换为`stream()`方法以创建顺序流。
- en: In the `SerialFileLoader` class, use `ArrayList` instead of `ConcurrentLinkedDeque`.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`SerialFileLoader`类中，使用`ArrayList`而不是`ConcurrentLinkedDeque`。
- en: Comparing the solutions
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较解决方案
- en: Let's compare the solutions of the serial and concurrent versions of all the
    methods we have implemented. We have executed them using the JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/)),
    which allows you to implement micro benchmarks in Java. Using a framework for
    benchmarking is a better solution that simply measures time using methods such
    as `currentTimeMillis()` or `nanoTime()`. We have executed them 10 times in a
    computer with a four-core processor so a concurrent algorithm can become theoretically
    four times faster than a serial one. Take into account that we have implemented
    a special class to execute the JMH tests. You can find these classes in the `com.javferna.packtpub.mastering.irsystem.benchmark`
    package of the source code.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较我们实现的所有方法的串行和并行版本的解决方案。我们使用JMH框架（[http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/)）执行它们，该框架允许您在Java中实现微基准测试。使用基准测试框架比仅使用`currentTimeMillis()`或`nanoTime()`等方法测量时间更好。我们在具有四核处理器的计算机上执行了10次，因此并行算法在理论上可以比串行算法快四倍。请注意，我们已经实现了一个特殊的类来执行JMH测试。您可以在源代码的`com.javferna.packtpub.mastering.irsystem.benchmark`包中找到这些类。
- en: 'For the first query, with the words `james` and `bond`, these are the execution
    times obtained in milliseconds:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个查询，使用单词`james`和`bond`，这些是以毫秒为单位获得的执行时间：
- en: '|   | **Serial** | **Concurrent** |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|   | **串行** | **并行** |'
- en: '| --- | --- | --- |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Basic search | 3516.674 | 3301.334 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 基本搜索 | 3516.674 | 3301.334 |'
- en: '| Reduced search | 3458.351 | 3230.017 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 减少搜索 | 3458.351 | 3230.017 |'
- en: '| HTML search | 3298.996 | 3298.632 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| HTML搜索 | 3298.996 | 3298.632 |'
- en: '| Preload search | 153.414 | 105.195 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 预加载搜索 | 153.414 | 105.195 |'
- en: '| Executor search | 154.679 | 102.135 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 执行器搜索 | 154.679 | 102.135 |'
- en: 'For the second query, with the words `gone`, `with`, `the`, and `wind`, these
    are the execution times obtained in milliseconds:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个查询，使用单词`gone`，`with`，`the`和`wind`，这些是以毫秒为单位获得的执行时间：
- en: '|   | **Serial** | **Concurrent** |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|   | **串行** | **并行** |'
- en: '| --- | --- | --- |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Basic search | 3446.022 | 3441.002 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 基本搜索 | 3446.022 | 3441.002 |'
- en: '| Reduced search | 3249.930 | 3260.026 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 减少搜索 | 3249.930 | 3260.026 |'
- en: '| HTML search | 3299.625 | 3379.277 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| HTML搜索 | 3299.625 | 3379.277 |'
- en: '| Preload search | 154.631 | 113.757 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 预加载搜索 | 154.631 | 113.757 |'
- en: '| Executor search | 156.091 | 106.418 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 执行器搜索 | 156.091 | 106.418 |'
- en: 'For the third query, with the words `rocky`, these are the execution times
    obtained in milliseconds:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第三个查询，使用单词`rocky`，这些是以毫秒为单位获得的执行时间：
- en: '|   | Serial | Concurrent |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|   | 串行 | 并行 |'
- en: '| --- | --- | --- |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Basic search | 3271.308 | 3219.990 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 基本搜索 | 3271.308 | 3219.990 |'
- en: '| Reduced search | 3318.343 | 3279.247 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 减少搜索 | 3318.343 | 3279.247 |'
- en: '| HTML search | 3323.345 | 3333.624 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| HTML搜索 | 3323.345 | 3333.624 |'
- en: '| Preload search | 151.416 | 97.092 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 预加载搜索 | 151.416 | 97.092 |'
- en: '| Executor search | 155.033 | 103.907 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 执行器搜索 | 155.033 | 103.907 |'
- en: 'Finally, these are the average execution times in milliseconds for the methods
    that return information about the inverted index:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是返回有关倒排索引信息的方法的平均执行时间（毫秒）：
- en: '|   | Serial | Concurrent |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|   | 串行 | 并发 |'
- en: '| --- | --- | --- |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `getWordsInFile1` | 131.066 | 81.357 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| `getWordsInFile1` | 131.066 | 81.357 |'
- en: '| `getWordsInFile2` | 132.737 | 84.112 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| `getWordsInFile2` | 132.737 | 84.112 |'
- en: '| `getAverageTfxidf` | 253.067 | 166.009 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| `getAverageTfxidf` | 253.067 | 166.009 |'
- en: '| `maxTfxidf` | 90.714 | 66.976 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| `maxTfxidf` | 90.714 | 66.976 |'
- en: '| `minTfxidf` | 84.652 | 68.158 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| `minTfxidf` | 84.652 | 68.158 |'
- en: 'We can draw the following conclusions:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出以下结论：
- en: When we read the inverted index to obtain the list of relevant documents, we
    obtain worse execution times. In this case, the execution times between the concurrent
    and serial versions are very similar.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们读取倒排索引以获取相关文档列表时，执行时间变得更糟。在这种情况下，并发和串行版本之间的执行时间非常相似。
- en: When we work with a preload version of the inverted index, concurrent versions
    of the algorithms give us better performance in all cases.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们使用倒排索引的预加载版本时，算法的并发版本在所有情况下都给我们更好的性能。
- en: For the methods that give us information about the inverted index, concurrent
    versions of the algorithms always give us better performance.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给我们提供倒排索引信息的方法，并发版本的算法总是给我们更好的性能。
- en: 'We can compare the parallel and sequential streams for the three queries in
    this end using the speed-up:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过速度提升来比较并行和顺序流在这个结束的三个查询中的表现：
- en: '![Comparing the solutions](img/00019.jpeg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![比较解决方案](img/00019.jpeg)'
- en: 'Finally, in our third approach, we generate an HTML web page with the results
    of the queries. These are the first results with the query `james bond`:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们的第三种方法中，我们生成了一个包含查询结果的HTML网页。这是查询`james bond`的第一个结果：
- en: '![Comparing the solutions](img/00020.jpeg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![比较解决方案](img/00020.jpeg)'
- en: 'For the query `gone with the wind`, these are the first results:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 对于查询`gone with the wind`，这是第一个结果：
- en: '![Comparing the solutions](img/00021.jpeg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![比较解决方案](img/00021.jpeg)'
- en: 'Finally, these are the first results for the query `rocky`:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是查询`rocky`的第一个结果：
- en: '![Comparing the solutions](img/00022.jpeg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![比较解决方案](img/00022.jpeg)'
- en: Summary
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we were introduced to streams, a new feature introduced in
    Java 8 inspired by functional programming and got ready to work with the new lambda
    expressions. A stream is a sequence of data (it is not a data structure) that
    allows you to apply a sequence of operations in a sequential or concurrent way
    to filter, convert, sort, reduce, or organize those elements to obtain a final
    object.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了流，这是Java 8中引入的一个受函数式编程启发的新功能，并准备好使用新的lambda表达式。流是一系列数据（不是数据结构），允许您以顺序或并发的方式应用一系列操作来过滤、转换、排序、减少或组织这些元素以获得最终对象。
- en: You also learned the main characteristics of the streams that we have to take
    into account when we use streams in our sequential or concurrent applications.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 您还学习了流的主要特征，当我们在顺序或并发应用程序中使用流时，我们必须考虑这些特征。
- en: Finally, we used streams in two samples. In the first sample, we used almost
    all the methods provided by the `Stream` interface to calculate statistical data
    of a big dataset. We used the Bank Marketing dataset of the UCI Machine Learning
    Repository with its 45,211 records. In the second sample, we implemented different
    approaches to a search application in an inverted index to obtain the most relevant
    documents to a query. This is one of the most common tasks in the information
    retrieval field. For this purpose, we used the `reduce()` method as the terminal
    operation of our streams.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在两个示例中使用了流。在第一个示例中，我们使用了`Stream`接口提供的几乎所有方法来计算大型数据集的统计数据。我们使用了UCI机器学习库的银行营销数据集，其中包含45211条记录。在第二个示例中，我们实现了不同的方法来搜索倒排索引中与查询相关的最相关文档。这是信息检索领域中最常见的任务之一。为此，我们使用`reduce()`方法作为流的终端操作。
- en: In the next chapter, we will continue working with streams, but with more focus
    on the `collect()` terminal operation.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续使用流，但更专注于`collect()`终端操作。
