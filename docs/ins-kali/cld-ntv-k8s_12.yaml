- en: '*Chapter 9*: Observability on Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：Kubernetes上的可观测性'
- en: This chapter dives into capabilities that are highly recommended to implement
    when running Kubernetes in production. First, we discuss observability in the
    context of distributed systems such as Kubernetes. Then, we look at the built-in
    Kubernetes observability stack and what functionality it implements. Finally,
    we learn how to supplement the built-in observability tooling with additional
    observability, monitoring, logging, and metrics infrastructure from the ecosystem.
    The skills you learn in this chapter will help you deploy observability tools
    to your Kubernetes cluster and enable you to understand how your cluster (and
    applications running on it) are functioning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入讨论了在生产环境中运行Kubernetes时强烈建议实施的能力。首先，我们讨论了在分布式系统（如Kubernetes）的上下文中的可观测性。然后，我们看一下内置的Kubernetes可观测性堆栈以及它实现的功能。最后，我们学习如何通过生态系统中的额外可观测性、监控、日志记录和指标基础设施来补充内置的可观测性工具。本章中学到的技能将帮助您将可观测性工具部署到您的Kubernetes集群，并使您能够了解您的集群（以及在其上运行的应用程序）的运行方式。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding observability on Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上理解可观测性
- en: Using default observability tooling – metrics, logging, and the dashboard
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用默认的可观测性工具 - 指标、日志和仪表板
- en: Implementing the best of the ecosystem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施生态系统的最佳实践
- en: To start, we will learn the out-of-the-box tools and processes that Kubernetes
    provides for observability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将学习Kubernetes为可观测性提供的开箱即用工具和流程。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the kubectl tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，您需要一台支持`kubectl`命令行工具的计算机，以及一个正常运行的Kubernetes集群。请参阅[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*，了解快速启动和安装kubectl工具的几种方法。
- en: 'The code used in this chapter can be found in the book''s GitHub repository:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在该书的GitHub存储库中找到：
- en: '[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter9)'
- en: Understanding observability on Kubernetes
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上理解可观测性
- en: No production system is complete without a way to monitor it. In software, we
    define observability as the ability to, at any point in time, understand how our
    system is performing (and, in the best case, why). Observability grants significant
    benefits in security, performance, and operational capacity. By knowing how your
    system is responding at the VM, container, and application level, you can tune
    it to perform efficiently, react quickly to events, and more easily troubleshoot
    bugs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 没有监控的生产系统是不完整的。在软件中，我们将可观测性定义为在任何时间点都能够了解系统的性能（在最好的情况下，还能了解原因）。可观测性在安全性、性能和运行能力方面带来了显著的好处。通过了解您的系统在虚拟机、容器和应用程序级别的响应方式，您可以调整它以高效地运行，快速响应事件，并更容易地排除错误。
- en: For instance, let's take a scenario where your application is running extremely
    slowly. In order to find the bottleneck, you may look at the application code
    itself, the resource specifications of the Pod, the number of Pods in the deployment,
    the memory and CPU usage at the Pod level or Node level, and externalities such
    as a MySQL database running outside your cluster.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们来看一个场景，您的应用程序运行非常缓慢。为了找到瓶颈，您可能会查看应用程序代码本身，Pod的资源规格，部署中的Pod数量，Pod级别或节点级别的内存和CPU使用情况，以及外部因素，比如在集群外运行的MySQL数据库。
- en: By adding observability tooling, you would be able to diagnose many of these
    variables and figure out what issues may be contributing to your application slowdown.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加可观察性工具，您将能够诊断许多这些变量，并找出可能导致应用程序减速的问题。
- en: 'Kubernetes, as a production-ready container orchestration system, gives us
    some default tools to monitor our applications. For the purposes of this chapter,
    we will separate observability into four ideas: metrics, logs, traces, and alerts.
    Let''s look at each of them:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes作为一个成熟的容器编排系统，为我们提供了一些默认工具来监控我们的应用程序。在本章中，我们将把可观察性分为四个概念：指标、日志、跟踪和警报。让我们来看看每一个：
- en: '**Metrics** here represents the ability to see numerical representations of
    the system''s current state, with specific attention paid to CPU, memory, network,
    disk space, and more. These numbers allow us to judge the gap in current state
    with the system''s maximum capacity and ensure that the system remains available
    to users.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标**在这里代表着查看系统当前状态的数值表示能力，特别关注CPU、内存、网络、磁盘空间等。这些数字让我们能够判断当前状态与系统最大容量之间的差距，并确保系统对用户保持可用。'
- en: '**Logs** refers to the practice of collecting text logs from applications and
    systems. Logs will likely be a combination of Kubernetes control plane logs and
    logs from your application Pods themselves. Logs can help us diagnose the availability
    of the Kubernetes system, but they also can help with triaging application bugs.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志**指的是从应用程序和系统中收集文本日志的做法。日志可能是Kubernetes控制平面日志和应用程序Pod自身的日志的组合。日志可以帮助我们诊断Kubernetes系统的可用性，但它们也可以帮助我们排除应用程序错误。'
- en: '**Traces** refers to collecting distributed traces. Traces are an observability
    pattern that delivers end-to-end visibility of a chain of requests – which can
    be HTTP requests or otherwise. This topic is especially important in a distributed
    cloud-native setting where microservices are used. If you have many microservices
    and they call each other, it can be difficult to find bottlenecks or issues when
    many services are involved in a single end-to-end request. Traces allow you to
    view requests broken down by each leg of a service-to-service call.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪**指的是收集分布式跟踪。跟踪是一种可观察性模式，提供了对请求链的端到端可见性 - 这些请求可以是HTTP请求或其他类型的请求。在使用微服务的分布式云原生环境中，这个主题尤为重要。如果您有许多微服务并且它们相互调用，那么在涉及多个服务的单个端到端请求时，很难找到瓶颈或问题。跟踪允许您查看每个服务对服务调用的每个环节分解的请求。'
- en: '**Alerts** correspond to the practice of setting automated touch points when
    certain events happen. Alerts can be set on both *metrics* and *logs*, and delivered
    through a host of mediums, from text messages to emails to third-party applications
    and everything in between.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报**对应于在发生某些事件时设置自动触点的做法。警报可以设置在*指标*和*日志*上，并通过各种媒介传递，从短信到电子邮件再到第三方应用程序等等。'
- en: Between these four aspects of observability, we should be able to understand
    the health of our cluster. However, it is possible to configure many different
    possible data points for metrics, logs, and even alerting. Therefore, knowing
    what to look for is important. The next section will discuss the most important
    observability areas for Kubernetes cluster and application health.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这四个可观测性方面之间，我们应该能够了解我们集群的健康状况。然而，可以配置许多不同的可能的指标、日志甚至警报。因此，了解要寻找的内容非常重要。下一节将讨论Kubernetes集群和应用程序健康最重要的可观测性领域。
- en: Understanding what matters for Kubernetes cluster and application health
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解对Kubernetes集群和应用程序健康至关重要的内容
- en: Among the vast number of possible metrics and logs that Kubernetes or third-party
    observability solutions for Kubernetes can provide, we can narrow down some of
    the ones that are most likely to cause major issues with your cluster. You should
    keep these pieces front and center in whichever observability solution you end
    up using. First, let's look at the connection between CPU usage and cluster health.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes或第三方可观测性解决方案提供的大量可能的指标和日志中，我们可以缩小一些最有可能导致集群出现重大问题的指标。无论您最终选择使用哪种可观测性解决方案，您都应该将这些要点放在最显眼的位置。首先，让我们看一下CPU使用率与集群健康之间的关系。
- en: Node CPU usage
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点CPU使用率
- en: The state of CPU usage across the Nodes in your Kubernetes cluster is a very
    important metric to keep an eye on across your observability solution. We've discussed
    in previous chapters how Pods can define resource requests and limits for CPU
    usage. However, it is still possible for Nodes to oversubscribe their CPU usage
    when the limits are set higher than the maximum CPU capacity of the cluster. Additionally,
    the master Nodes that run our control plane can also encounter CPU capacity issues.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的可观测性解决方案中，跨Kubernetes集群节点的CPU使用率状态是一个非常重要的指标。我们在之前的章节中已经讨论过，Pod可以为CPU使用率定义资源请求和限制。然而，当限制设置得比集群的最大CPU容量更高时，节点仍然可能过度使用CPU。此外，运行我们控制平面的主节点也可能遇到CPU容量问题。
- en: Worker Nodes with maxed-out CPUs may perform poorly or throttle workloads running
    on Pods. This can easily occur if no limits are set on Pods – or if a Node's total
    Pod resource limits are greater than its max capacity, even if its total resource
    requests are lower. Master Nodes with capped-out CPUs may hurt the performance
    of the scheduler, kube-apiserver, or any of the other control plane components.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CPU使用率达到最大的工作节点可能会表现不佳，或者限制运行在Pod上的工作负载。如果Pod上没有设置限制，或者节点的总Pod资源限制大于其最大容量，即使其总资源请求较低，也很容易发生这种情况。CPU使用率达到最大的主节点可能会影响调度器、kube-apiserver或其他控制平面组件的性能。
- en: In general, CPU usage across worker and master Nodes should be visible in your
    observability solution. This is best done via a combination of metrics (for instance
    on a charting solution such as Grafana, which you'll learn about later in this
    chapter) – and alerts for high CPU usage across the nodes in your cluster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，工作节点和主节点的CPU使用率应该在您的可观测性解决方案中可见。最好的方法是通过一些指标（例如在本章后面将要介绍的Grafana等图表解决方案）以及对集群中节点的高CPU使用率的警报来实现。
- en: Memory usage is also an extremely important metric to keep track of, similar
    to with CPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 内存使用率也是一个非常重要的指标，与CPU类似，需要密切关注。
- en: Node memory usage
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点内存使用率
- en: As with CPU usage, memory usage is an extremely important metric to observe
    across your cluster. Memory usage can be oversubscribed using Pod Resource Limits
    – and many of the same issues as with CPU usage can apply for both the master
    and worker Nodes in the cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与CPU使用率一样，内存使用率也是集群中需要密切观察的一个极其重要的指标。内存使用率可以通过Pod资源限制进行过度使用，对于集群中的主节点和工作节点都可能出现与CPU使用率相同的问题。
- en: Again, a combination of alerting and metrics is important for visibility into
    cluster memory usage. We will learn some tools for this later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，警报和指标的组合对于查看集群内存使用情况非常重要。我们将在本章后面学习一些工具。
- en: For the next major observability piece, we will look not at metrics but at logs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要的可观察性部分，我们将不再关注指标，而是关注日志。
- en: Control plane logging
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制平面日志记录
- en: The components of the Kubernetes control plane, when running, output logs that
    can be used to get an in-depth view of cluster operations. These logs can also
    significantly help with troubleshooting, as we'll see in [*Chapter 10*](B14790_10_Final_PG_ePub.xhtml#_idTextAnchor230),
    *Troubleshooting Kubernetes*. Logs for the Kubernetes API server, controller manager,
    scheduler, kube proxy, and kubelet can all be very useful for certain troubleshooting
    or observability reasons.
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当运行时，Kubernetes控制平面的组件会输出日志，这些日志可以用于深入了解集群操作。正如我们将在《第10章》[*Chapter 10*](B14790_10_Final_PG_ePub.xhtml#_idTextAnchor230)中看到的那样，这些日志也可以在故障排除中起到重要作用，*故障排除Kubernetes*。Kubernetes
    API服务器、控制器管理器、调度程序、kube代理和kubelet的日志对于某些故障排除或可观察性原因都非常有用。
- en: Application logging
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序日志记录
- en: Application logging can also be incorporated into an observability stack for
    Kubernetes – being able to view application logs along with other metrics can
    be very helpful to operators.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序日志记录也可以并入Kubernetes的可观察性堆栈中——能够查看应用程序日志以及其他指标可能对操作员非常有帮助。
- en: Application performance metrics
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序性能指标
- en: As with application logging, application performance metrics and monitoring
    are highly relevant to the performance of your applications on Kubernetes. Memory
    usage and CPU profiling at the application level can be a valuable piece of the
    observability stack.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与应用程序日志记录一样，应用程序性能指标和监控对于在Kubernetes上运行的应用程序的性能非常重要。在应用程序级别进行内存使用和CPU分析可以成为可观察性堆栈中有价值的一部分。
- en: Generally, Kubernetes provides the data infrastructure for application monitoring
    and logging but stays away from providing higher-level functionality such as charting
    and searching. With this in mind, let's review the tools that Kubernetes gives
    us by default to address these concerns.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Kubernetes提供了应用程序监控和日志记录的数据基础设施，但不提供诸如图表和搜索等更高级的功能。考虑到这一点，让我们回顾一下Kubernetes默认提供的工具，以解决这些问题。
- en: Using default observability tooling
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用默认的可观察性工具
- en: Kubernetes provides observability tooling even without adding any third-party
    solutions. These native Kubernetes tools form the basis of many of the more robust
    solutions, so they are important to discuss. Since observability includes metrics,
    logs, traces, and alerts, we will discuss each in turn, focusing first on the
    Kubernetes-native solutions. First, let's discuss metrics.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes甚至在不添加任何第三方解决方案的情况下就提供了可观察性工具。这些本机Kubernetes工具构成了许多更强大解决方案的基础，因此讨论它们非常重要。由于可观察性包括指标、日志、跟踪和警报，我们将依次讨论每个内容，首先关注Kubernetes本机解决方案。首先，让我们讨论指标。
- en: Metrics on Kubernetes
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes上的指标
- en: A lot of information about your applications can be gained by simply running
    `kubectl describe pod`. We can see information about our Pod's spec, what state
    it is in, and key issues preventing its functionality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单运行`kubectl describe pod`，可以获得关于应用程序的大量信息。我们可以看到有关Pod规范的信息，它所处的状态以及阻止其功能的关键问题。
- en: 'Let''s assume we are having some trouble with our application. Specifically,
    the Pod is not starting. To investigate, we run `kubectl describe pod`. As a reminder
    on kubectl aliases mentioned in [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, `kubectl describe pod` is the same as `kubectl
    describe pods`. Here is an example output from the `describe pod` command – we''ve
    stripped out everything apart from the `Events` information:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的应用程序出现了一些问题。具体来说，Pod没有启动。为了调查，我们运行`kubectl describe pod`。作为[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)中提到的kubectl别名的提醒，`kubectl
    describe pod`与`kubectl describe pods`是相同的。这是`describe pod`命令的一个示例输出 - 我们除了`Events`信息之外剥离了所有内容：
- en: '![Figure 9.1 – Describe Pod Events output](image/B14790_09_001_new.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 - 描述Pod事件输出](image/B14790_09_001_new.jpg)'
- en: Figure 9.1 – Describe Pod Events output
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 - 描述Pod事件输出
- en: As you can see, this Pod is not being scheduled because our Nodes are all out
    of memory! That would be a good thing to investigate further.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这个Pod没有被调度，因为我们的Nodes都没有内存了！这将是一个值得进一步调查的好事。
- en: 'Let''s keep going. By running `kubectl describe nodes`, we can learn a lot
    about our Kubernetes Nodes. Some of this information can be very relevant to how
    our system is performing. Here''s another example output, this time from the `kubectl
    describe nodes` command. Rather than putting the entire output here, which can
    be quite lengthy, let''s zero in on two important sections – `Conditions` and
    `Allocated resources`. First, let''s review the `Conditions` section:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续。通过运行`kubectl describe nodes`，我们可以了解很多关于我们的Kubernetes Nodes的信息。其中一些信息对我们系统的性能非常重要。这是另一个示例输出，这次是来自`kubectl
    describe nodes`命令。而不是将整个输出放在这里，因为可能会相当冗长，让我们聚焦在两个重要部分 - `Conditions`和`Allocated
    resources`。首先，让我们回顾一下`Conditions`部分：
- en: '![Figure 9.2 – Describe Node Conditions output](image/B14790_09_002_new.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 - 描述Node条件输出](image/B14790_09_002_new.jpg)'
- en: Figure 9.2 – Describe Node Conditions output
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 - 描述Node条件输出
- en: As you can see, we have included the `Conditions` block of the `kubectl describe
    nodes` command output. It's a great place to look for any issues. As we can see
    here, our Node is actually experiencing issues. Our `MemoryPressure` condition
    is true, and the `Kubelet` has insufficient memory. No wonder our Pods won't schedule!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们已经包含了`kubectl describe nodes`命令输出的`Conditions`块。这是查找任何问题的好地方。正如我们在这里看到的，我们的Node实际上正在遇到问题。我们的`MemoryPressure`条件为true，而`Kubelet`内存不足。难怪我们的Pod无法调度！
- en: 'Next, check out the `Allocated resources` block:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，检查`分配的资源`块：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we're seeing some metrics! It looks like our Pods are requesting too much
    memory, leading to our Node and Pod issues. As you can tell from this output,
    Kubernetes is already collecting metrics data about our Nodes, by default. Without
    that data, the scheduler would not be able to do its job properly, since maintaining
    Pod resources requests with Node capacity is one of its most important functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到了一些指标！看起来我们的Pod正在请求过多的内存，导致我们的Node和Pod出现问题。从这个输出中可以看出，Kubernetes默认已经在收集有关我们的Nodes的指标数据。没有这些数据，调度器将无法正常工作，因为维护Pod资源请求与Node容量是其最重要的功能之一。
- en: However, by default, these metrics are not surfaced to the user. They are in
    fact being collected by each Node's `Kubelet` and delivered to the scheduler for
    it to do its job. Thankfully, we can easily get these metrics by deploying Metrics
    Server to our cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，默认情况下，这些指标不会向用户显示。实际上，它们是由每个Node的`Kubelet`收集并传递给调度器来完成其工作。幸运的是，我们可以通过部署Metrics
    Server轻松地获取这些指标到我们的集群中。
- en: Metrics Server is an officially supported Kubernetes application that collects
    metrics information and surfaces it on an API endpoint for use. Metrics Server
    is in fact required to make the Horizontal Pod Autoscaler work, but it is not
    always included by default, depending on the Kubernetes distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Metrics Server是一个官方支持的Kubernetes应用程序，它收集指标信息并在API端点上公开它以供使用。实际上，Metrics Server是使水平Pod自动缩放器工作所必需的，但它并不总是默认包含在内，这取决于Kubernetes发行版。
- en: 'Deploying Metrics Server is very quick. As of the writing of this book, the
    newest version can be installed using the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Metrics Server非常快速。在撰写本书时，可以使用以下命令安装最新版本：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Important note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Full documentation on how to use Metrics Server can be found at [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用Metrics Server的完整文档可以在[https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)找到。
- en: Once Metrics Server is running, we can use a brand-new Kubernetes command. The
    `kubectl top` command can be used with either Pods or Nodes to see granular information
    about how much memory and CPU capacity is in use.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Metrics Server运行起来，我们就可以使用一个全新的Kubernetes命令。`kubectl top`命令可用于Pod或节点，以查看有关内存和CPU使用量的详细信息。
- en: 'Let''s take a look at some example usage. Run `kubectl top nodes` to see Node-level
    metrics. Here''s the output of the command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些示例用法。运行`kubectl top nodes`以查看节点级别的指标。以下是命令的输出：
- en: '![Figure 9.3 – Node Metrics output](image/B14790_09_003_new.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3-节点指标输出](image/B14790_09_003_new.jpg)'
- en: Figure 9.3 – Node Metrics output
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3-节点指标输出
- en: As you can see, we are able to see both absolute and relative CPU and memory
    usage.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们能够看到绝对和相对的CPU和内存使用情况。
- en: Important note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: CPU cores are measured in `millcpu` or `millicores`. 1,000 `millicores` is equivalent
    to one virtual CPU. Memory is measured in bytes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: CPU核心以`millcpu`或`millicores`来衡量。1000`millicores`相当于一个虚拟CPU。内存以字节为单位。
- en: Next, let's take a look at the `kubectl top pods` command. Run it with the `–namespace
    kube-system` flag to see Pods in the `kube-system` namespace.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看一下`kubectl top pods`命令。使用`-namespace kube-system`标志运行它，以查看`kube-system`命名空间中的Pod。
- en: 'To do this, we run the following command:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们运行以下命令：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And we get the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到以下输出：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, this command uses the same absolute units as `kubectl top nodes`
    – millicores and bytes. There are no relative percentages when looking at Pod-level
    metrics.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，该命令使用与`kubectl top nodes`相同的绝对单位-毫核和字节。在查看Pod级别的指标时，没有相对百分比。
- en: Next, we'll look at how Kubernetes handles logging.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下Kubernetes如何处理日志记录。
- en: Logging on Kubernetes
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes上的日志记录
- en: We can split up logging on Kubernetes into two areas – *application logs* and
    *control plane logs*. Let's start with control plane logs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将Kubernetes上的日志记录分为两个领域- *应用程序日志* 和 *控制平面日志*。让我们从控制平面日志开始。
- en: Control plane logs
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制平面日志
- en: 'Control plane logs refers to the logs created by the Kubernetes control plane
    components, such as the scheduler, API server, and others. For a vanilla Kubernetes
    install, control plane logs can be found on the Nodes themselves and require direct
    access to the Nodes in order to see. For clusters with components set up to use
    `systemd`, logs are found using the `journalctl` CLI tool (refer to the following
    link for more information: [https://manpages.debian.org/stretch/systemd/journalctl.1.en.html](https://manpages.debian.org/stretch/systemd/journalctl.1.en.html)
    ).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面日志是指由Kubernetes控制平面组件（如调度程序、API服务器等）创建的日志。对于纯净的Kubernetes安装，控制平面日志可以在节点本身找到，并且需要直接访问节点才能查看。对于设置为使用`systemd`的组件的集群，日志可以使用`journalctl`CLI工具找到（有关更多信息，请参阅以下链接：[https://manpages.debian.org/stretch/systemd/journalctl.1.en.html](https://manpages.debian.org/stretch/systemd/journalctl.1.en.html)）。
- en: 'On master Nodes, you can find logs in the following locations on the filesystem:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在主节点上，您可以在文件系统的以下位置找到日志：
- en: At `/var/log/kube-scheduler.log`, you can find the Kubernetes scheduler logs.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`/var/log/kube-scheduler.log`中，您可以找到Kubernetes调度器的日志。
- en: At `/var/log/kube-controller-manager.log`, you can find the controller manager
    logs (for instance, to see scaling events).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`/var/log/kube-controller-manager.log`中，您可以找到控制器管理器的日志（例如，查看扩展事件）。
- en: At `/var/log/kube-apiserver.log`, you can find the Kubernetes API server logs.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`/var/log/kube-apiserver.log`中，您可以找到Kubernetes API服务器的日志。
- en: 'On worker Nodes, logs are available in two locations on the filesystem:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作节点上，日志可以在文件系统的两个位置找到：
- en: At `/var/log/kubelet.log`, you can find the kubelet logs.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`/var/log/kubelet.log`中，您可以找到kubelet的日志。
- en: At `/var/log/kube-proxy.log`, you can find the kube proxy logs.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`/var/log/kube-proxy.log`中，您可以找到kube代理的日志。
- en: Although, generally, cluster health is influenced by the health of the Kubernetes
    master and worker Node components, it is of course also important to keep track
    of your application's logs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通常情况下，集群健康受Kubernetes主节点和工作节点组件的影响，但跟踪应用程序日志也同样重要。
- en: Application logs
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用程序日志
- en: It's very easy to find application logs on Kubernetes. Before we explain how
    it works, let's look at an example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上查找应用程序日志非常容易。在我们解释它是如何工作之前，让我们看一个例子。
- en: 'To check logs for a specific Pod, you can use the `kubectl logs <pod_name>`
    command. The output of the command will display any text written to the container''s
    `stdout` or `stderr`. If a Pod has multiple containers, you must include the container
    name in the command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查特定Pod的日志，可以使用`kubectl logs <pod_name>`命令。该命令的输出将显示写入容器的`stdout`或`stderr`的任何文本。如果一个Pod有多个容器，您必须在命令中包含容器名称：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Under the hood, Kubernetes handles Pod logs by using the container engine's
    logging driver. Typically, any logs to `stdout` or `stderr` are persisted to each
    Node's disk in the `/var/logs` folder. Depending on the Kubernetes distribution,
    log rotations may be set up to prevent overuse of Node disk space by logs. In
    addition, Kubernetes components such as the scheduler, kubelet, and kube-apiserver
    also persist logs to Node disk space, usually within the `/var/logs` folder. It
    is important to note how limited this default logging capability is – a robust
    observability stack for Kubernetes would certainly include a third-party solution
    for log forwarding, as we'll see shortly.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，Kubernetes通过使用容器引擎的日志驱动程序来处理Pod日志。通常，任何写入`stdout`或`stderr`的日志都会被持久化到每个节点的磁盘中的`/var/logs`文件夹中。根据Kubernetes的分发情况，可能会设置日志轮换，以防止日志占用节点磁盘空间过多。此外，Kubernetes组件，如调度器、kubelet和kube-apiserver也会将日志持久化到节点磁盘空间中，通常在`/var/logs`文件夹中。重要的是要注意默认日志记录功能的有限性
    - Kubernetes的强大可观察性堆栈肯定会包括第三方解决方案用于日志转发，我们很快就会看到。
- en: Next, for general Kubernetes observability, we can use Kubernetes Dashboard.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于一般的Kubernetes可观察性，我们可以使用Kubernetes仪表板。
- en: Installing Kubernetes Dashboard
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Kubernetes仪表板
- en: Kubernetes Dashboard provides all of the functionality of kubectl – including
    viewing logs and editing resources – in a GUI. It's very easy to get the dashboard
    set up – let's see how.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes仪表板提供了kubectl的所有功能，包括查看日志和编辑资源，都可以在图形界面中完成。设置仪表板非常容易，让我们看看如何操作。
- en: The dashboard can be installed in a single `kubectl apply` command. For customizations,
    check out the Kubernetes Dashboard GitHub page at [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板可以通过单个`kubectl apply`命令安装。要进行自定义，请查看Kubernetes仪表板GitHub页面[https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard)。
- en: 'To install a version of Kubernetes Dashboard, run the following `kubectl` command,
    replacing the `<VERSION>` tag with your desired version, based on the version
    of Kubernetes you are using (again, check the Dashboard GitHub page for version
    compatibility):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Kubernetes仪表板的版本，请运行以下`kubectl`命令，将`<VERSION>`标签替换为您所需的版本，根据您正在使用的Kubernetes版本（再次检查Dashboard
    GitHub页面以获取版本兼容性）：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In our case, as of the writing of this book, we will use v2.0.4 – the final
    command looks like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，截至本书撰写时，我们将使用v2.0.4 - 最终的命令看起来像这样：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once Kubernetes Dashboard has been installed, there are a few methods to access
    it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了Kubernetes仪表板后，有几种方法可以访问它。
- en: Important note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is not usually recommended to use Ingress or a public load balancer service,
    because Kubernetes Dashboard allows users to update cluster objects. If for some
    reason your login methods for the dashboard are compromised or easy to figure
    out, you could be looking at a large security risk.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通常不建议使用Ingress或公共负载均衡器服务，因为Kubernetes仪表板允许用户更新集群对象。如果由于某种原因您的仪表板登录方法受到损害或容易被发现，您可能面临着很大的安全风险。
- en: With that in mind, we can use either `kubectl port-forward` or `kubectl proxy`
    in order to view our dashboard from our local machine.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们可以使用`kubectl port-forward`或`kubectl proxy`来从本地机器查看我们的仪表板。
- en: For this example, we will use the `kubectl proxy` command, because we haven't
    used it in an example yet.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用`kubectl proxy`命令，因为我们还没有在示例中使用过它。
- en: The `kubectl proxy` command, unlike the `kubectl port-forward` command, requires
    only one command to proxy to every service running on your cluster. It does this
    by proxying the Kubernetes API directly to a port on your local machine, which
    is by default `8081`. For a full discussion of the `Kubectl proxy` command, check
    the docs at [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl proxy`命令与`kubectl port-forward`命令不同，它只需要一个命令即可代理到集群上运行的每个服务。它通过直接将Kubernetes
    API代理到本地机器上的一个端口来实现这一点，默认端口为`8081`。有关`kubectl proxy`命令的详细讨论，请查看[https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#proxy)上的文档。'
- en: 'In order to access a specific Kubernetes service using `kubectl proxy`, you
    just need to have the right path. The path to access Kubernetes Dashboard after
    running `kubectl proxy` will be the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用`kubectl proxy`访问特定的Kubernetes服务，您只需要正确的路径。运行`kubectl proxy`后访问Kubernetes仪表板的路径将如下所示：
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the `kubectl proxy` path we put in our browser is on localhost
    port `8001`, and mentions the namespace (`kubernetes-dashboard`), the service
    name and selector (`https:kubernetes-dashboard`), and a proxy path.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们在浏览器中放置的`kubectl proxy`路径是在本地主机端口`8001`上，并提到了命名空间（`kubernetes-dashboard`）、服务名称和选择器（`https:kubernetes-dashboard`）以及代理路径。
- en: 'Let''s put our Kubernetes Dashboard URL in a browser and see the result:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将Kubernetes仪表板的URL放入浏览器中并查看结果：
- en: '![Figure 9.4 – Kubernetes Dashboard login](image/B14790_09_004_new.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 - Kubernetes仪表板登录](image/B14790_09_004_new.jpg)'
- en: Figure 9.4 – Kubernetes Dashboard login
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 - Kubernetes仪表板登录
- en: When we deploy and access Kubernetes Dashboard, we are met with a login screen.
    We can either create a Service Account (or use our own) to log in to the dashboard,
    or simply link our local `Kubeconfig` file. By logging in to Kubernetes Dashboard
    with a specific Service Account's token, the dashboard user will inherit that
    Service Account's permissions. This allows you to specify what type of actions
    a user will be able to take using Kubernetes Dashboard – for instance, read-only
    permissions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署和访问 Kubernetes 仪表板时，我们会看到一个登录界面。我们可以创建一个服务账户（或使用我们自己的）来登录仪表板，或者简单地链接我们的本地
    `Kubeconfig` 文件。通过使用特定服务账户的令牌登录到 Kubernetes 仪表板，仪表板用户将继承该服务账户的权限。这允许您指定用户将能够使用
    Kubernetes 仪表板执行哪种类型的操作 - 例如，只读权限。
- en: 'Let''s go ahead and create a brand-new Service Account for our Kubernetes Dashboard.
    You could customize this Service Account and limit its permissions, but for now
    we will give it admin permissions. To do this, follow these steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续为我们的 Kubernetes 仪表板创建一个全新的服务账户。您可以自定义此服务账户并限制其权限，但现在我们将赋予它管理员权限。要做到这一点，请按照以下步骤操作：
- en: 'We can create a Service Account imperatively using the following Kubectl command:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下 Kubectl 命令来命令式地创建一个服务账户：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output, confirming the creation of our Service
    Account:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出，确认了我们服务账户的创建：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we need to link our Service Account to a ClusterRole. You could also use
    a Role, but we want our dashboard user to be able to access all namespaces. To
    link a Service Account to the `cluster-admin` default ClusterRole using a single
    command, we can run the following:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要将我们的服务账户链接到一个 ClusterRole。您也可以使用 Role，但我们希望我们的仪表板用户能够访问所有命名空间。为了使用单个命令将服务账户链接到
    `cluster-admin` 默认 ClusterRole，我们可以运行以下命令：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This command will result in the following output:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将产生以下输出：
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After this command is run, we should be able to log in to our dashboard! First,
    we just need to find the token that we will use to log in. A Service Account''s
    token is stored as a Kubernetes secret, so let''s see what it looks like. Run
    the following command to see which secret our token is stored in:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此命令后，我们应该能够登录到我们的仪表板！首先，我们只需要找到我们将用于登录的令牌。服务账户的令牌存储为 Kubernetes 秘密，所以让我们看看它是什么样子。运行以下命令以查看我们的令牌存储在哪个秘密中：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the output, you should see a secret that looks like the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，您应该会看到一个类似以下的秘密：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, to get our token for signing in to the dashboard, we only need to describe
    the secret contents using the following:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了获取我们用于登录到仪表板的令牌，我们只需要使用以下命令描述秘密内容：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting output will look like the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出将如下所示：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To log in to the dashboard, copy the string next to `token`, copy it into the
    token input on the Kubernetes Dashboard login screen, and click **Sign In**. You
    should be greeted with the Kubernetes Dashboard overview page!
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要登录到仪表板，复制`token`旁边的字符串，将其复制到 Kubernetes 仪表板登录界面上的令牌输入中，然后点击**登录**。您应该会看到 Kubernetes
    仪表板概览页面！
- en: Go ahead and click around the dashboard – you should be able to see all the
    same resources you would be able to using kubectl, and you can filter by namespace
    in the left-hand sidebar. For instance, here's a view of the **Namespaces** page:![Figure
    9.5 – Kubernetes Dashboard detail](image/B14790_09_005_new.jpg)
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续在仪表板上点击 - 您应该能够看到您可以使用 kubectl 查看的所有相同资源，并且您可以在左侧边栏中按命名空间进行过滤。例如，这是一个**命名空间**页面的视图：![图
    9.5 - Kubernetes 仪表板详细信息](image/B14790_09_005_new.jpg)
- en: Figure 9.5 – Kubernetes Dashboard detail
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 - Kubernetes 仪表板详细信息
- en: You can also click on individual resources, and even edit those resources using
    the dashboard as long as the Service Account you used to log in has the proper
    permissions.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以单击单个资源，甚至使用仪表板编辑这些资源，只要您用于登录的服务帐户具有适当的权限。
- en: 'Here''s a view of editing a Deployment resource from the deployment detail
    page:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从部署详细页面编辑部署资源的视图：
- en: '![Figure 9.6 – Kubernetes Dashboard edit view](image/B14790_09_006_new.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – Kubernetes仪表板编辑视图](image/B14790_09_006_new.jpg)'
- en: Figure 9.6 – Kubernetes Dashboard edit view
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – Kubernetes仪表板编辑视图
- en: Kubernetes Dashboard also lets you view Pod logs and dive into many other resource
    types in your cluster. To understand the full capabilities of the dashboard, check
    the docs at the previously mentioned GitHub page.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes仪表板还允许您查看Pod日志，并深入了解集群中的许多其他资源类型。要了解仪表板的全部功能，请查看先前提到的GitHub页面上的文档。
- en: Finally, to round out our discussion of default observability on Kubernetes,
    let's take a look at alerting.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了完成我们对Kubernetes上默认可观察性的讨论，让我们来看一下警报。
- en: Alerts and traces on Kubernetes
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes上的警报和跟踪
- en: Unfortunately, the last two pieces of the observability puzzle – *alerts* and
    *traces* – are not yet native pieces of functionality on Kubernetes. In order
    to create this type of functionality, let's move on to our next section – incorporating
    open source tooling from the Kubernetes ecosystem.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，可观察性谜题的最后两个部分——*警报*和*跟踪*——目前还不是Kubernetes上的本机功能。为了创建这种类型的功能，让我们继续我们的下一节——从Kubernetes生态系统中整合开源工具。
- en: Enhancing Kubernetes observability using the best of the ecosystem
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用生态系统最佳增强Kubernetes的可观察性
- en: As we've discussed, though Kubernetes provides the basis for powerful visibility
    functionality, it is generally up to the community and vendor ecosystem to create
    higher-level tooling for metrics, logging, traces, and alerting. For the purposes
    of this book, we will focus on fully open source, self-hosted solutions. Since
    many of these solutions fulfill multiple visibility pillars between metrics, logs,
    traces, and alerting, instead of categorizing solutions into each visibility pillar
    during our review, we will review each solution separately.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，尽管Kubernetes提供了强大的可见性功能的基础，但通常是由社区和供应商生态系统来创建用于度量、日志、跟踪和警报的高级工具。对于本书的目的，我们将专注于完全开源的自托管解决方案。由于许多这些解决方案在度量、日志、跟踪和警报之间满足多个可见性支柱的需求，因此在我们的审查过程中，我们将分别审查每个解决方案，而不是将解决方案分类到每个可见性支柱中。
- en: 'Let''s start with an often-used combination of technologies for metrics and
    alerts: **Prometheus** and **Grafana**.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从用于度量和警报的技术常用组合**Prometheus**和**Grafana**开始。
- en: Introducing Prometheus and Grafana
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Prometheus和Grafana
- en: Prometheus and Grafana are a typical combination of visibility technologies
    on Kubernetes. Prometheus is a time series database, query layer, and alerting
    system with many integrations, while Grafana is a sophisticated graphing and visualization
    layer that integrates with Prometheus. We'll walk you through the installation
    and usage of these tools, starting with Prometheus.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus和Grafana是Kubernetes上典型的可见性技术组合。Prometheus是一个时间序列数据库、查询层和具有许多集成的警报系统，而Grafana是一个复杂的图形和可视化层，与Prometheus集成。我们将带您了解这些工具的安装和使用，从Prometheus开始。
- en: Installing Prometheus and Grafana
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Prometheus和Grafana
- en: There are many ways to install Prometheus on Kubernetes, but most use Deployments
    in order to scale the service. For our purposes, we will be using the `kube-prometheus`
    project ([https://github.com/coreos/kube-prometheus](https://github.com/coreos/kube-prometheus)).
    This project includes an `operator` as well as several **custom resource definitions**
    (**CRDs**). It will also automatically install Grafana for us!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多在 Kubernetes 上安装 Prometheus 的方法，但大多数都使用部署来扩展服务。对于我们的目的，我们将使用 `kube-prometheus`
    项目（[https://github.com/coreos/kube-prometheus](https://github.com/coreos/kube-prometheus)）。该项目包括一个
    `operator` 以及几个**自定义资源定义**（**CRDs**）。它还将自动为我们安装 Grafana！
- en: An operator is essentially an application controller on Kubernetes (deployed
    like other applications in a Pod) that happens to make commands to the Kubernetes
    API in order to correctly run or operate its application.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 操作员本质上是 Kubernetes 上的一个应用控制器（像其他 Pod 中部署的应用程序一样部署），它恰好会向 Kubernetes API 发出命令，以便正确运行或操作其应用程序。
- en: A CRD, on the other hand, allows us to model custom functionality inside of
    the Kubernetes API. We'll learn a lot more about operators and CRDs in [*Chapter
    13*](B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289), *Extending Kubernetes with
    CRDs*, but for now just think of operators as a way to create *smart deployments*
    where the application can control itself properly and spin up other Pods and Deployments
    as necessary – and think of CRDs as a way to use Kubernetes to store application-specific
    concerns.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，CRD 允许我们在 Kubernetes API 内部建模自定义功能。我们将在[*第 13 章*](B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289)中学到更多关于操作员和
    CRDs 的知识，但现在只需将操作员视为创建*智能部署*的一种方式，其中应用程序可以正确地控制自身并根据需要启动其他 Pod 和部署 – 将 CRD 视为一种使用
    Kubernetes 存储特定应用程序关注点的方式。
- en: 'To install Prometheus, first we need to download a release, which may be different
    depending on the newest version of Prometheus or your intended version of Kubernetes:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Prometheus，首先我们需要下载一个发布版，这可能会因 Prometheus 的最新版本或您打算使用的 Kubernetes 版本而有所不同：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, unzip the file using any tool. First, we're going to need to install the
    CRDs. In general, most Kubernetes tooling installation instructions will have
    you create the CRDs on Kubernetes first, since any additional setup that uses
    the CRD will fail if the underlying CRD has not already been created on Kubernetes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用任何工具解压文件。首先，我们需要安装 CRDs。一般来说，大多数 Kubernetes 工具安装说明都会让您首先在 Kubernetes 上创建
    CRDs，因为如果底层 CRD 尚未在 Kubernetes 上创建，那么任何使用 CRD 的其他设置都将失败。
- en: 'Let''s install them using the following command:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令安装它们：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''ll need to wait a few seconds while the CRDs are created. This command
    will also create a `monitoring` namespace for our resources to live in. Once everything
    is ready, let''s spin up the rest of the Prometheus and Grafana resources using
    the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 CRDs 时，我们需要等待几秒钟。此命令还将为我们的资源创建一个 `monitoring` 命名空间。一旦一切准备就绪，让我们使用以下命令启动其余的
    Prometheus 和 Grafana 资源：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s talk about what this command will actually create. The entire stack
    consists of the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈这个命令实际上会创建什么。整个堆栈包括以下内容：
- en: '**Prometheus Deployment**: Pods of the Prometheus application'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus 部署**：Prometheus 应用程序的 Pod'
- en: '**Prometheus Operator**: Controls and operates the Prometheus app Pods'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus 操作员**：控制和操作 Prometheus 应用程序 Pod'
- en: '**Alertmanager Deployment**: A Prometheus component to specify and trigger
    alerts'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alertmanager 部署**：用于指定和触发警报的 Prometheus 组件'
- en: '**Grafana**: A powerful visualization dashboard'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Grafana**：一个强大的可视化仪表板'
- en: '**Kube-state-metrics agent**: Generates metrics from the Kubernetes API state'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kube-state-metrics 代理**：从 Kubernetes API 状态生成指标'
- en: '**Prometheus Node Exporter**: Exports Node hardware- and OS-level metrics to
    Prometheus'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus 节点导出器**：将节点硬件和操作系统级别的指标导出到 Prometheus'
- en: '**Prometheus Adapter for Kubernetes Metrics**: Adapter for Kubernetes Resource
    Metrics API and Custom Metrics API for ingest into Prometheus'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于Kubernetes指标的Prometheus适配器**：用于将Kubernetes资源指标API和自定义指标API摄入到Prometheus中'
- en: Together, all these components will provide sophisticated visibility into our
    cluster, from the command plane down to the application containers themselves.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些组件将为我们的集群提供复杂的可见性，从命令平面到应用程序容器本身。
- en: Once the stack has been created (check by using the `kubectl get po -n monitoring`
    command), we can start using our components. Let's dive into usage, starting with
    plain Prometheus.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦堆栈已经创建（通过使用`kubectl get po -n monitoring`命令进行检查），我们就可以开始使用我们的组件。让我们从简单的Prometheus开始使用。
- en: Using Prometheus
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Prometheus
- en: Though the real power of Prometheus is in its data store, query, and alert layer,
    it does provide a simple UI to developers. As you'll see later, Grafana provides
    many more features and customizations, but it is worth it to get acquainted with
    the Prometheus UI.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Prometheus的真正力量在于其数据存储、查询和警报层，但它确实为开发人员提供了一个简单的UI。正如您将在后面看到的，Grafana提供了更多功能和自定义选项，但值得熟悉Prometheus
    UI。
- en: By default, `kube-prometheus` will only create ClusterIP services for Prometheus,
    Grafana, and Alertmanager. It's up to us to expose them outside the cluster. For
    the purposes of this tutorial, we're simply going to port forward the service
    to our local machine. For production, you may want to use Ingress to route requests
    to the three services.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`kube-prometheus`只会为Prometheus、Grafana和Alertmanager创建ClusterIP服务。我们需要将它们暴露到集群外部。在本教程中，我们只是将服务端口转发到我们的本地机器。对于生产环境，您可能希望使用Ingress将请求路由到这三个服务。
- en: 'In order to `port-forward` to the Prometheus UI service, use the `port-forward`
    kubectl command:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了`port-forward`到Prometheus UI服务，使用`port-forward` kubectl命令：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We need to use port `9090` for the Prometheus UI. Access the service on your
    machine at `http://localhost:3000`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用端口`9090`来访问Prometheus UI。在您的机器上访问服务`http://localhost:3000`。
- en: 'You should see something like the following screenshot:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似以下截图的内容：
- en: '![Figure 9.7 – Prometheus UI](image/B14790_09_007_new.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – Prometheus UI](image/B14790_09_007_new.jpg)'
- en: Figure 9.7 – Prometheus UI
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – Prometheus UI
- en: As you can see, the Prometheus UI has a **Graph** page, which is what you can
    see in *Figure 9.4*. It also has its own UI for seeing configured alerts – but
    it doesn't allow you to create alerts via the UI. Grafana and Alertmanager will
    help us for that task.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，Prometheus UI有一个**Graph**页面，这就是您在*图9.4*中看到的内容。它还有自己的UI用于查看配置的警报 – 但它不允许您通过UI创建警报。Grafana和Alertmanager将帮助我们完成这项任务。
- en: 'To perform a query, navigate to the **Graph** page and enter the query command
    into the **Expression** bar, then click **Execute**. Prometheus uses a query language
    called `PromQL` – we won''t present it fully to you in this book, but the Prometheus
    docs are a great way to learn. You can refer to it using the following link: [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行查询，导航到**Graph**页面，并将查询命令输入到**Expression**栏中，然后单击**Execute**。Prometheus使用一种称为`PromQL`的查询语言
    – 我们不会在本书中完全向您介绍它，但Prometheus文档是学习的好方法。您可以使用以下链接进行参考：[https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/)。
- en: 'To show how this works, let''s enter a basic query, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这是如何工作的，让我们输入一个基本的查询，如下所示：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This query will list the total number of HTTP requests made to the kubelet
    on each Node, for each request category, as shown in the following screenshot:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询将列出每个节点上发送到kubelet的HTTP请求的总数，对于每个请求类别，如下截图所示：
- en: '![Figure 9.8 – HTTP requests query](image/B14790_09_008_new.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – HTTP请求查询](image/B14790_09_008_new.jpg)'
- en: Figure 9.8 – HTTP requests query
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – HTTP请求查询
- en: 'You can also see the requests in graph form by clicking the **Graph** tab next
    to **Table** as shown in the following screenshot:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过单击**表**旁边的**图表**选项卡以图形形式查看请求，如下截图所示：
- en: '![Figure 9.9 – HTTP requests query – graph view](image/B14790_09_009_new.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – HTTP请求查询 – 图表视图](image/B14790_09_009_new.jpg)'
- en: Figure 9.9 – HTTP requests query – graph view
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – HTTP请求查询 – 图表视图
- en: This provides a time series graph view of the data from the preceding screenshot.
    As you can see, the graphing capability is fairly simple.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了来自前面截图数据的时间序列图表视图。正如您所见，图表功能相当简单。
- en: Prometheus also provides an **Alerts** tab for configuring Prometheus alerts.
    Typically, these alerts are configured via code instead of using the **Alerts**
    tab UI, so we will skip that page in our review. For more information, you can
    check the official Prometheus documentation at [https://prometheus.io/docs/alerting/latest/overview/](https://prometheus.io/docs/alerting/latest/overview/).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus还提供了一个**警报**选项卡，用于配置Prometheus警报。通常，这些警报是通过代码配置而不是使用**警报**选项卡UI配置的，所以我们将在审查中跳过该页面。有关更多信息，您可以查看官方的Prometheus文档[https://prometheus.io/docs/alerting/latest/overview/](https://prometheus.io/docs/alerting/latest/overview/)。
- en: Let's move on to Grafana, where we can extend Prometheus powerful data tooling
    with visualizations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前往Grafana，在那里我们可以通过可视化扩展Prometheus强大的数据工具。
- en: Using Grafana
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Grafana
- en: Grafana provides powerful tools for visualizing metrics, with many supported
    charting types that can update in real time. We can connect Grafana to Prometheus
    in order to see our cluster metrics charted on the Grafana UI.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana提供了强大的工具来可视化指标，支持许多可以实时更新的图表类型。我们可以将Grafana连接到Prometheus，以便在Grafana UI上查看我们的集群指标图表。
- en: 'To get started with Grafana, do the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Grafana，请执行以下操作：
- en: 'We will end our current port forwarding (*CTRL* + *C* will do the trick) and
    set up a new port forward listener to the Grafana UI:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将结束当前的端口转发（*CTRL* + *C*即可），并设置一个新的端口转发监听器到Grafana UI：
- en: '[PRE21]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Again, navigate to `localhost:3000` to see the Grafana UI. You should be able
    to log in with **Username**: `admin` and **Password**: `admin`, at which point
    you should be able to change the initial password as shown in the following screenshot:![Figure
    9.10 – Grafana Change Password screen](image/B14790_09_010_new.jpg)'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次导航到`localhost:3000`以查看Grafana UI。您应该能够使用**用户名**：`admin`和**密码**：`admin`登录，然后您应该能够按照以下截图更改初始密码：![图9.10
    – Grafana更改密码屏幕](image/B14790_09_010_new.jpg)
- en: Figure 9.10 – Grafana Change Password screen
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – Grafana更改密码屏幕
- en: Upon login, you will see the following screen. Grafana does not come preconfigured
    with any dashboards, but we can add them easily by clicking the **+** sign as
    shown in the following screenshot:![Figure 9.11 – Grafana main page](image/B14790_09_011_new.jpg)
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，您将看到以下屏幕。Grafana不会预先配置任何仪表板，但我们可以通过单击如下截图所示的**+**号轻松添加它们：![图9.11 – Grafana主页](image/B14790_09_011_new.jpg)
- en: Figure 9.11 – Grafana main page
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – Grafana主页
- en: Each Grafana dashboard includes one or more graphs for different sets of metrics.
    To add a preconfigured dashboard (instead of creating one yourself), click the
    plus sign (**+**) on the left-hand menu bar and click **Import**. You should see
    a page like the following screenshot:![Figure 9.12 – Grafana Dashboard Import](image/B14790_09_012_new.jpg)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个Grafana仪表板都包括一个或多个不同集合的指标图。要添加一个预配置的仪表板（而不是自己创建一个），请单击左侧菜单栏上的加号（**+**）并单击**导入**。您应该会看到以下截图所示的页面：![图9.12
    – Grafana仪表板导入](image/B14790_09_012_new.jpg)
- en: Figure 9.12 – Grafana Dashboard Import
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – Grafana仪表板导入
- en: We can add a dashboard via this page either using the JSON configuration or
    by pasting in a public dashboard ID.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过此页面使用JSON配置或粘贴公共仪表板ID来添加仪表板。
- en: 'You can find public dashboards and their associated IDs at [https://grafana.com/grafana/dashboards/315](https://grafana.com/grafana/dashboards/315).
    Dashboard #315 is a great starter dashboard for Kubernetes – let''s add it to
    the textbox labeled **Grafana.com Dashboard** and click **Load**.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在 [https://grafana.com/grafana/dashboards/315](https://grafana.com/grafana/dashboards/315)
    找到公共仪表板及其关联的 ID。仪表板＃315 是 Kubernetes 的一个很好的起始仪表板 - 让我们将其添加到标有**Grafana.com 仪表板**的文本框中，然后单击**Load**。
- en: 'Then, on the next page, select the **Prometheus** data source from the **Prometheus**
    option dropdown, which is used to pick between multiple data sources if available.
    Click **Import**, and the dashboard should be loaded, which will look like the
    following screenshot:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在下一页中，从**Prometheus**选项下拉菜单中选择**Prometheus**数据源，用于在多个数据源之间进行选择（如果可用）。单击**Import**，应该加载仪表板，看起来像以下截图：
- en: '![Figure 9.13 – Grafana dashboard](image/B14790_09_013_new.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图9.13 – Grafana 仪表盘](image/B14790_09_013_new.jpg)'
- en: Figure 9.13 – Grafana dashboard
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – Grafana 仪表盘
- en: This particular Grafana dashboard provides a good high-level overview of network,
    memory, CPU, and filesystem utilization across the cluster, and it is broken down
    per Pod and container. It is configured with real-time graphs for **Network I/O
    pressure**, **Cluster memory usage**, **Cluster CPU usage**, and **Cluster filesystem
    usage** – though this last option may not be enabled depending on how you have
    installed Prometheus.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的 Grafana 仪表板提供了对集群中网络、内存、CPU 和文件系统利用率的良好高级概述，并且按照 Pod 和容器进行了细分。它配置了**网络
    I/O 压力**、**集群内存使用**、**集群 CPU 使用**和**集群文件系统使用**的实时图表 - 尽管最后一个选项可能根据您安装 Prometheus
    的方式而不启用。
- en: Finally, let's look at the Alertmanager UI.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看一下 Alertmanager UI。
- en: Using Alertmanager
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Alertmanager
- en: 'Alertmanager is an open source solution for managing alerts generated from
    Prometheus alerts. We installed Alertmanager previously as part of our stack –
    let''s take a look at what it can do:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Alertmanager 是一个用于管理从 Prometheus 警报生成的警报的开源解决方案。我们之前作为堆栈的一部分安装了 Alertmanager
    - 让我们看看它能做什么：
- en: 'First, let''s `port-forward` the Alertmanager service using the following command:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用以下命令`port-forward` Alertmanager 服务：
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As usual, navigate to `localhost:3000` to see the UI as shown in the following
    screenshot. It looks similar to the Prometheus UI:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，导航到 `localhost:3000`，查看如下截图所示的 UI。它看起来与 Prometheus UI 类似：
- en: '![Figure 9.14 – Alertmanager UI](image/B14790_09_014_new.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14 – Alertmanager UI](image/B14790_09_014_new.jpg)'
- en: Figure 9.14 – Alertmanager UI
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 – Alertmanager UI
- en: Alertmanager works together with Prometheus alerts. You can use the Prometheus
    server to specify alert rules, and then use Alertmanager to group similar alerts
    into single notifications, perform deduplications, and create *silences*, which
    are essentially a way to mute alerts if they match specific rules.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Alertmanager 与 Prometheus 警报一起工作。您可以使用 Prometheus 服务器指定警报规则，然后使用 Alertmanager
    将类似的警报分组为单个通知，执行去重，并创建*静音*，这实质上是一种静音警报的方式，如果它们符合特定规则。
- en: Next, we will review a popular logging stack for Kubernetes – Elasticsearch,
    FluentD, and Kibana.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾 Kubernetes 的一个流行日志堆栈 - Elasticsearch、FluentD 和 Kibana。
- en: Implementing the EFK stack on Kubernetes
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上实现 EFK 堆栈
- en: Similar to the popular ELK stack (Elasticsearch, Logstash, and Kibana), the
    EFK stack swaps out Logstash for the FluentD log forwarder, which is well supported
    on Kubernetes. Implementing this stack is easy and allows us to get started with
    log aggregation and search functionalities using purely open source tooling on
    Kubernetes.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于流行的 ELK 堆栈（Elasticsearch、Logstash 和 Kibana），EFK 堆栈将 Logstash 替换为 FluentD
    日志转发器，在 Kubernetes 上得到了很好的支持。实现这个堆栈很容易，让我们可以使用纯开源工具在 Kubernetes 上开始日志聚合和搜索功能。
- en: Installing the EFK stack
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 EFK 堆栈
- en: 'There are many ways to install the EFK Stack on Kubernetes, but the Kubernetes
    GitHub repository itself has some supported YAML, so let''s just use that:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上安装EFK Stack有很多种方法，但Kubernetes GitHub存储库本身有一些支持的YAML，所以让我们就使用那个吧：
- en: 'First, clone or download the Kubernetes repository using the following command:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令克隆或下载Kubernetes存储库：
- en: '[PRE23]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The manifests are located in the `kubernetes/cluster/addons` folder, specifically
    under `fluentd-elasticsearch`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清单位于`kubernetes/cluster/addons`文件夹中，具体位于`fluentd-elasticsearch`下：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For a production workload, we would likely make some changes to these manifests
    in order to properly customize the configuration for our cluster, but for the
    purposes of this tutorial we will leave everything as default. Let's start the
    process of bootstrapping our EFK stack.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产工作负载，我们可能会对这些清单进行一些更改，以便为我们的集群正确定制配置，但出于本教程的目的，我们将保留所有内容为默认值。让我们开始引导我们的EFK堆栈的过程。
- en: 'First, let''s create the Elasticsearch cluster itself. This runs as a StatefulSet
    on Kubernetes, and also provides a Service. To create the cluster, we need to
    run two `kubectl` commands:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们创建Elasticsearch集群本身。这在Kubernetes上作为一个StatefulSet运行，并提供一个Service。要创建集群，我们需要运行两个`kubectl`命令：
- en: '[PRE25]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Important note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A word of warning for the Elasticsearch StatefulSet – by default, the resource
    request for each Pod is 3 GB of memory, so if none of your Nodes have that available,
    you will not be able to deploy it as configured by default.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Elasticsearch StatefulSet的一个警告 - 默认情况下，每个Pod的资源请求为3GB内存，因此如果您的节点没有足够的可用内存，您将无法按默认配置部署它。
- en: Next, let's deploy the FluentD logging agents. These will run as a DaemonSet
    – one per Node – and forward logs from the Nodes to Elasticsearch. We also need
    to create the ConfigMap YAML, which contains the base FluentD agent configuration.
    This can be further customized to add things such as log filters and new sources.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们部署FluentD日志代理。这些将作为一个DaemonSet运行 - 每个节点一个 - 并将日志从节点转发到Elasticsearch。我们还需要创建包含基本FluentD代理配置的ConfigMap
    YAML。这可以进一步定制以添加诸如日志过滤器和新来源之类的内容。
- en: 'To install the DaemonSet for the agents and their configuration, run the following
    two `kubectl` commands:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装代理和它们的配置的DaemonSet，请运行以下两个`kubectl`命令：
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now that we''ve created the ConfigMap and the FluentD DaemonSet, we can create
    our Kibana application, which is a GUI for interacting with Elasticsearch. This
    piece runs as a Deployment, with a Service. To deploy Kibana to our cluster, run
    the final two `kubectl` commands:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了ConfigMap和FluentD DaemonSet，我们可以创建我们的Kibana应用程序，这是一个用于与Elasticsearch交互的GUI。这一部分作为一个Deployment运行，带有一个Service。要将Kibana部署到我们的集群，运行最后两个`kubectl`命令：
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once everything has been initiated, which may take several minutes, we can
    access the Kibana UI in the same way that we did Prometheus and Grafana. To check
    the status of the resources we just created, we can run the following:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有东西都已启动，这可能需要几分钟，我们就可以像我们之前对Prometheus和Grafana做的那样访问Kibana UI。要检查我们刚刚创建的资源的状态，我们可以运行以下命令：
- en: '[PRE28]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once all Pods for FluentD, Elasticsearch, and Kibana are in the **Ready** state,
    we can move on. If any of your Pods are in the **Error** or **CrashLoopBackoff**
    stage, consult the Kubernetes GitHub documentation in the `addons` folder for
    more information.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦FluentD、Elasticsearch和Kibana的所有Pod都处于**Ready**状态，我们就可以继续进行。如果您的任何Pod处于**Error**或**CrashLoopBackoff**阶段，请参阅`addons`文件夹中的Kubernetes
    GitHub文档以获取更多信息。
- en: 'Once we''ve confirmed that our components are working properly, let''s use
    the `port-forward` command to access the Kibana UI. By the way, our EFK stack
    pieces will live in the `kube-system` namespace – so our command needs to reflect
    that. So, let''s use the following command:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们确认我们的组件正常工作，让我们使用`port-forward`命令来访问Kibana UI。顺便说一句，我们的EFK堆栈组件将位于`kube-system`命名空间中
    - 因此我们的命令需要反映这一点。因此，让我们使用以下命令：
- en: '[PRE29]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This command will start a `port-forward` to your local machine's port `8080`
    from the Kibana UI.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将从Kibana UI开始一个`port-forward`到您本地机器的端口`8080`。
- en: Let's check out the Kibana UI at `localhost:8080`. It should look something
    like the following, depending on your exact version and configuration:![Figure
    9.15 – Basic Kibana UI](image/B14790_09_015_new.jpg)
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在`localhost:8080`上查看Kibana UI。根据您的确切版本和配置，它应该看起来像下面这样：![图9.15 – 基本Kibana
    UI](image/B14790_09_015_new.jpg)
- en: Figure 9.15 – Basic Kibana UI
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 – 基本Kibana UI
- en: Kibana provides several different features for searching and visualizing logs,
    metrics, and more. The most important section of the dashboard for our purposes
    is **Logging**, since we are using Kibana solely as a log search UI in our example.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana为搜索和可视化日志、指标等提供了几种不同的功能。对于我们的目的来说，仪表板中最重要的部分是**日志**，因为在我们的示例中，我们仅将Kibana用作日志搜索UI。
- en: However, Kibana has many other functions, some of which are comparable to Grafana.
    For instance, it includes a full visualization engine, **application performance
    monitoring** (**APM**) capabilities, and Timelion, an expression engine for time
    series data very similar to what is found in Prometheus's PromQL. Kibana's metrics
    functionality is similar to Prometheus and Grafana.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kibana还有许多其他功能，其中一些与Grafana相当。例如，它包括一个完整的可视化引擎，**应用程序性能监控**（**APM**）功能，以及Timelion，一个用于时间序列数据的表达式引擎，非常类似于Prometheus的PromQL。Kibana的指标功能类似于Prometheus和Grafana。
- en: In order to get Kibana working, we will first need to specify an index pattern.
    To do this, click on the **Visualize** button, then click **Add an Index Pattern**.
    Select an option from the list of patterns and choose the index with the current
    date on it, then create the index pattern.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了让Kibana工作，我们首先需要指定一个索引模式。要做到这一点，点击**可视化**按钮，然后点击**添加索引模式**。从模式列表中选择一个选项，并选择带有当前日期的索引，然后创建索引模式。
- en: 'Now that we''re set up, the **Discover** page will provide you with search
    functionality. This uses the Apache Lucene query syntax ([https://www.elastic.co/guide/en/elasticsearch/reference/6.7/query-dsl-query-string-query.html#query-string-syntax](https://www.elastic.co/guide/en/elasticsearch/reference/6.7/query-dsl-query-string-query.html#query-string-syntax))
    and can handle everything from simple string matching expressions to extremely
    complex queries. In the following screenshot, we are doing a simple string match
    for the letter `h`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了，**发现**页面将为您提供搜索功能。这使用Apache Lucene查询语法（[https://www.elastic.co/guide/en/elasticsearch/reference/6.7/query-dsl-query-string-query.html#query-string-syntax](https://www.elastic.co/guide/en/elasticsearch/reference/6.7/query-dsl-query-string-query.html#query-string-syntax)），可以处理从简单的字符串匹配表达式到非常复杂的查询。在下面的屏幕截图中，我们正在对字母`h`进行简单的字符串匹配。
- en: '![Figure 9.16 – Discover UI](image/B14790_09_016_new.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16 – 发现UI](image/B14790_09_016_new.jpg)'
- en: Figure 9.16 – Discover UI
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 – 发现UI
- en: When Kibana cannot find any results, it gives you a handy set of possible solutions
    including query examples, as you can see in *Figure 9.13*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kibana找不到任何结果时，它会为您提供一组方便的可能解决方案，包括查询示例，正如您在*图9.13*中所看到的。
- en: 'Now that you know how to create search queries, you can create visualizations
    from queries on the **Visualize** page. These can be chosen from a selection of
    visualization types including graphs, charts, and more, and then customized with
    specific queries as shown in the following screenshot:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何创建搜索查询，可以在**可视化**页面上从查询中创建可视化。这些可视化可以从包括图形、图表等在内的可视化类型中选择，然后使用特定查询进行自定义，如下面的屏幕截图所示：
- en: '![Figure 9.17 – New visualization](image/B14790_09_017_new.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17 – 新可视化](image/B14790_09_017_new.jpg)'
- en: Figure 9.17 – New visualization
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – 新可视化
- en: Next, these visualizations can be combined into dashboards. This works similarly
    to Grafana where multiple visualizations can be added to a dashboard, which can
    then be saved and reused.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这些可视化可以组合成仪表板。这与Grafana类似，多个可视化可以添加到仪表板中，然后可以保存和重复使用。
- en: 'You can also use the search bar to further filter your dashboard visualizations
    – pretty nifty! The following screenshot shows how a dashboard can be tied to
    a specific query:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用搜索栏进一步过滤您的仪表板可视化 - 非常巧妙！下面的屏幕截图显示了如何将仪表板与特定查询关联起来：
- en: '![Figure 9.18 – Dashboard UI](image/B14790_09_018_new.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图9.18 – 仪表板 UI](image/B14790_09_018_new.jpg)'
- en: Figure 9.18 – Dashboard UI
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 – 仪表板 UI
- en: As you can see, a dashboard can be created for a specific query using the **Add**
    button.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，可以使用**添加**按钮为特定查询创建仪表板。
- en: 'Next, Kibana provides a tool called *Timelion*, which is a time series visualization
    synthesis tool. Essentially, it allows you to combine separate data sources into
    a single visualization. Timelion is very powerful, but a full discussion of its
    feature set is outside the scope of this book. The following screenshot shows
    the Timelion UI – you may notice some similarities to Grafana, as these two sets
    of tools offer very similar capabilities:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Kibana提供了一个名为*Timelion*的工具，这是一个时间序列可视化综合工具。基本上，它允许您将单独的数据源合并到单个可视化中。Timelion非常强大，但其功能集的全面讨论超出了本书的范围。下面的屏幕截图显示了Timelion
    UI - 您可能会注意到与Grafana的一些相似之处，因为这两组工具提供了非常相似的功能：
- en: '![Figure 9.19 – Timelion UI](image/B14790_09_019_new.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图9.19 – Timelion UI](image/B14790_09_019_new.jpg)'
- en: Figure 9.19 – Timelion UI
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 – Timelion UI
- en: As you can see, in Timelion a query can be used to drive a real-time updating
    graph, just like in Grafana.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在Timelion中，查询可以用于驱动实时更新的图形，就像在Grafana中一样。
- en: Additionally, though less relevant to this book, Kibana provides APM functionality,
    which requires some further setup, especially with Kubernetes. In this book we
    lean on Prometheus for this type of information while using the EFK stack to search
    logs from our applications.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然与本书关联较小，但Kibana提供了APM功能，这需要一些进一步的设置，特别是在Kubernetes中。在本书中，我们依赖Prometheus获取这种类型的信息，同时使用EFK堆栈搜索我们应用程序的日志。
- en: Now that we've covered Prometheus and Grafana for metrics and alerting, and
    the EFK stack for logging, only one piece of the observability puzzle is left.
    To solve this, we will use another excellent piece of open source software – Jaeger.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了用于度量和警报的Prometheus和Grafana，以及用于日志记录的EFK堆栈，观察性谜题中只剩下一个部分。为了解决这个问题，我们将使用另一个优秀的开源软件
    - Jaeger。
- en: Implementing distributed tracing with Jaeger
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Jaeger实现分布式跟踪
- en: Jaeger is an open source distributed tracing solution compatible with Kubernetes.
    Jaeger implements the OpenTracing specification, which is a set of standards for
    defining distributed traces.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger是一个与Kubernetes兼容的开源分布式跟踪解决方案。Jaeger实现了OpenTracing规范，这是一组用于定义分布式跟踪的标准。
- en: Jaeger exposes a UI for viewing traces and integrates with Prometheus. The official
    Jaeger documentation can be found at [https://www.jaegertracing.io/docs/](https://www.jaegertracing.io/docs/).
    Always check the docs for new information, since things may have changed since
    the publishing of this book.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger提供了一个用于查看跟踪并与Prometheus集成的UI。官方Jaeger文档可以在[https://www.jaegertracing.io/docs/](https://www.jaegertracing.io/docs/)找到。始终检查文档以获取新信息，因为自本书出版以来可能已经发生了变化。
- en: Installing Jaeger using the Jaeger Operator
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Jaeger Operator安装Jaeger
- en: To install Jaeger, we are going to use the Jaeger Operator, which is the first
    operator that we've come across in this book. An *operator* in Kubernetes is simply
    a pattern for creating custom application controllers that speak Kubernetes's
    language. This means that instead of having to deploy all the various Kubernetes
    resources for an application, you can deploy a single Pod (or usually, single
    Deployment) and that application will talk to Kubernetes and spin up all the other
    required resources for you. It can even go further and self-operate the application,
    making resource changes when necessary. Operators can be highly complex, but they
    make it easier for us as end users to deploy commercial or open source software
    on our Kubernetes clusters.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Jaeger，我们将使用Jaeger Operator，这是本书中首次遇到的操作员。在Kubernetes中，*操作员*只是一种创建自定义应用程序控制器的模式，它们使用Kubernetes的语言进行通信。这意味着，您不必部署应用程序的各种Kubernetes资源，您可以部署一个单独的Pod（通常是单个部署），该应用程序将与Kubernetes通信并为您启动所有其他所需的资源。它甚至可以进一步自我操作应用程序，在必要时进行资源更改。操作员可能非常复杂，但它们使我们作为最终用户更容易在我们的Kubernetes集群上部署商业或开源软件。
- en: To get started with the Jaeger Operator, we need to create a few initial resources
    for Jaeger, and then the operator will do the rest. A prerequisite for this installation
    of Jaeger is that the `nginx-ingress` controller is installed on our cluster,
    since that is how we will access the Jaeger UI.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Jaeger Operator，我们需要为Jaeger创建一些初始资源，然后操作员将完成其余工作。安装Jaeger的先决条件是在我们的集群上安装了`nginx-ingress`控制器，因为这是我们将访问Jaeger
    UI的方式。
- en: 'First, we need to create a namespace for Jaeger to live in. We can get this
    via the `kubectl create namespace` command:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为Jaeger创建一个命名空间。我们可以通过`kubectl create namespace`命令获取它：
- en: '[PRE30]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that our namespace is created, we need to create some **CRDs** that Jaeger
    and the operator will use. We will discuss CRDs in depth in our chapter on extending
    Kubernetes, but for now, think of them as a way to co-opt the Kubernetes API to
    build custom functionality for applications. Using the following steps, let''s
    install Jaeger:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的命名空间已创建，我们需要创建一些Jaeger和操作员将使用的**CRDs**。我们将在我们的Kubernetes扩展章节中深入讨论CRDs，但现在，把它们看作是一种利用Kubernetes
    API来构建应用程序自定义功能的方式。使用以下步骤，让我们安装Jaeger：
- en: 'To create the Jaeger CRDs, run the following command:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建Jaeger CRDs，请运行以下命令：
- en: '[PRE31]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With our CRDs created, the operator needs a few Roles and Bindings to be created
    in order to do its work.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了我们的CRDs后，操作员需要创建一些角色和绑定以便进行工作。
- en: 'We want Jaeger to have cluster-wide permission in our cluster, so we will create
    some optional ClusterRoles and ClusterRoleBindings as well. To accomplish this,
    we run the following commands:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望Jaeger在我们的集群中拥有全局权限，因此我们将创建一些可选的ClusterRoles和ClusterRoleBindings。为了实现这一点，我们运行以下命令：
- en: '[PRE32]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we finally have all the pieces necessary for our operator to work. Let''s
    install the operator with one last `kubectl` command:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们终于拥有了操作员工作所需的所有要素。让我们用最后一个`kubectl`命令安装操作员：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, check to see if the operator is running, using the following command:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下命令检查操作员是否正在运行：
- en: '[PRE34]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If the operator is running correctly, you will see something similar to the
    following output, with one available Pod for the deployment:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果操作员正常运行，您将看到类似以下输出，部署中将有一个可用的Pod：
- en: '![Figure 9.20 – Jaeger Operator Pod output](image/B14790_09_020_new.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图9.20 - Jaeger Operator Pod输出](image/B14790_09_020_new.jpg)'
- en: Figure 9.20 – Jaeger Operator Pod output
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 - Jaeger Operator Pod输出
- en: We now have our Jaeger Operator up and running – but Jaeger itself isn't running.
    Why is this the case? Jaeger is a highly complex system and can run in different
    configurations, and the operator makes it easier to deploy these configurations.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经启动并运行了我们的Jaeger Operator - 但是Jaeger本身并没有在运行。为什么会这样？Jaeger是一个非常复杂的系统，可以以不同的配置运行，操作员使得部署这些配置变得更加容易。
- en: The Jaeger Operator uses a CRD called `Jaeger` to read a configuration for your
    Jaeger instance, at which time the operator will deploy all the necessary Pods
    and other resources on Kubernetes.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger Operator使用一个名为`Jaeger`的CRD来读取您的Jaeger实例的配置，此时操作员将在Kubernetes上部署所有必要的Pod和其他资源。
- en: 'Jaeger can run in three main configurations: *AllInOne*, *Production*, and
    *Streaming*. A full discussion of these configurations is outside the scope of
    this book (check the Jaeger docs link shared previously), but we will be using
    the AllInOne configuration. This configuration combines the Jaeger UI, Collector,
    Agent, and Ingestor into a single Pod, without any persistent storage included.
    This is perfect for demo purposes – to see production-ready configurations, check
    the Jaeger docs.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger可以以三种主要配置运行：*AllInOne*，*Production*和*Streaming*。对这些配置的全面讨论超出了本书的范围（请查看之前分享的Jaeger文档链接），但我们将使用AllInOne配置。这个配置将Jaeger
    UI，Collector，Agent和Ingestor组合成一个单独的Pod，不包括任何持久存储。这非常适合演示目的 - 要查看生产就绪的配置，请查看Jaeger文档。
- en: 'In order to create our Jaeger deployment, we need to tell the Jaeger Operator
    about our chosen configuration. We do that using the CRD that we created earlier
    – the Jaeger CRD. Create a new file for this CRD instance:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的Jaeger部署，我们需要告诉Jaeger Operator我们选择的配置。我们使用之前创建的CRD - Jaeger CRD来做到这一点。为此创建一个新文件：
- en: Jaeger-allinone.yaml
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger-allinone.yaml
- en: '[PRE35]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We are just using a small subset of the possible Jaeger type configurations
    – again, check the docs for the full story.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是使用了可能的Jaeger类型配置的一个小子集 - 再次查看完整的文档以了解全部情况。
- en: 'Now, we can create our Jaeger instance by running the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行以下命令来创建我们的Jaeger实例：
- en: '[PRE36]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This command creates an instance of the Jaeger CRD we installed previously.
    At this point, the Jaeger Operator should realize that the CRD has been created.
    In less than a minute, our actual Jaeger Pod should be running. We can check for
    it by listing all the Pods in the observability namespace, with the following
    command:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令创建了我们之前安装的Jaeger CRD的一个实例。此时，Jaeger Operator应该意识到已经创建了CRD。不到一分钟，我们的实际Jaeger
    Pod应该正在运行。我们可以通过以下命令列出observability命名空间中的所有Pod来检查：
- en: '[PRE37]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As an output, you should see the newly created Jaeger Pod for our all-in-one
    instance:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输出，您应该看到为我们的全功能实例新创建的Jaeger Pod：
- en: '[PRE38]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The Jaeger Operator creates an Ingress record when we also have an Ingress controller
    running on our cluster. This means that we can simply list our Ingress entries
    using kubectl to see where to access the Jaeger UI.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的集群上也运行有Ingress控制器时，Jaeger Operator会创建一个Ingress记录。这意味着我们可以简单地使用kubectl列出我们的Ingress条目，以查看如何访问Jaeger
    UI。
- en: 'You can list ingresses using this command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用这个命令列出Ingress：
- en: '[PRE39]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output should show your new Ingress for the Jaeger UI as shown:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该显示您的Jaeger UI的新Ingress，如下所示：
- en: '![Figure 9.21 – Jaeger UI Service output](image/B14790_09_021_new.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图9.21 - Jaeger UI服务输出](image/B14790_09_021_new.jpg)'
- en: Figure 9.21 – Jaeger UI Service output
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21 - Jaeger UI服务输出
- en: 'Now you can navigate to the address listed in your cluster''s Ingress record
    to see the Jaeger UI. It should look like the following:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以导航到集群Ingress记录中列出的地址，查看Jaeger UI。它应该看起来像下面这样：
- en: '![Figure 9.22 – Jaeger UI](image/B14790_09_022_new.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图9.22 – Jaeger UI](image/B14790_09_022_new.jpg)'
- en: Figure 9.22 – Jaeger UI
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.22 – Jaeger UI
- en: As you can see, the Jaeger UI is pretty simple. There are three tabs at the
    top – **Search**, **Compare**, and **System Architecture**. We will focus on the
    **Search** tab, but for more information about the other two, check the Jaeger
    docs at [https://www.jaegertracing.io](https://www.jaegertracing.io).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Jaeger UI非常简单。顶部有三个标签-**搜索**、**比较**和**系统架构**。我们将专注于**搜索**标签，但是要了解其他两个标签的更多信息，请查看Jaeger文档[https://www.jaegertracing.io](https://www.jaegertracing.io)。
- en: The Jaeger **Search** page lets us search for traces based on many inputs. We
    can search based on which Service is included in the trace, or based on tags,
    duration, or more. However, right now there's nothing in our Jaeger system.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Jaeger **搜索** 页面允许我们根据许多输入搜索跟踪。我们可以根据跟踪中包含的服务来搜索，也可以根据标签、持续时间等进行搜索。然而，现在我们的Jaeger系统中什么都没有。
- en: The reason for this is that even though we have Jaeger up and running, our apps
    still need to be configured to send traces to Jaeger. This usually needs to be
    done at the code or framework level and is out of the scope of this book. If you
    want to play around with Jaeger's tracing capabilities, a sample app is available
    to install – see the Jaeger docs page at [https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod](https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是，即使我们已经启动并运行了Jaeger，我们的应用程序仍然需要配置为将跟踪发送到Jaeger。这通常需要在代码或框架级别完成，超出了本书的范围。如果您想尝试Jaeger的跟踪功能，可以安装一个示例应用程序-请参阅Jaeger文档页面[https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod](https://www.jaegertracing.io/docs/1.18/getting-started/#sample-app-hotrod)。
- en: 'With services sending traces to Jaeger, it is possible to see traces. A trace
    in Jaeger looks like the following. We''ve cropped out some of the later parts
    of the trace for readability, but this should give you a good idea of what a trace
    can look like:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过服务将跟踪发送到Jaeger，可以查看跟踪。Jaeger中的跟踪如下所示。为了便于阅读，我们裁剪了跟踪的后面部分，但这应该可以让您对跟踪的外观有一个很好的想法：
- en: '![Figure 9.23 – Trace view in Jaeger](image/B14790_09_023_new.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图9.23 – Jaeger中的跟踪视图](image/B14790_09_023_new.jpg)'
- en: Figure 9.23 – Trace view in Jaeger
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.23 – Jaeger中的跟踪视图
- en: As you can see, the Jaeger UI view for a trace splits up service traces into
    constituent parts. Each service-to-service call, as well as any specific calls
    within the services themselves, have their own line in the trace. The horizontal
    bar chart you see moves from left to right with time, and each individual call
    in the trace has its own line. In this trace, you can see we have HTTP calls,
    SQL calls, as well as some Redis statements.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Jaeger UI视图将服务跟踪分成组成部分。每个服务之间的调用，以及服务内部的任何特定调用，在跟踪中都有自己的行。您看到的水平条形图从左到右移动，每个跟踪中的单独调用都有自己的行。在这个跟踪中，您可以看到我们有HTTP调用、SQL调用，以及一些Redis语句。
- en: You should be able to see how Jaeger and tracing in general can help developers
    make sense of a web of service-to-service calls and can help find bottlenecks.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能够看到Jaeger和一般跟踪如何帮助开发人员理清服务之间的网络调用，并帮助找到瓶颈所在。
- en: With that review of Jaeger, we have a fully open source solution to every problem
    in the observability bucket. However, that does not mean that there is no use
    case where a commercial solution makes sense – in many cases it does.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对Jaeger的回顾，我们对可观察性桶中的每个问题都有了一个完全开源的解决方案。然而，这并不意味着没有商业解决方案有用的情况-在许多情况下是有用的。
- en: Third-party tooling
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三方工具
- en: In addition to many open source libraries, there are many commercially available
    products for metrics, logging, and alerting on Kubernetes. Some of these can be
    much more powerful than the open source options.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 除了许多开源库之外，还有许多商业产品可用于Kubernetes上的指标、日志和警报。其中一些可能比开源选项更强大。
- en: Generally, most tooling in metrics and logging will require you to provision
    resources on your cluster to forward metrics and logs to your service of choice.
    In the examples we've used in this chapter, these services are running in the
    cluster, though in commercial products these can often be separate SaaS applications
    where you log on to analyze your logs and see your metrics. For instance, with
    the EFK stack we provisioned in this chapter, you can pay Elastic for a hosted
    solution where the Elasticsearch and Kibana pieces of the solution would be hosted
    on Elastic's infrastructure, reducing complexity in the solution. There are also
    many other solutions in this space, from vendors including Sumo Logic, Logz.io,
    New Relic, DataDog, and AppDynamics.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，大多数指标和日志工具都需要您在集群上配置资源，以将指标和日志转发到您选择的服务。在本章中我们使用的示例中，这些服务在集群中运行，尽管在商业产品中，这些通常可以是单独的SaaS应用程序，您可以登录分析日志并查看指标。例如，在本章中我们配置的EFK堆栈中，您可以支付Elastic提供的托管解决方案，其中解决方案的Elasticsearch和Kibana部分将托管在Elastic的基础设施上，从而降低了解决方案的复杂性。此外，还有许多其他解决方案，包括Sumo
    Logic、Logz.io、New Relic、DataDog和AppDynamics等供应商提供的解决方案。
- en: For a production environment, it is common to use separate compute (either a
    separate cluster, service, or SaaS tool) to perform log and metric analytics.
    This ensures that the cluster running your actual software can be dedicated to
    the application alone, and any costly log searching or querying functionality
    can be handled separately. It also means that if our application cluster goes
    down, we can still view logs and metrics up until the point of the failure.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产环境，通常会使用单独的计算资源（可以是单独的集群、服务或SaaS工具）来执行日志和指标分析。这确保了运行实际软件的集群可以专门用于应用程序，并且任何昂贵的日志搜索或查询功能可以单独处理。这也意味着，如果我们的应用程序集群崩溃，我们仍然可以查看日志和指标，直到故障发生的时刻。
- en: Summary
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about observability on Kubernetes. We first learned
    about the four major tenets of observability: metrics, logging, traces, and alerts.
    Then we discovered how Kubernetes itself provides tooling for observability, including
    how it manages logs and resource metrics and how to deploy Kubernetes Dashboard.
    Finally, we learned how to implement and use some key open source tools to provide
    visualization, searching, and alerting for the four pillars. This knowledge will
    help you build robust observability infrastructure for your future Kubernetes
    clusters and help you decide what is most important to observe in your cluster.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关于Kubernetes上的可观察性。我们首先了解了可观察性的四个主要原则：指标、日志、跟踪和警报。然后我们发现了Kubernetes本身提供的可观察性工具，包括它如何管理日志和资源指标，以及如何部署Kubernetes仪表板。最后，我们学习了如何实施和使用一些关键的开源工具，为这四个支柱提供可视化、搜索和警报。这些知识将帮助您为未来的Kubernetes集群构建健壮的可观察性基础设施，并帮助您决定在集群中观察什么最重要。
- en: In the next chapter, we will use what we learned about observability to help
    us troubleshoot applications on Kubernetes.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将运用我们在Kubernetes上学到的可观察性知识来帮助我们排除应用程序故障。
- en: Questions
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Explain the difference between metrics and logs.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释指标和日志之间的区别。
- en: Why would you use Grafana instead of simply using the Prometheus UI?
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么要使用Grafana而不是简单地使用Prometheus UI？
- en: When running an EFK stack in production (so as to keep as much compute off the
    production app cluster as possible), which piece(s) of the stack would run on
    the production app cluster? And which piece(s) would run off the cluster?
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生产环境中运行EFK堆栈（以尽量减少生产应用集群的计算负载），堆栈的哪些部分会在生产应用集群上运行？哪些部分会在集群外运行？
- en: Further reading
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In-depth review of Kibana Timelion: [https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html](https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana Timelion的深度审查：[https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html](https://www.elastic.co/guide/en/kibana/7.10/timelion-tutorial-create-time-series-visualizations.html)
