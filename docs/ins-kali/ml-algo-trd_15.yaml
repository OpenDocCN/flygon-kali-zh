- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Topic Modeling – Summarizing Financial News
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模-总结财经新闻
- en: In the last chapter, we used the **bag-of-words** (**BOW**) model to convert
    unstructured text data into a numerical format. This model abstracts from word
    order and represents documents as word vectors, where each entry represents the
    relevance of a token to the document. The resulting **document-term matrix** (**DTM**)—or
    transposed as the term-document matrix—is useful for comparing documents to each
    other or a query vector for similarity based on their token content and, therefore,
    finding the proverbial needle in a haystack. It provides informative features
    to classify documents, such as in our sentiment analysis examples.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用**词袋**（**BOW**）模型将非结构化文本数据转换为数字格式。这种模型抽象出了词序，并将文档表示为单词向量，其中每个条目表示一个标记与文档的相关性。由此产生的**文档-术语矩阵**（**DTM**）-或作为术语-文档矩阵的转置-对于将文档相互比较或基于它们的标记内容进行相似性查询，因此可以找到干草堆中的谚语针。它提供了信息丰富的特征来对文档进行分类，例如在我们的情感分析示例中。
- en: However, this document model produces both high-dimensional data and very sparse
    data, yet it does little to summarize the content or get closer to understanding
    what it is about. In this chapter, we will use **unsupervised machine learning**
    to extract hidden themes from documents using **topic modeling**. These themes
    can produce detailed insights into a large body of documents in an automated way.
    They are very useful in order to understand the haystack itself and allow us to
    tag documents based on their affinity with the various topics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种文档模型既产生高维数据，又产生非常稀疏的数据，但它对内容的总结或更接近理解内容的作用很小。在本章中，我们将使用**无监督机器学习**来使用**主题建模**从文档中提取隐藏的主题。这些主题可以以自动化的方式产生对大量文档的详细见解。它们非常有用，可以帮助我们理解干草堆本身，并允许我们根据它们与各种主题的关联来标记文档。
- en: '**Topic models** generate sophisticated, interpretable text features that can
    be a first step toward extracting trading signals from large collections of documents.
    They speed up the review of documents, help identify and cluster similar documents,
    and support predictive modeling.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题模型**生成复杂的、可解释的文本特征，可以成为从大量文档中提取交易信号的第一步。它们加快了文档的审查，帮助识别和聚类相似的文档，并支持预测建模。'
- en: '**Applications** include the unsupervised discovery of potentially insightful
    themes in company disclosures or earnings call transcripts, customer reviews,
    or contracts. Furthermore, the document-topic associations facilitate the labeling
    by assigning, for example, sentiment metrics or, more directly, subsequent relevant
    asset returns.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用**包括在公司披露或盈利电话转录、客户评论或合同中无监督地发现潜在有见地的主题。此外，文档-主题关联有助于通过分配情绪度量或更直接地，后续相关资产回报来进行标记。'
- en: 'More specifically, after reading this chapter, you''ll understand:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在阅读本章后，您将了解：
- en: How topic modeling has evolved, what it achieves, and why it matters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模的发展历程，它的成就以及为什么重要
- en: Reducing the dimensionality of the DTM using **latent semantic indexing** (**LSI**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**潜在语义索引**（**LSI**）来降低DTM的维度
- en: Extracting topics with **probabilistic latent semantic analysis** (**pLSA**)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**概率潜在语义分析**（**pLSA**）提取主题
- en: How **latent Dirichlet allocation** (**LDA**) improves pLSA to become the most
    popular topic model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）如何改进pLSA成为最流行的主题模型'
- en: Visualizing and evaluating topic modeling results
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和评估主题建模结果
- en: Running LDA using sklearn and Gensim
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用sklearn和Gensim运行LDA
- en: How to apply topic modeling to collections of earnings calls and financial news articles
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将主题建模应用于盈利电话和财经新闻文章的集合
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: Learning latent topics – Goals and approaches
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习潜在主题-目标和方法
- en: Topic modeling discovers hidden themes that capture semantic information beyond
    individual words in a body of documents. It aims to address a key challenge for
    a machine learning algorithm that learns from text data by transcending the lexical
    level of "what actually has been written" to the semantic level of "what was intended."
    The resulting topics can be used to annotate documents based on their association
    with various topics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模发现了在一系列文件中捕捉语义信息的隐藏主题。它旨在解决机器学习算法从文本数据中学习的一个关键挑战，即超越“实际写了什么”这个词汇级别，到“意图是什么”这个语义级别。由此产生的主题可以用于根据它们与各种主题的关联来注释文档。
- en: In practical terms, topic models automatically **summarize large collections
    of documents** to facilitate organization and management as well as search and
    recommendations. At the same time, it enables the understanding of documents to
    the extent that humans can interpret the descriptions of topics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，主题模型自动**总结大量文件**，以便组织和管理，以及搜索和推荐。同时，它使人们能够理解文件的程度，以至于人类可以解释主题的描述。
- en: Topic models also mitigate the **curse of dimensionality** that often plagues
    the BOW model; representing documents with high-dimensional, sparse vectors can
    make similarity measures noisy, lead to inaccurate distance measurements, and
    result in the overfitting of text classification models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型还可以缓解经常困扰BOW模型的**维度诅咒**；用高维稀疏向量表示文档可能会使相似性度量变得嘈杂，导致不准确的距离测量，并导致文本分类模型的过拟合。
- en: Moreover, the BOW model loses context as well as semantic information since
    it ignores word order. It is also unable to capture synonymy (where several words
    have the same meaning) or polysemy (where one word has several meanings). As a
    result of the latter, document retrieval or similarity search may miss the point
    when the documents are not indexed by the terms used to search or compare.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，BOW模型失去了上下文和语义信息，因为它忽略了词序。它也无法捕捉同义词（几个词具有相同的含义）或多义词（一个词有几个含义）。由于后者，当文档没有被用于搜索或比较的术语索引时，文档检索或相似性搜索可能会失去重点。
- en: 'These shortcomings of the BOW model prompt the question: how can we learn meaningful
    topics from data that facilitate a more productive interaction with documentary
    data?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: BOW模型的这些缺点引发了一个问题：我们如何从数据中学习有意义的主题，以促进与文档数据更有成效的互动？
- en: Initial attempts by topic models to improve on the vector space model (developed
    in the mid-1970s) applied linear algebra to reduce the dimensionality of the DTM.
    This approach is similar to the algorithm that we discussed as principal component
    analysis in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with
    Unsupervised Learning*. While effective, it is difficult to evaluate the results
    of these models without a benchmark model. In response, probabilistic models have
    emerged that assume an explicit document generation process and provide algorithms
    to reverse engineer this process and recover the underlying topics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型最初尝试改进向量空间模型（在20世纪70年代中期开发）的努力应用线性代数来降低DTM的维度。这种方法类似于我们在*第13章*中讨论的主成分分析。虽然有效，但在没有基准模型的情况下很难评估这些模型的结果。作为回应，出现了概率模型，假设存在一个明确的文档生成过程，并提供算法来逆向工程这个过程并恢复潜在的主题。
- en: 'The following table highlights key milestones in the model evolution, which
    we will address in more detail in the following sections:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格突出了模型演变中的关键里程碑，我们将在接下来的章节中更详细地讨论：
- en: '| Model | Year | Description |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 描述 |'
- en: '| **Latent semantic indexing (LSI)** | 1988 | Captures the semantic document-term
    relationship by reducing the dimensionality of the word space |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **潜在语义索引（LSI）** | 1988 | 通过降低词空间的维度来捕捉语义文档-术语关系 |'
- en: '| **Probabilistic latent semantic analysis (pLSA)** | 1999 | Reverse engineers
    a generative process that assumes words generate a topic and documents as a mix
    of topics |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **概率潜在语义分析（pLSA）** | 1999 | 逆向工程一个生成过程，假设单词生成一个主题和文档作为主题的混合 |'
- en: '| **Latent Dirichlet allocation (LDA)** | 2003 | Adds a generative process
    for documents: a three-level hierarchical Bayesian model |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **潜在狄利克雷分配（LDA）** | 2003 | 为文档添加了一个生成过程：一个三级分层贝叶斯模型 |'
- en: Latent semantic indexing
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在语义索引
- en: '**Latent semantic indexing** (**LSI**)—also called **latent semantic analysis**
    (**LSA**)—set out to improve the results of queries that omitted relevant documents
    containing synonyms of query terms (Dumais et al. 1988). Its goal was to model
    the relationships between documents and terms so that it could predict that a
    term should be associated with a document, even though, because of the variability
    in word use, no such association was observed.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在语义索引**（**LSI**）—也称为**潜在语义分析**（**LSA**）—旨在改善省略了包含查询词同义词的相关文档的查询结果（Dumais等人，1988年）。它的目标是建模文档和术语之间的关系，以便它可以预测一个术语应该与一个文档相关联，即使由于词语使用的变化，没有观察到这样的关联。'
- en: LSI uses linear algebra to find a given number *k* of latent topics by decomposing
    the DTM. More specifically, it uses the **singular value decomposition** (**SVD**)
    to find the best lower-rank DTM approximation using *k* singular values and vectors.
    In other words, LSI builds on some of the dimensionality reduction techniques
    we encountered in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation
    with Unsupervised Learning*. The authors also experimented with hierarchical clustering
    but found it too restrictive for this purpose.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LSI使用线性代数来通过分解DTM找到给定数量*k*的潜在主题。更具体地说，它使用**奇异值分解**（**SVD**）来找到使用*k*个奇异值和向量的最佳低秩DTM近似。换句话说，LSI建立在我们在*第13章*中遇到的一些降维技术上。作者还尝试了分层聚类，但发现对此目的来说太过限制性。
- en: 'In this context, SVD identifies a set of uncorrelated indexing variables or
    factors that represent each term and document by its vector of factor values.
    *Figure 15.1* illustrates how SVD decomposes the DTM into three matrices: two
    matrices that contain orthogonal singular vectors and a diagonal matrix with singular
    values that serve as scaling factors.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，SVD识别一组不相关的索引变量或因子，通过其因子值的向量来表示每个术语和文档。*图15.1*说明了SVD如何将DTM分解为三个矩阵：两个包含正交奇异向量的矩阵和一个包含奇异值的对角矩阵，这些奇异值充当缩放因子。
- en: Assuming some correlation in the input DTM, singular values decay in value.
    Therefore, selecting the *T*-largest singular values yields a lower-dimensional
    approximation of the original DTM that loses relatively little information. In
    the compressed version, the rows or columns that had *N* items only have *T* <
    *N* entries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入DTM中存在一些相关性，奇异值会衰减。因此，选择*T*个最大的奇异值会产生原始DTM的低维近似，且丢失的信息相对较少。在压缩版本中，原本有*N*个条目的行或列只有*T*
    < *N*个条目。
- en: 'The LSI decomposition of the DTM can be interpreted as shown in *Figure 15.1*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DTM的LSI分解可以解释如*图15.1*所示：
- en: The first ![](img/B15439_15_001.png) matrix represents the relationships between
    documents and topics.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个![](img/B15439_15_001.png)矩阵表示文档和主题之间的关系。
- en: The diagonal matrix scales the topics by their corpus strength.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对角矩阵通过其语料库强度来缩放主题。
- en: The third matrix models the term-topic relationship.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个矩阵建模了术语-主题关系。
- en: '![](img/B15439_15_01.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_01.png)'
- en: 'Figure 15.1: LSI and the SVD'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：LSI和SVD
- en: The rows of the matrix produced by multiplying the first two matrices ![](img/B15439_15_002.png)
    correspond to the locations of the original documents projected into the latent
    topic space.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将前两个矩阵相乘产生的矩阵的行！[](img/B15439_15_002.png)对应于投影到潜在主题空间中的原始文档的位置。
- en: How to implement LSI using sklearn
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现LSI
- en: We will illustrate LSI using the BBC articles data that we introduced in the
    last chapter because they are small enough for quick training and allow us to
    compare topic assignments with category labels. Refer to the notebook `latent_semantic_indexing`
    for additional implementation details.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们在上一章介绍的BBC文章数据来说明LSI，因为它们足够小，可以快速训练，并且可以让我们将主题分配与类别标签进行比较。有关其他实施细节，请参阅笔记本`latent_semantic_indexing`。
- en: 'We begin by loading the documents and creating a train and (stratified) test
    set with 50 articles. Then, we vectorize the data using `TfidfVectorizer` to obtain
    weighted DTM counts and filter out words that appear in less than 1 percent or
    more than 25 percent of the documents, as well as generic stopwords, to obtain
    a vocabulary of around 2,900 words:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载文档，并创建一个包含50篇文章的训练和（分层）测试集。然后，我们使用`TfidfVectorizer`对数据进行向量化，以获得加权的DTM计数，并过滤掉出现在不到1％或超过25％的文档中的词语，以及通用的停用词，以获得大约2900个词的词汇表：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use scikit-learn's `TruncatedSVD` class, which only computes the *k*-largest
    singular values, to reduce the dimensionality of the DTM. The deterministic `arpack`
    algorithm delivers an exact solution, but the default "randomized" implementation
    is more efficient for large matrices.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用scikit-learn的`TruncatedSVD`类，它只计算*k*个最大的奇异值，以减少DTM的维度。确定性的`arpack`算法提供了精确的解决方案，但默认的“随机化”实现对于大矩阵更有效。
- en: 'We compute five topics to match the five categories, which explain only 5.4
    percent of the total DTM variance, so a larger number of topics would be reasonable:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算五个主题以匹配五个类别，这解释了总DTM方差的仅5.4％，因此更多的主题数量是合理的：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'LSI identifies a new orthogonal basis for the DTM that reduces the rank to
    the number of desired topics. The `.transform()` method of the trained `svd` object
    projects the documents into the new topic space. This space results from reducing
    the dimensionality of the document vectors and corresponds to the ![](img/B15439_15_002.png)
    transformation illustrated earlier in this section:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LSI识别了DTM的新正交基，将秩降低到所需主题的数量。训练后的`svd`对象的`.transform()`方法将文档投影到新的主题空间中。这个空间是通过减少文档向量的维度得到的，并对应于本节前面所示的![](img/B15439_15_002.png)变换：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can sample an article to view its location in the topic space. We draw a
    "Politics" article that is most (positively) associated with topics 1 and 2:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以抽样一篇文章来查看它在主题空间中的位置。我们选择了一个与“政治”最（积极）相关的文章，它与主题1和2最相关：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The topic assignments for this sample align with the average topic weights for
    each category illustrated in *Figure 15.2* ("Politics" is the rightmost bar).
    They illustrate how LSI expresses the *k* topics as directions in a *k*-dimensional
    space (the notebook includes a projection of the average topic assignments per
    category into two-dimensional space).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个样本的主题分配与*图15.2*中每个类别的平均主题权重一致（“政治”是最右边的条）。它们说明了LSI如何将*k*个主题表达为*k*维空间中的方向（笔记本包括每个类别的平均主题分配在二维空间中的投影）。
- en: Each category is clearly defined, and the test assignments match with train
    assignments. However, the weights are both positive and negative, making it more
    difficult to interpret the topics.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别都清晰定义，测试分配与训练分配相匹配。然而，权重既有正数又有负数，这使得更难解释主题。
- en: '![A screenshot of a video game  Description automatically generated](img/B15439_15_02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的视频游戏描述的截图](img/B15439_15_02.png)'
- en: 'Figure 15.2: LSI topic weights for train and test data'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：LSI主题在训练和测试数据中的权重
- en: We can also display the words that are most closely associated with each topic
    (in absolute terms). The topics appear to capture some semantic information but
    are not clearly differentiated (refer to *Figure 15.3*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示与每个主题最密切关联的词语（绝对值）。主题似乎捕捉了一些语义信息，但并没有明显区分（参见*图15.3*）。
- en: '![](img/B15439_15_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_03.png)'
- en: 'Figure 15.3: Top 10 words per LSI topic'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：LSI主题的前10个词语
- en: Strengths and limitations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势和局限性
- en: The strengths of LSI include the removal of noise and the mitigation of the
    curse of dimensionality. It also captures some semantic aspects, like synonymy,
    and clusters both documents and terms via their topic associations. Furthermore,
    it does not require knowledge of the document language, and both information retrieval
    queries and document comparisons are easy to do.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LSI的优点包括去除噪音和减轻维度灾难。它还捕捉了一些语义方面，如同义词，并通过它们的主题关联来聚类文档和术语。此外，它不需要了解文档语言，信息检索查询和文档比较都很容易。
- en: However, the results of LSI are difficult to interpret because topics are word
    vectors with both positive and negative entries. In addition, there is no underlying
    model that would permit the evaluation of fit or provide guidance when selecting
    the number of dimensions or topics to use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LSI的结果很难解释，因为主题是具有正负条目的词向量。此外，没有基础模型可以允许拟合的评估或在选择要使用的维度或主题数量时提供指导。
- en: Probabilistic latent semantic analysis
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率潜在语义分析
- en: '**Probabilistic latent semantic analysis** (**pLSA**) takes a **statistical
    perspective** on LSI/LSA and creates a generative model to address the lack of
    theoretical underpinnings of LSA (Hofmann 2001).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率潜在语义分析**（**pLSA**）对LSI/LSA采取了**统计角度**，并创建了一个生成模型，以解决LSA缺乏理论基础的问题（Hofmann 2001）。'
- en: pLSA explicitly models the probability word *w* appearing in document *d*, as
    described by the DTM as a mixture of conditionally independent multinomial distributions
    that involve topics *t*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: pLSA明确地对LSI/LSA采取了统计角度，并创建了一个生成模型，以解决LSA缺乏理论基础的问题（Hofmann 2001）。
- en: There are both **symmetric and asymmetric formulations** of how word-document
    co-occurrences come about. The former assumes that both words and documents are
    generated by the latent topic class. In contrast, the asymmetric model assumes
    that topics are selected given the document, and words result in a second step
    given the topic.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有关词-文档共现的**对称和非对称形式**。前者假设词和文档都是由潜在主题类生成的。相反，非对称模型假设主题是在给定文档的情况下选择的，而词是在给定主题的情况下第二步产生的。
- en: '![](img/B15439_15_004.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_004.png)'
- en: The number of topics is a **hyperparameter** chosen prior to training and is
    not learned from the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 主题数量是在训练之前选择的**超参数**，不是从数据中学习的。
- en: 'The **plate notation** in *Figure 15.4* describes the statistical dependencies
    in a probabilistic model. More specifically, it encodes the relationship just
    described for the asymmetric model. Each rectangle represents multiple items:
    the outer block stands for *M* documents, while the inner shaded rectangle symbolizes
    *N* words for each document. We only observe the documents and their content;
    the model infers the hidden or latent topic distribution:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.4*中的**板符号**描述了概率模型中的统计依赖关系。更具体地说，它编码了刚才描述的非对称模型的关系。每个矩形代表多个项目：外部块代表*M*个文档，而内部阴影矩形代表每个文档的*N*个单词。我们只观察文档及其内容；模型推断隐藏或潜在的主题分布：'
- en: '![](img/B15439_15_04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_04.png)'
- en: 'Figure 15.4: The statistical dependencies modeled by pLSA in plate notation'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.4*：pLSA在板符号中建模的统计依赖关系'
- en: Let's now take a look at how we can implement this model in practice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在实践中实现这个模型。
- en: How to implement pLSA using sklearn
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现pLSA
- en: pLSA is equivalent to **non-negative matrix factorization** (**NMF**) using
    a Kullback-Leibler divergence objective (view the references on GitHub). Therefore,
    we can use the `sklearn.decomposition.NMF` class to implement this model following
    the LSI example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: pLSA等价于使用Kullback-Leibler散度目标的**非负矩阵分解**（**NMF**）（请参阅GitHub上的参考资料）。因此，我们可以使用`sklearn.decomposition.NMF`类来实现这个模型，就像LSI示例一样。
- en: 'Using the same train-test split of the DTM produced by `TfidfVectorizer`, we
    fit pLSA like so:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`TfidfVectorizer`生成的相同的训练-测试拆分的DTM，我们可以这样拟合pLSA：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get a measure of the reconstruction error that is a substitute for the explained
    variance measure from earlier:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个重构误差的度量，这是对之前解释方差度量的替代：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Due to its probabilistic nature, pLSA produces only positive topic weights
    that result in more straightforward topic-category relationships for the test
    and training sets, as shown in *Figure 15.5*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其概率性质，pLSA只产生正主题权重，这导致了测试和训练集的更直接的主题-类别关系，如*图15.5*所示：
- en: '![](img/B15439_15_05.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_05.png)'
- en: 'Figure 15.5: pLSA weights by topic for train and test data'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.5*：pLSA对训练和测试数据的主题权重'
- en: 'We also note that the word lists that describe each topic begin to make more
    sense; for example, the "Entertainment" category is most directly associated with
    Topic 4, which includes the words "film," "star," and so forth, as you can see
    in *Figure 15.6*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到，描述每个主题的单词列表开始变得更有意义；例如，“娱乐”类别与主题4最直接相关，其中包括“电影”，“明星”等词语，正如您在*图15.6*中所看到的：
- en: '![](img/B15439_15_06.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_06.png)'
- en: 'Figure 15.6: Top words per topic for pLSA'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.6*：pLSA每个主题的前几个单词'
- en: Strengths and limitations
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势和局限性
- en: The benefit of using a probability model is that we can now compare the performance
    of different models by evaluating the probability they assign to new documents
    given the parameters learned during training. It also means that the results have
    a clear probabilistic interpretation. In addition, pLSA captures more semantic
    information, including polysemy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用概率模型的好处是，我们现在可以通过评估它们在训练期间学习的参数给出的新文档的概率来比较不同模型的性能。这也意味着结果有一个清晰的概率解释。此外，pLSA捕获了更多的语义信息，包括多义性。
- en: On the other hand, pLSA increases the computational complexity compared to LSI,
    and the algorithm may only yield a local as opposed to a global maximum. Finally,
    it does not yield a generative model for new documents because it takes them as
    given.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，与LSI相比，pLSA增加了计算复杂性，算法可能只产生局部最大值而不是全局最大值。最后，它不会为新文档产生生成模型，因为它将它们视为给定的。
- en: Latent Dirichlet allocation
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: '**Latent Dirichlet allocation** (**LDA**) extends pLSA by adding a generative
    process for topics (Blei, Ng, and Jordan 2003). It is the most popular topic model
    because it tends to produce meaningful topics that humans can relate to, can assign
    topics to new documents, and is extensible. Variants of LDA models can include
    metadata, like authors or image data, or learn hierarchical topics.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）通过添加主题的生成过程（Blei，Ng和Jordan 2003）扩展了pLSA。它是最流行的主题模型，因为它倾向于产生人类可以相关的有意义的主题，可以为新文档分配主题，并且是可扩展的。LDA模型的变体可以包括元数据，如作者或图像数据，或学习分层主题。'
- en: How LDA works
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDA的工作原理
- en: LDA is a **hierarchical Bayesian model** that assumes topics are probability
    distributions over words, and documents are distributions over topics. More specifically,
    the model assumes that topics follow a sparse Dirichlet distribution, which implies
    that documents reflect only a small set of topics, and topics use only a limited
    number of terms frequently.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一个**分层贝叶斯模型**，假设主题是单词的概率分布，文档是主题的分布。更具体地说，该模型假设主题遵循稀疏的Dirichlet分布，这意味着文档只反映了一小部分主题，主题只使用了有限数量的术语。
- en: The Dirichlet distribution
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 狄利克雷分布
- en: 'The Dirichlet distribution produces probability vectors that can be used as
    a discrete probability distribution. That is, it randomly generates a given number
    of values that are positive and sum to one. It has a parameter ![](img/B15439_15_005.png)
    of positive real value that controls the concentration of the probabilities. Values
    closer to zero mean that only a few values will be positive and receive most of
    the probability mass. *Figure 15.7* illustrates three draws of size 10 for ![](img/B15439_15_006.png)
    = 0.1:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 狄利克雷分布产生可以用作离散概率分布的概率向量。也就是说，它随机生成一定数量的正值，并且总和为一。它具有一个正实数值的参数![](img/B15439_15_005.png)，控制概率的集中度。接近零的值意味着只有少数值将为正，并且将获得大部分的概率质量。*图15.7*说明了![](img/B15439_15_006.png)
    = 0.1的大小为10的三次抽样：
- en: '![](img/B15439_15_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_07.png)'
- en: 'Figure 15.7: Three draws from the Dirichlet distribution'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7：从狄利克雷分布中抽取的三个样本
- en: The notebook `dirichlet_distribution` contains a simulation that lets you experiment
    with different parameter values.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`dirichlet_distribution`包含一个模拟，让您可以尝试不同的参数值。
- en: The generative model
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型
- en: 'The LDA topic model assumes the following generative process when an author
    adds an article to a body of documents:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当作者向一组文档添加文章时，LDA主题模型假定以下生成过程：
- en: Randomly mix a small subset of topics with proportions defined by the Dirichlet
    probabilities.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机混合一小部分主题，其比例由狄利克雷概率定义。
- en: For each word in the text, select one of the topics according to the document-topic
    probabilities.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文本中的每个单词，根据文档-主题概率之一选择一个主题。
- en: Select a word from the topic's word list according to the topic-word probabilities.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据主题-单词概率从主题的单词列表中选择一个单词。
- en: As a result, the article content depends on the weight of each topic and the
    terms that make up each topic. The Dirichlet distribution governs the selection
    of topics for documents and words for topics. It encodes the idea that a document
    only covers a few topics, while each topic uses only a small number of words frequently.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，文章内容取决于每个主题的权重以及构成每个主题的术语。狄利克雷分布控制了为文档选择主题和为主题选择单词的过程。它编码了这样一个观念，即一个文档只涵盖少数主题，而每个主题只使用少数词频高的单词。
- en: 'The **plate notation** for the LDA model in *Figure 15.8* summarizes these
    relationships and highlights the key model parameters:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: LDA模型的**板符号**在*图15.8*中总结了这些关系，并突出了关键的模型参数：
- en: '![](img/B15439_15_08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_08.png)'
- en: 'Figure 15.8: The statistical dependencies of the LDA model in plate notation'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8：LDA模型在板符号中的统计依赖关系
- en: Reverse engineering the process
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逆向工程的过程
- en: 'The generative process is clearly fictional but turns out to be useful because
    it permits the recovery of the various distributions. The LDA algorithm reverse
    engineers the work of the imaginary author and arrives at a summary of the document-topic-word
    relationships that concisely describes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 生成过程显然是虚构的，但事实证明它是有用的，因为它允许恢复各种分布。LDA算法通过逆向工程虚构作者的工作，并得出了对文档-主题-单词关系的摘要，这个摘要简洁地描述了：
- en: The percentage contribution of each topic to a document
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主题对文档的百分比贡献
- en: The probabilistic association of each word with a topic
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词与主题的概率关联
- en: LDA solves the **Bayesian inference** problem of recovering the distributions
    from the body of documents and the words they contain by reverse engineering the
    assumed content generation process. The original paper by Blei et al. (2003) uses
    **variational Bayes** (**VB**) to approximate the posterior distribution. Alternatives
    include Gibbs sampling and expectation propagation. We will illustrate, shortly,
    the implementations by the sklearn and Gensim libraries.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LDA通过逆向工程假定的内容生成过程来解决从文档集和它们包含的单词中恢复分布的**贝叶斯推断**问题。Blei等人（2003）的原始论文使用**变分贝叶斯**（**VB**）来近似后验分布。替代方法包括吉布斯抽样和期望传播。我们将很快说明sklearn和Gensim库的实现。
- en: How to evaluate LDA topics
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何评估LDA主题
- en: Unsupervised topic models do not guarantee that the result will be meaningful
    or interpretable, and there is no objective metric to assess the quality of the
    result as in supervised learning. Human topic evaluation is considered the gold
    standard, but it is potentially expensive and not readily available at scale.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督主题模型不能保证结果是有意义的或可解释的，也没有客观的度量标准来评估结果的质量，就像在监督学习中一样。人工主题评估被认为是金标准，但可能昂贵，并且在规模上不容易获得。
- en: Two options to evaluate results more objectively include **perplexity**, which
    evaluates the model on unseen documents, and **topic coherence** metrics, which
    aim to evaluate the semantic quality of the uncovered patterns.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 更客观地评估结果的两个选项包括**困惑度**，它评估模型对未见过的文档的表现，以及**主题连贯性**指标，旨在评估发现的模式的语义质量。
- en: Perplexity
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 困惑度
- en: 'Perplexity, when applied to LDA, measures how well the topic-word probability
    distribution recovered by the model predicts a sample of unseen text documents.
    It is based on the entropy *H*(*p*) of this distribution *p* and is computed with
    respect to the set of tokens *w*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于LDA时，困惑度衡量模型恢复的主题-单词概率分布对未见文本文档样本的预测能力。它基于此分布*p*的熵*H*(*p*)，并针对标记*w*的集合进行计算：
- en: '![](img/B15439_15_007.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_007.png)'
- en: Measures closer to zero imply the distribution is better at predicting the sample.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接近零的度量意味着分布在预测样本方面表现更好。
- en: Topic coherence
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主题连贯性
- en: Topic coherence measures the semantic consistency of the topic model results,
    that is, whether humans would perceive the words and their probabilities associated
    with topics as meaningful.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 主题连贯性衡量主题模型结果的语义一致性，即人类是否会认为与主题相关的单词及其概率是有意义的。
- en: To this end, it scores each topic by measuring the degree of semantic similarity
    between the words most relevant to the topic. More specifically, coherence measures
    are based on the probability of observing the set of words *W* that defines a
    topic together.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，它通过衡量与主题最相关的单词之间的语义相似度来对每个主题进行评分。更具体地说，连贯性度量是基于观察定义主题的单词集合*W*的概率来计算的。
- en: There are two measures of coherence that have been designed for LDA and are
    shown to align with human judgments of topic quality, namely the UMass and the
    UCI metrics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种连贯性度量是专门为LDA设计的，并且已经证明与人类对主题质量的判断一致，即UMass和UCI度量。
- en: 'The UCI metric (Stevens et al. 2012) defines a word pair''s score to be the
    sum of the **pointwise mutual information** (**PMI**) between two distinct pairs
    of (top) topic words *w*[i], *w*[j] ![](img/B15439_15_008.png) *w* and a smoothing
    factor ![](img/B15439_15_009.png):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: UCI度量（Stevens等人，2012）定义了一个词对的分数，即两个不同的（顶部）主题词*w*[i]，*w*[j]之间的**点对点互信息**（**PMI**）的总和![](img/B15439_15_008.png)和一个平滑因子![](img/B15439_15_009.png)：
- en: '![](img/B15439_15_010.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_010.png)'
- en: The probabilities are computed from word co-occurrence frequencies in a sliding
    window over an external corpus like Wikipedia so that this metric can be thought
    of as an external comparison to semantic ground truth.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概率是从一个外部语料库（如维基百科）上的单词共现频率中计算出来的，因此这个度量可以被认为是对语义基本事实的外部比较。
- en: 'In contrast, the UMass metric (Mimno et al. 2011) uses the co-occurrences in
    a number of documents *D* from the training corpus to compute a coherence score:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，UMass度量（Mimno等人，2011）使用训练语料库中一定数量的文档*D*中的共现来计算连贯性得分：
- en: '![](img/B15439_15_011.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_011.png)'
- en: Rather than comparing the model result to extrinsic ground truth, this measure
    reflects intrinsic coherence. Both measures have been evaluated to align well
    with human judgment (Röder, Both, and Hinneburg 2015). In both cases, values closer
    to zero imply that a topic is more coherent.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与将模型结果与外在的基本事实进行比较不同，这个度量反映了内在的连贯性。这两种度量都经过评估，与人类判断一致（Röder，Both和Hinneburg，2015）。在这两种情况下，值越接近零意味着主题更连贯。
- en: How to implement LDA using sklearn
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现LDA
- en: 'We will use the BBC data as before and train an LDA model using sklearn''s
    `decomposition.LatentDirichletAllocation` class with five topics (refer to the
    sklearn documentation for details on the parameters and the notebook `lda_with_sklearn`
    for implementation details):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像以前一样使用BBC数据，并使用sklearn的`decomposition.LatentDirichletAllocation`类训练一个包含五个主题的LDA模型（有关参数的详细信息，请参考sklearn文档，有关实现细节，请参考笔记本`lda_with_sklearn`）：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The model tracks the in-sample perplexity during training and stops iterating
    once this measure stops improving. We can persist and load the result as usual
    with sklearn objects:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在训练过程中跟踪样本内困惑度，并在这个度量停止改善时停止迭代。我们可以像通常一样持久化和加载结果与sklearn对象：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How to visualize LDA results using pyLDAvis
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用pyLDAvis可视化LDA结果
- en: Topic visualization facilitates the evaluation of topic quality using human
    judgment. pyLDAvis is a Python port of LDAvis, developed in R and `D3.js` (Sievert
    and Shirley 2014). We will introduce the key concepts; each LDA application notebook
    contains examples.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 主题可视化有助于使用人类判断评估主题质量。pyLDAvis是LDAvis的Python版本，它是在R和`D3.js`（Sievert和Shirley，2014）中开发的。我们将介绍关键概念；每个LDA应用笔记本都包含示例。
- en: pyLDAvis displays the global relationships among topics while also facilitating
    their semantic evaluation by inspecting the terms most closely associated with
    each individual topic and, inversely, the topics associated with each term. It
    also addresses the challenge that terms that are frequent in a corpus tend to
    dominate the distribution over words that define a topic.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: pyLDAvis显示了主题之间的全局关系，同时通过检查与每个单独主题最相关的术语以及与每个术语相关的主题来促进它们的语义评估。它还解决了在语料库中频繁出现的术语往往会主导定义主题的单词分布的挑战。
- en: 'To this end, LDAVis introduces the **relevance** *r* of term *w* to topic *t*.
    The relevance produces a flexible ranking of terms by topic, by computing a weighted
    average of two metrics:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，LDAVis引入了术语*w*对主题*t*的**相关性** *r*。相关性通过计算两个指标的加权平均值来产生对主题的术语的灵活排名：
- en: The degree of association of topic *t* with term *w*, expressed as the conditional
    probability *p*(*w* | *t*)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题*t*与术语*w*的关联程度，表示为条件概率*p*(*w* | *t*)
- en: The saliency, or lift, which measures how the frequency of term *w* for the
    topic t, *p*(*w* | *t*), compares to its overall frequency across all documents,
    *p*(*w*)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著性或提升度量了术语*w*对主题*t*的频率*p*(*w* | *t*)与其在所有文档中的总体频率*p*(*w*)的比较
- en: 'More specifically, we can compute the relevance *r* for a term *w* and a topic
    *t* given a user-defined weight ![](img/B15439_15_012.png), like the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们可以计算给定用户定义的权重![](img/B15439_15_012.png)的术语*w*和主题*t*的相关性*r，如下所示：
- en: '![](img/B15439_15_013.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_013.png)'
- en: The tool allows the user to interactively change ![](img/B15439_15_014.png)
    to adjust the relevance, which updates the ranking of terms. User studies have
    found ![](img/B15439_15_015.png) to produce the most plausible results.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具允许用户交互地更改![](img/B15439_15_014.png)以调整相关性，从而更新术语的排名。用户研究发现![](img/B15439_15_015.png)产生了最合理的结果。
- en: How to implement LDA using Gensim
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用Gensim实现LDA
- en: Gensim is a specialized **natural language processing** (**NLP**) library with
    a fast LDA implementation and many additional features. We will also use it in
    the next chapter on word vectors (refer to the notebook `lda_with_gensim` for
    details and the installation directory for related instructions).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim是一个专门的**自然语言处理**（**NLP**）库，具有快速的LDA实现和许多其他功能。我们还将在下一章关于词向量的内容中使用它（有关详细信息和相关说明的安装目录，请参考笔记本`lda_with_gensim`）。
- en: 'We convert the DTM produced by sklearn''s `CountVectorizer` or `TfIdfVectorizer`
    into Gensim data structures as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将sklearn的`CountVectorizer`或`TfIdfVectorizer`生成的DTM转换为Gensim数据结构，如下所示：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Gensim''s LDA algorithm includes numerous settings:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim的LDA算法包括许多设置：
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Gensim also provides an `LdaMulticore` model for parallel training that may
    speed up training using Python's multiprocessing features for parallel computation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还提供了一个`LdaMulticore`模型，用于并行训练，可以利用Python的多进程功能加快训练速度。
- en: 'Model training just requires instantiating `LdaModel`, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练只需要实例化`LdaModel`，如下所示：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Gensim evaluates topic coherence, as introduced in the previous section, and
    shows the most important words per topic:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim评估主题连贯性，如前一节介绍的，并显示每个主题的最重要词语：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can display the results as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下显示结果：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This shows the following top words for each topic:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了每个主题的前几个词语：
- en: '| Topic 1 | Topic 2 | Topic 3 | Topic 4 | Topic 5 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 主题1 | 主题2 | 主题3 | 主题4 | 主题5 |'
- en: '| Probability | Term | Probability | Term | Probability | Term | Probability
    | Term | Probability | Term |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 概率 | 术语 | 概率 | 术语 | 概率 | 术语 | 概率 | 术语 | 概率 | 术语 |'
- en: '| 0.55% | online | 0.90% | best | 1.04% | mobile | 0.64% | market | 0.94% |
    labour |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 0.55% | 在线 | 0.90% | 最好 | 1.04% | 移动 | 0.64% | 市场 | 0.94% | 劳动 |'
- en: '| 0.51% | site | 0.87% | game | 0.98% | phone | 0.53% | growth | 0.72% | blair
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 0.51% | 网站 | 0.87% | 游戏 | 0.98% | 电话 | 0.53% | 增长 | 0.72% | 布莱尔 |'
- en: '| 0.46% | game | 0.62% | play | 0.51% | music | 0.52% | sales | 0.72% | brown
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 0.46% | 游戏 | 0.62% | 玩 | 0.51% | 音乐 | 0.52% | 销售 | 0.72% | 布朗 |'
- en: '| 0.45% | net | 0.61% | won | 0.48% | film | 0.49% | economy | 0.65% | election
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 0.45% | 网络 | 0.61% | 赢 | 0.48% | 电影 | 0.49% | 经济 | 0.65% | 选举 |'
- en: '| 0.44% | used | 0.56% | win | 0.48% | use | 0.45% | prices | 0.57% | united
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 0.44% | 使用 | 0.56% | 赢 | 0.48% | 使用 | 0.45% | 价格 | 0.57% | 联合 |'
- en: 'The left panel of *Figure 15.9* displays the topic coherence scores, which
    highlight the decay of topic quality (at least, in part, due to the relatively
    small dataset):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.9*的左侧面板显示了主题连贯性得分，突出显示了主题质量的下降（至少部分原因是由于相对较小的数据集）：'
- en: '![](img/B15439_15_09.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_09.png)'
- en: 'Figure 15.9: Topic coherence and test set assignments'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9：主题连贯性和测试集分配
- en: The right panel displays the evaluation of our test set of 50 articles with
    our trained model. The model makes four mistakes for an accuracy of 92 percent.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧面板显示了我们训练模型的50篇文章的测试集的评估。该模型出现了四个错误，准确率为92%。
- en: Modeling topics discussed in earnings calls
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对收益电话讨论的主题进行建模
- en: In *Chapter 3*, *Alternative Data for Finance – Categories and Use Cases*, we
    learned how to scrape earnings call data from the SeekingAlpha site. In this section,
    we will illustrate topic modeling using this source. I'm using a sample of some
    700 earnings call transcripts between 2018 and 2019\. This is a fairly small dataset;
    for a practical application, we would need a larger dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*金融的替代数据-类别和用例*中，我们学习了如何从SeekingAlpha网站上爬取收益电话数据。在本节中，我们将使用这个来源来说明主题建模。我使用了2018年至2019年间的大约700个收益电话成绩单的样本。这是一个相当小的数据集；对于实际应用，我们需要一个更大的数据集。
- en: The directory `earnings_calls` contains several files with the code examples
    used in this section. Refer to the notebook `lda_earnings_calls` for details on
    loading, exploring, and preprocessing the data, as well as training and evaluating
    individual models, and the `run_experiments.py` file for the experiments described
    next.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目录`earnings_calls`包含本节中使用的代码示例的多个文件。有关加载、探索和预处理数据的详细信息，请参阅笔记本`lda_earnings_calls`，以及用于描述下一步实验的`run_experiments.py`文件。
- en: Data preprocessing
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'The transcripts consist of individual statements by company representatives,
    an operator, and a Q&A session with analysts. We will treat each of these statements
    as separate documents, ignoring operator statements, to obtain 32,047 items with
    mean and median word counts of 137 and 62, respectively:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 成绩单包括公司代表的个人声明，操作员和分析师问答环节。我们将把这些声明视为单独的文件，忽略操作员的声明，以获得32,047个项目，平均词数为137个，中位数为62个：
- en: '[PRE13]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We use spaCy to preprocess these documents, as illustrated in *Chapter 13*,
    *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*, (refer
    to the notebook), and store the cleaned and lemmatized text as a new text file.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用spaCy对这些文件进行预处理，如*第13章*，*数据驱动的风险因素和无监督学习的资产配置*中所示（请参阅笔记本），并将清理和词形还原后的文本存储为新的文本文件。
- en: Exploration of the most common tokens, as shown in *Figure 15.10*, reveals domain-specific
    stopwords like "year" and "quarter" that we remove in a second step, where we
    also filter out statements with fewer than 10 words so that some 22,582 remain.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图15.10*所示，对最常见的标记进行探索，揭示了领域特定的停用词，如“年”和“季度”，我们在第二步中将其删除，同时过滤掉少于10个词的声明，剩下约22,582个。
- en: '![](img/B15439_15_10.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_10.png)'
- en: 'Figure 15.10: Most common earnings call tokens'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10：最常见的收益电话标记
- en: Model training and evaluation
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练和评估
- en: For illustration, we create a DTM containing terms appearing in between 0.5
    and 25 percent of documents that results in 1,529 features. Now we proceed to
    train a 15-topic model using 25 passes over the corpus. This takes a bit over
    two minutes on a 4-core i7.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，我们创建了一个包含出现在0.5%到25%文件中的术语的DTM，结果是1,529个特征。现在我们继续对语料库进行25次训练，训练一个包含15个主题的模型。在4核i7上，这需要花费两分钟多一点的时间。
- en: The top 10 words per topic, as shown in *Figure 15.11*, identify several distinct
    themes that range from obvious financial information to clinical trials (Topic
    5), China and tariff issues (Topic 9), and technology issues (Topic 11).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图15.11*所示，每个主题的前10个词语确定了几个不同的主题，从明显的财务信息到临床试验（主题5），中国和关税问题（主题9），以及技术问题（主题11）。
- en: '![](img/B15439_15_11.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_11.png)'
- en: 'Figure 15.11: Most important words for earnings call topics'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.11：收益电话主题的最重要词语
- en: 'Using pyLDAvis'' relevance metric with a 0.6 weighting of unconditional frequency
    relative to lift, topic definitions become more intuitive, as illustrated in *Figure
    15.12* for Topic 7 about China and the trade wars:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pyLDAvis的相关度指标，将无条件频率与提升的0.6加权，主题定义变得更加直观，如*图15.12*所示，关于中国和贸易战的第7个主题：
- en: '![](img/B15439_15_12.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_12.png)'
- en: 'Figure 15.12: pyLDAVis'' interactive topic explorer'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.12：pyLDAVis的交互式主题浏览器
- en: The notebook also illustrates how you can look up documents by their topic association.
    In this case, an analyst can review relevant statements for nuances, use sentiment
    analysis to further process the topic-specific text data, or assign labels derived
    from market prices.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本还说明了如何通过它们的主题关联查找文档。在这种情况下，分析师可以审查相关声明以获取细微差别，使用情感分析进一步处理特定主题的文本数据，或者分配从市场价格派生的标签。
- en: Running experiments
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行实验
- en: To illustrate the impact of different parameter settings, we run a few hundred
    experiments for different DTM constraints and model parameters. More specifically,
    we let the `min_df` and `max_df` parameters range from 50-500 words and 10 to
    100 percent of documents, respectively, using alternatively binary and absolute
    counts. We then train LDA models with 3 to 50 topics, using 1 and 25 passes over
    the corpus.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明不同参数设置的影响，我们对不同的DTM约束和模型参数进行了几百次实验。更具体地说，我们让`min_df`和`max_df`参数分别从50-500个单词和10-100%的文档范围内变化，交替使用二进制和绝对计数。然后我们使用3到50个主题对语料库进行1次和25次训练。
- en: The chart in *Figure 15.13* illustrates the results in terms of topic coherence
    (higher is better) and perplexity (lower is better). Coherence drops after 25-30
    topics, and perplexity similarly increases.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.13*中的图表说明了主题连贯性（越高越好）和困惑度（越低越好）的结果。连贯性在25-30个主题后下降，困惑度也相应增加。'
- en: '![](img/B15439_15_13.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_13.png)'
- en: 'Figure 15.13: Impact of LDA hyperparameter settings on topic quality'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.13：LDA超参数设置对主题质量的影响
- en: The notebook includes regression results that quantify the relationships between
    parameters and outcomes. We generally get better results using absolute counts
    and a smaller vocabulary.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包括量化参数和结果之间关系的回归结果。我们通常使用绝对计数和较小的词汇表获得更好的结果。
- en: Topic modeling for with financial news
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 财经新闻的主题建模
- en: The notebook `lda_financial_news` contains an example of LDA applied to a subset
    of over 306,000 financial news articles from the first five months of 2018\. The
    datasets have been posted on Kaggle, and the articles have been sourced from CNBC,
    Reuters, the Wall Street Journal, and more. The notebook contains download instructions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`lda_financial_news`包含了对2018年前五个月的超过306,000篇财经新闻文章的LDA应用的示例。这些数据集已经发布在Kaggle上，文章来源于CNBC、路透社、华尔街日报等。笔记本包含下载说明。
- en: We select the most relevant 120,000 articles based on their section titles with
    a total of 54 million tokens for an average word count of 429 words per article.
    To prepare the data for the LDA model, we rely on spaCy to remove numbers and
    punctuation and lemmatize the results.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据其部分标题选择了最相关的12万篇文章，总共有5400万个标记，平均每篇文章429个单词。为了准备数据用于LDA模型，我们依赖spaCy来去除数字和标点，并对结果进行词形还原。
- en: '*Figure 15.14* highlights the remaining most frequent tokens and the article
    length distribution with a median length of 231 tokens; the 90th percentile is
    642 words.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.14*突出了剩余最频繁的标记和文章长度分布，其中标记的中位数长度为231个；第90百分位数为642个单词。'
- en: '![](img/B15439_15_14.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_14.png)'
- en: 'Figure 15.14: Corpus statistics for financial news data'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.14：财经新闻数据的语料库统计
- en: In *Figure 15.15*, we show results for one model using a vocabulary of 3,570
    tokens based on `min_df`=0.005 and `max_df`=0.1, with a single pass to avoid the
    length training time for 15 topics. We can use the `top_topics` attribute of the
    trained `LdaModel` to obtain the most likely words for each topic (refer to the
    notebook for more details).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图15.15*中，我们展示了一个模型的结果，该模型使用了3,570个标记的词汇表，基于`min_df`=0.005和`max_df`=0.1，进行了一次遍历以避免为15个主题进行训练所需的时间。我们可以使用训练好的`LdaModel`的`top_topics`属性来获取每个主题最可能的词语（更多细节请参考笔记本）。
- en: '![](img/B15439_15_15.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_15_15.png)'
- en: 'Figure 15.15: Top 15 words for financial news topics'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.15：财经新闻主题的前15个词
- en: The topics outline several issues relevant to the time period, including Brexit
    (Topic 8), North Korea (Topic 4), and Tesla (Topic 14).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主题概述了与该时期相关的几个问题，包括英国脱欧（主题8）、朝鲜（主题4）和特斯拉（主题14）。
- en: Gensim provides a `LdaMultiCore` implementation that allows for parallel training
    using Python's multiprocessing module and improves performance by 50 percent when
    using four workers. More workers do not further reduce training time, though,
    due to I/O bottlenecks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim提供了`LdaMultiCore`实现，允许使用Python的多进程模块进行并行训练，并且当使用四个工作进程时，性能提高了50%。然而，由于I/O瓶颈，更多的工作进程并不能进一步减少训练时间。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the use of topic modeling to gain insights into
    the content of a large collection of documents. We covered latent semantic indexing
    that uses dimensionality reduction of the DTM to project documents into a latent
    topic space. While effective in addressing the curse of dimensionality caused
    by high-dimensional word vectors, it does not capture much semantic information.
    Probabilistic models make explicit assumptions about the interplay of documents,
    topics, and words that allow algorithms to reverse engineer the document generation
    process and evaluate the model fit on new documents. We learned that LDA is capable
    of extracting plausible topics that allow us to gain a high-level understanding
    of large amounts of text in an automated way, while also identifying relevant
    documents in a targeted way.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用主题建模来洞察大量文档内容的用途。我们涵盖了使用潜在语义索引，该索引使用DTM的降维来将文档投影到潜在主题空间中。虽然在解决高维词向量引起的维度灾难方面很有效，但它并没有捕捉到太多的语义信息。概率模型对文档、主题和词语的相互作用做出了明确的假设，这些假设允许算法逆向工程文档生成过程，并评估新文档的模型拟合度。我们了解到LDA能够提取出合理的主题，使我们能够以自动化的方式对大量文本进行高层次理解，同时以有针对性的方式识别相关文档。
- en: In the next chapter, we will learn how to train neural networks that embed individual
    words in a high-dimensional vector space that captures important semantic information
    and allows us to use the resulting word vectors as high-quality text features.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何训练神经网络，将单词嵌入到一个高维向量空间中，捕捉重要的语义信息，并允许我们使用生成的单词向量作为高质量的文本特征。
