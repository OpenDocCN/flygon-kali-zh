["```py\nScipy.cluster \n\n%matplotlib inline \nimport matplotlib.pyplot as plt \n\n## Import ndimage to read the image \nfrom scipy import ndimage \n\n## Import cluster for clustering algorithms \nfrom scipy import cluster \n\n## Read the image \nimage = ndimage.imread(\"cluster_test_image.jpg\") \n## Image is 1000x1000 pixels and it has 3 channels. \nimage.shape \n\n(1000, 1000, 3) \n```", "```py\narray([[[30, 30, 30], \n        [16, 16, 16], \n        [14, 14, 14], \n        ..., \n        [14, 14, 14], \n        [16, 16, 16], \n        [29, 29, 29]], \n\n       [[13, 13, 13], \n        [ 0,  0,  0], \n        [ 0,  0,  0], \n        ..., \n        [ 0,  0,  0], \n        [ 0,  0,  0], \n        [12, 12, 12]], \n\n       [[16, 16, 16], \n        [ 3,  3,  3], \n        [ 1,  1,  1], \n        ..., \n        [ 0,  0,  0], \n        [ 2,  2,  2], \n        [16, 16, 16]], \n\n       ..., \n\n       [[17, 17, 17], \n        [ 3,  3,  3], \n        [ 1,  1,  1], \n        ..., \n        [34, 26, 39], \n        [27, 21, 33], \n        [59, 55, 69]], \n\n       [[15, 15, 15], \n        [ 2,  2,  2], \n        [ 0,  0,  0], \n        ..., \n        [37, 31, 43], \n        [34, 28, 42], \n        [60, 56, 71]], \n\n       [[33, 33, 33], \n        [20, 20, 20], \n        [17, 17, 17], \n        ..., \n        [55, 49, 63], \n        [47, 43, 57], \n        [65, 61, 76]]], dtype=uint8) \n\n```", "```py\nplt.figure(figsize = (15,8)) \nplt.imshow(image) \n```", "```py\nx, y, z = image.shape \nimage_2d = image.reshape(x*y, z).astype(float) \nimage_2d.shape \n\n(1000000, 3) \n\nimage_2d \n\narray([[30., 30., 30.], \n       [16., 16., 16.], \n       [14., 14., 14.], \n       ..., \n       [55., 49., 63.], \n       [47., 43., 57.], \n       [65., 61., 76.]]) \n\n## kmeans will return cluster centers and the distortion \ncluster_centers, distortion = cluster.vq.kmeans(image_2d, k_or_guess=2) \n\nprint(cluster_centers, distortion) \n\n[[179.28653454 179.30176248 179.44142117] \n [  3.75308484   3.83491111   4.49236356]] 26.87835069294931 \n\nimage_2d_labeled = image_2d.copy() \n\nlabels = [] \n\nfrom scipy.spatial.distance import euclidean \nimport numpy as np \n\nfor i in range(image_2d.shape[0]): \n    distances = [euclidean(image_2d[i], center) for center in cluster_centers] \n    labels.append(np.argmin(distances)) \n\nplt.figure(figsize = (15,8)) \nplt.imshow(cluster_centers[labels].reshape(x, y, z)) \n```", "```py\nfrom sklearn import datasets \n%matplotlib inline \nimport matplotlib.pyplot as plt \n\n## Boston House Prices dataset \nboston = datasets.load_boston() \nx = boston.data \ny = boston.target \n\nboston.feature_names \n\narray(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', \n       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7') \n\nx.shape \n(506, 13) \n\ny.shape \n(506,) \n\n## We will consider \"lower status of population\" as independent variable for its importance \nlstat = x[0:,-1] \nlstat.shape \n(506,) \n\nfrom scipy import stats \n\nslope, intercept, r_value, p_value, std_err = stats.linregress(lstat, y) \n\nprint(slope, intercept, r_value, p_value, std_err) \n\n-0.9500493537579909 34.55384087938311 -0.737662726174015 5.081103394387796e-88 0.03873341621263942 \n\nprint(\"r-squared:\", r_value**2) \nr-squared: 0.5441462975864798 \n\nplt.plot(lstat, y, 'o', label='original data') \nplt.plot(lstat, intercept + slope*lstat, 'r', label='fitted line') \nplt.legend() \nplt.show() \n```", "```py\nrm = x[0:,5] \n\nslope, intercept, r_value, p_value, std_err = stats.linregress(rm, y) \n\nprint(slope, intercept, r_value, p_value, std_err) \n\nprint(\"r-squared:\", r_value**2) \n\n## 9.102108981180308 -34.670620776438554 0.6953599470715394 2.48722887100781e-74 0.4190265601213402 \n## r-squared: 0.483525455991334 \n```", "```py\nplt.plot(rm, y, 'o', label='original data') \nplt.plot(rm, intercept + slope*rm, 'r', label='fitted line') \nplt.legend() \nplt.show() \n```", "```py\nimport pandas as pd \nfrom sklearn import datasets \n\n%matplotlib inline \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\ndiabetes = datasets.load_diabetes() \n\ndf = pd.DataFrame(diabetes.data, columns=diabetes.feature_names) \n\ndiabetes.feature_names \n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'] \n\ndf.head(10) \n```", "```py\ndf['Target'] = diabetes.target \ndf.head(10) \n```", "```py\n## Descriptive statistics \ndf.describe() \n```", "```py\nplt.hist(df['Target']) \n```", "```py\n## Since 'sex' is categorical, excluding it from numerical columns \nnumeric_cols = [col for col in df.columns if col != 'sex'] \n\nnumeric_cols \n## ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'Target'] \n\n## You can have a look at variable distributions individually, but there's a better way \ndf[numeric_cols].hist(figsize=(20, 20), bins=30, xlabelsize=12, ylabelsize=12) \n\n## You can also choose create dataframes for numerical and categorical variables \n```", "```py\n## corr method will give you the correlation between features \ndf[numeric_cols].corr() \n```", "```py\nplt.figure(figsize=(15, 15)) \nsns.heatmap(df[numeric_cols].corr(), annot=True) \n```", "```py\nplt.figure(figsize=(18, 15)) \nsns.heatmap(df[numeric_cols].corr() \n            [(df[numeric_cols].corr() >= 0.3) & (df[numeric_cols].corr() <= 0.5)],  \n            annot=True) \n```", "```py\nfig, ax = plt.subplots(3, 3, figsize = (18, 12)) \nfor i, ax in enumerate(fig.axes): \n    if i < 9: \n        sns.regplot(x=df[numeric_cols[i]],y='Target', data=df, ax=ax) \n```", "```py\nimport quandl msft = quandl.get('WIKI/MSFT') \n\nmsft.columns \n## Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Ex-Dividend', 'Split Ratio', 'Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close', 'Adj. Volume'], dtype='object') \n\nmsft.tail()\n```", "```py\nmatplotlib.font_manager as font_manager font_path = '/Library/Fonts/Cochin.ttc' font_prop = font_manager.FontProperties(fname=font_path, size=24) axis_font = {'fontname':'Arial', 'size':'18'} title_font = {'fontname':'Arial', 'size':'22', 'color':'black', 'weight':'normal', 'verticalalignment':'bottom'} plt.figure(figsize=(10, 8)) plt.plot(msft['Adj. Close'], label='Adj. Close') plt.xticks(fontsize=22) plt.yticks(fontsize=22) plt.xlabel(\"Date\", **axis_font) plt.ylabel(\"Adj. Close\", **axis_font) plt.title(\"MSFT\", **title_font) plt.legend(loc='upper left', prop=font_prop, numpoints=1) plt.show()\n```", "```py\nmsft['Daily Pct. Change'] = (msft['Adj. Close'] - msft['Adj. Open']) / msft['Adj. Open'] \n\nmsft.tail(10)\n```", "```py\nplt.figure(figsize=(22, 8)) \nplt.hist(msft['Daily Pct. Change'], bins=100)\n```", "```py\nimport plotly.plotly as py \nimport plotly.graph_objs as go \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot init_notebook_mode(connected=True) \nfrom datetime import datetime \nimport pandas_datareader.data as web \nimport quandl \n\nmsft = quandl.get('WIKI/MSFT') \nmsft['Daily Pct. Change'] = (msft['Adj. Close'] - msft['Adj. Open']) / msft['Adj. Open'] \n\ndata = [go.Scatter(x=msft.index, y=msft['Adj. Close'])] \n\nplot(data)\n```", "```py\ncharts trace = go.Ohlc(x=msft.index, open=msft['Adj. Open'], high=msft['Adj. High'], low=msft['Adj. Low'], close=msft['Adj. Close']) \n\ndata = [trace] \n\nplot(data)\n```", "```py\ntrace = go.Candlestick(x=msft.index, open=msft['Adj. Open'], high=msft['Adj. High'], low=msft['Adj. Low'], close=msft['Adj. Close']) \n\ndata = [trace] \n\nplot(data)\n```", "```py\nimport plotly.figure_factory as ff \n\nfig = ff.create_distplot([msft['Daily Pct. Change'].values], ['MSFT Daily Returns'], show_hist=False) \n\nplot(fig)\n```", "```py\nmsft['200MA'] = msft['Adj. Close'].rolling(window=200).mean() \nmsft['100MA'] = msft['Adj. Close'].rolling(window=100).mean() \nmsft['50MA'] = msft['Adj. Close'].rolling(window=50).mean() \n\nmsft.tail(10)\n```", "```py\ntrace_adjclose = go.Scatter( x=msft[-2000:].index, y=msft[-2000:]['Adj. Close'], name = \"Adj. Close\", line = dict(color = '#000000'), opacity = 0.8) \n\ntrace_200 = go.Scatter( x=msft[-2000:].index, y=msft[-2000:]['200MA'], name = \"200MA\", line = dict(color = '#FF0000'), opacity = 0.8) \n\ntrace_100 = go.Scatter( x=msft[-2000:].index, y=msft[-2000:]['100MA'], name = \"100MA\", line = dict(color = '#0000FF'), opacity = 0.8) \n\ntrace_50 = go.Scatter( x=msft[-2000:].index, y=msft[-2000:]['50MA'], name = \"50MA\", line = dict(color = '#FF00FF'), opacity = 0.8) \n\ndata = [trace_adjclose, trace_200, trace_100, trace_50] \n\nlayout = dict( title = \"MSFT Moving Averages: 200, 100, 50 days\", ) \n\nfig = dict(data=data, layout=layout) \n\nplot(fig)\n```", "```py\nmsft_monthly = msft.resample('M').mean() \n\nmsft_monthly.tail(10)\n```", "```py\ndata = [go.Scatter(x=msft_monthly[-24:].index, y = msft_monthly[-24:]['Adj. Close'])] \n\nplot(data)\n```", "```py\nplt.figure(figsize=(22, 14)) pd.plotting.autocorrelation_plot(msft_monthly['Daily Pct. Change'])\n```", "```py\nfrom sklearn import datasets, linear_model \nfrom sklearn.metrics import mean_squared_error, r2_score \n\ndiabetes = datasets.load_diabetes() \n\nlinreg = linear_model.LinearRegression() \n\nlinreg.fit(diabetes.data, diabetes.target) \n\n## You can inspect the results by looking at evaluation metrics \nprint('Coeff.: n', linreg.coef_) \nprint(\"MSE: {}\".format(mean_squared_error(diabetes.target, linreg.predict(diabetes.data)))) print('Variance Score: {}'.format(r2_score(diabetes.target, linreg.predict(diabetes.data))))\n```", "```py\nfrom sklearn.cluster import KMeans from sklearn.datasets import load_boston boston = load_boston()\n## As previously, you have implemented the KMeans from scracth and in this example, you use sklearns API k_means = KMeans(n_clusters=3) # Training k_means.fit(boston.data)\nKMeans(algorithm='auto', copy_x=True, init='K \u5747\u503c++', max_iter=300, n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0)\nprint(k_means.labels_)\n```", "```py\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 0 2 2\n2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n2 2 0 0 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2\n2 0 2 2 2 2 0 2 2 2 0 0 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1]\n```", "```py\nprint(k_means.cluster_centers_)\n```", "```py\n[[ 1.49558803e+01 -5.32907052e-15 1.79268421e+01 2.63157895e-02\n6.73710526e-01 6.06550000e+00 8.99052632e+01 1.99442895e+00\n2.25000000e+01 6.44736842e+02 1.99289474e+01 5.77863158e+01\n2.04486842e+01]\n[ 3.74992678e-01 1.57103825e+01 8.35953552e+00 7.10382514e-02\n5.09862568e-01 6.39165301e+00 6.04133880e+01 4.46074481e+00\n4.45081967e+00 3.11232240e+02 1.78177596e+01 3.83489809e+02\n1.03886612e+01]\n[ 1.09105113e+01 5.32907052e-15 1.85725490e+01 7.84313725e-02\n6.71225490e-01 5.98226471e+00 8.99137255e+01 2.07716373e+00\n2.30196078e+01 6.68205882e+02 2.01950980e+01 3.71803039e+02\n1.78740196e+01]]\n```"]