["```\n$ gcloud init\n```", "```\n$ gcloud compute networks create my-auto-network --mode auto\n```", "```\n//create custom mode VPC which is named my-custom-network\n$ gcloud compute networks create my-custom-network --mode custom  \n```", "```\n$ gcloud compute networks subnets create subnet-a --network=my-custom-network --range=10.0.1.0/24 --region=us-west1\n$ gcloud compute networks subnets create subnet-b --network=my-custom-network --range=172.16.1.0/24 --region=us-east1\n$ gcloud compute networks subnets create subnet-c --network=my-custom-network --range=192.168.1.0/24 --region=asia-northeast1  \n```", "```\n//create ssh access for public host\n$ gcloud compute firewall-rules create public-ssh --network=my-custom-network --allow=\"tcp:22\" --source-ranges=\"0.0.0.0/0\" --target-tags=\"public\"\n\n//create http access (80/tcp for public host)\n$ gcloud compute firewall-rules create public-http --network=my-custom-network --allow=\"tcp:80\" --source-ranges=\"0.0.0.0/0\" --target-tags=\"public\"\n\n//create ssh access for private host (allow from host which has \"public\" tag)\n$ gcloud compute firewall-rules create private-ssh --network=my-custom-network --allow=\"tcp:22\" --source-tags=\"public\" --target-tags=\"private\"\n\n//create icmp access for internal each other (allow from host which has either \"public\" or \"private\")\n$ gcloud compute firewall-rules create internal-icmp --network=my-custom-network --allow=\"icmp\" --source-tags=\"public,private\"\n```", "```\n//this command create new ssh key pair\n$ gcloud compute config-ssh\n\n//key will be stored as ~/.ssh/google_compute_engine(.pub)\n$ cd ~/.ssh\n$ ls -l google_compute_engine*\n-rw-------  1 saito  admin  1766 Aug 23 22:58 google_compute_engine\n-rw-r--r--  1 saito  admin   417 Aug 23 22:58 google_compute_engine.pub  \n```", "```\n//create public instance (\"public\" tag) on subnet-a\n$ gcloud compute instances create public-on-subnet-a --machine-type=f1-micro --network=my-custom-network --subnet=subnet-a --zone=us-west1-a --tags=public\n\n//create public instance (\"public\" tag) on subnet-b\n$ gcloud compute instances create public-on-subnet-b --machine-type=f1-micro --network=my-custom-network --subnet=subnet-b --zone=us-east1-c --tags=public\n\n//create private instance (\"private\" tag) on subnet-a with larger size (g1-small)\n$ gcloud compute instances create private-on-subnet-a --machine-type=g1-small --network=my-custom-network --subnet=subnet-a --zone=us-west1-a --tags=private\n\n//Overall, there are 3 VM instances has been created in this example as below\n$ gcloud compute instances list\nNAME                                           ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP      STATUS\npublic-on-subnet-b                             us-east1-c     f1-micro                   172.16.1.2   35.196.228.40    RUNNING\nprivate-on-subnet-a                            us-west1-a     g1-small                   10.0.1.2     104.199.121.234  RUNNING\npublic-on-subnet-a                             us-west1-a     f1-micro                   10.0.1.3     35.199.171.31    RUNNING  \n```", "```\n$ ssh-add ~/.ssh/google_compute_engine\nEnter passphrase for /Users/saito/.ssh/google_compute_engine: \nIdentity added: /Users/saito/.ssh/google_compute_engine (/Users/saito/.ssh/google_compute_engine)  \n```", "```\n//logout from VM instance, then back to your machine\n$ exit\n\n//install nginx from your machine via ssh\n$ ssh 35.196.228.40 \"sudo apt-get -y install nginx\"\n$ ssh 35.199.171.31 \"sudo apt-get -y install nginx\"\n\n//check whether firewall rule (public-http) work or not\n$ curl -I http://35.196.228.40/\nHTTP/1.1 200 OK\nServer: nginx/1.10.3\nDate: Sun, 27 Aug 2017 07:07:01 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Fri, 25 Aug 2017 05:48:28 GMT\nConnection: keep-alive\nETag: \"599fba2c-264\"\nAccept-Ranges: bytes  \n```", "```\n$ curl http://104.199.121.234:8080/examples/\ncurl: (7) Failed to connect to 104.199.121.234 port 8080: Operation timed out  \n```", "```\n//create instance groups for HTTP instances and tomcat instance\n$ gcloud compute instance-groups unmanaged create http-ig-us-west --zone us-west1-a\n$ gcloud compute instance-groups unmanaged create http-ig-us-east --zone us-east1-c\n$ gcloud compute instance-groups unmanaged create tomcat-ig-us-west --zone us-west1-a\n\n//because tomcat uses 8080/tcp, create a new named port as tomcat:8080\n$ gcloud compute instance-groups unmanaged set-named-ports tomcat-ig-us-west --zone us-west1-a --named-ports tomcat:8080\n\n//register an existing VM instance to correspond instance group\n$ gcloud compute instance-groups unmanaged add-instances http-ig-us-west --instances public-on-subnet-a --zone us-west1-a\n$ gcloud compute instance-groups unmanaged add-instances http-ig-us-east --instances public-on-subnet-b --zone us-east1-c\n$ gcloud compute instance-groups unmanaged add-instances tomcat-ig-us-west --instances private-on-subnet-a --zone us-west1-a  \n```", "```\n//create health check for http (80/tcp) for \"/\"\n$ gcloud compute health-checks create http my-http-health-check --check-interval 5 --healthy-threshold 2 --unhealthy-threshold 3 --timeout 5 --port 80 --request-path /\n\n//create health check for Tomcat (8080/tcp) for \"/examples/\"\n$ gcloud compute health-checks create http my-tomcat-health-check --check-interval 5 --healthy-threshold 2 --unhealthy-threshold 3 --timeout 5 --port 8080 --request-path /examples/  \n```", "```\n//create backend service for http (default) and named port tomcat (8080/tcp)\n$ gcloud compute backend-services create my-http-backend-service --health-checks my-http-health-check --protocol HTTP --global\n$ gcloud compute backend-services create my-tomcat-backend-service --health-checks my-tomcat-health-check --protocol HTTP --port-name tomcat --global\n\n//add http instance groups (both us-west1 and us-east1) to http backend service\n$ gcloud compute backend-services add-backend my-http-backend-service --instance-group http-ig-us-west --instance-group-zone us-west1-a --balancing-mode UTILIZATION --max-utilization 0.8 --capacity-scaler 1 --global\n$ gcloud compute backend-services add-backend my-http-backend-service --instance-group http-ig-us-east --instance-group-zone us-east1-c --balancing-mode UTILIZATION --max-utilization 0.8 --capacity-scaler 1 --global\n\n//also add tomcat instance group to tomcat backend service\n$ gcloud compute backend-services add-backend my-tomcat-backend-service --instance-group tomcat-ig-us-west --instance-group-zone us-west1-a --balancing-mode UTILIZATION --max-utilization 0.8 --capacity-scaler 1 --global  \n```", "```\n//create load balancer(url-map) to associate my-http-backend-service as default\n$ gcloud compute url-maps create my-loadbalancer --default-service my-http-backend-service\n\n//add /examples and /examples/* mapping to my-tomcat-backend-service\n$ gcloud compute url-maps add-path-matcher my-loadbalancer --default-service my-http-backend-service --path-matcher-name tomcat-map --path-rules /examples=my-tomcat-backend-service,/examples/*=my-tomcat-backend-service\n\n//create target-http-proxy that associate to load balancer(url-map)\n$ gcloud compute target-http-proxies create my-target-http-proxy --url-map=my-loadbalancer\n\n//allocate static global ip address and check assigned address\n$ gcloud compute addresses create my-loadbalancer-ip --global\n$ gcloud compute addresses describe my-loadbalancer-ip --global\naddress: 35.186.192.6\n\n//create forwarding rule that associate static IP to target-http-proxy\n$ gcloud compute forwarding-rules create my-frontend-rule --global --target-http-proxy my-target-http-proxy --address 35.186.192.6 --ports 80\n```", "```\n//add one more Firewall Rule that allow Load Balancer to Tomcat (8080/tcp)\n$ gcloud compute firewall-rules create private-tomcat --network=my-custom-network --source-ranges 130.211.0.0/22,35.191.0.0/16 --target-tags private --allow tcp:8080  \n```", "```\n$ gcloud compute instances list\nNAME                                           ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP      STATUS\npublic-on-subnet-b                             us-east1-c     f1-micro                   172.16.1.2   35.196.228.40    RUNNING\nprivate-on-subnet-a                            us-west1-a     g1-small                   10.0.1.2     104.199.121.234  RUNNING\npublic-on-subnet-a                             us-west1-a     f1-micro                   10.0.1.3     35.199.171.31    RUNNING  \n```", "```\n//create 20GB PD on us-west1-a with standard type\n$ gcloud compute disks create my-disk-us-west1-a --zone us-west1-a --type pd-standard --size 20\n\n//after a few seconds, check status, you can see existing boot disks as well\n$ gcloud compute disks list\nNAME                                           ZONE           SIZE_GB  TYPE         STATUS\npublic-on-subnet-b                             us-east1-c     10       pd-standard  READY\nmy-disk-us-west1-a                             us-west1-a     20       pd-standard  READY\nprivate-on-subnet-a                            us-west1-a     10       pd-standard  READY\npublic-on-subnet-a                             us-west1-a     10       pd-standard  READY\n\n//attach PD(my-disk-us-west1-a) to the VM instance(public-on-subnet-a)\n$ gcloud compute instances attach-disk public-on-subnet-a --disk my-disk-us-west1-a --zone us-west1-a\n\n//login to public-on-subnet-a to see the status\n$ ssh 35.199.171.31\nLinux public-on-subnet-a 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u3 (2017-08-06) x86_64\n\nThe programs included with the Debian GNU/Linux system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\npermitted by applicable law.\nLast login: Fri Aug 25 03:53:24 2017 from 107.196.102.199\n**saito@public-on-subnet-a**:**~**$ sudo su\nroot@public-on-subnet-a:/home/saito# dmesg | tail\n[ 7377.421190] systemd[1]: apt-daily-upgrade.timer: Adding 25min 4.773609s random time.\n[ 7379.202172] systemd[1]: apt-daily-upgrade.timer: Adding 6min 37.770637s random time.\n[243070.866384] scsi 0:0:2:0: Direct-Access     Google   PersistentDisk   1    PQ: 0 ANSI: 6\n[243070.875665] sd 0:0:2:0: [sdb] 41943040 512-byte logical blocks: (21.5 GB/20.0 GiB)\n[243070.883461] sd 0:0:2:0: [sdb] 4096-byte physical blocks\n[243070.889914] sd 0:0:2:0: Attached scsi generic sg1 type 0\n[243070.900603] sd 0:0:2:0: [sdb] Write Protect is off\n[243070.905834] sd 0:0:2:0: [sdb] Mode Sense: 1f 00 00 08\n[243070.905938] sd 0:0:2:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\n[243070.925713] sd 0:0:2:0: [sdb] Attached SCSI disk  \n```", "```\n//install kubectl command\n$ gcloud components install kubectl  \n```", "```\n$ gcloud container clusters create my-k8s-cluster --cluster-version 1.6.7 --machine-type f1-micro --num-nodes 3 --network my-custom-network --subnetwork subnet-c --zone asia-northeast1-a --tags private\n\nCreating cluster my-k8s-cluster...done. \nCreated [https://container.googleapis.com/v1/projects/devops-with-kubernetes/zones/asia-northeast1-a/clusters/my-k8s-cluster].\nkubeconfig entry generated for my-k8s-cluster.\nNAME            ZONE               MASTER_VERSION  MASTER_IP      MACHINE_TYPE  NODE_VERSION  NUM_NODES  STATUS\nmy-k8s-cluster  asia-northeast1-a  1.6.7           35.189.135.13  f1-micro      1.6.7         3          RUNNING\n\n//check node status\n$ kubectl get nodes\nNAME                                            STATUS    AGE       VERSION\ngke-my-k8s-cluster-default-pool-ae180f53-47h5   Ready     1m        v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-6prb   Ready     1m        v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-z6l1   Ready     1m        v1.6.7  \n```", "```\n//run kubectl proxy on your machine, that will bind to 127.0.0.1:8001\n$ kubectl proxy\nStarting to serve on 127.0.0.1:8001\n\n//use Web browser on your machine to access to 127.0.0.1:8001/ui/\nhttp://127.0.0.1:8001/ui/\n```", "```\n//run resize command to change number of nodes to 5\n$ gcloud container clusters resize my-k8s-cluster --size 5 --zone asia-northeast1-a\n\n//after a few minutes later, you may see additional nodes\n$ kubectl get nodes\nNAME                                            STATUS    AGE       VERSION\ngke-my-k8s-cluster-default-pool-ae180f53-47h5   Ready     5m        v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-6prb   Ready     5m        v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-f8ps   Ready     30s       v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-qzxz   Ready     30s       v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-z6l1   Ready     5m        v1.6.7  \n```", "```\n//create and add node pool which is named \"large-mem-pool\"\n$ gcloud container node-pools create large-mem-pool --cluster my-k8s-cluster --machine-type g1-small --num-nodes 2 --tags private --zone asia-northeast1-a\n\n//after a few minustes, large-mem-pool instances has been added\n$ kubectl get nodes\nNAME                                              STATUS    AGE       VERSION\ngke-my-k8s-cluster-default-pool-ae180f53-47h5     Ready     13m       v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-6prb     Ready     13m       v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-f8ps     Ready     8m        v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-qzxz     Ready     8m        v1.6.7\ngke-my-k8s-cluster-default-pool-ae180f53-z6l1     Ready     13m       v1.6.7\ngke-my-k8s-cluster-large-mem-pool-f87dd00d-9v5t   Ready     5m        v1.6.7\ngke-my-k8s-cluster-large-mem-pool-f87dd00d-fhpn   Ready     5m        v1.6.7  \n```", "```\n//nodeSelector specifies f1-micro\n$ cat nginx-pod-selector.yml \napiVersion: v1\nkind: Pod\nmetadata:\n name: nginx\nspec:\n containers:\n - name: nginx\n image: nginx\n nodeSelector:\n beta.kubernetes.io/instance-type: f1-micro\n\n//deploy pod\n$ kubectl create -f nginx-pod-selector.yml \npod \"nginx\" created\n\n//it uses default pool\n$ kubectl get pods nginx -o wide\nNAME      READY     STATUS    RESTARTS   AGE       IP           NODE\nnginx     1/1       Running   0          7s        10.56.1.13   gke-my-k8s-cluster-default-pool-ae180f53-6prb\n```", "```\n//list Node Pool\n$ gcloud container node-pools list --cluster my-k8s-cluster --zone asia-northeast1-a\nNAME            MACHINE_TYPE  DISK_SIZE_GB  NODE_VERSION\ndefault-pool    f1-micro      100           1.6.7\nlarge-mem-pool  g1-small      100           1.6.7\n\n//delete default-pool\n$ gcloud container node-pools delete default-pool --cluster my-k8s-cluster --zone asia-northeast1-a\n\n//after a few minutes, default-pool nodes x 5 has been deleted\n$ kubectl get nodes\nNAME                                              STATUS    AGE       VERSION\ngke-my-k8s-cluster-large-mem-pool-f87dd00d-9v5t   Ready     16m       v1.6.7\ngke-my-k8s-cluster-large-mem-pool-f87dd00d-fhpn   Ready     16m       v1.6.7  \n```", "```\n//delete cluster first\n$ gcloud container clusters delete my-k8s-cluster --zone asia-northeast1-a\n\n//create a new cluster with --additional-zones option but 2 nodes only\n$ gcloud container clusters create my-k8s-cluster --cluster-version 1.6.7 --machine-type f1-micro --num-nodes 2 --network my-custom-network --subnetwork subnet-c --zone asia-northeast1-a --tags private --additional-zones asia-northeast1-b,asia-northeast1-c  \n```", "```\n$ kubectl get nodes\nNAME                                            STATUS    AGE       VERSION\ngke-my-k8s-cluster-default-pool-0c4fcdf3-3n6d   Ready     44s       v1.6.7\ngke-my-k8s-cluster-default-pool-0c4fcdf3-dtjj   Ready     48s       v1.6.7\ngke-my-k8s-cluster-default-pool-2407af06-5d28   Ready     41s       v1.6.7\ngke-my-k8s-cluster-default-pool-2407af06-tnpj   Ready     45s       v1.6.7\ngke-my-k8s-cluster-default-pool-4c20ec6b-395h   Ready     49s       v1.6.7\ngke-my-k8s-cluster-default-pool-4c20ec6b-rrvz   Ready     49s       v1.6.7  \n```", "```\n$ gcloud container get-server-config\n\nFetching server config for us-east4-b\ndefaultClusterVersion: 1.6.7\ndefaultImageType: COS\nvalidImageTypes:\n- CONTAINER_VM\n- COS\n- UBUNTU\nvalidMasterVersions:\n- 1.7.3\n- 1.6.8\n- 1.6.7\nvalidNodeVersions:\n- 1.7.3\n- 1.7.2\n- 1.7.1\n- 1.6.8\n- 1.6.7\n- 1.6.6\n- 1.6.4\n- 1.5.7\n- 1.4.9  \n```", "```\n//upgrade master using --master option\n$ gcloud container clusters upgrade my-k8s-cluster --zone asia-northeast1-a --cluster-version 1.7.3 --master\nMaster of cluster [my-k8s-cluster] will be upgraded from version \n[1.6.7] to version [1.7.3]. This operation is long-running and will \nblock other operations on the cluster (including delete) until it has \nrun to completion.\n\nDo you want to continue (Y/n)?  y\n\nUpgrading my-k8s-cluster...done. \nUpdated [https://container.googleapis.com/v1/projects/devops-with-kubernetes/zones/asia-northeast1-a/clusters/my-k8s-cluster].  \n```", "```\n//master upgrade has been successfully to done\n$ gcloud container clusters list --zone asia-northeast1-a\nNAME            ZONE               MASTER_VERSION  MASTER_IP       MACHINE_TYPE  NODE_VERSION  NUM_NODES  STATUS\nmy-k8s-cluster  asia-northeast1-a  1.7.3           35.189.141.251  f1-micro      1.6.7 *       6          RUNNING  \n```", "```\n//node upgrade (not specify --master)\n$ gcloud container clusters upgrade my-k8s-cluster --zone asia-northeast1-a --cluster-version 1.7.3 \nAll nodes (6 nodes) of cluster [my-k8s-cluster] will be upgraded from \nversion [1.6.7] to version [1.7.3]. This operation is long-running and will block other operations on the cluster (including delete) until it has run to completion.\n\nDo you want to continue (Y/n)?  y  \n```", "```\nNAME                                            STATUS                        AGE       VERSION\ngke-my-k8s-cluster-default-pool-0c4fcdf3-3n6d   Ready                         37m       v1.6.7\ngke-my-k8s-cluster-default-pool-0c4fcdf3-dtjj   Ready                         37m       v1.6.7\ngke-my-k8s-cluster-default-pool-2407af06-5d28   NotReady,SchedulingDisabled   37m       v1.6.7\ngke-my-k8s-cluster-default-pool-2407af06-tnpj   Ready                         37m       v1.6.7\ngke-my-k8s-cluster-default-pool-4c20ec6b-395h   Ready                         5m        v1.7.3\ngke-my-k8s-cluster-default-pool-4c20ec6b-rrvz   Ready                         1m        v1.7.3  \n```", "```\n$ kubectl get storageclass\nNAME                 TYPE\nstandard (default)   kubernetes.io/gce-pd \n\n$ kubectl describe storageclass standard\nName:       standard\nIsDefaultClass:   Yes\nAnnotations:      storageclass.beta.kubernetes.io/is-default-class=true\nProvisioner:      kubernetes.io/gce-pd\nParameters: type=pd-standard\nEvents:           <none>  \n```", "```\n$ cat pvc-gke.yml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n name: pvc-gke-1\nspec:\n storageClassName: \"standard\"\n accessModes:\n - ReadWriteOnce\n resources:\n requests:\n storage: 10Gi\n\n//create Persistent Volume Claim\n$ kubectl create -f pvc-gke.yml \npersistentvolumeclaim \"pvc-gke-1\" created\n\n//check Persistent Volume\n$ kubectl get pv\nNAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM               STORAGECLASS   REASON    AGE\npvc-bc04e717-8c82-11e7-968d-42010a920fc3   10Gi       RWO           Delete          Bound     default/pvc-gke-1   standard                 2s\n\n//check via gcloud command\n$ gcloud compute disks list \nNAME                                                             ZONE               SIZE_GB  TYPE         STATUS\ngke-my-k8s-cluster-d2e-pvc-bc04e717-8c82-11e7-968d-42010a920fc3  asia-northeast1-a  10       pd-standard  READY  \n```", "```\n$ cat grafana.yml \napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n name: grafana\nspec:\n replicas: 1\n template:\n metadata:\n labels:\n run: grafana\n spec:\n containers:\n - image: grafana/grafana\n name: grafana\n ports:\n - containerPort: 3000\n---\napiVersion: v1\nkind: Service\nmetadata:\n name: grafana\nspec:\n ports:\n - port: 80\n targetPort: 3000\n type: LoadBalancer\n selector:\n run: grafana\n\n//deploy grafana with Load Balancer service\n$ kubectl create -f grafana.yml \ndeployment \"grafana\" created\nservice \"grafana\" created\n\n//check L4 Load balancer IP address\n$ kubectl get svc grafana\nNAME      CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\ngrafana   10.59.249.34   35.189.128.32   80:30584/TCP   5m\n\n//can reach via GCP L4 Load Balancer\n$ curl -I 35.189.128.32\nHTTP/1.1 302 Found\nLocation: /login\nSet-Cookie: grafana_sess=f92407d7b266aab8; Path=/; HttpOnly\nSet-Cookie: redirect_to=%252F; Path=/\nDate: Wed, 30 Aug 2017 07:05:20 GMT\nContent-Type: text/plain; charset=utf-8  \n```", "```\n$ cat nginx-tomcat-ingress.yaml \napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n name: nginx-tomcat-ingress\nspec:\n rules:\n - http:\n paths:\n - path: /\n backend:\n serviceName: nginx\n servicePort: 80\n - path: /examples\n backend:\n serviceName: tomcat\n servicePort: 8080\n - path: /examples/*\n backend:\n serviceName: tomcat\n servicePort: 8080\n\n$ kubectl create -f nginx-tomcat-ingress.yaml \ningress \"nginx-tomcat-ingress\" created  \n```", "```\n$ kubectl get ing\nNAME                   HOSTS     ADDRESS           PORTS     AGE\nnginx-tomcat-ingress   *         107.178.253.174   80        1m  \n```", "```\n//allocate static IP as my-nginx-tomcat\n$ gcloud compute addresses create my-nginx-tomcat --global\n\n//check assigned IP address\n$ gcloud compute addresses list \nNAME             REGION  ADDRESS         STATUS\nmy-nginx-tomcat          35.186.227.252  IN_USE\n\n//add annotations definition\n$ cat nginx-tomcat-static-ip-ingress.yaml \napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n name: nginx-tomcat-ingress\n annotations:\n kubernetes.io/ingress.global-static-ip-name: my-nginx- \ntomcat\nspec:\n rules:\n - http:\n paths:\n - path: /\n backend:\n serviceName: nginx\n servicePort: 80\n - path: /examples\n backend:\n serviceName: tomcat\n servicePort: 8080\n - path: /examples/*\n backend:\n serviceName: tomcat\n servicePort: 8080\n\n//apply command to update Ingress\n$ kubectl apply -f nginx-tomcat-static-ip-ingress.yaml \n\n//check Ingress address that associate to static IP\n$ kubectl get ing\nNAME                   HOSTS     ADDRESS          PORTS     AGE\nnginx-tomcat-ingress   *         35.186.227.252   80        48m  \n```"]