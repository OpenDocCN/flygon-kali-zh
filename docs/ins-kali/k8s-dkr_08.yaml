- en: '*Chapter 6*: Services, Load Balancing, and External DNS'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：服务、负载均衡和外部DNS'
- en: When you deploy an application to a Kubernetes cluster, your pods are assigned
    ephemeral IP addresses. Since the assigned addresses are likely to change as pods
    are restarted, you should never target a service using a pod IP address; instead,
    you should use a service object, which will map a service IP address to backend
    pods based on labels. If you need to offer service access to external requests,
    you can deploy an Ingress controller, which will expose your service to external
    traffic on a per-URL basis. For more advanced workloads, you can deploy a load
    balancer, which provides your service with an external IP address, allowing you
    to expose any IP-based service to external requests.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将应用程序部署到Kubernetes集群时，您的pod将被分配临时IP地址。由于分配的地址可能会随着pod的重新启动而更改，您不应该使用pod IP地址来定位服务；相反，您应该使用一个服务对象，它将基于标签将服务IP地址映射到后端pod。如果您需要向外部请求提供服务访问，您可以部署一个Ingress控制器，它将根据每个URL公开您的服务以接受外部流量。对于更高级的工作负载，您可以部署一个负载均衡器，它将为您的服务提供外部IP地址，从而允许您将任何基于IP的服务暴露给外部请求。
- en: We will explain how to implement each of these by deploying them on our KinD
    cluster. To help us understand how the Ingress works, we will deploy a NGINX Ingress
    controller to the cluster and expose a web server. Since Ingress rules are based
    on the incoming URL name, we need to be able to provide stable DNS names. In an
    enterprise environment, this would be accomplished using standard DNS. Since we
    are using a development environment without a DNS server, we will use a popular
    service from nip.io.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解释如何通过在我们的KinD集群上部署它们来实现这些功能。为了帮助我们理解Ingress的工作原理，我们将在集群中部署一个NGINX Ingress控制器并公开一个Web服务器。由于Ingress规则是基于传入的URL名称的，我们需要能够提供稳定的DNS名称。在企业环境中，这将通过使用标准DNS来实现。由于我们使用的是没有DNS服务器的开发环境，我们将使用nip.io的流行服务。
- en: To end the chapter, we will explain how you can dynamically register service
    names using an ETCD-integrated DNS zone with the Kubernetes incubator project,
    external-dns.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将解释如何使用Kubernetes孵化器项目external-dns动态注册服务名称，使用ETCD集成的DNS区域。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Exposing workloads to requests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将工作负载暴露给请求
- en: Introduction to load balancers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器简介
- en: Layer 7 load balancers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7层负载均衡器
- en: Layer 4 load balancers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4层负载均衡器
- en: Making service names available externally
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使服务名称在外部可用
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具有以下技术要求：
- en: A new Ubuntu 18.04 server with a minimum of 4 GB of RAM.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个新的Ubuntu 18.04服务器，至少有4GB的RAM。
- en: A KinD cluster configured using the configuration from [*Chapter 4*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083),
    Deploying Kubernetes using KinD.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[*第4章*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083)中的配置部署KinD的集群。
- en: You can access the code for this chapter at GitHub repository [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库[https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide)中访问本章的代码。
- en: Exposing workloads to requests
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将工作负载暴露给请求
- en: Three of the most misunderstood objects in Kubernetes are services, Ingress
    controllers, and load balancers. In order to expose your workloads, you need to
    understand how each object works and the options that are available to you. Let's
    look at these in detail.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中最被误解的三个对象是服务、Ingress控制器和负载均衡器。为了暴露您的工作负载，您需要了解每个对象的工作原理以及可用的选项。让我们详细看看这些。
- en: Understanding how services work
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解服务的工作原理
- en: As we mentioned in the introduction, any pod that is running a workload is assigned
    an IP address at pod startup. Many events will cause a deployment to restart a
    pod, and when the pod is restarted, it will likely receive a new IP address. Since
    the addresses that are assigned to pods may change, you should never target a
    pod's workload directly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在介绍中提到的，任何运行工作负载的pod在启动时都被分配一个IP地址。许多事件会导致部署重新启动一个pod，当pod重新启动时，它可能会收到一个新的IP地址。由于分配给pod的地址可能会改变，您不应直接针对pod的工作负载。
- en: One of the most powerful features that Kubernetes offers is the ability to scale
    your deployments. When a deployment is scaled, Kubernetes will create additional
    pods to handle any additional resource requirements. Each pod will have an IP
    address, and as you may know, most applications only target a single IP address
    or name. If your application were to scale from a single pod to ten pods, how
    would you utilize the additional pods?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供的最强大的功能之一是能够扩展您的部署。当部署被扩展时，Kubernetes将创建额外的pod来处理任何额外的资源需求。每个pod都将有一个IP地址，正如您可能知道的，大多数应用程序只针对单个IP地址或名称。如果您的应用程序从一个pod扩展到十个pod，您将如何利用额外的pod？
- en: 'Services use Kubernetes labels to create a dynamic mapping between the service
    itself and the pods running the workload. The pods that are running the workload
    are labeled when they start up. Each pod has the same label that is defined in
    the deployment. For example, if we were using a NGINX web server in our deployment,
    we would create a deployment with the following manifest:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 服务使用Kubernetes标签来创建服务本身与运行工作负载的pod之间的动态映射。在启动时，运行工作负载的pod被标记。每个pod都具有与部署中定义的相同的标签。例如，如果我们在部署中使用NGINX
    web服务器，我们将创建一个具有以下清单的部署：
- en: 'apiVersion: apps/v1'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: api版本：apps/v1
- en: 'kind: Deployment'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：部署
- en: 'metadata:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'creationTimestamp: null'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 创建时间戳：null
- en: 'labels:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：
- en: 'run: nginx-frontend'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'run: nginx-frontend'
- en: 'name: nginx-frontend'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-frontend
- en: 'spec:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'replicas: 3'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 副本：3
- en: 'selector:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'matchLabels:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配标签：
- en: 'run: nginx-frontend'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'run: nginx-frontend'
- en: 'strategy: {}'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 策略：{}
- en: 'template:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 模板：
- en: 'metadata:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'labels:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：
- en: 'run: nginx-frontend'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'run: nginx-frontend'
- en: 'spec:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'containers:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 容器：
- en: '- image: bitnami/nginx'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '- 镜像：bitnami/nginx'
- en: 'name: nginx-frontend'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-frontend
- en: This deployment will create three NGINX servers and each pod will be labeled
    with **run=nginx-frontend**. We can verify whether the pods are labeled correctly
    by listing the pods using kubectl, adding **the --show-labels** option, **kubectl
    get pods --show-labels.**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署将创建三个NGINX服务器，每个pod将被标记为**run=nginx-frontend**。我们可以通过使用kubectl列出pod并添加**--show-labels**选项来验证pod是否正确标记，**kubectl
    get pods --show-labels.**
- en: 'This will list each pod and any associated labels:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这将列出每个pod和任何相关的标签：
- en: '**nginx-frontend-6c4dbf86d4-72cbc           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**nginx-frontend-6c4dbf86d4-72cbc           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
- en: '**nginx-frontend-6c4dbf86d4-8zlwc           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**nginx-frontend-6c4dbf86d4-8zlwc           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
- en: '**nginx-frontend-6c4dbf86d4-xfz6m           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**nginx-frontend-6c4dbf86d4-xfz6m           1/1     Running            0          19s    pod-template-hash=6c4dbf86d4,run=nginx-frontend**'
- en: As you can see from the preceding output, each pod has a label, **run=nginx-frontend**.
    You will use this label when you create your service for the application, configuring
    the service to use the label to create the endpoints.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出中可以看到，每个pod都有一个标签**run=nginx-frontend**。在为应用程序创建服务时，您将使用此标签，配置服务以使用标签创建端点。
- en: Creating a service
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建服务
- en: Now that you know how a service will use labels to create endpoints, let's discuss
    the service options we have in Kubernetes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道服务将如何使用标签来创建端点，让我们讨论一下Kubernetes中我们拥有的服务选项。
- en: This section will introduce each service type and show you how to create a service
    object. Each type will be detailed in its own section after the general introduction.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍每种服务类型，并向您展示如何创建服务对象。每种类型将在一般介绍之后的各自部分中详细介绍。
- en: 'Kubernetes services can be created using one of four types:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务可以使用四种类型之一创建：
- en: '![Table 6.1: Kubernetes service types'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![表6.1：Kubernetes服务类型'
- en: '](image/Table_1.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Table_1.jpg)'
- en: 'Table 6.1: Kubernetes service types'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1：Kubernetes服务类型
- en: 'To create a service, you need to create a service object that includes the
    **kind**, a **selector**, a **type**, and any **ports** that will be used to connect
    to the service. For our NGINX deployment, we want to expose the service on ports
    80 and 443\. We labeled the deployment with **run=nginx-frontend**, so when we
    create a manifest, we will use that name as our selector:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个服务，您需要创建一个包括**类型**、**选择器**、**类型**和将用于连接到服务的任何**端口**的服务对象。对于我们的NGINX部署，我们希望在端口80和443上公开服务。我们使用**run=nginx-frontend**标记了部署，因此在创建清单时，我们将使用该名称作为我们的选择器：
- en: 'apiVersion: v1'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: api版本：v1
- en: 'kind: Service'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：服务
- en: 'metadata:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'labels:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：
- en: 'run: nginx-frontend'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：nginx-frontend
- en: 'name: nginx-frontend'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-frontend
- en: 'spec:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'selector:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'run: nginx-frontend'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：nginx-frontend
- en: 'ports:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- name: http'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：http'
- en: 'port: 80'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：80
- en: 'protocol: TCP'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：TCP
- en: 'targetPort: 80'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端口：80
- en: '- name: https'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：https'
- en: 'port: 443'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：443
- en: 'protocol: TCP'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：TCP
- en: 'targetPort: 443'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端口：443
- en: 'type: ClusterIP'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：ClusterIP
- en: If a type is not defined in a service manifest, Kubernetes will assign a default
    type of **ClusterIP**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务清单中未定义类型，Kubernetes将分配默认类型**ClusterIP**。
- en: 'Now that a service has been created, we can verify that it was correctly defined
    using a few **kubectl** commands. The first check we will perform is to verify
    that the service object was created. To check our service, we use the **kubectl
    get services** command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经创建了一个服务，我们可以使用一些**kubectl**命令来验证它是否被正确定义。我们将执行的第一个检查是验证服务对象是否已创建。要检查我们的服务，我们使用**kubectl
    get services**命令：
- en: NAME                   TYPE          CLUSTER-IP    EXTERNAL-IP   PORT(S)                  AGE
    nginx-frontend   ClusterIP   10.43.142.96  <none>            80/TCP,443/TCP   3m49s
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 名称                   类型          集群IP      外部IP      端口                          年龄
    nginx-frontend   ClusterIP   10.43.142.96  <none>            80/TCP,443/TCP   3m49s
- en: 'After verifying that the service has been created, we can verify that the endpoints
    were created. Using kubectl, we can verify the endpoints by executing **kubectl
    get ep <service name>**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证服务已创建后，我们可以验证端点是否已创建。使用kubectl，我们可以通过执行**kubectl get ep <service name>**来验证端点：
- en: NAME                  ENDPOINTS                                                                                            AGE
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 名称                  端点                                                                                            年龄
- en: nginx-frontend   10.42.129.9:80,10.42.170.91:80,10.42.183.124:80 + 3 more...   7m49s
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: nginx-frontend   10.42.129.9:80,10.42.170.91:80,10.42.183.124:80 + 3 more...   7m49s
- en: 'We can see that the service shows three endpoints, but it also shows a **+3
    more** in the endpoint list. Since the output is truncated, the output from a
    get is limited and it cannot show all of the endpoints. Since we cannot see the
    entire list, we can get a more detailed list if we describe the endpoints. Using
    kubectl, you can execute the **kubectl describe ep <service name>** command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到服务显示了三个端点，但在端点列表中还显示了**+3 more**。由于输出被截断，get的输出是有限的，无法显示所有端点。由于我们无法看到整个列表，如果我们描述端点，我们可以获得更详细的列表。使用kubectl，您可以执行**kubectl
    describe ep <service name>**命令：
- en: Name:         nginx-frontend
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-frontend
- en: Namespace:    default
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：默认
- en: Labels:       run=nginx-frontend
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '标签: run=nginx-frontend'
- en: 'Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2020-04-06T14:26:08Z'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '注释: endpoints.kubernetes.io/last-change-trigger-time: 2020-04-06T14:26:08Z'
- en: 'Subsets:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '子集:'
- en: Addresses:          10.42.129.9,10.42.170.91,10.42.183.124
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '地址: 10.42.129.9,10.42.170.91,10.42.183.124'
- en: NotReadyAddresses:  <none>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'NotReadyAddresses: <none>'
- en: 'Ports:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '端口:'
- en: Name   Port  Protocol
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 端口 协议
- en: '----         ----    --------'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '---- ---- --------'
- en: http      80      TCP
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: http 80 TCP
- en: https  443   TCP
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: https 443 TCP
- en: Events:  <none>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '事件: <none>'
- en: 'If you compare the output from our **get** and **describe** commands, it may
    appear that there is a mismatch in endpoints. The **get** command showed a total
    of six endpoints: it showed three IP endpoints and because it was truncated, it
    also listed a **+3**, for a total of six endpoints. The output from the **describe**
    command shows only three IP addresses, not six. Why do the two outputs appear
    to show different results?'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您比较我们的**get**和**describe**命令的输出，可能会发现端点不匹配。**get**命令显示了总共六个端点：它显示了三个IP端点，并且因为它被截断了，它还列出了**+3**，总共六个端点。**describe**命令的输出只显示了三个IP地址，而不是六个。为什么这两个输出似乎显示了不同的结果？
- en: The **get** command will list each endpoint and port in the list of addresses.
    Since our service is defined to expose two ports, each address will have two entries,
    one for each exposed port. The address list will always contain every socket for
    the service, which may list the endpoint addresses multiple times, once for each
    socket.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**get**命令将在地址列表中列出每个端点和端口。由于我们的服务被定义为公开两个端口，每个地址将有两个条目，一个用于每个公开的端口。地址列表将始终包含服务的每个套接字，这可能会多次列出端点地址，每个套接字一次。'
- en: The **describe** command handles the output differently, listing the addresses
    on one line with all of the ports listed below the addresses. At first glance,
    it may look like the **describe** command is missing three address, but since
    it breaks the output into multiple sections, it will only list the addresses once.
    All ports are broken out below the address list; in our example, it shows port
    80 and 443.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**describe**命令以不同的方式处理输出，将地址列在一行上，下面列出所有端口。乍一看，**describe**命令可能看起来缺少三个地址，但由于它将输出分成多个部分，它只会列出地址一次。所有端口都在地址列表下面分开列出；在我们的示例中，它显示端口80和443。'
- en: Both commands show the same data, but it's presented in a different format.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个命令显示相同的数据，但以不同的格式呈现。
- en: Now that the service is exposed to the cluster, you could use the assigned service
    IP address to connect to the application. While this would work, the address may
    change if the service object is deleted and recreated. Rather than target an IP
    address, you should use the DNS that was assigned to the service when it was created.
    In the next section, we will explain how to use internal DNS names to resolve
    services.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经暴露给集群，您可以使用分配的服务IP地址来连接应用程序。虽然这样可以工作，但是如果服务对象被删除并重新创建，地址可能会更改。您应该使用在创建服务时分配给服务的DNS，而不是针对IP地址。在下一节中，我们将解释如何使用内部DNS名称来解析服务。
- en: Using DNS to resolve services
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用DNS解析服务
- en: In the world of physical machines and virtual servers, you have probably targeted
    a DNS record to communicate with a server. If the IP address of the server changed,
    then assuming you had dynamic DNS enabled, it would not have any effect on the
    application. This is the advantage of using names rather than IP addresses as
    endpoints.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理机和虚拟服务器的世界中，您可能已经针对DNS记录以与服务器通信。如果服务器的IP地址更改了，那么假设您启用了动态DNS，它对应用程序不会产生任何影响。这就是使用名称而不是IP地址作为端点的优势。
- en: 'When you create a service, an internal DNS record is created that can be queried
    by other workloads in the cluster. If all pods are in the same namespace, then
    we can target the services using a simple, short name like, **mysql-web**; however,
    you may have some services that will be used by multiple namespaces, and when
    workloads need to communicate to a service outside of their own namespace, you
    must target the service using the full name. The following is an example table
    showing how a service may be targeted from namespaces:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个服务时，将创建一个内部DNS记录，其他工作负载可以查询该记录。如果所有pod都在同一个命名空间中，那么我们可以使用简单的短名称如**mysql-web**来定位服务；但是，您可能有一些服务将被多个命名空间使用，当工作负载需要与自己命名空间之外的服务通信时，必须使用完整的名称来定位服务。以下是一个示例表格，显示了如何从命名空间中定位服务：
- en: '![Table 6.2: Internal DNS examples'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![表6.2：内部DNS示例'
- en: '](image/Table_2.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Table_2.jpg)'
- en: 'Table 6.2: Internal DNS examples'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2：内部DNS示例
- en: As you can see from the preceding table, you can target a service that is in
    another namespace by using a standard naming convention, *.<namespace>.svc.<cluster
    name>*. In most cases, when you are accessing a service in a different namespace,
    you do not need to add the cluster name since it should be appended automatically.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的表格中可以看出，您可以使用标准命名约定*.<namespace>.svc.<cluster name>*来定位另一个命名空间中的服务。在大多数情况下，当您访问不同命名空间中的服务时，您不需要添加集群名称，因为它应该会自动添加。
- en: To build on the general services concept, let's get into the details of each
    of the types and how we can use them to access our workloads.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加强对一般服务概念的理解，让我们深入了解每种类型的细节以及如何使用它们来访问我们的工作负载。
- en: Understanding different service types
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解不同的服务类型
- en: When you create a service, you need to specify a service type. The service type
    that is assigned will configure how the service is exposed to either the cluster
    or external traffic.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务时，您需要指定服务类型。分配的服务类型将配置服务如何向集群或外部流量暴露。
- en: The ClusterIP service
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClusterIP服务
- en: The most commonly used, and misunderstood, service type is ClusterIP. If you
    look back at our table, you can see that the description for the ClusterIP type
    states that the service allows connectivity to the service from within the cluster.
    The ClusterIP type does not allow any external traffic to the exposed service.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用且被误解的服务类型是ClusterIP。如果您回顾一下我们的表格，您会看到ClusterIP类型的描述指出该服务允许从集群内部连接到该服务。ClusterIP类型不允许任何外部流量进入暴露的服务。
- en: The idea of exposing a service to only internal cluster workloads can be a confusing
    concept. Why would you expose a service that can only be used by workloads in
    the cluster?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将服务仅暴露给内部集群工作负载的概念可能会让人感到困惑。为什么要暴露一个只能被集群中的工作负载使用的服务呢？
- en: For a minute, let's forget about external traffic entirely. We need to concentrate
    on our current deployment and how each component interacts to create our application.
    Using the NGINX example, we will expand the deployment to include a backend database
    that services the web server.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时忘记外部流量。我们需要集中精力关注当前的部署以及每个组件如何相互交互来创建我们的应用程序。以NGINX示例为例，我们将扩展部署以包括为Web服务器提供服务的后端数据库。
- en: Our application will have two deployments, one for the NGINX servers and one
    for the database server. The NGINX deployment will create five replicas while
    the database server will consist of a single replica. The NGINX servers need to
    connect to the database server to pull data for the web pages.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序将有两个部署，一个用于NGINX服务器，一个用于数据库服务器。 NGINX部署将创建五个副本，而数据库服务器将由单个副本组成。 NGINX服务器需要连接到数据库服务器，以获取网页数据。
- en: 'So far, this is a simple application: we have our deployments created, a service
    for the NGINX servers called the web frontend, and a database service called **mysql-web**.
    To configure the database connection from the web servers, we have decided to
    use a ConfigMap that will target the database service. What do we use in the ConfigMap
    as the destination for the database?'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这是一个简单的应用程序：我们已经创建了部署，为NGINX服务器创建了一个名为web frontend的服务，以及一个名为**mysql-web**的数据库服务。为了从Web服务器配置数据库连接，我们决定使用一个ConfigMap，该ConfigMap将针对数据库服务。我们在ConfigMap中使用什么作为数据库的目的地？
- en: You may be thinking that since we are using a single database server, we could
    simply use the IP address. While this would initially work, any restarts to the
    pod would change the address and the web servers would fail to connect to the
    database. A service should always be used, even if you are only targeting a single
    pod. Since the database deployment is called mysql-web, our ConfigMap should use
    that name as the database server.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会认为，由于我们只使用单个数据库服务器，我们可以简单地使用IP地址。虽然这一开始可能有效，但是对Pod的任何重启都会更改地址，Web服务器将无法连接到数据库。即使只针对单个Pod，也应始终使用服务。由于数据库部署称为mysql-web，我们的ConfigMap应该使用该名称作为数据库服务器。
- en: By using the service name, we will not run into issues when the pod is restarted
    since the service targets the labels rather than an IP address. Our web servers
    will simply query the Kubernetes DNS server for the service name, which will contain
    the endpoints of any pod that has a matching label.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用服务名称，当Pod重新启动时，我们不会遇到问题，因为服务针对的是标签而不是IP地址。我们的Web服务器将简单地查询Kubernetes DNS服务器以获取服务名称，其中将包含具有匹配标签的任何Pod的端点。
- en: The NodePort service
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NodePort服务
- en: A NodePort service will expose your service internally to the cluster, as well
    as externally to the network. At a first glance, this may look like the go-to
    service when you want to expose a service. It exposes your service to everybody,
    but it does this by using something called a NodePort, and using it for external
    service access can become difficult to maintain. It is also very confusing for
    users to use a NodePort or remember when they need to access a service over the
    network.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort服务将在集群内部和网络外部公开您的服务。乍一看，这可能看起来像是要公开服务的首选服务。它会向所有人公开您的服务，但它是通过使用称为NodePort的东西来实现的，对于外部服务访问，这可能变得难以维护。对于用户来说，使用NodePort或记住何时需要通过网络访问服务也非常令人困惑。
- en: 'To create a service that uses the NodePort type, you just need to set the type
    to NodePort in your manifest. We can use the same manifest that we used earlier
    to expose a NGINX deployment from the ClusterIP example, only changing the **type**
    to **NodePort**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建使用NodePort类型的服务，您只需在清单中将类型设置为NodePort。我们可以使用之前用于从ClusterIP示例中公开NGINX部署的相同清单，只需将**类型**更改为**NodePort**：
- en: 'apiVersion: v1'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: api版本：v1
- en: 'kind: Service'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：服务
- en: 'metadata:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'labels:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：
- en: 'run: nginx-frontend'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：nginx-frontend
- en: 'name: nginx-frontend'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-frontend
- en: 'spec:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'selector:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'run: nginx-frontend'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：nginx-frontend
- en: 'ports:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- name: http'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：http'
- en: 'port: 80'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：80
- en: 'protocol: TCP'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：TCP
- en: 'targetPort: 80'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端口：80
- en: '- name: https'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：https'
- en: 'port: 443'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：443
- en: 'protocol: TCP'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：TCP
- en: 'targetPort: 443'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端口：443
- en: 'type: NodePort'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：NodePort
- en: 'We can view the endpoints in the same way that we did for a ClusterIP service,
    using kubectl. Running a **kubectl get services** will show you the newly created
    service:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以与ClusterIP服务相同的方式查看端点，使用kubectl。运行**kubectl get services**将显示新创建的服务：
- en: NAME                    TYPE           CLUSTER-IP         EXTERNAL-IP   PORT(S)                                         AGE
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 类型 CLUSTER-IP 外部IP 端口 年龄
- en: nginx-frontend    NodePort   10.43.164.118   <none>            80:31574/TCP,443:32432/TCP   4s
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: nginx-frontend NodePort 10.43.164.118 <none> 80:31574/TCP,443:32432/TCP 4s
- en: The output shows that the type is NodePort and that we have exposed the service
    IP address and the ports. If you look at the ports, you will notice that unlike
    a ClusterIP service, a NodePort service shows two ports rather than one. The first
    port is the exposed port that the internal cluster services can target and the
    second port number is the randomly generated port that is accessible from outside
    of the cluster.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示类型为NodePort，并且我们已公开了服务IP地址和端口。如果您查看端口，您会注意到，与ClusterIP服务不同，NodePort服务显示两个端口而不是一个。第一个端口是内部集群服务可以定位的公开端口，第二个端口号是从集群外部可访问的随机生成的端口。
- en: 'Since we exposed both ports 80 and 443 for the service, we will have two NodePorts
    assigned. If someone needs to target the service from outside of the cluster,
    they can target any worker node with the supplied port to access the service:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为服务公开了80端口和443端口，我们将分配两个NodePort。如果有人需要从集群外部定位服务，他们可以定位任何带有提供的端口的工作节点来访问服务：
- en: '![Figure 6.1 – NGINX service using NodePort'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 - 使用NodePort的NGINX服务'
- en: '](image/Fig_6.1_B15514.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.1_B15514.jpg)'
- en: Figure 6.1 – NGINX service using NodePort
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 - 使用NodePort的NGINX服务
- en: Each node maintains a list of the NodePorts and their assigned services. Since
    the list is shared with all nodes, you can target any functioning node using the
    port and Kubernetes will route it to a running pod.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点维护NodePorts及其分配的服务列表。由于列表与所有节点共享，您可以使用端口定位任何运行中的节点，Kubernetes将将其路由到运行中的pod。
- en: 'To visualize the traffic flow, we have created a graphic showing the web request
    to our NGINX pod:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化流量流向，我们创建了一个图形，显示了对我们的NGINX pod的web请求：
- en: '![Figure 6.2 – NodePort traffic flow overview'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 - NodePort流量流向概述'
- en: '](image/Fig_6.2_B15514.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.2_B15514.jpg)'
- en: Figure 6.2 – NodePort traffic flow overview
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 - NodePort流量流向概述
- en: 'There are some issues to consider when using a NodePort to expose a service:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用NodePort公开服务时，有一些问题需要考虑：
- en: If you delete and recreate the service, the assigned NodePort will change.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您删除并重新创建服务，则分配的NodePort将更改。
- en: If you target a node that is offline or having issues, your request will fail.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您瞄准的节点处于离线状态或出现问题，您的请求将失败。
- en: Using NodePort for too many services may get confusing. You need to remember
    the port for each service, and remember that there are no *external* names associated
    with the service. This may get confusing for users that are targeting services
    in the cluster.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于太多服务使用NodePort可能会令人困惑。您需要记住每个服务的端口，并记住服务没有与之关联的*外部*名称。这可能会让瞄准集群中的服务的用户感到困惑。
- en: Because of the limitations listed here, you should limit using NodePort services.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这里列出的限制，您应该限制使用NodePort服务。
- en: The LoadBalancer service
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡器服务
- en: Many people starting out in Kubernetes read about services and discover that
    the LoadBalancer type will assign an external IP address to a service. Since an
    external IP address can be addressed directly by any machine on the network, this
    is an attractive option for a service, which is why many people try to use it
    first. Unfortunately, since many users start by using an on-premise Kubernetes
    cluster, they run into headaches trying to create a LoadBalancer service.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 许多刚开始使用Kubernetes的人会阅读有关服务的信息，并发现LoadBalancer类型将为服务分配外部IP地址。由于外部IP地址可以被网络上的任何计算机直接寻址，这对于服务来说是一个有吸引力的选项，这就是为什么许多人首先尝试使用它的原因。不幸的是，由于许多用户首先使用本地Kubernetes集群，他们在尝试创建LoadBalancer服务时遇到了麻烦。
- en: The LoadBalancer service relies on an external component that integrates with
    Kubernetes to create the IP address assigned to the service. Most on-premise Kubernetes
    installations do not include this type of service. When you try to use a LoadBalancer
    service without the support infrastructure, you will find that your service shows
    **<pending>** in the **EXTERNAL-IP** status column.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer服务依赖于与Kubernetes集成的外部组件，以创建分配给服务的IP地址。大多数本地Kubernetes安装不包括这种类型的服务。当您尝试在没有支持基础设施的情况下使用LoadBalancer服务时，您会发现您的服务在**EXTERNAL-IP**状态列中显示**<pending>**。
- en: We will explain the LoadBalancer service and how to implement it later in the
    chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面解释LoadBalancer服务以及如何实现它。
- en: The ExternalName service
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExternalName服务
- en: The ExternalName service is a unique service type with a specific use case.
    When you query a service that uses an ExternalName type, the final endpoint is
    not a pod that is running in the cluster, but an external DNS name.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName服务是一种具有特定用例的独特服务类型。当您查询使用ExternalName类型的服务时，最终端点不是运行在集群中的pod，而是外部DNS名称。
- en: To use an example that you may be familiar with outside of Kubernetes, this
    is similar to using **c-name** to alias a host record. When you query a **c-name**
    record in DNS, it resolves to a host record rather than an IP address.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用您可能在Kubernetes之外熟悉的示例，这类似于使用**c-name**来别名主机记录。当您在DNS中查询**c-name**记录时，它会解析为主机记录，而不是IP地址。
- en: Before using this service type, you need to understand potential issues that
    it may cause for your application. You may run into issues if the target endpoint
    is using SSL certificates. Since the hostname you are querying may not be the
    same as the name on the destination server's certificate, your connection may
    not succeed because of the name mismatch. If you find yourself in this situation,
    you may be able to use a certificate that has **subject alternative names** (**SAN**)
    added to the certificate. Adding alternative names to a certificate allow you
    to associate multiple names with a certificate.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用此服务类型之前，您需要了解它可能对您的应用程序造成的潜在问题。如果目标端点使用SSL证书，您可能会遇到问题。由于您查询的主机名可能与目标服务器证书上的名称不同，您的连接可能无法成功，因为名称不匹配。如果您发现自己处于这种情况，您可能可以使用在证书中添加**主题替代名称**（**SAN**）的证书。向证书添加替代名称允许您将多个名称与证书关联起来。
- en: 'To explain why you may want to use an ExternalName service, let''s use the
    following example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释为什么您可能希望使用ExternalName服务，让我们使用以下示例：
- en: '![](image/Table_3.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](image/Table_3.jpg)'
- en: Based on the requirements, using an ExternalName service is the perfect solution.
    So, how would we accomplish the requirements? (This is a theoretical exercise;
    you do not need to execute anything on your KinD cluster)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 基于要求，使用ExternalName服务是完美的解决方案。那么，我们如何实现这些要求呢？（这是一个理论练习；您不需要在您的KinD集群上执行任何操作）
- en: 'The first step is to create a manifest that will create the ExternalName service
    for the database server:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是创建一个清单，将为数据库服务器创建ExternalName服务：
- en: 'apiVersion: v1'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: api版本：v1
- en: 'kind: Service'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：服务
- en: 'metadata:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: sql-db'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：sql-db
- en: 'namespace: finance'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：财务
- en: 'spec:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'type: ExternalName'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：ExternalName
- en: 'externalName: sqlserver1.foowidgets.com'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'externalName: sqlserver1.foowidgets.com'
- en: With the service created, the next step is to configure the application to use
    the name of our new service. Since the service and the application are in the
    same namespace, you can configure the application to target the name **sql-db**.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了服务之后，下一步是配置应用程序以使用我们新服务的名称。由于服务和应用程序在同一个命名空间中，您可以配置应用程序以定位名称**sql-db**。
- en: Now, when the application queries for **sql-db**, it will resolve to **sqlserver1.foowidgets.com**,
    and, ultimately, the IP address of 192.168.10.200.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，当应用程序查询**sql-db**时，它将解析为**sqlserver1.foowidgets.com**，最终解析为IP地址192.168.10.200。
- en: This accomplishes the initial requirement, connecting the application to the
    external database server using only the Kubernetes DNS server.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这实现了最初的要求，即仅使用Kubernetes DNS服务器将应用程序连接到外部数据库服务器。
- en: You may be wondering why we didn't simply configure the application to use the
    database server name directly. The key is the second requirement, limiting any
    reconfiguration when the SQL server is migrated to a container.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想知道为什么我们不直接配置应用程序使用数据库服务器名称。关键在于第二个要求，即在将SQL服务器迁移到容器时限制任何重新配置。
- en: Since we cannot reconfigure the application after the SQL server is migrated
    to the cluster, we will not be able to change the name of the SQL server in the
    application settings. If we configured the application to use the original name,
    **sqlserver1.foowidgets.com**, the application would not work after the migration.
    By using the ExternalName service, we have the ability to change the internal
    DNS service name by replacing the ExternalHost service name with a standard Kubernetes
    service that points to the SQL server.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在将SQL服务器迁移到集群后无法重新配置应用程序，因此我们将无法更改应用程序设置中SQL服务器的名称。如果我们配置应用程序使用原始名称**sqlserver1.foowidgets.com**，则迁移后应用程序将无法工作。通过使用ExternalName服务，我们可以通过将ExternalHost服务名称替换为指向SQL服务器的标准Kubernetes服务来更改内部DNS服务名称。
- en: 'To accomplish the second goal, go through the following steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现第二个目标，请按照以下步骤进行：
- en: Delete the **ExternalName** service.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除**ExternalName**服务。
- en: 'Create a new service using the name **ext-sql-db** that uses **app=sql-app**
    as the selector. The manifest would look like the one shown here:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用名称**ext-sql-db**创建一个新的服务，该服务使用**app=sql-app**作为选择器。清单看起来像这样：
- en: 'apiVersion: v1'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: api版本：v1
- en: 'kind: Service'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：服务
- en: 'metadata:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'labels:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：
- en: 'app: sql-db'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 'app: sql-db'
- en: 'name: sql-db'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：sql-db
- en: 'namespace: finance'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：财务
- en: 'ports:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- port: 1433'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '- 端口：1433'
- en: 'protocol: TCP'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：TCP
- en: 'targetPort: 1433'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端口：1433
- en: 'name: sql'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：sql
- en: 'selector:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'app: sql-app'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序：sql-app
- en: 'type: ClusterIP'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：ClusterIP
- en: Since we are using the same service name for the new service, no changes need
    to be made to the application. The app will still target the name **sql-db**,
    which will now use the SQL server deployed in the cluster.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为新服务使用相同的服务名称，因此无需对应用程序进行任何更改。该应用程序仍将以**sql-db**为目标名称，现在将使用集群中部署的SQL服务器。
- en: Now that you know about services, we can move on to load balancers, which will
    allow you to expose services externally using standard URL names and ports.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了服务，我们可以继续讨论负载均衡器，这将允许您使用标准URL名称和端口外部公开服务。
- en: Introduction to load balancers
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡器简介
- en: Before discussing different types of load balancers, it's important to understand
    the **Open Systems Interconnection** (**OSI**) model. Understanding the different
    layers of the OSI model will help you to understand how different solutions handle
    incoming requests.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论不同类型的负载均衡器之前，重要的是要了解**开放式系统互联**（**OSI**）模型。了解OSI模型的不同层将帮助您了解不同解决方案如何处理传入请求。
- en: Understanding the OSI model
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解OSI模型
- en: When you hear about different solutions to expose an application in Kubernetes,
    you will often here a reference to layer 7 or layer 4 load balancing. These designations
    refer to where each operates in the OSI model. Each layer offers different functionality;
    a component that runs at layer 7 offers different functionality than a component
    in layer 4.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当您听到有关在Kubernetes中暴露应用程序的不同解决方案时，您经常会听到对第7层或第4层负载均衡的引用。这些指示是指它们在OSI模型中的操作位置。每一层提供不同的功能；在第7层运行的组件提供的功能与第4层的组件不同。
- en: 'To begin, let''s look at a brief overview of the seven layers and a description
    of each. For this chapter, we are interested in the two highlighted sections,
    **layer 4 and layer 7**:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们简要概述一下七层，并对每一层进行描述。在本章中，我们对两个突出显示的部分感兴趣，**第4层和第7层**：
- en: '![](image/Table_4.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](image/Table_4.jpg)'
- en: Table 6.3 OSI model layers
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 OSI模型层
- en: You don't need to be an expert in the OSI layers, but you should understand
    what a layer 4 and layer 7 load balancer provide and how each may be used with
    a cluster.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要成为OSI层的专家，但您应该了解第4层和第7层负载均衡器提供的功能以及如何在集群中使用每一层。
- en: 'Let''s go deeper into the details of layer 4 and 7:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解第4层和第7层的细节：
- en: '**Layer 4**: As the description states in the chart, layer 4 is responsible
    for the communication traffic between devices. Devices that run at layer 4 have
    access to TCP/UPD information. Load balancers that are layer-4 based provide your
    applications with the ability to service incoming requests for any TCP/UDP port.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第4层**：正如图表中的描述所述，第4层负责设备之间的通信流量。在第4层运行的设备可以访问TCP/UPD信息。基于第4层的负载均衡器为您的应用程序提供了为任何TCP/UDP端口服务传入请求的能力。'
- en: '**Layer 7**: Layer 7 is responsible for providing network services to applications.
    When we say application traffic, we are not referring to applications such as
    Excel or Word; instead, we are referring to the protocols that support the applications,
    such as HTTP and HTTPS.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第7层**：第7层负责为应用程序提供网络服务。当我们说应用程序流量时，我们指的不是诸如Excel或Word之类的应用程序；而是指支持应用程序的协议，如HTTP和HTTPS。'
- en: In the next section, we will explain each load balancer type and how to use
    them in a Kubernetes cluster to expose your services.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将解释每种负载均衡器类型以及如何在Kubernetes集群中使用它们来暴露您的服务。
- en: Layer 7 load balancers
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7层负载均衡器
- en: 'Kubernetes provides layer 7 load balancers in the form of an Ingress controller.
    There are a number of solutions to provide Ingress to your clusters, including
    the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes以Ingress控制器的形式提供第7层负载均衡器。有许多解决方案可以为您的集群提供Ingress，包括以下内容：
- en: NGINX
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NGINX
- en: Envoy
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Envoy
- en: Traefik
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Traefik
- en: Haproxy
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haproxy
- en: Typically, a layer 7 load balancer is limited in the functions it can perform.
    In the Kubernetes world, they are implemented as Ingress controllers that can
    route incoming HTTP/HTTPS requests to your exposed services. We will go into detail
    on implementing NGINX as a Kubernetes Ingress controller in the *Creating Ingress
    rules* section.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，第7层负载均衡器在其可执行的功能方面受到限制。在Kubernetes世界中，它们被实现为Ingress控制器，可以将传入的HTTP/HTTPS请求路由到您暴露的服务。我们将在*创建Ingress规则*部分详细介绍如何实现NGINX作为Kubernetes
    Ingress控制器。
- en: Name resolution and layer 7 load balancers
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 名称解析和第7层负载均衡器
- en: To handle layer 7 traffic in a Kubernetes cluster, you deploy an Ingress controller.
    Ingress controllers are dependent on incoming names to route traffic to the correct
    service. In a legacy server deployment model, you would create a DNS entry and
    map it to an IP address.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理Kubernetes集群中的第7层流量，您需要部署一个Ingress控制器。Ingress控制器依赖于传入的名称来将流量路由到正确的服务。在传统的服务器部署模型中，您需要创建一个DNS条目并将其映射到一个IP地址。
- en: Applications that are deployed on a Kubernetes cluster are no different—the
    user will use a DNS name to access the application.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 部署在Kubernetes集群上的应用程序与此无异-用户将使用DNS名称访问应用程序。
- en: Oftentimes, you will create a new wildcard domain that will target the Ingress
    controller via an external load balancer, such as an F5, HAproxy, or SeeSaw.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您将创建一个新的通配符域，将其定位到Ingress控制器，通过外部负载均衡器，如F5、HAproxy或SeeSaw。
- en: 'Let''s assume that our company is called FooWidgets and we have three Kubernetes
    clusters, fronted by an external load balancer with multiple Ingress controller
    endpoints. Our DNS server would have entries for each cluster, using a wildcard
    domain that points to the load balancer''s virtual IP address:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的公司叫FooWidgets，我们有三个Kubernetes集群，由多个Ingress控制器端点作为前端的外部负载均衡器。我们的DNS服务器将为每个集群添加条目，使用通配符域指向负载均衡器的虚拟IP地址：
- en: '![](image/Table_5.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](image/Table_5.jpg)'
- en: Table 6.4 Example wildcard domain names for Ingress
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4 Ingress的通配符域名示例
- en: 'The following diagram shows the entire flow of the request:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了请求的整个流程：
- en: '![Figure 6.3 – Multiple-name Ingress traffic flow'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 - 多名称Ingress流量'
- en: '](image/Fig_6.3_B15514.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.3_B15514.jpg)'
- en: Figure 6.3 – Multiple-name Ingress traffic flow
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 - 多名称Ingress流量
- en: 'Each of the steps in diagram 6.3 are detailed here:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3中的每个步骤在这里详细说明：
- en: Using a browser, the user requests the URL [https://timesheets.cluster1.foowidgets.com](https://timesheets.cluster1.foowidgets.com).
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用浏览器，用户请求URL [https://timesheets.cluster1.foowidgets.com](https://timesheets.cluster1.foowidgets.com)。
- en: The DNS query is sent to a DNS server. The DNS server looks up the zone details
    for **cluster1.foowidgets.com**. There is a single entry in the DNS zone that
    resolves to the VIP assigned on the load balancer for the domain.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DNS查询被发送到DNS服务器。DNS服务器查找**cluster1.foowidgets.com**的区域详细信息。DNS区域中有一个单一条目解析为该域的负载均衡器分配的VIP。
- en: The load balancer's VIP for **cluster1.foowidgets.com** has three backend servers
    assigned, pointing to three worker nodes where we have deployed Ingress controllers.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**cluster1.foowidgets.com**的负载均衡器VIP分配了三个后端服务器，指向我们部署了Ingress控制器的三个工作节点。'
- en: Using one of the endpoints, the request is sent to the Ingress controller.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用其中一个端点，请求被发送到Ingress控制器。
- en: The Ingress controller will compare the requested URL to a list of Ingress rules.
    When a matching request is found, the Ingress controller will forward the request
    to the service that was assigned to the Ingress rule.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ingress控制器将请求的URL与Ingress规则列表进行比较。当找到匹配的请求时，Ingress控制器将请求转发到分配给Ingress规则的服务。
- en: To help reinforce how Ingress works, it will help to create Ingress rules on
    a cluster to see them in action. Right now, the key takeaways are that Ingress
    uses the requested URL to direct traffic to the correct Kubernetes services.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助加强Ingress的工作原理，创建Ingress规则并在集群上查看它们的运行情况将会有所帮助。现在，关键要点是Ingress使用请求的URL将流量定向到正确的Kubernetes服务。
- en: Using nip.io for name resolution
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用nip.io进行名称解析
- en: Most personal development clusters, such as our KinD installation, may not have
    enough access to add records to a DNS server. To test Ingress rules, we need to
    target unique host names that are mapped to Kubernetes services by the Ingress
    controller. Without a DNS server, you need to create a local host file with multiple
    names pointing to the IP address of the Ingress controller.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数个人开发集群，例如我们的KinD安装，可能没有足够的访问权限来向DNS服务器添加记录。为了测试Ingress规则，我们需要将唯一主机名定位到由Ingress控制器映射到Kubernetes服务的IP地址的本地主机文件中，而不需要DNS服务器。
- en: 'For example, if you deployed four web servers, you need to add all four names
    to your local hosts. An example of this is shown here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您部署了四个Web服务器，您需要将所有四个名称添加到您的本地主机。这里显示了一个示例：
- en: '**192.168.100.100 webserver1.test.local**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**192.168.100.100 webserver1.test.local**'
- en: '**192.168.100.100 webserver2.test.local**'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**192.168.100.100 webserver2.test.local**'
- en: '**192.168.100.100 webserver3.test.local**'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**192.168.100.100 webserver3.test.local**'
- en: '**192.168.100.100 webserver4.test.local**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**192.168.100.100 webserver4.test.local**'
- en: 'This can also be represented on a single line rather than multiple lines:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以表示为单行而不是多行：
- en: '**192.168.100.100 webserver1.test.local webserver2.test.local webserver3.test.local
    webserver4.test.local**'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**192.168.100.100 webserver1.test.local webserver2.test.local webserver3.test.local
    webserver4.test.local**'
- en: If you use multiple machines to test your deployments, you will need to edit
    the host file on every machine that you plan to use for testing. Maintaining multiple
    files on multiple machines is an administrative nightmare and will lead to issues
    that will make testing a challenge.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用多台机器来测试您的部署，您将需要编辑每台机器上的host文件。在多台机器上维护多个文件是一场管理噩梦，并将导致问题，使测试变得具有挑战性。
- en: Luckily, there are free services available that provide DNS services that we
    can use without configuring a complex DNS infrastructure for our KinD cluster.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有免费的服务可用，提供了DNS服务，我们可以在KinD集群中使用，而无需配置复杂的DNS基础设施。
- en: Nip.io is the service that we will use for our KinD cluster name resolution
    requirements. Using our previous web server example, we will not need to create
    any DNS records. We still need to send the traffic for the different servers to
    the NGINX server running on 192.168.100.100 so that Ingress can route the traffic
    to the appropriate service. Nip.io uses a naming format that includes the IP address
    in the hostname to resolve the name to an IP. For example, say that we have four
    web servers that we want to test called webserver1, webserver2, webserver3, and
    webserver4, with Ingress rules on an Ingress controller running on 192.168.100.100.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Nip.io是我们将用于KinD集群名称解析需求的服务。使用我们之前的Web服务器示例，我们将不需要创建任何DNS记录。我们仍然需要将不同服务器的流量发送到运行在192.168.100.100上的NGINX服务器，以便Ingress可以将流量路由到适当的服务。Nip.io使用包含IP地址在主机名中的命名格式来将名称解析为IP。例如，假设我们有四台我们想要测试的Web服务器，分别为webserver1、webserver2、webserver3和webserver4，Ingress控制器运行在192.168.100.100上。
- en: 'As we mentioned earlier, we do not need to create any records to accomplish
    this. Instead, we can use the naming convention to have nip.io resolve the name
    for us. Each of the web servers would use a name with the following naming standard:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们不需要创建任何记录来完成这个任务。相反，我们可以使用命名约定让nip.io为我们解析名称。每台Web服务器都将使用以下命名标准的名称：
- en: '**<desired name>.<INGRESS IP>.nip.io**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**<desired name>.<INGRESS IP>.nip.io**'
- en: 'The names for all four web servers are listed in the following table:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所有四台Web服务器的名称列在下表中：
- en: '![](image/Table_6.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](image/Table_6.jpg)'
- en: Table 6.5 – Nip.io example domain names
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5–Nip.io示例域名
- en: 'When you use any of the preceding names, nip.io will resolve them to 192.168.100.100\.
    You can see an example ping for each name in the following screenshot:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用任何上述名称时，nip.io将把它们解析为192.168.100.100。您可以在以下截图中看到每个名称的ping示例：
- en: '![Figure 6.4 – Example name resolution using nip.io'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4–使用nip.io进行名称解析的示例'
- en: '](image/Fig_6.4_B15514.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.4_B15514.jpg)'
- en: Figure 6.4 – Example name resolution using nip.io
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4–使用nip.io进行名称解析的示例
- en: This may look like it has very little benefit, since you are supplying the IP
    address in the name. Why would you need to bother using nip.io if you know the
    IP address?
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来好像没有什么好处，因为您在名称中提供了IP地址。如果您知道IP地址，为什么还需要使用nip.io呢？
- en: Remember that the Ingress rules require a unique name to route traffic to the
    correct service. While the name may not be required for you to know the IP address
    of the server, the name is required for the Ingress rules. Each name is unique,
    using the first part of the full name—in our example, that is **webserver1**,
    **webserver2**, **webserver3**, and **webserver4**.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Ingress规则需要一个唯一的名称来将流量路由到正确的服务。虽然对于您来说可能不需要知道服务器的IP地址，但是对于Ingress规则来说，名称是必需的。每个名称都是唯一的，使用完整名称的第一部分——在我们的示例中，即**webserver1**、**webserver2**、**webserver3**和**webserver4**。
- en: By providing this service, nip.io allows you to use any name for Ingress rules
    without the need to have a DNS server in your development cluster.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供这项服务，nip.io允许您在开发集群中使用任何名称的Ingress规则，而无需拥有DNS服务器。
- en: Now that you know how to use nip.io to resolve names for your cluster, let's
    explain how to use a nip.io name in an Ingress rule.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道如何使用nip.io来解析集群的名称，让我们解释如何在Ingress规则中使用nip.io名称。
- en: Creating an Ingress rules
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Ingress规则
- en: 'Remember, Ingress rules use names to route the incoming request to the correct
    service. The following is a graphical representation of an incoming request, showing
    how Ingress routes the traffic:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Ingress规则使用名称来将传入的请求路由到正确的服务。以下是传入请求的图形表示，显示了Ingress如何路由流量：
- en: '![Figure 6.5 – Ingress traffic flow'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 – Ingress流量流向'
- en: '](image/Fig_6.5_B15514.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.5_B15514.jpg)'
- en: Figure 6.5 – Ingress traffic flow
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – Ingress流量流向
- en: 'Figure 6.5 shows a high-level overview of how Kubernetes handles incoming Ingress
    requests. To help explain each step in more depth, let''s go over the five steps
    in greater detail. Using the graphic provided in Figure 6.5, we will explain each
    numbered step in detail to show how Igress processes the request:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5显示了Kubernetes如何处理传入的Ingress请求的高级概述。为了更深入地解释每个步骤，让我们更详细地介绍一下五个步骤。使用图6.5中提供的图形，我们将详细解释每个编号步骤，以展示Ingress如何处理请求：
- en: The user requests a URL in their browser named webserver1.192.168.200.20.nio.io.
    A DNS request is sent to the local DNS server, which is ultimately sent to the
    nip.io DNS server.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户在其浏览器中请求名为webserver1.192.168.200.20.nio.io的URL。DNS请求被发送到本地DNS服务器，最终发送到nip.io
    DNS服务器。
- en: The nip.io server resolves the domain name to the IP address of 192.168.200.20,
    which is returned to the client.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: nip.io服务器将域名解析为IP地址192.168.200.20，并返回给客户端。
- en: The client sends the request to the Ingress controller, which is running on
    192.168.200.20\. The request contains the complete URL name, **webserver1.192.168.200.20.nio.io**.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端将请求发送到运行在192.168.200.20上的Ingress控制器。请求包含完整的URL名称**webserver1.192.168.200.20.nio.io**。
- en: The Ingress controller looks up the requested URL name in the configured rules
    and matches the URL name to a service.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ingress控制器在配置的规则中查找请求的URL名称，并将URL名称与服务匹配。
- en: The service endpoint(s) will be used to route traffic to the assigned pods.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务端点将用于将流量路由到分配的pod。
- en: The request is routed to an endpoint pod running the web server.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求被路由到运行web服务器的端点pod。
- en: 'Using the preceding example traffic flow, let''s go over the Kubernetes objects
    that need to be created:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的示例流量流向，让我们来看看需要创建的Kubernetes对象：
- en: 'First, we need a simple webserver running in a namespace. We will simply deploy
    a base NGINX webserver in the default namespace. Rather than create a manifest
    manually, we can create a deployment quickly using the following **kubectl run**
    command:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要在一个命名空间中运行一个简单的web服务器。我们将在默认命名空间中简单部署一个基本的NGINX web服务器。我们可以使用以下**kubectl
    run**命令快速创建一个部署，而不是手动创建清单：
- en: '**kubectl run nginx-web --image bitnami/nginx**'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl run nginx-web --image bitnami/nginx**'
- en: Using the **run** option is a shortcut that will create a deployment called
    **nginx-web** in the default namespace. You may notice that the output will give
    you a warning that the run is being deprecated. This is just a warning; it will
    still create our deployment, although using **run** to create a deployment may
    not work in future Kubernetes versions.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**run**选项是一个快捷方式，它将在默认命名空间中创建一个名为**nginx-web**的部署。您可能会注意到输出会给出一个警告，即run正在被弃用。这只是一个警告；它仍然会创建我们的部署，尽管在将来的Kubernetes版本中使用**run**创建部署可能不起作用。
- en: 'Next, we need to create a service for the deployment. Again, we will create
    a service using a kubectl command, **kubectl expose**. The Bitnami NGINX image
    runs on port 8080, so we will use the same port to expose the service:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为部署创建一个服务。同样，我们将使用kubectl命令**kubectl expose**创建一个服务。Bitnami NGINX镜像在端口8080上运行，因此我们将使用相同的端口来暴露服务：
- en: '**kubectl expose deployment nginx-web --port 8080 --target-port 8080**'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl expose deployment nginx-web --port 8080 --target-port 8080**'
- en: This will create a new service called nginx-web for our deployment, called nginx-web.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们的部署创建一个名为nginx-web的新服务，名为nginx-web。
- en: Now that we have our deployment and service created, the last step is to create
    the Ingress rule. To create an Ingress rule, you create a manifest using the object
    type **Ingress**. The following is an example Ingress rule that assumes that the
    Ingress controller is running on 192.168.200.20\. If you are creating this rule
    on your host, you should use the **IP address of your Docker host**.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了部署和服务，最后一步是创建Ingress规则。要创建Ingress规则，您需要使用对象类型**Ingress**创建一个清单。以下是一个假设Ingress控制器正在运行在192.168.200.20上的示例Ingress规则。如果您在您的主机上创建此规则，您应该使用**您的Docker主机的IP地址**。
- en: 'Create a file called **nginx-ingress.yaml** with the following content:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为**nginx-ingress.yaml**的文件，其中包含以下内容：
- en: 'apiVersion: networking.k8s.io/v1beta1'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: networking.k8s.io/v1beta1'
- en: 'kind: Ingress'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: Ingress'
- en: 'metadata:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 'metadata:'
- en: 'name: nginx-web-ingress'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 'name: nginx-web-ingress'
- en: 'spec:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 'spec:'
- en: 'rules:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 规则：
- en: '- host: webserver1.192.168.200.20.nip.io'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '- host: webserver1.192.168.200.20.nip.io'
- en: 'http:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 'http:'
- en: 'paths:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'paths:'
- en: '- path: /'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '- path: /'
- en: 'backend:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 后端：
- en: 'serviceName: nginx-web'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 'serviceName: nginx-web'
- en: 'servicePort: 8080'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 'servicePort: 8080'
- en: 'Create the Ingress rune using **kubectl apply**:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**kubectl apply**创建Ingress规则：
- en: '**kubectl apply -f nginx-ingress.yaml**'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f nginx-ingress.yaml**'
- en: You can test the deployment from any client on your internal network by browsing
    to the Ingress URL, **http:// webserver1.192.168.200.20.nip.io**.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过浏览到Ingress URL **http:// webserver1.192.168.200.20.nip.io** 来从内部网络上的任何客户端测试部署。
- en: 'If everything was created successfully, you should see the NGINX welcome page:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一切创建成功，您应该看到NGINX欢迎页面：
- en: '![Figure 6.6 – NGINX web server using nip.io for Ingress'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 - 使用nip.io创建Ingress的NGINX web服务器'
- en: '](image/Fig_6.6_B15514.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.6_B15514.jpg)'
- en: Figure 6.6 – NGINX web server using nip.io for Ingress
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 - 使用nip.io创建Ingress的NGINX web服务器
- en: Using the information in this section, you can create Ingress rules for multiple
    containers using different hostnames. Of course, you aren't limited to using a
    service like nip.io to resolve names; you can use any name resolution method that
    you have available in your environment. In a production cluster, you will have
    an enterprise DNS infrastructure, but in a lab environment, such as our KinD cluster,
    nip.io is the perfect tool to test scenarios that require proper naming conventions.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本节中的信息，您可以使用不同的主机名为多个容器创建Ingress规则。当然，您并不局限于使用像nip.io这样的服务来解析名称；您可以使用您在环境中可用的任何名称解析方法。在生产集群中，您将拥有企业DNS基础设施，但在实验环境中，比如我们的KinD集群，nip.io是测试需要适当命名约定的场景的完美工具。
- en: We will use nip.io naming standards throughout the book, so it's important to
    understand the naming convention before moving on to the next chapter.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整本书中使用nip.io命名标准，因此在继续下一章之前了解命名约定非常重要。
- en: Layer 7 load balancers, such as NGINX Ingress, are used by many standard workloads,
    such as like web servers. There will be deployments that will require a more complex
    load balancer, one that runs at a lower layer of the OIS model. As we move down
    the model, we gain lower-level features. In our next section, we will discuss
    layer 4 load balancers.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 许多标准工作负载使用第7层负载均衡器，例如NGINX Ingress，比如Web服务器。将有一些部署需要更复杂的负载均衡器，这种负载均衡器在OIS模型的较低层运行。随着我们向模型下移，我们获得了更低级别的功能。在下一节中，我们将讨论第4层负载均衡器。
- en: Note
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you deployed the NGINX example on your cluster, you should delete the service
    and the Ingress rules:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在集群上部署了NGINX示例，应删除服务和Ingress规则：
- en: '• To delete the Ingress rule, execute the following: **kubectl delete ingress
    nginx-web-ingress**'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: • 要删除Ingress规则，请执行以下操作：**kubectl delete ingress nginx-web-ingress**
- en: '• To delete the service, execute the following: **kubectl delete service nginx-web**'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: • 要删除服务，请执行以下操作：**kubectl delete service nginx-web**
- en: You can leave the NGINX deployment running for the next section.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以让NGINX部署在下一节继续运行。
- en: Layer 4 load balancers
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4层负载均衡器
- en: Layer 4 of the OSI model is responsible for protocols such as TCP and UDP. A
    load balancer that is running in layer 4 accepts incoming traffic based on the
    only IP address and port. The incoming request is accepted by the load balancer,
    and based on a set of rules, the traffic is sent to the destination IP address
    and port.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: OSI模型的第4层负责TCP和UDP等协议。在第4层运行的负载均衡器根据唯一的IP地址和端口接受传入的流量。负载均衡器接受传入请求，并根据一组规则将流量发送到目标IP地址和端口。
- en: There are lower-level networking operations in the process that are out of the
    scope of this book. HAproxy has a good summary of the terminology and example
    configurations on their website at [https://www.haproxy.com/fr/blog/loadbalancing-faq/](https://www.haproxy.com/fr/blog/loadbalancing-faq/).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中有一些较低级别的网络操作超出了本书的范围。HAproxy在他们的网站上有术语和示例配置的很好总结，网址为[https://www.haproxy.com/fr/blog/loadbalancing-faq/](https://www.haproxy.com/fr/blog/loadbalancing-faq/)。
- en: Layer 4 load balancer options
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4层负载均衡器选项
- en: 'There are multiple options available to you if you want to configure a layer
    4 load balancer for a Kubernetes cluster. Some of the options include the following:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想为Kubernetes集群配置第4层负载均衡器，可以选择多种选项。其中一些选项包括以下内容：
- en: HAproxy
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAproxy
- en: NGINX Pro
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NGINX Pro
- en: SeeSaw
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秋千
- en: F5 Networks
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F5网络
- en: MetalLB
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetalLB
- en: And more…
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等...
- en: Each option provides layer 4 load balancing, but for the purpose of this book,
    we felt that MetalLB was the best choice.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 每个选项都提供第4层负载均衡，但出于本书的目的，我们认为MetalLB是最好的选择。
- en: Using MetalLB as a layer 4 load balancer
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用MetalLB作为第4层负载均衡器
- en: Important note
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remember that in [*Chapter 4*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083)
    *Deploying Kubernetes using KinD* we had a diagram showing the flow of traffic
    between a workstation and the KinD nodes. Because KinD was running in a nested
    Docker container, a layer 4 load balancer would have had certain limitations when
    it came to networking connectivity. Without additional network configuration on
    the Docker host, you will not be able to target the services that use the LoadBalancer
    type outside of the Docker host itself.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在[*第4章*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083) *使用KinD部署Kubernetes*中，我们有一个图表显示了工作站和KinD节点之间的流量流向。因为KinD在嵌套的Docker容器中运行，所以在涉及网络连接时，第4层负载均衡器会有一定的限制。如果没有在Docker主机上进行额外的网络配置，您将无法将LoadBalancer类型的服务定位到Docker主机之外。
- en: If you deploy MetalLB to a standard Kubernetes cluster running on a host, you
    will not be limited to accessing services outside of the host itself.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将MetalLB部署到运行在主机上的标准Kubernetes集群中，您将不受限于访问主机外的服务。
- en: MetalLB is a free, easy to configure layer 4 load balancer. It includes powerful
    configuration options that give it the ability to run in a development lab or
    an enterprise cluster. Since it is so versatile, it has become a very popular
    choice for clusters requiring layer 4 load balancing.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB是一个免费、易于配置的第4层负载均衡器。它包括强大的配置选项，使其能够在开发实验室或企业集群中运行。由于它如此多才多艺，它已成为需要第4层负载均衡的集群的非常受欢迎的选择。
- en: In this section, we will focus on installing MetalLB in layer 2 mode. This is
    an easy installation and works for development or small Kubernetes clusters. MetalLB
    also offers the option to deploy using BGP mode, which allows you to establish
    peering partners to exchange networking routes. If you would like to read about
    MetalLB's BGP mode, you can read about it on MetalLB's site at  [https://metallb.universe.tf/concepts/bgp/](https://metallb.universe.tf/concepts/bgp/).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于在第2层模式下安装MetalLB。这是一个简单的安装，适用于开发或小型Kubernetes集群。MetalLB还提供了使用BGP模式部署的选项，该选项允许您建立对等伙伴以交换网络路由。如果您想阅读MetalLB的BGP模式，请访问MetalLB网站
    [https://metallb.universe.tf/concepts/bgp/](https://metallb.universe.tf/concepts/bgp/)。
- en: Installing MetalLB
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装MetalLB
- en: 'To deploy MetalLB on your KinD cluster, use the manifests from MetalLB''s GitHub
    repository. To install MetalLB, go through the following steps:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的KinD集群上部署MetalLB，请使用MetalLB的GitHub存储库中的清单。要安装MetalLB，请按照以下步骤进行：
- en: 'The following will create a new namespace called **metallb-system** with a
    label of **app: metallb**:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '以下将创建一个名为**metallb-system**的新命名空间，并带有**app: metallb**的标签：'
- en: '**kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml**'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml**'
- en: 'This will deploy MetalLB to your cluster. It will create all required Kubernetes
    objects, including **PodSecurityPolicies**, **ClusterRoles**, **Bindings**, **DaemonSet**,
    and a **deployment**:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将在您的集群中部署MetalLB。它将创建所有必需的Kubernetes对象，包括**PodSecurityPolicies**，**ClusterRoles**，**Bindings**，**DaemonSet**和一个**deployment**：
- en: '**kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml**'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml**'
- en: 'The last command will create a secret in the **metalb-system** namespace that
    has a randomly generated value. This secret is used by MetalLB to encrypt communications
    between speakers:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一个命令将在**metalb-system**命名空间中创建一个具有随机生成值的秘密。MetalLB使用此秘密来加密发言者之间的通信：
- en: '**kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl
    rand -base64 128)"**'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl
    rand -base64 128)"**'
- en: Now that MetalLB has been deployed to the cluster, you need to supply a configuration
    file to complete the setup.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在MetalLB已部署到集群中，您需要提供一个配置文件来完成设置。
- en: Understanding MetalLB's configuration file
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解MetalLB的配置文件
- en: 'MetalLB is configured using a ConfigMap that contains the configuration. Since
    we will be using MetalLB in layer 2 mode, the required configuration file is fairly
    simple and only requires one piece of information: the IP range that you want
    to create for services.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB使用包含配置的ConfigMap进行配置。由于我们将在第2层模式下使用MetalLB，所需的配置文件相当简单，只需要一个信息：您想为服务创建的IP范围。
- en: To keep the configuration simple, we will use a small range from the Docker
    subnet in which KinD is running. If you were running MetalLB on a standard Kubernetes
    cluster, you could assign any range that is routable in your network, but we are
    limited with our KinD clusters.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持配置简单，我们将在KinD正在运行的Docker子网中使用一个小范围。如果您在标准Kubernetes集群上运行MetalLB，您可以分配任何在您的网络中可路由的范围，但我们在KinD集群中受到限制。
- en: 'To get the subnet that Docker is using, we can inspect the default bridge network
    that we are using:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取Docker正在使用的子网，我们可以检查我们正在使用的默认桥接网络：
- en: '**docker network inspect bridge**'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**docker网络检查桥接**'
- en: 'In the output, you will see the assigned subnet, similar to following:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，您将看到分配的子网，类似于以下内容：
- en: '**"Subnet": "172.17.0.0/16"**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**"子网"："172.17.0.0/16"**'
- en: This is an entire class-B address range. We know that we will not use all of
    the IP addresses for running containers, so we will use a small range from the
    subnet in our MetalLB configuration.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个完整的B类地址范围。我们知道我们不会使用所有IP地址来运行容器，因此我们将在MetalLB配置中使用子网中的一个小范围。
- en: 'Let''s create a new file called **metallb-config.yaml** and add the following
    to the file:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为**metallb-config.yaml**的新文件，并将以下内容添加到文件中：
- en: 'apiVersion: v1'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：v1
- en: 'kind: ConfigMap'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：ConfigMap
- en: 'metadata:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'namespace: metallb-system'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：metallb-system
- en: 'name: config'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：config
- en: 'data:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：
- en: 'config: |'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 配置：|
- en: 'address-pools:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 地址池：
- en: '- name: default'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：默认'
- en: 'protocol: layer2'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：layer2
- en: 'addresses:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 地址：
- en: '- 172.17.200.100-172.17.200.125'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '- 172.17.200.100-172.17.200.125'
- en: The manifest will create a ConfigMap in the **metallb-system** namespace called
    **config**. The configuration file will set MetalLB's mode to layer 2 with an
    IP pool called **default**, using the range of 172.16.200-100 through 172.16.200.125
    for LoadBalancer services.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 该清单将在**metallb-system**命名空间中创建一个名为**config**的ConfigMap。配置文件将设置MetalLB的模式为第2层，使用名为**default**的IP池，为负载均衡器服务使用172.16.200-100到172.16.200.125的范围。
- en: You can assign different addresses based on the configuration names. We will
    show this when we explain how to create a LoadBalancer service.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据配置名称分配不同的地址。我们将在解释如何创建负载均衡器服务时展示这一点。
- en: 'Finally, deploy the manifest using kubectl:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用kubectl部署清单：
- en: '**kubectl apply -f metallb-config.yaml**'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f metallb-config.yaml**'
- en: To understand how MetalLB works, you need to know the installed components and
    how they interact to assign IP addresses to services.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解MetalLB的工作原理，您需要了解安装的组件以及它们如何相互作用来为服务分配IP地址。
- en: MetalLB components
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MetalLB组件
- en: 'The second manifest in our deployment is what installs the MetalLB components
    to the cluster. It deploys a DaemonSet that includes the speaker image and a DaemonSet
    that includes the controller image. These components communicate with each other
    to maintain a list of services and assigned IP addresses:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 部署中的第二个清单是安装MetalLB组件到集群的清单。它部署了一个包含speaker镜像的DaemonSet和一个包含controller镜像的DaemonSet。这些组件相互通信，以维护服务列表和分配的IP地址：
- en: The speaker
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 发言者
- en: The speaker component is what MetaLB uses to announce the LoadBalancer services
    on the node. It is deployed as a DaemonSet since the deployments can be on any
    worker node, and therefore, each worker node needs to announce the workloads that
    are running. As services are created using a LoadBalancer type, the speaker will
    announce the service.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 发言者组件是MetaLB用来在节点上宣布负载均衡器服务的组件。它部署为DaemonSet，因为部署可以在任何工作节点上，因此每个工作节点都需要宣布正在运行的工作负载。当使用负载均衡器类型创建服务时，发言者将宣布该服务。
- en: 'If we look at the speaker log from a node, we can see the announcements following:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从节点查看发言者日志，我们可以看到以下公告：
- en: '**{"caller":"main.go:176","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437231123Z"}**'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"main.go:176","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437231123Z"}**'
- en: '**{"caller":"main.go:189","event":"endUpdate","msg":"end of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437516541Z"}**'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"main.go:189","event":"endUpdate","msg":"end of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437516541Z"}**'
- en: '**{"caller":"main.go:176","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464140524Z"}**'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"main.go:176","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464140524Z"}**'
- en: '**{"caller":"main.go:246","event":"serviceAnnounced","ip":"10.2.1.72","msg":"service
    has IP, announcing","pool":"default","protocol":"layer2","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464311087Z"}**'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"main.go:246","event":"serviceAnnounced","ip":"10.2.1.72","msg":"service
    has IP, announcing","pool":"default","protocol":"layer2","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464311087Z"}**'
- en: '**{"caller":"main.go:249","event":"endUpdate","msg":"end of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464470317Z"}**'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"main.go:249","event":"endUpdate","msg":"end of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.464470317Z"}**'
- en: The preceding announcement is for Grafana. After the announcement, you can see
    that it has been assigned an IP address of 10.2.1.72.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的公告是为Grafana。在公告之后，您可以看到它被分配了IP地址10.2.1.72。
- en: The controller
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 控制器
- en: 'The controller will receive the announcements from the speaker on each worker
    node. Using the same service announcement shown previously, the controller log
    shows the announcement and the IP address that the controller assigned to the
    service:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器将从每个工作节点的扬声器接收公告。使用先前显示的相同服务公告，控制器日志显示了公告和控制器为服务分配的IP地址：
- en: '**{"caller":"main.go:49","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437701161Z"}**'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"main.go:49","event":"startUpdate","msg":"start of service update","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.437701161Z"}**'
- en: '**{"caller":"service.go:98","event":"ipAllocated","ip":"10.2.1.72","msg":"IP
    address assigned by controller","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.438079774Z"}**'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"service.go:98","event":"ipAllocated","ip":"10.2.1.72","msg":"IP
    address assigned by controller","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.438079774Z"}**'
- en: '**{"caller":"main.go:96","event":"serviceUpdated","msg":"updated service object","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.467998702Z"}**'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"main.go:96","event":"serviceUpdated","msg":"updated service object","service":"my-grafana-operator/grafana-operator-metrics","ts":"2020-04-21T21:10:07.467998702Z"}**'
- en: In the second line of the log, you can see that the controller assigned the
    IP address of 10.2.1.72.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志的第二行中，您可以看到控制器分配了IP地址10.2.1.72。
- en: Creating a LoadBalancer service
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个LoadBalancer服务
- en: Now that you have installed MetalLB and understand how the components create
    the services, let's create our first LoadBalancer service on our KinD cluster.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经安装了MetalLB并了解了组件如何创建服务，让我们在我们的KinD集群上创建我们的第一个LoadBalancer服务。
- en: 'In the layer 7 load balancer section, we created a deployment running NGINX
    that we exposed by creating a service and an Ingress rule. At the end of the section,
    we deleted the service and the Ingress rule, but we kept the NGINX deployment
    for this section. If you followed the steps in the Ingress section and have not
    deleted the service and Ingress rule, please do so before creating the LoadBalancer
    service. If you did not create the deployment at all, you will need an NGINX deployment
    for this section:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7层负载均衡器部分，我们创建了一个运行NGINX的部署，并通过创建服务和Ingress规则来公开它。在本节的末尾，我们删除了服务和Ingress规则，但保留了NGINX部署。如果您按照Ingress部分的步骤并且尚未删除服务和Ingress规则，请在创建LoadBalancer服务之前这样做。如果您根本没有创建部署，则需要一个NGINX部署来完成本节：
- en: 'You can create a quick NGINX deployment by executing the following command:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过执行以下命令快速创建一个NGINX部署：
- en: '**kubectl run nginx-web --image bitnami/nginx**'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl run nginx-web --image bitnami/nginx
- en: To create a new service that will use the LoadBalancer type, you can create
    a new manifest or you can expose the deployment using only kubectl.
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建一个将使用LoadBalancer类型的新服务，您可以创建一个新的清单，或者只使用kubectl公开部署。
- en: 'To create a manifest, create a new file called **nginx-lb.yaml** and add the
    following:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个清单，请创建一个名为**nginx-lb.yaml**的新文件，并添加以下内容：
- en: 'apiVersion: v1'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: v1'
- en: 'kind: Service'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: Service'
- en: 'metadata:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 'metadata:'
- en: 'name: nginx-lb'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-lb
- en: 'spec:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 'spec:'
- en: 'ports:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- port: 8080'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '- 端口：8080'
- en: 'targetPort: 8080'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 'targetPort: 8080'
- en: 'selector:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 'selector:'
- en: 'run: nginx-web'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 'run: nginx-web'
- en: 'type: LoadBalancer'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 'type: LoadBalancer'
- en: 'Apply the file to the cluster using kubectl:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用kubectl将文件应用到集群：
- en: '**kubectl apply -f nginx-lb.yaml**'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f nginx-lb.yaml**'
- en: To verify that the service was created correctly, list the services using **kubectl
    get services**:![Figure 6.7 – Kubectl service output
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要验证服务是否正确创建，请使用**kubectl get services**列出服务：![图6.7 - Kubectl服务输出
- en: '](image/Fig_6.7_B15514.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.7_B15514.jpg)'
- en: Figure 6.7 – Kubectl service output
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 - Kubectl服务输出
- en: You will see that a new service was created using the LoadBalancer type and
    that MetalLB assigned an IP address from the configured pool we created earlier.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到使用LoadBalancer类型创建了一个新服务，并且MetalLB从我们之前创建的配置池中分配了一个IP地址。
- en: 'A quick look at the controller log will verify that the MetalLB controller
    assigned the service the IP address:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 快速查看控制器日志将验证MetalLB控制器分配了IP地址给服务：
- en: '**{"caller":"service.go:114","event":"ipAllocated","ip":"172.16.200.100","msg":"IP
    address assigned by controller","service":"default/nginx-lb","ts":"2020-04-25T23:54:03.668948668Z"}**'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"caller":"service.go:114","event":"ipAllocated","ip":"172.16.200.100","msg":"IP
    address assigned by controller","service":"default/nginx-lb","ts":"2020-04-25T23:54:03.668948668Z"}**'
- en: 'Now you can test the service by using **curl** on the Docker host. Using the
    IP address that was assigned to the service and port 8080, enter the following
    command:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您可以在Docker主机上使用**curl**来测试服务。使用分配给服务的IP地址和端口8080，输入以下命令：
- en: '**curl 172.17.200.100:8080**'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '**curl 172.17.200.100:8080**'
- en: 'You will receive the following output:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 您将收到以下输出：
- en: '![Figure 6.8 – Curl output to the LoadBalancer service running NGINX'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 - Curl输出到运行NGINX的LoadBalancer服务'
- en: '](image/Fig_6.8_B15514.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.8_B15514.jpg)'
- en: Figure 6.8 – Curl output to the LoadBalancer service running NGINX
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 - Curl输出到运行NGINX的LoadBalancer服务
- en: Adding MetalLB to a cluster allows you to expose applications that otherwise
    could not be exposed using a layer 7 balancer. Adding both layer 7 and layer 4
    services to your clusters allows you to expose almost any application type you
    can think of, including databases. What if you wanted to offer different IP pools
    to services? In the next section, we will explain how to create multiple IP pools
    that can be assigned to services using an annotation, allowing you to assign an
    IP range to services.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 将MetalLB添加到集群中，允许您暴露其他情况下无法使用第7层负载均衡器暴露的应用程序。在集群中添加第7层和第4层服务，允许您暴露几乎任何类型的应用程序，包括数据库。如果您想要为服务提供不同的IP池怎么办？在下一节中，我们将解释如何创建多个IP池，并使用注释将其分配给服务，从而允许您为服务分配IP范围。
- en: Adding multiple IP pools to MetalLB
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向MetalLB添加多个IP池
- en: There may be scenarios where you need to provide different subnets to specific
    workloads on a cluster. One scenario may be that when you created a range on the
    network for your services, you underestimated how many services would be created
    and you ran out of IP addresses.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有一些情况下，您需要为集群上的特定工作负载提供不同的子网。一个情况可能是，当您为您的服务在网络上创建一个范围时，您低估了会创建多少服务，导致IP地址用尽。
- en: Depending on the original range that you used, you may be able to just increase
    the range on your configuration. If you cannot extend the existing range, you
    will need to create a new range before any new LoadBalancer services can be created.
    You can also add additional IP ranges to the default pool, but for this example,
    we will create a new pool.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您使用的原始范围，您可能只需增加配置中的范围。如果无法扩展现有范围，则需要在创建任何新的LoadBalancer服务之前创建一个新范围。您还可以向默认池添加其他IP范围，但在本例中，我们将创建一个新池。
- en: 'We can edit the configuration file and add the new range information to the
    file. Using the original YAML file, **metallb-config.yaml**, we need to add the
    text in bold in the following code:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编辑配置文件，并将新的范围信息添加到文件中。使用原始的YAML文件**metallb-config.yaml**，我们需要在以下代码中添加粗体文本：
- en: 'apiVersion: v1'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：v1
- en: 'kind: ConfigMap'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：ConfigMap
- en: 'metadata:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'namespace: metallb-system'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：metallb-system
- en: 'name: config'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：配置
- en: 'data:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：
- en: 'config: |'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 配置：|
- en: 'address-pools:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 地址池：
- en: '- name: default'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：默认'
- en: 'protocol: layer2'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：layer2
- en: 'addresses:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 地址：
- en: '- 172.17.200.100-172.17.200.125'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '- 172.17.200.100-172.17.200.125'
- en: '- name: subnet-201'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：subnet-201'
- en: 'protocol: layer2'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：layer2
- en: 'addresses:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 地址：
- en: '- 172.17.201.100-172.17.201.125'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '- 172.17.200.100-172.17.200.125'
- en: 'Apply the updated ConfigMap using **kubectl**:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 应用使用**kubectl**更新ConfigMap：
- en: '**kubectl apply -f metallb-config.yaml**'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f metallb-config.yaml**'
- en: 'The updated ConfigMap will create a new pool called subnet-201\. MetalLB now
    has two pools that can be used to assign IP addresses to services: the default
    and subnet-201\.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的ConfigMap将创建一个名为subnet-201的新池。MetalLB现在有两个池，可以用来为服务分配IP地址：默认和subnet-201。
- en: If a user creates a LoadBalancer service and does not specify a pool name, Kubernetes
    will attempt to use the default pool. If the requested pool is out of address,
    the service will sit in a pending state until an address is available.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户创建了一个LoadBalancer服务，但没有指定池名称，Kubernetes将尝试使用默认池。如果请求的池中没有地址，服务将处于挂起状态，直到有地址可用。
- en: 'To create a new service from the second pool, you need to add an annotation
    to your service request. Using our NGINX deployment, we will create a second service
    called **nginx-web2** that will request an IP address from the subnet-201 pool:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 要从第二个池创建一个新服务，您需要向服务请求添加注释。使用我们的NGINX部署，我们将创建一个名为**nginx-web2**的第二个服务，该服务将从subnet-201池请求一个IP地址：
- en: 'Create a new file called **nginx-lb2.yaml** with the following content:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为**nginx-lb2.yaml**的新文件，其中包含以下内容：
- en: 'apiVersion: v1'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：v1
- en: 'kind: Service'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：服务
- en: 'metadata:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: nginx-lb2'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-lb2
- en: 'annotations:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 注释：
- en: 'metallb.universe.tf/address-pool: subnet-201'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 'metallb.universe.tf/address-pool: subnet-201'
- en: 'spec:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'ports:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- port: 8080'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '- 端口：8080'
- en: 'targetPort: 8080'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端口：8080
- en: 'selector:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'run: nginx-web'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：nginx-web
- en: 'type: LoadBalancer'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：负载均衡器
- en: 'To create the new service, deploy the manifest using kubectl:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建新服务，请使用kubectl部署清单：
- en: '**kubectl apply -f nginx-lb2.yaml**'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f nginx-lb2.yaml**'
- en: 'To verify that the service was created with an IP address from the subnet-201
    pool, list all of the services:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要验证服务是否使用了子网201地址池中的IP地址创建，请列出所有服务：
- en: '**kubectl get services**'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl get services**'
- en: 'You will receive the following output:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 您将收到以下输出：
- en: '![Figure 6.9 – Example services using LoadBalancer'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.9 - 使用LoadBalancer的示例服务'
- en: '](image/Fig_6.9_B15514.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.9_B15514.jpg)'
- en: Figure 6.9 – Example services using LoadBalancer
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 - 使用LoadBalancer的示例服务
- en: The last service in the list is our newly created **nginx-lb2** service. We
    can confirm that it has been assigned an external IP address of 172.17.20.100,
    which is from the subnet-201 pool.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的最后一个服务是我们新创建的**nginx-lb2**服务。我们可以确认它已被分配了一个外部IP地址172.17.20.100，这是来自子网201地址池的。
- en: 'And finally, we can test the service by using a **curl** command on the Docker
    host, to the assigned IP address on port 8080:'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过在Docker主机上使用**curl**命令，连接到分配的IP地址的8080端口来测试服务：
- en: '![Figure 6.10 – Curl NGINX on a LoadBalancer using a second IP pool'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10 - 在第二个IP池上使用Curl NGINX的负载均衡器'
- en: '](image/Fig_6.10_B15514.jpg)'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.10_B15514.jpg)'
- en: Figure 6.10 – Curl NGINX on a LoadBalancer using a second IP pool
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 - 在第二个IP池上使用Curl NGINX的负载均衡器
- en: Having the ability to offer different address pools allows you to assign a known
    IP address block to services. You may decide that address pool 1 will be used
    for web services, address pool 2 for databases, address pool 3 for file transfers,
    and so on. Some organizations do this to identify traffic based on the IP assignment,
    making it easier to track communication.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有提供不同地址池的能力，允许您为服务分配已知的IP地址块。您可以决定地址池1用于Web服务，地址池2用于数据库，地址池3用于文件传输，依此类推。一些组织这样做是为了根据IP分配来识别流量，使跟踪通信更容易。
- en: Adding a layer 4 load balancer to your cluster allows you to migrate applications
    that may not work with simple layer 7 traffic.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 向集群添加第4层负载均衡器允许您迁移可能无法处理简单第7层流量的应用程序。
- en: As more applications are migrated or refactored for containers, you will run
    into many applications that require multiple protocols for a single service. Natively,
    if you attempt to create a service with both TCP and UDP port mapping, you will
    receive an error that multiple protocols are not supported for the service object.
    This may not affect many applications, but why should you be limited to a single
    protocol for a service?
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 随着更多应用程序迁移到容器或进行重构，您将遇到许多需要单个服务的多个协议的应用程序。如果您尝试创建同时具有TCP和UDP端口映射的服务，您将收到一个错误，即服务对象不支持多个协议。这可能不会影响许多应用程序，但为什么您应该被限制为单个协议的服务呢？
- en: Using multiple protocols
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个协议
- en: All of our examples so far have used a TCP as the protocol. Of course, MetalLB
    supports using UDP as the service protocol as well, but what if you had a service
    that required you to use both protocols?
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的所有示例都使用TCP作为协议。当然，MetalLB也支持使用UDP作为服务协议，但如果您有一个需要同时使用两种协议的服务呢？
- en: Multiple protocol issues
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多协议问题
- en: 'Not all service types support assigning multiple protocols to a single service.
    The following table shows the three service types and their support for multiple
    protocols:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有服务类型都支持为单个服务分配多个协议。以下表格显示了三种服务类型及其对多个协议的支持：
- en: '![](image/Table_7.jpg)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](image/Table_7.jpg)'
- en: Table 6.6 – Service type protocol support
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.6 - 服务类型协议支持
- en: 'If you attempt to create a service that uses both protocols, you will receive
    an error message. We have highlighted the error in the following error message:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试创建一个同时使用两种协议的服务，你将会收到一个错误信息。我们已经在下面的错误信息中突出显示了这个错误：
- en: 'The Service "kube-dns-lb" is invalid: spec.ports: Invalid value: []core.ServicePort{core.ServicePort{Name:"dns",
    Protocol:"UDP", Port:53, TargetPort:intstr.IntOrString{Type:0, IntVal:53, StrVal:""},
    NodePort:0}, core.ServicePort{Name:"dns-tcp", Protocol:"TCP", Port:53, TargetPort:intstr.IntOrString{Type:0,
    IntVal:53, StrVal:""}, NodePort:0}}: **cannot create an external load balancer
    with mix protocols**'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '服务"kube-dns-lb"是无效的：spec.ports: 无效的值: []core.ServicePort{core.ServicePort{Name:"dns",
    Protocol:"UDP", Port:53, TargetPort:intstr.IntOrString{Type:0, IntVal:53, StrVal:""},
    NodePort:0}, core.ServicePort{Name:"dns-tcp", Protocol:"TCP", Port:53, TargetPort:intstr.IntOrString{Type:0,
    IntVal:53, StrVal:""}, NodePort:0}}: **无法创建混合协议的外部负载均衡器**'
- en: The service we were attempting to create would expose our CoreDNS service to
    an external IP using a LoadBalancer service. We need to expose the service on
    port 50 for both TCP and UDP.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图创建的服务将使用LoadBalancer服务将我们的CoreDNS服务暴露给外部IP。我们需要在端口50上同时为TCP和UDP暴露服务。
- en: 'MetalLB includes support for multiple protocols bound to a single IP address.
    The configuration requires the creation of two different services rather than
    a single service, which may seem a little odd at first. As we have shown previously,
    the API server will not allow you to create a service object with multiple protocols.
    The only way to work around this limitation is to create two different services:
    one that has the TCP ports assigned and another that has the UDP ports assigned.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB包括对绑定到单个IP地址的多个协议的支持。这种配置需要创建两个不同的服务，而不是一个单一的服务，这一开始可能看起来有点奇怪。正如我们之前所展示的，API服务器不允许你创建一个具有多个协议的服务对象。绕过这个限制的唯一方法是创建两个不同的服务：一个分配了TCP端口，另一个分配了UDP端口。
- en: Using our CoreDNS example, we will go through the steps to create an application
    that requires multiple protocols.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的CoreDNS例子，我们将逐步介绍创建一个需要多个协议的应用程序的步骤。
- en: Using multiple protocols with MetalLB
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用MetalLB的多个协议
- en: To enable support for an application that requires both TCP and UDP you need
    to create two separate services. If you have been paying close attention to how
    services are created, you may have noticed that each service receives an IP address.
    Logically, this means that when we create two services for our application, we
    would receive two different IP addresses.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持一个需要TCP和UDP的应用程序，你需要创建两个单独的服务。如果你一直在关注服务的创建方式，你可能已经注意到每个服务都会得到一个IP地址。逻辑上讲，这意味着当我们为我们的应用程序创建两个服务时，我们将得到两个不同的IP地址。
- en: In our example, we want to expose CoreDNS as a LoadBalancer service, which requires
    both TCP and UDP protocols. If we created two standard services, one with each
    protocol defined, we would receive two different IP address. How would you configure
    a system to use a DNS server that requires two different IP addresses for a connection?
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们想要将CoreDNS暴露为一个LoadBalancer服务，这需要TCP和UDP协议。如果我们创建了两个标准服务，一个定义了每种协议，我们将会得到两个不同的IP地址。你会如何配置一个需要两个不同IP地址的DNS服务器的连接？
- en: The simple answer is, **you can't**.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的答案是，**你不能**。
- en: But we just told you that you MetalLB supports this type of configuration. Stay
    with us—we are building up to explaining this by first explaining the issues that
    MetalLB will solve for us.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们刚告诉你，MetalLB支持这种类型的配置。跟着我们走——我们首先要解释MetalLB将为我们解决的问题。
- en: When we created the NGINX service that pulled from the subnet-201 IP pool earlier,
    we did so by adding an annotation to the load-balancer manifest. MetalLB has added
    support for multiple protocols by adding an annotation for **shared-IPs.**
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前创建了从subnet-201 IP池中提取的NGINX服务时，是通过向负载均衡器清单添加注释来实现的。 MetalLB通过为**shared-IPs**添加注释来添加对多个协议的支持。
- en: Using shared-IPs
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用共享IP
- en: Now that you understand the limitations around multiple protocol support in
    Kubernetes, let's use MetalLB to expose our CoreDNS service to external requests,
    using both TCP and UDP.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了Kubernetes中多协议支持的限制，让我们使用MetalLB来将我们的CoreDNS服务暴露给外部请求，同时使用TCP和UDP。
- en: As we mentioned earlier, Kubernetes will not allow you to create a single service
    with both protocols. To have a single load-balanced IP use both protocols, you
    need to create a service for both protocols, one for TCP and another for UDP.
    Each of the services will need an annotation that MetalLB will use to assign the
    same IP to both services.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Kubernetes不允许您创建具有两种协议的单个服务。要使单个负载均衡IP使用两种协议，您需要为两种协议创建一个服务，一个用于TCP，另一个用于UDP。每个服务都需要一个MetalLB将使用它来为两个服务分配相同IP的注释。
- en: For each service, you need to set the same value for the **metallb.universe.tf/allow-shared-ip**
    annotation. We will cover a complete example to expose CoreDNS to explain the
    entire process.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个服务，您需要为**metallb.universe.tf/allow-shared-ip**注释设置相同的值。我们将介绍一个完整的示例来公开CoreDNS以解释整个过程。
- en: Important note
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Most Kubernetes distributions use CoreDNS as the default DNS provider, but some
    of them still use the service name from when kube-dns was the default DNS provider.
    KinD is one of the distributions that may confuse you at first, since the service
    name is kube-dns, but rest assured, the deployment is using CoreDNS.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Kubernetes发行版使用CoreDNS作为默认的DNS提供程序，但其中一些仍然使用了kube-dns作为默认DNS提供程序时的服务名称。 KinD是其中一个可能会让你感到困惑的发行版，因为服务名称是kube-dns，但请放心，部署正在使用CoreDNS。
- en: 'So, let''s begin:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们开始：
- en: First, look at the services in the **kube-system** namespace:![Figure 6.11 –
    Default service list for kube-system
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，查看**kube-system**命名空间中的服务：![图6.11- kube-system的默认服务列表
- en: '](image/Fig_6.11_B15514.jpg)'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.11_B15514.jpg)'
- en: Figure 6.11 – Default service list for kube-system
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11- kube-system的默认服务列表
- en: The only service we have is the default **kube-dns** service, using the ClusterIP
    type, which means that it is only accessible internally to the cluster.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一拥有的服务是默认的**kube-dns**服务，使用ClusterIP类型，这意味着它只能在集群内部访问。
- en: You might have noticed that the service has multiple protocol support, having
    both port UDP and TCP assigned. Remember that, unlike the LoadBalancer service,
    a ClusterIP service **can** be assigned multiple protocols.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到该服务具有多协议支持，分配了UDP和TCP端口。请记住，与LoadBalancer服务不同，ClusterIP服务**可以**分配多个协议。
- en: The first step to add LoadBalancer support to our CoreDNS server is to create
    two manifests, one for each protocol.
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了为我们的CoreDNS服务器添加LoadBalancer支持的第一步是创建两个清单，每个协议一个。
- en: 'We will create the TCP service first. Create a file called **coredns-tcp.yaml**
    and add the content from the following example manifest. Note that the internal
    service for CoreDNS is using the **k8s-app: kube-dns** selector. Since we are
    exposing the same service, that''s the selector we will use in our manifests:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建TCP服务。创建一个名为**coredns-tcp.yaml**的文件，并添加以下示例清单中的内容。请注意，CoreDNS的内部服务使用**k8s-app：kube-dns**选择器。由于我们正在公开相同的服务，这就是我们在清单中将使用的选择器：
- en: 'apiVersion: v1'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：v1
- en: 'kind: Service'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：服务
- en: 'metadata:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: coredns-tcp'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：coredns-tcp
- en: 'namespace: kube-system'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：kube-system
- en: 'annotations:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 注释：
- en: 'metallb.universe.tf/allow-shared-ip: "coredns-ext"'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 'metallb.universe.tf/allow-shared-ip: "coredns-ext"'
- en: 'spec:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'selector:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'k8s-app: kube-dns'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: k8s-app：kube-dns
- en: 'ports:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- name: dns-tcp'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：dns-tcp'
- en: 'port: 53'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：53
- en: 'protocol: TCP'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：TCP
- en: 'targetPort: 53'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 'targetPort: 53'
- en: 'type: LoadBalancer'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：负载均衡器
- en: This file should be familiar by now, with the one exception in the annotations
    being the addition of the **metallb.universe.tf/allow-shared-ip** value. The use
    for this value will become clear when we create the next manifest for the UDP
    services.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件现在应该很熟悉了，唯一的例外是注释中添加了**metallb.universe.tf/allow-shared-ip**值。当我们为UDP服务创建下一个清单时，这个值的用途将变得清晰。
- en: Create a file called **coredns-udp.yaml** and add the content from the following
    example manifest.
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为**coredns-udp.yaml**的文件，并添加以下示例清单中的内容。
- en: 'apiVersion: v1'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: api版本：v1
- en: 'kind: Service'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：服务
- en: 'metadata:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: coredns-udp'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：coredns-udp
- en: 'namespace: kube-system'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：kube-system
- en: 'annotations:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 注释：
- en: 'metallb.universe.tf/allow-shared-ip: "coredns-ext"'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 'metallb.universe.tf/allow-shared-ip: "coredns-ext"'
- en: 'spec:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'selector:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'k8s-app: kube-dns'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: k8s-app：kube-dns
- en: 'ports:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- name: dns-tcp'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：dns-tcp'
- en: 'port: 53'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：53
- en: 'protocol: UDP'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：UDP
- en: 'targetPort: 53'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 'targetPort: 53'
- en: 'type: LoadBalancer'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：负载均衡器
- en: 'Note that we used the same annotation value from the TCP service manifest,
    **metallb.universe.tf/allow-shared-ip: "coredns-ext"**. This is the value that
    MetalLB will use to create a single IP address, even though two separate services
    are being requested.'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，我们从TCP服务清单中使用了相同的注释值**metallb.universe.tf/allow-shared-ip: "coredns-ext"**。这是MetalLB将使用的值，即使请求了两个单独的服务，也会创建一个单一的IP地址。'
- en: 'Finally, we can deploy the two services to the cluster using **kubectl apply**:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用**kubectl apply**将这两个服务部署到集群中：
- en: '**kubectl apply -f coredns-tcp.yaml kubectl apply -f coredns-udp.yaml**'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f coredns-tcp.yaml kubectl apply -f coredns-udp.yaml**'
- en: 'Once deployed, get the services in the **kube-system** namespace to verify
    that our services were deployed:'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦部署完成，获取**kube-system**命名空间中的服务，以验证我们的服务是否已部署：
- en: '![Figure 6.12 – Multiple protocols assigned using MetalLB'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12 - 使用MetalLB分配多个协议'
- en: '](image/Fig_6.12_B15514.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.12_B15514.jpg)'
- en: Figure 6.12 – Multiple protocols assigned using MetalLB
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 - 使用MetalLB分配多个协议
- en: 'You should see that two new services were created: the **coredns-tcp** and
    **coredns-udp** services. Under the **EXTERNAL-IP** column, you can see that both
    services have been assigned the same IP address, which allows the service to accept
    both protocols on the same IP address.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到已创建了两个新服务：**coredns-tcp**和**coredns-udp**服务。在**EXTERNAL-IP**列下，您可以看到这两个服务都被分配了相同的IP地址，这允许服务在同一个IP地址上接受两种协议。
- en: Adding MetalLB to a cluster gives your users the ability to deploy any application
    that they can containerize. It uses IP pools that dynamically assign an IP address
    for the service so that it is instantly accessible for servicing external requests.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 将MetalLB添加到集群中，使用户能够部署任何可以容器化的应用程序。它使用动态分配IP地址的IP池，以便立即为外部请求提供服务。
- en: One issue is that MetalLB does not provide name resolution for the service IPs.
    Users prefer to target an easy-to-remember name rather than random IP addresses
    when they want to access a service. Kubernetes does not provide the ability to
    create externally accessible names for services, but it does have an incubator
    project to enable this feature.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是MetalLB不为服务IP提供名称解析。用户更喜欢以易记的名称为目标，而不是在访问服务时使用随机IP地址。Kubernetes不提供为服务创建外部可访问名称的能力，但它有一个孵化器项目来启用此功能。
- en: In the next section, we will learn how to use CoreDNS to create service name
    entries in DNS using an incubator project called external-dns.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用CoreDNS使用一个名为external-dns的孵化器项目在DNS中创建服务名称条目。
- en: Making service names available externally
  id: totrans-548
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使服务名称在外部可用
- en: You may have been wondering why we were using the IP addresses to test the NGINX
    services that we created while we used domain names for our Ingress tests.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能一直在想为什么我们在测试我们创建的NGINX服务时使用IP地址，而在Ingress测试中使用域名。
- en: While a Kubernetes load balancer provides a standard IP address to a service,
    it does not create an external DNS name for users to connect to the service. Using
    IP addresses to connect to applications running on a cluster is not very efficient,
    and manually registering names in DNS for each IP assigned by MetalLB would be
    an impossible method to maintain. So how would you provide a more cloud-like experience
    to adding name resolution to our LoadBalancer services?
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Kubernetes负载均衡器为服务提供了标准的IP地址，但它并不为用户创建外部DNS名称以连接到服务。使用IP地址连接到集群上运行的应用程序并不是非常有效，手动为MetalLB分配的每个IP注册DNS名称将是一种不可能维护的方法。那么，如何为我们的负载均衡器服务添加名称解析提供更类似云的体验呢？
- en: Similar to the team that maintains KinD, there is a Kubernetes SIG that is working
    on this feature to Kubernetes called **external-dns**. The main project page is
    found on the SIG's Github at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于维护KinD的团队，有一个名为**external-dns**的Kubernetes SIG正在开发这个功能。主项目页面位于SIG的Github上[https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)。
- en: 'At the time of writing, the **external-dns** project supports a long list of
    compatible DNS servers, including the following:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，**external-dns**项目支持一长串兼容的DNS服务器，包括以下内容：
- en: Google's cloud DNS
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的云DNS
- en: Amazon's Route 53
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊的Route 53
- en: AzureDNS
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AzureDNS
- en: Cloudflare
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudflare
- en: CoreDNS
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreDNS
- en: RFC2136
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFC2136
- en: And more…
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有更多...
- en: As you know, our Kubernetes cluster is running CoreDNS to provide cluster DNS
    name resolution. Many people are not aware that CoreDNS is not limited to providing
    only internal cluster DNS resolution. It can also provide external name resolution,
    resolving names for any DNS zone that is managed by a CoreDNS deployment.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所知，我们的Kubernetes集群正在运行CoreDNS以提供集群DNS名称解析。许多人不知道CoreDNS不仅限于提供内部集群DNS解析。它还可以提供外部名称解析，解析由CoreDNS部署管理的任何DNS区域的名称。
- en: Setting up external-dns
  id: totrans-561
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置外部DNS
- en: Right now, our CoreDNS is only resolving names for internal cluster names, so
    we need to set up a zone for our new DNS entries. Since FooWidgets wanted all
    applications to go into **foowidgets.k8s**, we will use that as our new zone.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的CoreDNS只为内部集群名称解析名称，因此我们需要为我们的新DNS条目设置一个区域。由于FooWidgets希望所有应用程序都进入**foowidgets.k8s**，我们将使用它作为我们的新区域。
- en: Integrating external-dns and CoreDNS
  id: totrans-563
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成外部DNS和CoreDNS
- en: The final step to providing dynamic service registration to our cluster is to
    deploy and integrate **external-dns** with CoreDNS.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 向我们的集群提供动态服务注册的最后一步是部署并集成**external-dns**与CoreDNS。
- en: To configure **external-dns** and CoreDNS to work in the cluster, we need to
    configure each to use ETCD for the new DNS zone. Since our clusters are running
    KinD with a preinstalled ETCD, we will deploy a new ETCD pod dedicated to **external-dns**
    zones.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置**external-dns**和CoreDNS在集群中工作，我们需要配置每个使用ETCD的新DNS区域。由于我们的集群正在运行预安装的ETCD的KinD，我们将部署一个专用于**external-dns**区域的新ETCD
    pod。
- en: The quickest method to deploy a new ETCD service is to use the official ETCD
    operator Helm chart. Using the following single command, we can install the operator
    and a three-node ETCD cluster.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 部署新ETCD服务的最快方法是使用官方ETCD操作员Helm图表。使用以下单个命令，我们可以安装操作员和一个三节点的ETCD集群。
- en: 'First, we need install the Helm binary. We can install Helm quickly using the
    script provided by the Helm team:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装Helm二进制文件。我们可以使用Helm团队提供的脚本快速安装Helm：
- en: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
- en: chmod 700 get_helm.sh
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: chmod 700 get_helm.sh
- en: ./get_helm.sh
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: ./get_helm.sh
- en: 'Now, using Helm, we can create the ETCD cluster that we will integrate with
    CoreDNS. The following command will deploy the ETCD operator and create the ETCD
    cluster:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用Helm，我们可以创建与CoreDNS集成的ETCD集群。以下命令将部署ETCD运算符并创建ETCD集群：
- en: helm install etcd-dns --set customResources.createEtcdClusterCRD=true stable/etcd-operator
    --namespace kube-system
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: helm install etcd-dns --set customResources.createEtcdClusterCRD=true stable/etcd-operator
    --namespace kube-system
- en: 'It will take a few minutes to deploy the operator and the ETCD nodes. You can
    check on the status by looking at the pods in the **kube-system** namespace. Once
    fully installed, you will see three ETCD operator pods and three ETCD cluster
    pods:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 部署运算符和ETCD节点需要几分钟的时间。您可以通过查看**kube-system**命名空间中的pod的状态来检查状态。安装完成后，您将看到三个ETCD运算符pod和三个ETCD集群pod：
- en: '![Figure 6.13 – ETCD operator and nodes'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.13 - ETCD运算符和节点'
- en: '](image/Fig_6.13_B15514.jpg)'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.13_B15514.jpg)'
- en: Figure 6.13 – ETCD operator and nodes
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 - ETCD运算符和节点
- en: 'Once the deployment has completed, view the services in the **kube-system**
    namespace to get the IP address of the new ETCD service called **etcd-cluster-client**:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，查看**kube-system**命名空间中的服务，以获取名为**etcd-cluster-client**的新ETCD服务的IP地址：
- en: '![Figure 6.14 – ETCD service IP'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14 - ETCD服务IP'
- en: '](image/Fig_6.14_B15514.jpg)'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.14_B15514.jpg)'
- en: Figure 6.14 – ETCD service IP
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 - ETCD服务IP
- en: We will need the assigned IP address to configure **external-dns** and the CoreDNS
    zone file in the next section.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要分配的IP地址来配置**external-dns**和下一节中的CoreDNS区文件。
- en: Adding an ETCD zone to CoreDNS
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向CoreDNS添加ETCD区域
- en: '**external-dns** requires the CoreDNS zone to be stored on an ETCD server.
    Earlier, we created a new zone for foowidgets, but that was just a standard zone
    that would require manually adding new records for new services. Users do not
    have time to wait to test their deployments, and using an IP address may cause
    issues with proxy servers or internal policies. To help the users speed up their
    delivery and testing of application, we need to provide dynamic name resolution
    for their services. To enable an ETCD-integrated zone for foowidgets, edit the
    CoreDNS configmap, and add the following bold lines.'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '**external-dns**需要将CoreDNS区存储在ETCD服务器上。早些时候，我们为foowidgets创建了一个新区域，但那只是一个标准区域，需要手动添加新服务的新记录。用户没有时间等待测试他们的部署，并且使用IP地址可能会导致代理服务器或内部策略出现问题。为了帮助用户加快其服务的交付和测试，我们需要为他们的服务提供动态名称解析。要为foowidgets启用ETCD集成区域，请编辑CoreDNS
    configmap，并添加以下粗体行。'
- en: 'You may need to change the **endpoint** to the IP address of the new ETCD service
    that was retrieved on the previous page:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要将**端点**更改为在上一页检索到的新ETCD服务的IP地址：
- en: 'apiVersion: v1'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: v1'
- en: 'data:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：
- en: 'Corefile: |'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 'Corefile: |'
- en: .:53 {
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: .:53 {
- en: errors
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: errors
- en: health {
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 健康 {
- en: lameduck 5s
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: lameduck 5s
- en: '}'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ready
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 准备
- en: kubernetes cluster.local in-addr.arpa ip6.arpa {
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: kubernetes cluster.local in-addr.arpa ip6.arpa {
- en: pods insecure
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: pods insecure
- en: fallthrough in-addr.arpa ip6.arpa
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: fallthrough in-addr.arpa ip6.arpa
- en: ttl 30
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: ttl 30
- en: '}'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: prometheus :9153
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: prometheus :9153
- en: forward . /etc/resolv.conf
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: forward . /etc/resolv.conf
- en: '**etcd foowidgets.k8s {**'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '**etcd foowidgets.k8s {**'
- en: '**stubzones**'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '**stubzones**'
- en: '**path /skydns**'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '**路径/skydns**'
- en: '**endpoint http://10.96.181.53:2379**'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '**端点http://10.96.181.53:2379**'
- en: '**}**'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '**}**'
- en: cache 30
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: cache 30
- en: loop
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 循环
- en: reload
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加载
- en: loadbalance
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 负载平衡
- en: '}'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'kind: ConfigMap'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: ConfigMap'
- en: The next step is to deploy **external-dns** to the cluster.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将**external-dns**部署到集群中。
- en: We have provided a manifest in the GitHub repository in the **chapter6** directory
    that will patch the deployment with your ETCD service endpoint. You can deploy
    **external-dns** using this manifest by executing the following command, from
    the **chapter6** directory. The following command will query the service IP for
    the ETCD cluster and create a deployment file using that IP as the endpoint.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在GitHub存储库的**chapter6**目录中提供了一个清单，该清单将使用您的ETCD服务端点修补部署。您可以通过在**chapter6**目录中执行以下命令来使用此清单部署**外部DNS**。以下命令将查询ETCD集群的服务IP，并使用该IP创建一个部署文件作为端点。
- en: 'The newly created deployment will then install **external-dns** in your cluster:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，新创建的部署将在您的集群中安装**外部DNS**：
- en: ETCD_URL=$(kubectl -n kube-system get svc etcd-cluster-client -o go-template='{{
    .spec.clusterIP }}')
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: ETCD_URL=$(kubectl -n kube-system get svc etcd-cluster-client -o go-template='{{
    .spec.clusterIP }}')
- en: cat external-dns.yaml | sed -E "s/<ETCD_URL>/${ETCD_URL}/" > external-dns-deployment.yaml
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: cat external-dns.yaml | sed -E "s/<ETCD_URL>/${ETCD_URL}/" > external-dns-deployment.yaml
- en: kubectl apply -f external-dns-deployment.yaml
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl apply -f external-dns-deployment.yaml
- en: 'To deploy **external-dns** to your cluster manually, create a new manifest
    called **external-dns-deployment.yaml** with the following content, using your
    ETCD service IP address on the last line:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动将**外部DNS**部署到您的集群中，请创建一个名为**external-dns-deployment.yaml**的新清单，并在最后一行使用您的ETCD服务IP地址的以下内容：
- en: 'apiVersion: rbac.authorization.k8s.io/v1beta1'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: rbac.authorization.k8s.io/v1beta1'
- en: 'kind: ClusterRole'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：ClusterRole
- en: 'metadata:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: external-dns'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：外部DNS
- en: 'rules:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 规则：
- en: '- apiGroups: [""]'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '- apiGroups: [""]'
- en: 'resources: ["services","endpoints","pods"]'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：["services","endpoints","pods"]
- en: 'verbs: ["get","watch","list"]'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 动词：["get","watch","list"]
- en: '- apiGroups: ["extensions"]'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '- apiGroups: ["extensions"]'
- en: 'resources: ["ingresses"]'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：["ingresses"]
- en: 'verbs: ["get","watch","list"]'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 动词：["get","watch","list"]
- en: '- apiGroups: [""]'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '- apiGroups: [""]'
- en: 'resources: ["nodes"]'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：["nodes"]
- en: 'verbs: ["list"]'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 动词：["list"]
- en: '---'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '---'
- en: 'apiVersion: rbac.authorization.k8s.io/v1beta1'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: rbac.authorization.k8s.io/v1beta1'
- en: 'kind: ClusterRoleBinding'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：ClusterRoleBinding
- en: 'metadata:'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: external-dns-viewer'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：外部DNS查看器
- en: 'roleRef:'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 角色引用：
- en: 'apiGroup: rbac.authorization.k8s.io'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiGroup: rbac.authorization.k8s.io'
- en: 'kind: ClusterRole'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：ClusterRole
- en: 'name: external-dns'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：外部DNS
- en: 'subjects:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 主题：
- en: '- kind: ServiceAccount'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '- 类型：ServiceAccount'
- en: 'name: external-dns'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：外部DNS
- en: 'namespace: kube-system'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：kube-system
- en: '---'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '---'
- en: 'apiVersion: v1'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: v1'
- en: 'kind: ServiceAccount'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：ServiceAccount
- en: 'metadata:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: external-dns'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：外部DNS
- en: 'namespace: kube-system'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：kube-system
- en: '---'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '---'
- en: 'apiVersion: apps/v1'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: apps/v1'
- en: 'kind: Deployment'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：Deployment
- en: 'metadata:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: external-dns'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：外部DNS
- en: 'namespace: kube-system'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：kube-system
- en: 'spec:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'strategy:'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 策略：
- en: 'type: Recreate'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：重新创建
- en: 'selector:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'matchLabels:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配标签：
- en: 'app: external-dns'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 应用：外部DNS
- en: 'template:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 模板：
- en: 'metadata:'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'labels:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：
- en: 'app: external-dns'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 应用：外部DNS
- en: 'spec:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'serviceAccountName: external-dns'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 服务账户名称：外部DNS
- en: 'containers:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 容器：
- en: '- name: external-dns'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：外部DNS'
- en: 'image: registry.opensource.zalan.do/teapot/external-dns:latest'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像：registry.opensource.zalan.do/teapot/external-dns:latest
- en: 'args:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '- --source=service'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '- --source=service'
- en: '- --provider=coredns'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '- --provider=coredns'
- en: '- --log-level=info'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '- --log-level=info'
- en: 'env:'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：
- en: '- name: ETCD_URLS'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：ETCD_URLS'
- en: 'value: http://10.96.181.53:2379'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 值：http://10.96.181.53:2379
- en: Remember, if your ETCD server's IP address is not 10.96.181.53, change it before
    deploying the manifest.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果您的ETCD服务器IP地址不是10.96.181.53，请在部署清单之前更改它。
- en: Deploy the manifest using **kubectl apply -f external-dns-deployment.yaml**.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**kubectl apply -f external-dns-deployment.yaml**部署清单。
- en: Creating a LoadBalancer service with external-dns integration
  id: totrans-682
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建具有外部DNS集成的负载均衡器服务
- en: 'You should still have the NGINX deployment from the beginning of this chapter
    running. It has a few services tied to it. We will add another one to show you
    how to create a dynamic registration for the deployment:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该仍然拥有本章开头时运行的NGINX部署。它有一些与之相关的服务。我们将添加另一个服务，以向您展示如何为部署创建动态注册：
- en: 'To create a dynamic entry in the CoreDNS zone, you need to add an annotation
    in your service manifest. Create a new file called **nginx-dynamic.yaml** with
    the following content:'
  id: totrans-684
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在CoreDNS区域中创建动态条目，您需要在服务清单中添加一个注释。创建一个名为**nginx-dynamic.yaml**的新文件，内容如下：
- en: 'apiVersion: v1'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: api版本：v1
- en: 'kind: Service'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：服务
- en: 'metadata:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: '**annotations:**'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '**注释：**'
- en: '**external-dns.alpha.kubernetes.io/hostname: nginx.foowidgets.k8s**'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '**external-dns.alpha.kubernetes.io/hostname: nginx.foowidgets.k8s**'
- en: 'name: nginx-ext-dns'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：nginx-ext-dns
- en: 'namespace: default'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间：默认
- en: 'spec:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 规范：
- en: 'ports:'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 端口：
- en: '- port: 8080'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '- 端口：8080'
- en: 'protocol: TCP'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 协议：TCP
- en: 'targetPort: 8080'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 目标端口：8080
- en: 'selector:'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器：
- en: 'run: nginx-web'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 运行：nginx-web
- en: 'type: LoadBalancer'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：负载均衡器
- en: Note the annotation in the file. To instruct **external-dns** to create a record,
    you need to add an annotation that has the key **external-dns.alpha.kubernetes.io/hostname**
    with the desired name for the service—in this example, **nginx.foowidgets.k8s**.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 注意文件中的注释。要指示**external-dns**创建记录，您需要添加一个具有键**external-dns.alpha.kubernetes.io/hostname**的注释，其中包含服务的所需名称
    - 在本例中为**nginx.foowidgets.k8s**。
- en: Create the service using **kubectl apply -f nginx-dynamic.yaml**.
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**kubectl apply -f nginx-dynamic.yaml**创建服务。
- en: It takes about a minute for the **external-dns** to pick up on DNS changes.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '**external-dns**大约需要一分钟来获取DNS更改。'
- en: 'To verify that the record was created, check the **external-dns** pod logs
    using **kubectl logs -n kube-system -l app=external-dns**. Once the record has
    been picked up by **external-dns**, you will see an entry similar to the following:'
  id: totrans-703
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要验证记录是否已创建，请使用**kubectl logs -n kube-system -l app=external-dns**检查**external-dns**的pod日志。一旦**external-dns**捕获到记录，您将看到类似以下的条目：
- en: '**time="2020-04-27T18:14:38Z" level=info msg="Add/set key /skydns/k8s/foowidgets/nginx/03ebf8d8
    to Host=172.17.201.101, Text=\"heritage=external-dns,external-dns/owner=default,external-dns/resource=service/default/nginx-lb\",
    TTL=0"**'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '**time="2020-04-27T18:14:38Z" level=info msg="Add/set key /skydns/k8s/foowidgets/nginx/03ebf8d8
    to Host=172.17.201.101, Text=\"heritage=external-dns,external-dns/owner=default,external-dns/resource=service/default/nginx-lb\",
    TTL=0"**'
- en: The last step to confirm that external-dns is fully working is to test a connection
    to the application. Since we are using a KinD cluster, we must test this from
    a pod in the cluster. We will use a Netshoot container, as we have been doing
    throughout this book.
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认**external-dns**完全工作的最后一步是测试与应用程序的连接。由于我们使用的是KinD集群，我们必须从集群中的一个pod进行测试。我们将使用Netshoot容器，就像我们在本书中一直在做的那样。
- en: Important note
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At the end of this section, we will show the steps to integrate a Windows DNS
    server with our Kubernetes CoreDNS servers. The steps are being provided to provide
    you with a complete understanding of how you fully integrate the enterprise DNS
    server with delegation to our CoreDNS service.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后，我们将展示将Windows DNS服务器与我们的Kubernetes CoreDNS服务器集成的步骤。这些步骤旨在让您完全了解如何将企业DNS服务器完全集成到我们的CoreDNS服务中。
- en: 'Run a Netshoot container:'
  id: totrans-708
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行Netshoot容器：
- en: '**kubectl run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot
    -- /bin/bash**'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot
    -- /bin/bash**'
- en: To confirm that the entry has been created successfully, execute a **nslookup**
    for the host in a Netshoot shell:![Figure 6.15 – Nslookup for new record
  id: totrans-710
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要确认条目已成功创建，请在Netshoot shell中执行**nslookup**以查找主机：![图6.15 - Nslookup的新记录
- en: '](image/Fig_6.15_B15514.jpg)'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.15_B15514.jpg)'
- en: Figure 6.15 – Nslookup for new record
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 - Nslookup的新记录
- en: We can confirm that the DNS server in use is CoreDNS, based on the IP address,
    which is the assigned IP to the **kube-dns** service. (Again, the service is **kube-dns**,
    but the pods are running CoreDNS).
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确认正在使用的DNS服务器是CoreDNS，基于IP地址，这是分配给**kube-dns**服务的IP地址。（再次强调，服务是**kube-dns**，但是pod正在运行CoreDNS）。
- en: 'The 172.17.201.101 address is the IP that was assigned to the new NGINX service;
    we can confirm this by listing the services in the default namespace:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 172.17.201.101地址是分配给新NGINX服务的IP地址；我们可以通过列出默认命名空间中的服务来确认这一点：
- en: '![Figure 6.16 – NGINX external IP address'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.16 – NGINX外部IP地址'
- en: '](image/Fig_6.16_B15514.jpg)'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.16_B15514.jpg)'
- en: Figure 6.16 – NGINX external IP address
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – NGINX外部IP地址
- en: 'Finally, let''s confirm that the connection to NGINX works by connecting to
    the container using the name. Using a **curl** command in the Netshoot container,
    curl to the DNS name on port 8080:'
  id: totrans-718
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们通过使用名称连接到容器来确认连接到NGINX是否有效。在Netshoot容器中使用**curl**命令，curl到端口8080上的DNS名称：
- en: '![Figure 6.17 – Curl test using the external-dns name'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.17 – 使用external-dns名称进行Curl测试'
- en: '](image/Fig_6.17_B15514.jpg)'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.17_B15514.jpg)'
- en: Figure 6.17 – Curl test using the external-dns name
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 使用external-dns名称进行Curl测试
- en: The **curl** output confirms that we can use the dynamically created service
    name to access the NGINX web server.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '**curl**输出确认我们可以使用动态创建的服务名称访问NGINX Web服务器。'
- en: We realize that some of these tests aren't very exciting, since you can test
    them using a standard browser. In the next section, we will integrate the CoreDNS
    running in our cluster with a Windows DNS server.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 我们意识到其中一些测试并不是非常令人兴奋，因为您可以使用标准浏览器进行测试。在下一节中，我们将把我们集群中运行的CoreDNS与Windows DNS服务器集成起来。
- en: Integrating CoreDNS with an enterprise DNS
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将CoreDNS与企业DNS集成
- en: This section will show you how to forward the name resolution of the **foowidgets.k8s**
    zone to a CoreDNS server running on a Kubernetes cluster.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向您展示如何将**foowidgets.k8s**区域的名称解析转发到运行在Kubernetes集群上的CoreDNS服务器。
- en: Note
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This section has been included to provide an example of integrating an enterprise
    DNS server with a Kubernetes DNS service.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括了一个示例，演示如何将企业DNS服务器与Kubernetes DNS服务集成。
- en: Because of the external requirements and additional setup, the steps provided
    are for reference and **should not be executed** on your KinD cluster.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 由于外部要求和额外的设置，提供的步骤仅供参考，**不应**在您的KinD集群上执行。
- en: For this scenario, the main DNS server is running on a Windows 2016 server.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，主DNS服务器运行在Windows 2016服务器上。
- en: 'The components deployed are as follows:'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的组件如下：
- en: Windows 2016 Server running DNS
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行DNS的Windows 2016服务器
- en: A Kubernetes cluster
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Kubernetes集群
- en: Bitnami NGINX deployment
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bitnami NGINX部署
- en: LoadBalancer service created, assigned IP 10.2.1.74
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建了LoadBalancer服务，分配的IP为10.2.1.74
- en: CoreDNS service configured to use hostPort 53
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreDNS服务配置为使用hostPort 53
- en: Deployed add-ons, using the configuration from this chapter such as external-dns,
    ETCD cluster for CoreDNS, CoreDNS ETCD zone added, and MetalLB using an address
    pool of 10.2.1.60-10.2.1.80
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署了附加组件，使用本章的配置，如external-dns，CoreDNS的ETCD集群，添加了CoreDNS ETCD区域，并使用地址池10.2.1.60-10.2.1.80的MetalLB
- en: Now, let's go through the configuration steps to integrate our DNS servers.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照配置步骤来集成我们的DNS服务器。
- en: Configuring the primary DNS server
  id: totrans-738
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置主DNS服务器
- en: The first step is to create a conditional forwarder to the node running the
    CoreDNS pod.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个有条件的转发器到运行CoreDNS pod的节点。
- en: 'On the Windows DNS host, we need to create a new conditional forwarder for
    **foowidgets.k8s** pointing to the host that is running the CoreDNS pod. In our
    example, the CoreDNS pod has been assigned to the host 10.240.100.102:'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows DNS主机上，我们需要为**foowidgets.k8s**创建一个新的有条件的转发器，指向运行CoreDNS pod的主机。在我们的示例中，CoreDNS
    pod已分配给主机10.240.100.102：
- en: '![Figure 6.18 – Windows conditional forwarder setup'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.18 - Windows条件转发器设置'
- en: '](image/Fig_6.18_B15514.jpg)'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.18_B15514.jpg)'
- en: Figure 6.18 – Windows conditional forwarder setup
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 - Windows条件转发器设置
- en: This configures the Windows DNS server to forward any request for a host in
    the **foowidgets.k8s** domain to CoreDNS pod.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 这将配置Windows DNS服务器，将对**foowidgets.k8s**域中主机的任何请求转发到CoreDNS pod。
- en: Testing DNS forwarding to CoreDNS
  id: totrans-745
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试DNS转发到CoreDNS
- en: To test the configuration, we will use a workstation on the main network that
    has been configured to use the Windows DNS server.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试配置，我们将使用主网络上已配置为使用Windows DNS服务器的工作站。
- en: 'The first test we will run is a **nslookup** of the NGINX record that was created
    by the MetalLB annotation:'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行的第一个测试是对MetalLB注释创建的NGINX记录进行**nslookup**：
- en: 'From a command prompt, we execute a **nslookup nginx.foowidgets.k8s**:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令提示符中，我们执行**nslookup nginx.foowidgets.k8s**：
- en: '![Figure 6.19 – Nslookup confirmation for registered name'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.19 - 注册名称的Nslookup确认'
- en: '](image/Fig_6.19_B15514.jpg)'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.19_B15514.jpg)'
- en: Figure 6.19 – Nslookup confirmation for registered name
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 - 注册名称的Nslookup确认
- en: Since the query returned the IP address we expected for the record, we can confirm
    that the Windows DNS server is forwarding requests to CoreDNS correctly.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 由于查询返回了我们期望的记录的IP地址，我们可以确认Windows DNS服务器正确地将请求转发到CoreDNS。
- en: 'We can do one more additional NGINX test from the laptop''s browser:'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从笔记本电脑的浏览器进行一次额外的NGINX测试：
- en: '![Figure 6.20 – Success browsing from an external workstation using CoreDNS'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.20 - 使用CoreDNS从外部工作站成功浏览'
- en: '](image/Fig_6.20_B15514.jpg)'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.20_B15514.jpg)'
- en: Figure 6.20 – Success browsing from an external workstation using CoreDNS
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 - 使用CoreDNS从外部工作站成功浏览
- en: One test confirms that the forwarding works, but we aren't comfortable that
    the system is fully working.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 一个测试确认了转发的工作，但我们不确定系统是否完全正常工作。
- en: To test a new service, we deploy a different NGINX server called microbot, with
    a service that has an annotation assigning the name **microbot.foowidgets.k8s**.
    MetalLB has assigned the service the IP address of 10.2.1.65.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试一个新的服务，我们部署了一个名为microbot的不同NGINX服务器，该服务具有一个注释，分配了名称**microbot.foowidgets.k8s**。MetalLB已经分配了该服务的IP地址为10.2.1.65。
- en: 'Like our previous test, we test the name resolution using nslookup:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的测试一样，我们使用nslookup测试名称解析：
- en: '![Figure 6.21 – Nslookup confirmation for an additional registered name'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.21 - 用于额外注册名称的Nslookup确认'
- en: '](image/Fig_6.21_B15514.jpg)'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.21_B15514.jpg)'
- en: Figure 6.21 – Nslookup confirmation for an additional registered name
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 - 用于额外注册名称的Nslookup确认
- en: 'To confirm that the web server is running correctly, we browse to the URL from
    a workstation:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认Web服务器是否正常运行，我们从工作站浏览到URL：
- en: '![Figure 6.22 – Successful browsing from an external workstation using CoreDNS'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.22 - 使用CoreDNS从外部工作站成功浏览'
- en: '](image/Fig_6.22_B15514.jpg)'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_6.22_B15514.jpg)'
- en: Figure 6.22 – Successful browsing from an external workstation using CoreDNS
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 - 使用CoreDNS从外部工作站成功浏览
- en: Success! We have now integrated an enterprise DNS server with a CoreDNS server
    running on a Kubernetes cluster. This integration provides users with the ability
    to register service names dynamically by simply adding an annotation to the service.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们现在已将企业DNS服务器与在Kubernetes集群上运行的CoreDNS服务器集成在一起。这种集成使用户能够通过简单地向服务添加注释来动态注册服务名称。
- en: Summary
  id: totrans-768
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about two important objects in Kubernetes that
    expose your deployments to other cluster resources and users.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了Kubernetes中两个重要的对象，这些对象将您的部署暴露给其他集群资源和用户。
- en: We started the chapter by going over services and the multiple types that can
    be assigned. The three major service types are ClusterIP, NodePort, and LoadBalancer.
    Selecting the type of service will configure how your application is accessed.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始本章时讨论了服务和可以分配的多种类型。三种主要的服务类型是ClusterIP、NodePort和LoadBalancer。选择服务类型将配置应用程序的访问方式。
- en: Typically, services alone are not the only objects that are used to provide
    access to applications running in the cluster. You will often use a ClusterIP
    service along with an Ingress controller to provide access to services that use
    layer 7\. Some applications may require additional communication, that is not
    provided by a layer-7 load balancer. These applications may need a layer-4 load
    balancer to expose their services to the users. In the load balancing section,
    we demonstrated the installation and use of MetalLB, a commonly used open source
    layer-7 load balancer.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，仅使用服务并不是提供对在集群中运行的应用程序的访问的唯一对象。您通常会使用ClusterIP服务以及Ingress控制器来提供对使用第7层的服务的访问。一些应用程序可能需要额外的通信，第7层负载均衡器无法提供这种通信。这些应用程序可能需要第4层负载均衡器来向用户公开其服务。在负载均衡部分，我们演示了MetalLB的安装和使用，这是一个常用的开源第7层负载均衡器。
- en: In the last section, we explained how to integrate a dynamic CoreDNS zone with
    an external enterprise DNS server using conditional forwarding. Integrating the
    two naming systems provides a method to allow the dynamic registration of any
    layer-4 load-balanced service in the cluster.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们解释了如何使用条件转发将动态CoreDNS区集成到外部企业DNS服务器。集成这两个命名系统提供了一种方法，允许在集群中动态注册任何第4层负载均衡服务。
- en: Now that you know how to expose services on the cluster to users, how do we
    control who has access to the cluster to create a new service? In the next chapter,
    we will explain how to integrate authentication with your cluster. We will deploy
    an OIDC provider into our KinD clusters and connect with an external SAML2 lab
    server for identities.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道如何向用户公开集群上的服务，那么我们如何控制谁可以访问集群来创建新服务呢？在下一章中，我们将解释如何将身份验证集成到您的集群中。我们将在我们的KinD集群中部署一个OIDC提供程序，并与外部SAML2实验室服务器连接以获取身份。
- en: Questions
  id: totrans-774
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How does a service know what pods should be used as endpoints for the service?
  id: totrans-775
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务如何知道应该使用哪些pod作为服务的端点？
- en: A. By the service port
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: A. 通过服务端口
- en: B. By the namespace
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: B. 通过命名空间
- en: C. By the author
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: C. 由作者
- en: D. By the selector label
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: D. 通过选择器标签
- en: What kubectl command helps you to troubleshoot services that may not be working
    properly?
  id: totrans-780
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个kubectl命令可以帮助您排除可能无法正常工作的服务？
- en: A. **kubectl get services <service name>**
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: A. **kubectl get services <service name>**
- en: B. **kubectl get ep <service name>**
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: B. **kubectl get ep <service name>**
- en: C. **kubectl get pods <service name>**
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: C. **kubectl get pods <service name>**
- en: D. **kubectl get servers <service name>**
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: D. **kubectl get servers <service name>**
- en: All Kubernetes distributions include support for services that use the **LoadBalancer**
    type.
  id: totrans-785
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有Kubernetes发行版都支持使用**LoadBalancer**类型的服务。
- en: A. True
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: A. 真
- en: B. False
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: B. 错误
- en: Which load balancer type supports all TCP/UDP ports and accepts traffic regardless
    of the packet's contents?
  id: totrans-788
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种负载均衡器类型支持所有TCP/UDP端口并接受数据包内容？
- en: A. Layer 7
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: A. 第7层
- en: B. Cisco layer
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: B. Cisco层
- en: C. Layer 2
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: C. 第2层
- en: D. Layer 4
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: D. 第4层
- en: Without any added components, you can use multiple protocols using which of
    the following service types?
  id: totrans-793
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在没有任何附加组件的情况下，您可以使用以下哪种服务类型来使用多个协议？
- en: A. **NodePort** and **ClusterIP**
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: A. **NodePort**和**ClusterIP**
- en: B. **LoadBalancer** and **NodePort**
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: B. **LoadBalancer**和**NodePort**
- en: C. **NodePort**, **LoadBalancer**, and **ClusterIP**
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: C. **NodePort**、**LoadBalancer**和**ClusterIP**
- en: D. **LoadBalancer** and **ClusterIP**
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: D. **负载均衡器**和**ClusterIP**
