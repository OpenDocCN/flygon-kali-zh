- en: Chapter 4. Creating a Production-Grade Swarm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。创建生产级别的Swarm
- en: 'In this chapter, you will learn how to create real Swarm clusters with thousands
    of nodes; specifically we''ll cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您将学习如何创建拥有数千个节点的真实Swarm集群；具体来说，我们将涵盖以下主题：
- en: Tools to deploy large Swarms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署大型Swarm的工具
- en: 'Swarm2k: One of the largest Swarm mode cluster ever built, made of 2,300 nodes'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm2k：有史以来构建的最大Swarm模式集群之一，由2,300个节点组成
- en: 'Swarm3k: The second experiment, a cluster with 4,700 nodes'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm3k：第二个实验，一个拥有4,700个节点的集群
- en: How to plan hardware resources
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何规划硬件资源
- en: HA cluster topologies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HA集群拓扑
- en: Swarm infrastructures management, networking, and security
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm基础设施管理、网络和安全
- en: Monitoring dashboards
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控仪表板
- en: What you learned from the Swarm2k and Swarm3k experiments
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Swarm2k和Swarm3k实验中学到的东西
- en: Tools
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: With Swarm Mode, we can easily design a production-grade cluster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Swarm模式，我们可以轻松设计生产级别的集群。
- en: The principles and architecture we're illustrating here are important in general
    and give a foundation on how to architect production installations, regardless
    of the tools. However, from a practical point of view, the tools to use are also
    important.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里阐述的原则和架构在一般情况下非常重要，并为如何设计生产安装提供了基础，无论使用何种工具。然而，从实际角度来看，使用的工具也很重要。
- en: 'At the time of writing this book, Docker Machine was not the ideal single tool
    to use for large swarms setups, so we''re demonstrating our production-scale deployments
    with a tool born alongside with this book that we already introduced in [Chapter
    1](ch01.html "Chapter 1. Welcome to Docker Swarm"), *Welcome to Docker Swarm*:
    belt ([https://github.com/chanwit/belt](https://github.com/chanwit/belt)). We''ll
    use it in conjunction with Docker Machine, Docker Networking, and the DigitalOcean''s
    `doctl` command.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，Docker Machine并不是用于大规模Swarm设置的理想单一工具，因此我们将使用一个与本书同时诞生的工具来演示我们的生产规模部署，该工具已在[第1章](ch01.html
    "第1章。欢迎来到Docker Swarm")中介绍过，*欢迎来到Docker Swarm*：belt ([https://github.com/chanwit/belt](https://github.com/chanwit/belt))。我们将与Docker
    Machine、Docker Networking和DigitalOcean的`doctl`命令一起使用它。
- en: In [Chapter 5](ch05.html "Chapter 5. Administer a Swarm Cluster"), *Administer
    a Swarm Cluster* you'll learn how it's possible to automate the creation of Swarms;
    especially, how to quickly join a massive number of workers with scripts and other
    mechanisms, such as Ansible.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html "第5章。管理Swarm集群")中，*管理Swarm集群*，您将学习如何自动化创建Swarm；特别是如何通过脚本和其他机制（如Ansible）快速加入大量的工作节点。
- en: '![Tools](images/image_04_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![工具](images/image_04_001.jpg)'
- en: An HA Topology for Swarm2k
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm2k的HA拓扑
- en: Swarm2k and Swarm3k were collaborative experiments. We raised funds in terms
    of Docker Hosts, instead of money, with a call to participate. The result was
    astonishing-Swarm2k and Swarm3k were joined by dozens of individuals and corporate
    geographically distributed contributors. In total, for Swarm2k, we collected around
    2,300 nodes, while for Swarm3k, around 4,700.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm2k和Swarm3k是协作实验。我们通过呼吁参与者以Docker主机而不是金钱来筹集资金，结果令人惊讶-Swarm2k和Swarm3k得到了数十个个人和公司地理分布的贡献者的支持。总共，对于Swarm2k，我们收集了大约2,300个节点，而对于Swarm3k，大约有4,700个节点。
- en: Let's discuss the architecture of *Swarm2k*. In the preceding figure, there
    are three managers, denoted as **mg0**, **mg1**, and **mg2**. We will use the
    three managers because it is the optimum number of managers, suggested by the
    Docker core team. Managers formed a quorum on a high-speed network link and raft
    nodes employ employed a significant amount of resources to synchronize their activities.
    So, we decided to deploy our managers on a 40GB Ethernet link into the same data
    center.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论*Swarm2k*的架构。在前面的图中，有三个管理者，分别标记为**mg0**、**mg1**和**mg2**。我们将使用三个管理者，因为这是Docker核心团队建议的最佳管理者数量。管理者在高速网络链路上形成法定人数，并且Raft节点需要大量资源来同步它们的活动。因此，我们决定将我们的管理者部署在同一数据中心的40GB以太网链路上。
- en: 'At the beginning of the experiment, we had the following configuration:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验开始时，我们有以下配置：
- en: mg0 was the cluster's manager leader
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mg0是集群的管理者领导者
- en: mg1 hosted the stat collector
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mg1托管了统计收集器
- en: mg2 was a ready (reserve) manager
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mg2是一个准备（备用）管理者
- en: Instead, the **W** nodes were Swarm workers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，**W**节点是Swarm工作者。
- en: The stat collector installed on mg1 queries the information out of the local
    Docker Engine and sends them to store in a remote time-series database, *InfluxDB*.
    We chose InfluxDB because it's natively supported by *Telegraf*, our monitoring
    agent. To display the cluster's statistics, we used *Grafana* as a dashboard,
    which we'll see later.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 安装在mg1上的统计收集器从本地Docker Engine查询信息，并将其发送到远程时间序列数据库*InfluxDB*中存储。我们选择了InfluxDB，因为它是*Telegraf*监控代理的本地支持。为了显示集群的统计信息，我们使用*Grafana*作为仪表板，稍后我们会看到。
- en: Managers specifications
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理者规格
- en: Managers are CPU bound rather than memory bound. For a 500-1,000 nodes Swarm
    cluster, we observed empirically that three managers with 8 vCPUs each are enough
    to keep the load. However, if it's beyond 2,000 nodes, we recommend, for each
    manager, at least 16-20 vCPUs to meet eventual Raft recoveries.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 管理者受CPU限制而不是内存限制。对于一个500-1,000节点的Swarm集群，我们经验性地观察到每个管理者具有8个虚拟CPU足以承担负载。然而，如果超过2,000个节点，我们建议每个管理者至少具有16-20个虚拟CPU以满足可能的Raft恢复。
- en: In case of Raft recovery
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Raft恢复的情况下
- en: The following diagram shows the CPU usage during a hardware upgrade and under
    a massive number of workers join process. During the hardware upgrade to 8 vCPUs
    (the machines downtime is represented by lines disconnections), we can see that
    the CPU usage of mg0, the leader, spiked to 75-90% when mg**1** and mg**2** rejoined
    the cluster. The event that triggered this spike is the Raft log synchronization
    and recovery.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了硬件升级期间的CPU使用情况以及大量工作者加入过程中的情况。在将硬件升级到8个虚拟CPU时（机器的停机时间由线条表示），我们可以看到领导者mg0的CPU使用率在mg**1**和mg**2**重新加入集群时飙升至75-90%。触发此飙升的事件是Raft日志的同步和恢复。
- en: Under normal conditions, with no required recoveries, CPU usage of each manager
    stays low, as shown in the following image.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有必要恢复的正常情况下，每个管理者的CPU使用率保持较低，如下图所示。
- en: '![In case of Raft recovery](images/image_04_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![在Raft恢复的情况下](images/image_04_002.jpg)'
- en: Raft files
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Raft文件
- en: In manager hosts, Swarm data is saved in `/var/lib/docker/swarm`, called the
    *swarm directory*. Specifically, Raft data is saved in `/var/lib/docker/swarm/raft`
    and consists of the Write Ahead Log (WAL) and snapshot files.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理主机上，Swarm数据保存在`/var/lib/docker/swarm`中，称为*swarm目录*。具体来说，Raft数据保存在`/var/lib/docker/swarm/raft`中，包括预写式日志（WAL）和快照文件。
- en: In these files, there are entries for nodes, services, and tasks, as defined
    by the Protobuf format.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些文件中，有关节点、服务和任务的条目，按照Protobuf格式定义。
- en: WAL and snapshot files are frequently written to disk. In SwarmKit and Docker
    Swarm Mode, they are written to disk every 10,000 entries. Per this behavior,
    we mapped the swarm directory to a fast and dedicated disk with increased throughput,
    specifically an SSD drive.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: WAL和快照文件经常写入磁盘。在SwarmKit和Docker Swarm模式中，它们每10,000个条目写入一次磁盘。根据这种行为，我们将swarm目录映射到具有增加吞吐量的快速和专用磁盘，特别是SSD驱动器。
- en: We will explain backup and restore procedures in case of the swarm directory
    corruption in [Chapter 5](ch05.html "Chapter 5. Administer a Swarm Cluster"),
    *Administer a Swarm Cluster*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第5章](ch05.html "第5章。管理Swarm集群") *管理Swarm集群*中解释在Swarm目录损坏的情况下的备份和恢复程序。
- en: Running tasks
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行任务
- en: The goal of Swarm clusters is to run services, for example, large-scale Web
    applications made from a big number of containers. We will call this deployment
    type the *Mono* model. In this model, network ports are considered resources that
    must be published globally. With *namespaces* in the future versions of Docker
    Swarm Mode, the deployment can be in the *Multi* model, where we are allowed to
    have multiple subclusters, which expose the same port for different services.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm集群的目标是运行服务，例如，由大量容器组成的大规模Web应用程序。我们将这种部署类型称为*单一*模型。在这个模型中，网络端口被视为必须全局发布的资源。在未来版本的Docker
    Swarm模式中，使用*命名空间*，部署可以是*多*模型，允许我们拥有多个子集群，这些子集群为不同的服务公开相同的端口。
- en: In small-size clusters, we can decide to allow managers to host worker tasks
    with some prudence. For larger setups, instead, managers use more resources. Moreover,
    in case a manager load saturates its resources, the cluster will become unstable
    and unresponsive and will not take any commands. We call this state the *Berserk*
    *state*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在小型集群中，我们可以决定允许管理者谨慎地托管工作任务。对于更大的设置，管理者使用更多的资源。此外，如果管理者的负载饱和了其资源，集群将变得不稳定和无响应，并且不会执行任何命令。我们称这种状态为*狂暴*
    *状态*。
- en: 'To make a large cluster, such as Swarm2k or Swarm3k stable, all managers''
    availability must be set to the "Drained" state so that all the tasks will not
    be scheduled on them, only on workers, with:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使大型集群，如Swarm2k或Swarm3k稳定，所有管理者的可用性必须设置为“排水”状态，以便所有任务不会被安排在它们上面，只会在工作节点上，具体为：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Manager topologies
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理者拓扑结构
- en: 'We''ll discuss this HA property in [Chapter 5](ch05.html "Chapter 5. Administer
    a Swarm Cluster"), *Administer a Swarm Cluster* again, but here, we will introduce
    it to illustrate some Swarm topologies theory. The HA theory makes it mandatory
    to form a HA cluster with an odd number of nodes. The following table shows the
    fault tolerance factors for a single data center. In this chapter, we''ll call
    the 5(1)-3-2 formula for a cluster size of 5 over 1 data center with 3-node quorum
    that allows 2 nodes to fail:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第5章](ch05.html "第5章。管理Swarm集群") *管理Swarm集群*中再次讨论这个HA属性，但在这里，我们将介绍它来说明一些Swarm拓扑理论。HA理论要求形成一个具有奇数节点数的HA集群。以下表格显示了单个数据中心的容错因素。在本章中，我们将称之为5(1)-3-2公式，用于5个节点在1个数据中心上的集群大小，其中3个节点法定人数允许2个节点故障。
- en: '| **Cluster Size** | **Quorum** | **Node Failure allowed** |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **集群大小** | **法定人数** | **允许节点故障** |'
- en: '| 3 | 2 | 1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 1 |'
- en: '| 5 | 3 | 2 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 3 | 2 |'
- en: '| 7 | 4 | 3 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 4 | 3 |'
- en: '| 9 | 5 | 4 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5 | 4 |'
- en: 'However, there are several manager topologies that can be designed to use in
    production environments with multiple data centers. For example, the 3(3) manager
    topology can be distributed as 1 + 1 + 1, while the 5(3) manager topology can
    be distributed as 2 + 2 + 1\. The following image shows the optimum 5(3) manager
    topology:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在多个数据中心的生产环境中可以设计出几种管理者拓扑结构。例如，3(3)管理者拓扑结构可以分布为1 + 1 + 1，而5(3)管理者拓扑结构可以分布为2
    + 2 + 1。以下图片显示了最佳的5(3)管理者拓扑结构：
- en: '![Manager topologies](images/image_04_003.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![管理者拓扑结构](images/image_04_003.jpg)'
- en: With the same level of tolerance, the next image shows an alternative 5(4) topology
    containing 5 managers across 4 data centers. There are 2 managers, mg0, and mg1,
    running in Data Center 1, while each of the remaining managers, mg2, mg3, and
    mg4, run in Data Center 2, 3, and 4 respectively. The mg0 and mg1 managers are
    connected on high-speed network, while mg2, mg3, and mg4 can use a slower link.
    Therefore, 2 + 2 + 1 across 3 data centers will be rearranged as 2 + 1 + 1 + 1
    over 4 data centers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的容错水平下，下一张图片显示了一个替代的5(4)拓扑，其中包含了4个数据中心的5个管理者。在数据中心1中运行了2个管理者mg0和mg1，而剩下的管理者mg2、mg3和mg4分别在数据中心2、3和4中运行。mg0和mg1管理者在高速网络上连接，而mg2、mg3和mg4可以使用较慢的链接。因此，在3个数据中心中的2
    + 2 + 1将被重新排列为在4个数据中心中的2 + 1 + 1 + 1。
- en: '![Manager topologies](images/image_04_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![管理者拓扑结构](images/image_04_004.jpg)'
- en: Finally, there is another distributed topology, 6(4), which is much more performant
    because, at its heart, there are 3 nodes forming a central quorum on high-speed
    links. The 6-manager cluster needs a quorum size of 4\. If Data Center 1 fails,
    the control plane of the cluster will stop working. In normal case, 2 nodes or
    2 data centers, except the main one, can be down.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有另一种分布式拓扑，6(4)，它的性能更好，因为在其核心有3个节点在高速链接上形成中央仲裁。6个管理者的集群需要一个4的仲裁大小。如果数据中心1失败，集群的控制平面将停止工作。在正常情况下，除了主要数据中心外，可以关闭2个节点或2个数据中心。
- en: '![Manager topologies](images/image_04_005.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![管理者拓扑结构](images/image_04_005.jpg)'
- en: To sum it up, stick with odd numbers of managers whenever possible. If you want
    stability of manager quorum, form it over high-speed links. If you want to avoid
    single point of failures, distribute them as much as possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，尽可能使用奇数个管理者。如果您想要管理者仲裁的稳定性，请在高速链接上形成它。如果您想要避免单点故障，请尽可能将它们分布开来。
- en: To confirm which topology will work for you, try forming it and test the manager
    latency by intentionally putting some managers down and then measure how fast
    they recover.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要确认哪种拓扑结构适合您，请尝试形成它，并通过有意将一些管理者关闭然后测量它们恢复的速度来测试管理者的延迟。
- en: For Swarm2k and Swarm3k, we chose to form topology with all three managers being
    on a single data center because we wanted to achieve the best performances.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Swarm2k和Swarm3k，我们选择在单个数据中心上形成拓扑结构，因为我们希望实现最佳性能。
- en: Provisioning the infrastructure with belt
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用belt进行基础设施的配置。
- en: 'First, we created a cluster template named `swarm2k` for DigitalOcean with
    the following command:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用以下命令为DigitalOcean创建了一个名为`swarm2k`的集群模板：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding command creates a configuration template file in the current directory
    called `.belt/swarm2k/config.yml`. This was our starting point to define other
    attributes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令在当前目录中创建了一个名为`.belt/swarm2k/config.yml`的配置模板文件。这是我们定义其他属性的起点。
- en: 'We checked if our cluster was defined by running the following command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过运行以下命令来检查我们的集群是否已定义：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the use command, we can switch and used the available `swarm2k` clusters,
    as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该命令，我们可以切换并使用可用的`swarm2k`集群，如下所示：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At this point, we refined the `swarm2k` template's attributes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们完善了`swarm2k`模板的属性。
- en: 'By setting  the DigitalOcean''s instance region to be `sgp1` by issuing the
    following command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过发出以下命令将DigitalOcean的实例区域设置为`sgp1`：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Belt required to define all necessary values with this command. Here''s a list
    of the required template keys for the DigitalOcean driver that we specified in
    `config.yml`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Belt需要使用该命令定义所有必要的值。以下是我们在`config.yml`中指定的DigitalOcean驱动程序所需的模板键列表：
- en: '`image`: This is to specify the DigitalOcean image ID or snapshot ID'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`：这是为了指定DigitalOcean镜像ID或快照ID'
- en: '`region`: This is to specify the DigitalOcean region, for example, sgp1 or
    nyc3'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`region`：这是为了指定DigitalOcean区域，例如sgp1或nyc3'
- en: '`ssh_key_fingerprint`: This is to specify the DigitalOcean SSH Key ID or fingerprint'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssh_key_fingerprint`：这是为了指定DigitalOcean SSH密钥ID或指纹'
- en: '`ssh_user`: This is to specify the username used by the image, for example,
    root'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssh_user`：这是为了指定镜像使用的用户名，例如root'
- en: '`access_token`: This is to specify DigitalOcean''s access token; it is recommended
    to not put any of the tokens here'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`access_token`：这是为了指定DigitalOcean的访问令牌；建议不要在这里放任何令牌'
- en: Tip
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Every template attribute has its environment variable counterpart. For example,
    the `access_token` attribute can be set via `DIGITALOCEAN_ACCESS_TOKEN`. So, in
    practice, we can also export `DIGITALOCEAN_ACCESS_TOKEN` as a shell variable before
    proceeding.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模板属性都有其对应的环境变量。例如，`access_token`属性可以通过`DIGITALOCEAN_ACCESS_TOKEN`来设置。因此，在实践中，我们也可以在继续之前将`DIGITALOCEAN_ACCESS_TOKEN`导出为一个shell变量。
- en: 'With the configuration in place, we verified the current template attributes
    by running the following piece of code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 配置就绪后，我们通过运行以下代码验证了当前的模板属性：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we created a set of 3 512MB manger nodes called mg0, mg1, and mg2 with
    the following syntax:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用以下语法创建了一组3个512MB的管理节点，分别称为mg0、mg1和mg2：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All new nodes are initialized and go to a new status.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有新节点都被初始化并进入新状态。
- en: 'We can use the following command to wait until all 3 nodes become active:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令等待所有3个节点变为活动状态：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we set node1 to be the active manger host, and our Swarm will be ready
    to be formed. Setting the active host can be done by running the active command,
    as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将node1设置为活动的管理主机，我们的Swarm将准备好形成。通过运行active命令可以设置活动主机，如下所示：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At this point, we formed a swarm. We initialized mg0 as the manager leader,
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经形成了一个Swarm。我们将mg0初始化为管理者领导者，如下所示：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding command outputs the strings to copy and paste to join other managers
    and workers, for example, take a look at the following command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令输出了要复制和粘贴以加入其他管理者和工作者的字符串，例如，看一下以下命令：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Belt provides a convenient shortcut to join nodes with the following syntax,
    that was what we used to join mg1 and mg2 to the swarm.:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Belt提供了一个方便的快捷方式来加入节点，使用以下语法，这就是我们用来加入mg1和mg2到Swarm的方法：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, we have the mg0, mg1, and mg2 managers configured and ready to get the
    swarm of workers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经配置好了mg0、mg1和mg2管理者，并准备好获取工作者的Swarm。
- en: Securing Managers with Docker Machine
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Machine保护管理者
- en: Docker Machine won't scale well for massive Docker Engine deployments, but it
    turns out to be very useful for automatically securing small number of nodes.
    In the following section, we'll use Docker Machine to secure our Swarm manager
    using the generic driver, a driver that allows us to control existing hosts.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Machine对于大规模的Docker Engine部署不会很好，但事实证明它非常适用于自动保护少量节点。在接下来的部分中，我们将使用Docker
    Machine使用通用驱动程序来保护我们的Swarm管理器，这是一种允许我们控制现有主机的驱动程序。
- en: In our case, we already did set up a Docker Swarm manager on mg0\. Furthermore,
    we want to secure Docker Engine by enabling the TLS connection for its remote
    endpoint.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们已经在mg0上设置了一个Docker Swarm管理器。此外，我们希望通过为其远程端点启用TLS连接来保护Docker Engine。
- en: How can Docker Machine do the work for us? First, Docker Machine connects to
    the host via SSH; detects the operating system of mg0, in our case, Ubuntu; and
    the provisioner, in our case, systemd.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Machine如何为我们工作？首先，Docker Machine通过SSH连接到主机；检测mg0的操作系统，在我们的情况下是Ubuntu；以及provisioner，在我们的情况下是systemd。
- en: After that, it installs the Docker Engine; however, in case one is already in
    place, like here, it will skip this step.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，它安装了Docker Engine；但是，如果已经有一个存在，就像这里一样，它会跳过这一步。
- en: Then, as the most important part, it generates a Root CA certificate, as well
    as all certificates, and stores them on the host. It also automatically configures
    Docker to use those certificates. Finally, it restarts Docker.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，作为最重要的部分，它生成了一个根CA证书，以及所有证书，并将它们存储在主机上。它还自动配置Docker使用这些证书。最后，它重新启动Docker。
- en: If everything goes well, Docker Engine will be started again with TLS enabled.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，Docker Engine将再次启动，并启用TLS。
- en: We then used Docker Machine to generate a Root CA for the Engine on mg0, mg1
    and mg2, and configure a TLS connection. Then, we later used the Docker client
    to further control Swarm without the need of SSH, which is slower.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用Docker Machine在mg0、mg1和mg2上生成了Engine的根CA，并配置了TLS连接。然后，我们稍后使用Docker客户端进一步控制Swarm，而无需使用较慢的SSH。
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Also, `docker node ls` will work normally with this setup We verified now that
    the 3 managers formed the initial swarm, and were able to accept a bunch of workers:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`docker node ls`将在这个设置中正常工作。我们现在验证了3个管理者组成了初始的Swarm，并且能够接受一堆工作节点：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**How secure is this cluster?**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个集群有多安全？**'
- en: We will use the Docker client to connect to the Docker Engine equipped TLS;
    and, there's another TLS connection among the swarm's node with CA expiring in
    three months, and it will be auto-rotated. Advanced security setup will be discussed
    in [Chapter 9](ch09.html "Chapter 9. Securing a Swarm Cluster and the Docker Software
    Supply Chain"), *Securing a Swarm Cluster and the Docker Software Supply Chain*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Docker客户端连接到配备TLS的Docker Engine；此外，swarm节点之间还有另一个TLS连接，CA在三个月后到期，将自动轮换。高级安全设置将在[第9章](ch09.html
    "第9章。保护Swarm集群和Docker软件供应链")中讨论，*保护Swarm集群和Docker软件供应链*。
- en: Understanding some Swarm internals
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解一些Swarm内部机制
- en: 'At this point, we checked that the Swarm was operative, by creating a service
    nginx with 3 replicas:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们通过创建一个带有3个副本的nginx服务来检查Swarm是否可操作：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Afteer that, we found where the net namespace ID of the running Nginx was. We
    connected to mg0 via SSH to mg0 via SSH. The network namespace for Swarm's routing
    mesh was the one having the same timestamp as the special network namespace, `1-5t4znibozx`.
    In this example, the namespace we're looking for is `fe3714ca42d0`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们找到了运行Nginx的net命名空间ID。我们通过SSH连接到mg0。Swarm的路由网格的网络命名空间是具有与特殊网络命名空间`1-5t4znibozx`相同时间戳的命名空间。在这个例子中，我们要找的命名空间是`fe3714ca42d0`。
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can figure out our IPVS entries with ipvsadm, and run it inside the net
    namespace using the nsenter tool ([https://github.com/jpetazzo/nsenter](https://github.com/jpetazzo/nsenter)),
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用ipvsadm找出我们的IPVS条目，并使用nsenter工具（[https://github.com/jpetazzo/nsenter](https://github.com/jpetazzo/nsenter)）在net命名空间内运行它，如下所示：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, we can notice that there is an active round-robin IPVS entry. IPVS is
    the kernel-level load balancer, and it's used by Swarm to balance traffic in conjunction
    with iptables, which is used to forward and filter packets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以注意到有一个活动的轮询IPVS条目。IPVS是内核级负载均衡器，与iptables一起用于Swarm来平衡流量，iptables用于转发和过滤数据包。
- en: 'After cleaning the nginx test service (`docker service rm nginx`), we will
    set the managers in Drain mode, so to avoid them to take tasks:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 清理nginx测试服务（`docker service rm nginx`）后，我们将设置管理者为Drain模式，以避免它们接受任务：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, we are ready to announce the availability of our managers on Twitter and
    Github and start the experiment!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备在Twitter和Github上宣布我们的管理者的可用性，并开始实验！
- en: Joining workers
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入工作节点
- en: 'Our contributors started joining their nodes to manager **mg0** as workers.
    Anyone used their own favorite method, including the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献者开始将他们的节点作为工作节点加入到管理者**mg0**。任何人都可以使用自己喜欢的方法，包括以下方法：
- en: Looping `docker-machine ssh sudo docker swarm join` commands
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环`docker-machine ssh sudo docker swarm join`命令
- en: Ansible
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansible
- en: Custom scripts and programs
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义脚本和程序
- en: We'll cover some of these methods in [Chapter 5](ch05.html "Chapter 5. Administer
    a Swarm Cluster"), *Administer a Swarm Cluster* .
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第5章](ch05.html "第5章。管理Swarm集群")中涵盖其中一些方法，*管理Swarm集群*。
- en: 'After some time, we reached the quota of 2,300 workers and launched an **alpine**
    service with a replica factor of 100,000:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 过了一段时间，我们达到了2,300个工作节点的配额，并使用了100,000个副本因子启动了一个**alpine**服务：
- en: '![Joining workers](images/image_04_006.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![加入工人](images/image_04_006.jpg)'
- en: Upgrading Managers
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级管理器
- en: After some time, we reached the maximum of capacity for our managers, and we
    had to increase their physical resources. Live upgrading and maintenance of managers
    may be an expected operation in production. Here is how we did this operation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 过了一段时间，我们达到了管理器的最大容量，并且不得不增加它们的物理资源。在生产中，实时升级和维护管理器可能是一项预期的操作。以下是我们执行此操作的方法。
- en: Live Upgrading the Managers
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时升级管理器
- en: With an odd number for the quorum, it is safe to demote a manager for maintenance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用奇数作为法定人数，安全地将管理器降级进行维护。
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we had mg1 as a reachable manager, and we demoted it to worker with the
    following syntax:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将mg1作为可达的管理器，并使用以下语法将其降级为工作节点：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see that the `Reachable` status of `mg1` disappears from node ls output
    when it becomes a worker.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到当mg1成为工作节点时，`mg1`的`Reachable`状态从节点列表输出中消失。
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When the node is not a manager anymore, it''s safe to shut it down, for example,
    with the DigitalOcean CLI, as we did:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点不再是管理器时，可以安全地关闭它，例如，使用DigitalOcean CLI，就像我们做的那样：
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Listing the nodes, we noticed that mg1 was already down.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列出节点时，我们注意到mg1已经宕机了。
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We upgraded its resources to have 16G of memory, and then we powered the machine
    on again:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其资源升级为16G内存，然后再次启动该机器：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When listing this time, we can expect some delay as mg1 is being back and reentered
    the cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在列出这个时间时，我们可以预期一些延迟，因为mg1正在重新加入集群。
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we can promote it back to the manager, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将其重新提升为管理器，如下所示：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Once this is done, the cluster operated normally. So, we repeated the operation
    for mg0 and mg2.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这个操作，集群就正常运行了。所以，我们对mg0和mg2重复了这个操作。
- en: Monitoring Swarm2k
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控Swarm2k
- en: For the production-grade cluster, we usually want to set up some kind of monitoring.
    At the date, there is not a specific way to monitor Docker service and tasks in
    Swarm mode. We did this for Swarm2k with Telegraf, InfluxDB, and Grafana.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产级集群，通常希望设置某种监控。到目前为止，还没有一种特定的方法来监视Swarm模式中的Docker服务和任务。我们在Swarm2k中使用了Telegraf、InfluxDB和Grafana来实现这一点。
- en: InfluxDB Time-Series Database
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InfluxDB时间序列数据库
- en: InfluxDB is a time-series database, which is easy to install because of no dependency.
    InfluxDB is useful to store metrics, information about events, and use them for
    later analysis. For Swarm2k, we used InfluxDB to store information of cluster,
    nodes, events, and for tasks with Telegraf.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: InfluxDB是一个易于安装的时间序列数据库，因为它没有依赖关系。InfluxDB对于存储指标、事件信息以及以后用于分析非常有用。对于Swarm2k，我们使用InfluxDB来存储集群、节点、事件以及使用Telegraf进行任务的信息。
- en: Telegraf is pluggable and has a certain number of input plugins useful to observe
    the system environment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Telegraf是可插拔的，并且具有一定数量的输入插件，用于观察系统环境。
- en: Telegraf Swarm plugin
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Telegraf Swarm插件
- en: We developed a new plugin for Telegraf to store stats into InfluxDB. This plugin
    can be found at [http://github.com/chanwit/telegraf](http://github.com/chanwit/telegraf).
    Data may contain *values*, *tags*, and *timestamp*. Values will be computed or
    aggregated based on timestamp. Additionally, tags will allow you to group these
    values together based on timestamp.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为Telegraf开发了一个新的插件，可以将统计数据存储到InfluxDB中。该插件可以在[http://github.com/chanwit/telegraf](http://github.com/chanwit/telegraf)找到。数据可能包含*值*、*标签*和*时间戳*。值将根据时间戳进行计算或聚合。此外，标签将允许您根据时间戳将这些值分组在一起。
- en: 'The Telegraf Swarm plugin collects data and creates the following series containing
    values, which we identified as the most interesting for Swarmk2, tags, and timestamp
    into InfluxDB:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Telegraf Swarm插件收集数据并创建以下系列，其中包含我们认为对Swarmk2最有趣的值、标签和时间戳到InfluxDB中：
- en: 'Series `swarm_node`: This series contains `cpu_shares` and `memory` as values
    and allow you to be grouped by `node_id` and `node_hostname` tags.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系列`swarm_node`：该系列包含`cpu_shares`和`memory`作为值，并允许按`node_id`和`node_hostname`标签进行分组。
- en: 'Series `swarm`: This series contains `n_nodes` for number of nodes, `n_services`
    for number of services, and `n_tasks` for number of tasks. This series does not
    contain tags.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系列`swarm`：该系列包含`n_nodes`表示节点数量，`n_services`表示服务数量，`n_tasks`表示任务数量。该系列不包含标签。
- en: 'Series `swarm_task_status`: This series contains number of tasks grouped by
    status at a time. Tags of this series are tasks status names, for example, Started,
    Running, and Failed.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系列`swarm_task_status`：该系列包含按状态分组的任务数量。该系列的标签是任务状态名称，例如Started、Running和Failed。
- en: 'To enable the Telegraf Swarm plugin, we will need to tweak `telegraf.conf` by
    adding the following configuration:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用Telegraf Swarm插件，我们需要通过添加以下配置来调整`telegraf.conf`：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'First, set up an instance of InfluxDB as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，按以下方式设置InfluxDB实例：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, set up an instance of Grafana, as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，按以下方式设置Grafana：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After we setup an instance of Grafana, we can create the dashboard from the
    following JSON configuration:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置Grafana实例后，我们可以从以下JSON配置创建仪表板：
- en: '[https://objects-us-west-1.dream.io/swarm2k/swarm2k_final_grafana_dashboard.json](https://objects-us-west-1.dream.io/swarm2k/swarm2k_final_grafana_dashboard.json)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://objects-us-west-1.dream.io/swarm2k/swarm2k_final_grafana_dashboard.json](https://objects-us-west-1.dream.io/swarm2k/swarm2k_final_grafana_dashboard.json)'
- en: To connect the dashboard to InfluxDB, we will have to define the default data
    source and point it to the InfluxDB host port `8086`. Here's the JSON configuration
    to define the data source. Replace `$INFLUX_DB_IP` with your InfluxDB instance.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要将仪表板连接到InfluxDB，我们将不得不定义默认数据源并将其指向InfluxDB主机端口`8086`。以下是定义数据源的JSON配置。将`$INFLUX_DB_IP`替换为您的InfluxDB实例。
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After linking everything together, we''ll see a dashboard like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容链接在一起后，我们将看到一个像这样的仪表板：
- en: '![Telegraf Swarm plugin](images/image_04_007.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Telegraf Swarm插件](images/image_04_007.jpg)'
- en: Swarm3k
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm3k
- en: Swarm3k was the second collaborative project trying to form a very large Docker
    cluster with the Swarm mode. It was fired up on 28th October 2016 with more than
    50 individuals and companies joining this project.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm3k是第二个协作项目，试图使用Swarm模式形成一个非常大的Docker集群。它于2016年10月28日启动，有50多个个人和公司加入了这个项目。
- en: Sematext was one of the very first companies that offered to help us by offering
    their Docker monitoring and logging solution. They became the official monitoring
    system for Swarm3k. Stefan, Otis, and their team provided wonderful support for
    us from the very beginning.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Sematext是最早提供帮助的公司之一，他们提供了他们的Docker监控和日志解决方案。他们成为了Swarm3k的官方监控系统。Stefan、Otis和他们的团队从一开始就为我们提供了很好的支持。
- en: '![Swarm3k](images/image_04_008.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![Swarm3k](images/image_04_008.jpg)'
- en: '*Sematext Dashboard*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sematext仪表板*'
- en: Sematext is the one and only Docker monitoring company that allows us to deploy
    the monitoring agents as the global Docker service at the moment. This deployment
    model provides for a greatly simplified monitoring process.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Sematext是唯一一家允许我们将监控代理部署为全局Docker服务的Docker监控公司。这种部署模型大大简化了监控过程。
- en: Swarm3k Setup and Workload
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swarm3k设置和工作负载
- en: We aimed at 3000 nodes, but in the end, we successfully formed a working, geographically
    distributed 4,700-node Docker Swarm cluster.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是3000个节点，但最终，我们成功地形成了一个工作的、地理分布的4700个节点的Docker Swarm集群。
- en: The managers' specifications were high-mem 128GB DigitalOcean nodes in the same
    Data Center, each with 16 vCores.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 经理们的规格要求是在同一数据中心中使用高内存128GB的DigitalOcean节点，每个节点有16个vCores。
- en: The cluster init configuration included an undocumented "KeepOldSnapshots",
    which tells the Swarm mode not to delete but preserve all data snapshots for later
    analysis. The Docker daemon of each manager started in the DEBUG mode so to have
    more information on the go..
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 集群初始化配置包括一个未记录的"KeepOldSnapshots"，告诉Swarm模式不要删除，而是保留所有数据快照以供以后分析。每个经理的Docker守护程序都以DEBUG模式启动，以便在移动过程中获得更多信息。
- en: We used belt to set up the managers, as we showed in the previous section, and
    waited for contributors to join their workers.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用belt来设置经理，就像我们在前一节中展示的那样，并等待贡献者加入他们的工作节点。
- en: Docker 1.12.3 was used on managers, while workers were a mix of 1.12.2 and 1.12.3\.
    We organized the services on the *ingress* and *overlay* networks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 经理们使用的是Docker 1.12.3，而工作节点则是1.12.2和1.12.3的混合。我们在*ingress*和*overlay*网络上组织了服务。
- en: 'We planned the following two workloads:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划了以下两个工作负载：
- en: MySQL with Wordpress cluster
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MySQL与Wordpress集群
- en: C1M (Container-1-Million)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C1M（Container-1-Million）
- en: '25 nodes were intended to form a MySQL cluster. First, we created an overlay
    network, `mydb`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 打算使用25个节点形成一个MySQL集群。首先，我们创建了一个overlay网络`mydb`：
- en: '[PRE30]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we prepared the following `entrypoint.sh` script:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们准备了以下`entrypoint.sh`脚本：
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we will prepare a new Dockerfile for our special version of Etcd, as
    follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将为我们特殊版本的Etcd准备一个新的Dockerfile，如下所示：
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Don't forget to build it with `$ docker build -t chanwit/etcd.` before you start
    using it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用之前，不要忘记使用`$ docker build -t chanwit/etcd.`来构建它。
- en: 'Third, we started an Etcd node as a central discovery service for MySQL cluster
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们启动了一个Etcd节点作为MySQL集群的中央发现服务，如下所示：
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'By inspecting the Virtual IP of Etcd, we will get the service VIP as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查Etcd的虚拟IP，我们将得到以下服务VIP：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With this information, we created our `mysql` service, which can scale at any
    degree. Take a look at the following example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们创建了我们的`mysql`服务，可以在任何程度上进行扩展。看看以下示例：
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We experienced some IP addresses issues from both mynet and ingress networks
    because of a Libnetwork bug; check out [https://github.com/docker/docker/issues/24637](https://github.com/docker/docker/issues/24637)
    for more information. We work around this bug by binding the cluster only to a
    *single* overlay network, `mydb.`
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Libnetwork的一个bug，我们在mynet和ingress网络中遇到了一些IP地址问题；请查看[https://github.com/docker/docker/issues/24637](https://github.com/docker/docker/issues/24637)获取更多信息。我们通过将集群绑定到一个*单一*overlay网络`mydb`来解决了这个bug。
- en: Now, we attempted a `docker service create` with the replica factor 1 for a
    WordPress container. We intentionally didn't control where the Wordpress container
    would be scheduled. When we were trying to wire this Wordpress service to the
    MySQL service, however, the connection repeatedly timed out. We concluded that
    for a Wordpress + MySQL combo at this scale, it's much better to put a few constraints
    on the cluster to make all the services run together in the same data center.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们尝试使用复制因子1创建一个WordPress容器的`docker service create`。我们故意没有控制WordPress容器的调度位置。然而，当我们试图将这个WordPress服务与MySQL服务连接时，连接一直超时。我们得出结论，对于这种规模的WordPress
    + MySQL组合，最好在集群上加一些约束，使所有服务在同一数据中心中运行。
- en: Swarm performance at a scale
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模上的Swarm性能
- en: 'What you also learned from this issue was that the performance of the overlay
    network greatly depends on the correct tuning of network configuration on each
    host. As suggested by a Docker engineer, we may experience the "Neighbour Table
    Overflow" error when there are too many ARP requests (when the network is very
    big) and each host is not able to reply. These were the tunables we increased
    on the Docker hosts to fix the following behavior:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个问题中我们还学到，覆盖网络的性能在很大程度上取决于每个主机上网络配置的正确调整。正如一位Docker工程师建议的那样，当有太多的ARP请求（当网络非常大时）并且每个主机无法回复时，我们可能会遇到“邻居表溢出”错误。这些是我们在Docker主机上增加的可调整项，以修复以下行为：
- en: '[PRE36]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, `gc_thresh1` is the expected number of hosts, where `gc_thresh2` is the
    soft limit and `gc_thresh3` is the hard limit.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`gc_thresh1`是预期的主机数量，`gc_thresh2`是软限制，`gc_thresh3`是硬限制。
- en: So, when the MySQL + Wordpress test failed, we changed our plan to experiment
    NGINX on a Routing Mesh.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当MySQL + Wordpress测试失败时，我们改变了计划，尝试在路由网格上实验NGINX。
- en: The ingress network was set up with a /16 pool so, it could accommodate a maximum
    of 64,000 IP addresses. From a suggestion by Alex Ellis, we started 4,000 (four
    thousands!) NGINX containers on the cluster. During this test, nodes were still
    coming in and out. Eventually, a few minutes later, the NGINX service started
    and the Routing Mesh was formed. It could correctly serve, even as some nodes
    kept failing, so this test verified that the Routing Mesh in 1.12.3 is rock solid
    and production ready.We then stopped the NGINX service and started to test the
    scheduling of as many containers as possible, aiming at 1,000,000, one million.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 入口网络设置为/16池，因此最多可以容纳64,000个IP地址。根据Alex Ellis的建议，我们在集群上启动了4,000（四千个！）个NGINX容器。在这个测试过程中，节点仍在不断进出。几分钟后，NGINX服务开始运行，路由网格形成。即使一些节点不断失败，它仍然能够正确提供服务，因此这个测试验证了1.12.3版本中的路由网格是非常稳定且可以投入生产使用的。然后我们停止了NGINX服务，并开始测试尽可能多地调度容器，目标是1,000,000个，一百万个。
- en: So, we created an "alpine top" service, as we did for Swarm2k. However, the
    scheduling rate was a bit slower this time. We reached 47,000 containers in approximately
    30 minutes. Therefore, we foresaw it was going to take approximately ~10.6 hours
    to fill the cluster with our 1,000,000 containers.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们创建了一个“alpine top”服务，就像我们为Swarm2k所做的那样。然而，这次调度速率稍慢。大约30分钟内我们达到了47,000个容器。因此，我们预计填满集群需要大约10.6小时来达到我们的1,000,000个容器。
- en: As that was expected to take too much time, we decided to change plans again
    and go for 70,000 containers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预计会花费太多时间，我们决定再次改变计划，转而选择70,000个容器。
- en: '![Swarm performance at a scale](images/image_04_009.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![规模下的Swarm性能](images/image_04_009.jpg)'
- en: Scheduling a huge number of containers (**docker scale alpine=70000**) stressed
    out the cluster. This created a huge scheduling queue that would not commit until
    all 70,000 containers finished their scheduling. Therefore, when we decided to
    shut down the managers, all scheduling tasks disappeared and the cluster became
    unstable, for the Raft log got corrupted.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 调度大量的容器（**docker scale alpine=70000**）使集群压力山大。这创建了一个巨大的调度队列，直到所有70,000个容器完成调度才会提交。因此，当我们决定关闭管理节点时，所有调度任务都消失了，集群变得不稳定，因为Raft日志已损坏。
- en: On the way, one of the most interesting things we wanted to check by collecting
    CPU profile information was to see what Swarm primitives were loading the cluster
    more.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们想通过收集CPU配置文件信息来检查Swarm原语加载集群的情况。
- en: '![Swarm performance at a scale](images/image_04_010.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![规模下的Swarm性能](images/image_04_010.jpg)'
- en: Here, we can see that only 0.42% of the CPU was spent on the scheduling algorithm.
    We concluded with some approximations that the Docker Swarm scheduling algorithm
    in version 1.12 is quite fast. This means that there is an opportunity to introduce
    a more sophisticated scheduling algorithm that could result in an even better
    resource utilization in future versions of Swarm, by adding just some acceptable
    overhead.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到只有0.42%的CPU用于调度算法。我们得出一些近似值的结论，即Docker Swarm 1.12版本的调度算法非常快。这意味着有机会引入一个更复杂的调度算法，在未来的Swarm版本中可能会导致更好的资源利用，只需增加一些可接受的开销。
- en: '![Swarm performance at a scale](images/image_04_011.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![规模下的Swarm性能](images/image_04_011.jpg)'
- en: Also, we found that a lot of CPU cycles were spent on nodes communication. Here,
    we can see the Libnetwork member list layer. It used ~12% of the overall CPU.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们发现大量的CPU周期被用于节点通信。在这里，我们可以看到Libnetwork成员列表层。它使用了整体CPU的约12%。
- en: '![Swarm performance at a scale](images/image_04_012.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![规模下的Swarm性能](images/image_04_012.jpg)'
- en: However, it seemed that the major CPU consumer was Raft, which also invoked
    the Go garbage collector significantly here. This used ~30% of the overall CPU.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，似乎主要的CPU消耗者是Raft，在这里还显著调用了Go垃圾收集器。这使用了整体CPU的约30%。
- en: Swarm2k and Swarm3k lessons learned
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm2k和Swarm3k的经验教训
- en: 'Here''s a summary of what you learned from these experiments:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从这些实验中学到的总结：
- en: For a large set of workers, managers require a lot of CPUs. CPUs will spike
    whenever the Raft recovery process kicks in.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大量的工作节点，管理者需要大量的CPU。每当Raft恢复过程启动时，CPU就会飙升。
- en: If the leading manager dies, it's better to stop Docker on that node and wait
    until the cluster becomes stable again with n-1 managers.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果领先的管理者死了，最好停止该节点上的Docker，并等待集群再次稳定下来，直到剩下n-1个管理者。
- en: Keep snapshot reservation as small as possible. The default Docker Swarm configuration
    will do. Persisting Raft snapshots uses extra CPU.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量保持快照保留尽可能小。默认的Docker Swarm配置就可以了。持久化Raft快照会额外使用CPU。
- en: Thousands of nodes require a huge set of resources to manage, both in terms
    of CPU and network bandwidth. Try to keep services and the managers' topology
    geographically compact.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成千上万的节点需要大量的资源来管理，无论是在CPU还是网络带宽方面。尽量保持服务和管理者的拓扑地理上紧凑。
- en: Hundreds of thousand tasks require high memory nodes.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数十万个任务需要高内存节点。
- en: Now, a maximum of 500-1000 nodes are recommended for stable production setups.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，稳定的生产设置建议最多500-1000个节点。
- en: If managers seem to be stuck, wait; they'll recover eventually.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果管理者似乎被卡住了，等一等；他们最终会恢复过来。
- en: The `advertise-addr` parameter is mandatory for Routing Mesh to work.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`advertise-addr`参数对于路由网格的工作是必需的。'
- en: Put your compute nodes as close to your data nodes as possible. The overlay
    network is great and will require tweaking Linux net configuration for all hosts
    to make it work best.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将计算节点尽可能靠近数据节点。覆盖网络很好，但需要调整所有主机的Linux网络配置，以使其发挥最佳作用。
- en: Docker Swarm Mode is robust. There were no task failures, even with unpredictable
    network connecting this huge cluster together.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm模式很强大。即使在将这个庞大的集群连接在一起的不可预测的网络情况下，也没有任务失败。
- en: 'For Swarm3k, we would like to thank all the heroes: `@FlorianHeigl`; `@jmaitrehenry`
    from PetalMD; `@everett_toews` from Rackspace, Internet Thailand; `@squeaky_pl`,
    `@neverlock`, `@tomwillfixit` from Demonware; `@sujaypillai` from Jabil; `@pilgrimstack`
    from OVH; `@ajeetsraina` from Collabnix; `@AorJoa` and `@PNgoenthai` from Aiyara
    Cluster; `@GroupSprint3r`, `@toughIQ`, `@mrnonaki`, `@zinuzoid` from HotelQuickly;
    `@_EthanHunt_`; `@packethost` from Packet.io; `@ContainerizeT-ContainerizeThis`,
    The Conference; `@_pascalandy` from FirePress; @lucjuggery from TRAXxs; @alexellisuk;
    @svega from Huli; @BretFisher; `@voodootikigod` from Emerging Technology Advisors;
    `@AlexPostID`; `@gianarb` from ThumpFlow; `@Rucknar`, `@lherrerabenitez`; `@abhisak`
    from Nipa Technology; and `@djalal` from NexwayGroup.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Swarm3k，我们要感谢所有的英雄：来自PetalMD的`@FlorianHeigl`、`@jmaitrehenry`；来自Rackspace的`@everett_toews`、来自Demonware的`@squeaky_pl`、`@neverlock`、`@tomwillfixit`；来自Jabil的`@sujaypillai`；来自OVH的`@pilgrimstack`；来自Collabnix的`@ajeetsraina`；来自Aiyara
    Cluster的`@AorJoa`和`@PNgoenthai`；来自HotelQuickly的`@GroupSprint3r`、`@toughIQ`、`@mrnonaki`、`@zinuzoid`；`@_EthanHunt_`；来自Packet.io的`@packethost`；来自The
    Conference的`@ContainerizeT-ContainerizeThis`；来自FirePress的`@_pascalandy`；来自TRAXxs的@lucjuggery；@alexellisuk；来自Huli的@svega；@BretFisher；来自Emerging
    Technology Advisors的`@voodootikigod`；`@AlexPostID`；来自ThumpFlow的`@gianarb`；`@Rucknar`、`@lherrerabenitez`；来自Nipa
    Technology的`@abhisak`；以及来自NexwayGroup的`@djalal`。
- en: We would also like to thank Sematext again for the best-of-class Docker monitoring
    system; and DigitalOcean for providing us with all resources.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还要再次感谢Sematext提供的最佳Docker监控系统；以及DigitalOcean提供给我们的所有资源。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we showed you how we deployed two huge Swarm clusters over
    Digital Ocean by using belt. These stories gave you much to learn from. We summarized
    the lessons and outlined some tips for running huge production swarms. On the
    go, we also introduced some Swarm features, such as services and security, and
    we discussed managers' topologies. In the next chapter, we'll discuss in detail
    how to administer Swarm. Topics included will be deploying workers with belt,
    scripts and Ansible, managing nodes, monitoring, and graphical interfaces.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您展示了如何使用belt在Digital Ocean上部署了两个庞大的Swarm集群。这些故事给了您很多值得学习的东西。我们总结了这些教训，并概述了一些运行庞大生产集群的技巧。同时，我们还介绍了一些Swarm的特性，比如服务和安全性，并讨论了管理者的拓扑结构。在下一章中，我们将详细讨论如何管理Swarm。包括使用belt、脚本和Ansible部署工作节点，管理节点，监控以及图形界面。
