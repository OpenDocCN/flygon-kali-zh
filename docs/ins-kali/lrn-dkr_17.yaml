- en: Zero-Downtime Deployments and Secrets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署和Secrets
- en: In the previous chapter, we explored Docker Swarm and its resources in detail.
    We learned how to build a highly available swarm locally and in the cloud. Then,
    we discussed Swarm services and stacks in depth. Finally, we created services
    and stacks in the swarm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们详细探讨了Docker Swarm及其资源。我们学习了如何在本地和云中构建高可用的swarm。然后，我们深入讨论了Swarm服务和堆栈。最后，我们在swarm中创建了服务和堆栈。
- en: In this chapter, we will show you how we can update services and stacks running
    in Docker Swarm without interrupting their availability. This is called zero-downtime
    deployment. We are also going to introduce swarm secrets as a means to securely
    provide sensitive information to containers of a service using those secrets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您展示如何在Docker Swarm中更新服务和堆栈而不中断其可用性。这被称为零停机部署。我们还将介绍swarm secrets作为一种安全地向服务的容器提供敏感信息的手段。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Zero-downtime deployment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零停机部署
- en: Storing configuration data in the swarm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在swarm中存储配置数据
- en: Protecting sensitive data with Docker Secrets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker Secrets保护敏感数据
- en: 'After finishing this chapter, you will be able to do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够做到以下事情：
- en: List two to three different deployment strategies commonly used to update a
    service without downtime.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列举两到三种常用的部署策略，用于在不中断的情况下更新服务。
- en: Update a service in batches without causing a service interruption.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量更新服务而不会造成服务中断。
- en: Define a rollback strategy for a service that is used if an update fails.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为服务定义回滚策略，如果更新失败则使用。
- en: Store non-sensitive configuration data using Docker configs.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker配置存储非敏感配置数据。
- en: Use a Docker secret with a service.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker secret与服务。
- en: Update the value of a secret without causing downtime.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新secret的值而不会造成停机时间。
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code files for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition).
    If you have checked out the repository as indicated in [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml),
    *Setting up a Working Environment*, then you'll find the code at `~/fod-solution/ch14`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到[https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition)。如果您已经按照第2章中指示的*设置工作环境*检出了存储库，那么您可以在`~/fod-solution/ch14`找到代码。
- en: Zero-downtime deployment
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署
- en: One of the most important aspects of a mission-critical application that needs
    frequent updates is the ability to do updates in a fashion that requires no outage
    at all. We call this a zero-downtime deployment. At all times, the application
    that is updated must be fully operational.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 需要频繁更新的关键应用程序最重要的一个方面是能够以完全无中断的方式进行更新。我们称之为零停机部署。更新后的应用程序必须始终完全可操作。
- en: Popular deployment strategies
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行的部署策略
- en: 'There are various ways to achieve this. Some of them are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种方法可以实现这一点。其中一些如下：
- en: Rolling updates
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新
- en: Blue-green deployments
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Canary releases
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: Docker Swarm supports rolling updates out of the box. The other two types of
    deployments can be achieved with some extra effort from our side.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm支持开箱即用的滚动更新。其他两种部署类型需要我们额外的努力才能实现。
- en: Rolling updates
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滚动更新
- en: In a mission-critical application, each application service has to run in multiple
    replicas. Depending on the load, that can be as few as two to three instances
    and as many as dozens, hundreds, or thousands of instances. At any given time,
    we want to have a clear majority when it comes to all the service instances running.
    So, if we have three replicas, we want to have at least two of them up and running
    at all times. If we have 100 replicas, we can be content with a minimum of, say,
    90 replicas, being available. By doing this, we can define a batch size of replicas
    that we may take down to upgrade. In the first case, the batch size would be 1
    and in the second case, it would be 10.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键任务应用中，每个应用服务必须以多个副本运行。根据负载的大小，副本可以少至两到三个实例，多至数十、数百或数千个实例。在任何给定时间，我们希望所有服务实例的运行都有明确的多数。因此，如果我们有三个副本，我们希望至少有两个副本一直在运行。如果我们有100个副本，我们可以满足于至少有90个副本可用。通过这样做，我们可以定义一个批量大小的副本，我们可以关闭以进行升级。在第一种情况下，批量大小将为1，在第二种情况下，将为10。
- en: 'When we take replicas down, Docker Swarm will automatically take those instances
    out of the load balancing pool and all traffic will be load balanced across the
    remaining active instances. Those remaining instances will thus experience a slight
    increase in traffic. In the following diagram, prior to the start of the rolling
    update, if **Task A3** wanted to access **Service B**, it could have been load
    balanced to any of the three tasks of **Service B** by SwarmKit. Once the rolling
    update started, SwarmKit took down **Task B1** for updates. Automatically, this
    task is then taken out of the pool of targets. So, if **Task A3** now requests
    to connect to **Service B**, load balancing will only select from the remaining
    tasks, that is, **B2** and **B3**. Thus, those two tasks might experience a higher
    load temporarily:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们关闭副本时，Docker Swarm将自动将这些实例从负载均衡池中移除，所有流量将在剩余的活动实例之间进行负载均衡。因此，这些剩余实例将暂时经历流量的轻微增加。在下图中，在滚动更新开始之前，如果**Task
    A3**想要访问**Service B**，它可能已经被SwarmKit负载均衡到**Service B**的任何三个任务中的一个。一旦滚动更新开始，SwarmKit将关闭**Task
    B1**进行更新。自动地，这个任务就被从目标池中移除。因此，如果**Task A3**现在请求连接到**Service B**，负载均衡将只从剩余的任务中选择，即**B2**和**B3**。因此，这两个任务可能暂时经历更高的负载：
- en: '![](assets/b5692dbe-f8b2-4050-bc4b-04147a063825.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b5692dbe-f8b2-4050-bc4b-04147a063825.png)'
- en: Task B1 is taken down to be updated
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Task B1**被关闭以进行更新'
- en: The stopped instances are then replaced by an equivalent number of new instances
    of the new version of the application service. Once the new instances are up and
    running, we can have the Swarm observe them for a given period of time and make
    sure they're healthy. If all is well, then we can continue by taking down the
    next batch of instances and replacing them with instances of the new version.
    This process is repeated until all the instances of the application service have
    been replaced.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后停止实例，用新版本的应用服务的等效数量的新实例替换它们。一旦新实例正常运行，我们可以让Swarm在一定时间内观察它们，确保它们健康。如果一切正常，那么我们可以继续关闭下一批实例，并用新版本的实例替换它们。这个过程重复进行，直到所有应用服务的实例都被替换。
- en: 'In the following diagram, we can see that **Task B1** of **Service B** has
    been updated to version 2\. The container of **Task B1** was assigned a new **IP**
    address, and it was deployed to another worker node with free resources:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到**Service B**的**Task B1**已更新为版本2。**Task B1**的容器被分配了一个新的**IP**地址，并部署到另一个具有空闲资源的工作节点上：
- en: '![](assets/2e0094c4-5dce-4763-8401-394a87cc79b3.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2e0094c4-5dce-4763-8401-394a87cc79b3.png)'
- en: The first batch being updated in a rolling update
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正在进行滚动更新的第一批
- en: It is important to understand that when the task of a service is updated, in
    most cases, it gets deployed to a different worker node than the one it used to
    live on. But that should be fine as long as the corresponding service is stateless.
    If we have a stateful service that is location- or node-aware and we'd like to
    update it, then we have to adjust our approach, but this is outside of the scope
    of this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，当服务的任务被更新时，在大多数情况下，它会被部署到与其原来所在的不同的工作节点上。但只要相应的服务是无状态的，这应该没问题。如果我们有一个有状态的服务，它是位置或节点感知的，并且我们想要对其进行更新，那么我们必须调整我们的方法，但这超出了本书的范围。
- en: 'Now, let''s look at how we can actually instruct the Swarm to perform a rolling
    update of an application service. When we declare a service in a stack file, we
    can define multiple options that are relevant in this context. Let''s look at
    a snippet of a typical stack file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何实际指示Swarm执行应用服务的滚动更新。当我们在堆栈文件中声明一个服务时，我们可以定义在这种情况下相关的多个选项。让我们看一个典型堆栈文件的片段：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this snippet, we can see a section, `update_config`, with the `parallelism`
    and `delay` properties. `parallelism` defines the batch size of how many replicas
    are going to be updated at a time during a rolling update. `delay` defines how
    long Docker Swarm is going to wait between updating individual batches. In the
    preceding case, we have `10` replicas that are being updated in two instances
    at a time and, between each successful update, Docker Swarm waits for `10` seconds.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段中，我们可以看到一个名为`update_config`的部分，其中包含`parallelism`和`delay`属性。`parallelism`定义了在滚动更新期间一次要更新多少个副本的批处理大小。`delay`定义了Docker
    Swarm在更新单个批次之间要等待多长时间。在前面的例子中，我们有`10`个副本，每次更新两个实例，并且在每次成功更新之间，Docker Swarm等待`10`秒。
- en: Let's test such a rolling update. Navigate to the `ch14` subfolder of our `labs`
    folder and use the `stack.yaml` file to create a web service that's been configured
    for a rolling update. The service uses an Alpine-based Nginx image whose version
    is `1.12-alpine`. We will update the service to a newer version, that is, `1.13-alpine`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试这样一个滚动更新。导航到我们`labs`文件夹的`ch14`子文件夹，并使用`stack.yaml`文件创建一个已配置为滚动更新的web服务。该服务使用基于Alpine的Nginx镜像，版本为`1.12-alpine`。我们将把服务更新到一个更新的版本，即`1.13-alpine`。
- en: 'To start, we will deploy this service to our swarm that we created locally
    in VirtualBox. Let''s take a look:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将把这个服务部署到我们在VirtualBox中本地创建的Swarm。让我们来看一下：
- en: 'First, we need to make sure that we have our Terminal window configured so
    that we can access one of the master nodes of our cluster. Let''s take the leader,
    that is, `node-1`:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要确保我们的终端窗口已配置，以便我们可以访问我们集群的主节点之一。让我们选择领导者，即`node-1`：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can deploy the service using the stack file:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用堆栈文件部署服务：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding command looks like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下：
- en: '![](assets/1f326e7f-883f-4cc7-b643-3844164cc739.png)Deployment of the web stack'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/1f326e7f-883f-4cc7-b643-3844164cc739.png)部署web堆栈'
- en: 'Once the service has been deployed, we can monitor it using the following command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务部署后，我们可以使用以下命令对其进行监视：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will see the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '![](assets/909a831e-a9a3-4ae8-98b1-addeb1ac75a7.png)Service web of the web
    stack running in Swarm with 10 replicasIf you''re working on a macOS machine,
    you need to make sure your watch tool is installed. Use the `brew install watch`
    command to do so.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/909a831e-a9a3-4ae8-98b1-addeb1ac75a7.png)运行在Swarm中的web堆栈的web服务，有10个副本。如果您在macOS机器上工作，您需要确保您安装了watch工具。使用`brew
    install watch`命令来安装。'
- en: The previous command will continuously update the output and provide us with
    a good overview of what happens during the rolling update.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将持续更新输出，并为我们提供滚动更新期间发生的情况的良好概述。
- en: 'Now, we need to open a second Terminal and configure it for remote access for
    the manager node of our swarm. Once we have done that, we can execute the `docker`
    command, which will update the image of the `web` service of the stack, also called
    `web`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要打开第二个终端，并为我们的Swarm的管理节点配置远程访问。一旦我们完成了这一步，我们可以执行`docker`命令，它将更新堆栈的`web`服务的镜像，也称为`web`：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding command leads to the following output, indicating the progress
    of the rolling update:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令导致以下输出，指示滚动更新的进度：
- en: '![](assets/80e17241-6cbe-414b-b393-b874ba9f475a.png)Screen showing the progress
    of the rolling update'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/80e17241-6cbe-414b-b393-b874ba9f475a.png)显示滚动更新进度的屏幕'
- en: The preceding output indicates that the first two batches, each with two tasks,
    have been successful and that the third batch is preparing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出表明，前两批每批两个任务已成功，并且第三批正在准备中。
- en: 'In the first Terminal window, where we''re watching the stack, we should now
    see how Docker Swarm updates the service batch by batch with an interval of `10
    seconds`. After the first batch, it should look like the following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在观看堆栈的第一个终端窗口中，我们现在应该看到Docker Swarm如何以`10秒`的间隔逐批更新服务。第一批之后，它应该看起来像以下截图：
- en: '![](assets/0c203143-48d9-4eb6-8207-bf098224f2d8.png)Rolling update for a service
    in Docker Swarm'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/0c203143-48d9-4eb6-8207-bf098224f2d8.png)Docker Swarm中服务的滚动更新'
- en: In the preceding screenshot, we can see that the first batch of the two tasks,
    `8` and `9`, has been updated. Docker Swarm is waiting for `10 seconds` to proceed
    with the next batch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述截图中，我们可以看到前两个任务`8`和`9`已经更新。Docker Swarm正在等待`10秒`后继续下一批。
- en: It is interesting to note that in this particular case, SwarmKit deploys the
    new version of the task to the same node as the previous version. This is accidental
    since we have five nodes and two tasks on each node. SwarmKit always tries to
    balance the workload evenly across the nodes. So, when SwarmKit takes down a task,
    the corresponding node has a smaller workload than all the others, so the new
    instance is scheduled to it. Normally, you cannot expect to find the new instance
    of a task on the same node. Just try it out yourself by deleting the stack with
    `docker stack rm web` and changing the number of replicas to say, seven, and then
    redeploy and update it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在这种特殊情况下，SwarmKit将任务的新版本部署到与先前版本相同的节点。这是偶然的，因为我们有五个节点，每个节点上有两个任务。SwarmKit始终尝试在节点之间均匀平衡工作负载。因此，当SwarmKit关闭一个任务时，相应的节点的工作负载小于所有其他节点，因此新实例被调度到该节点。通常情况下，您不能期望在同一节点上找到任务的新实例。只需尝试通过删除具有`docker
    stack rm web`并将副本数更改为例如七个，然后重新部署和更新来自己尝试。
- en: 'Once all the tasks have been updated, the output of our `docker stack ps web`
    command will look similar to the following screenshot:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有任务都已更新，我们的`docker stack ps web`命令的输出将类似于以下截图：
- en: '![](assets/4def2ed1-1be6-4416-9e98-041b0ec8d8d6.png)All tasks have been updated
    successfully'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/4def2ed1-1be6-4416-9e98-041b0ec8d8d6.png)所有任务已成功更新'
- en: Please note that SwarmKit does not immediately remove the containers of the
    previous versions of the tasks from the corresponding nodes. This makes sense
    as we might want to, for example, retrieve the logs from those containers for
    debugging purposes, or we might want to retrieve their metadata using `docker
    container inspect`. SwarmKit keeps the four latest terminated task instances around
    before it purges older ones so that it doesn't clog the system with unused resources.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SwarmKit不会立即从相应节点中删除任务的先前版本的容器。这是有道理的，因为我们可能希望，例如，检索这些容器的日志以进行调试，或者我们可能希望使用`docker
    container inspect`检索它们的元数据。SwarmKit在清除旧实例之前会保留最近的四个终止任务实例，以防止它用未使用的资源堵塞系统。
- en: We can use the `--update-order` parameter to instruct Docker to start the new
    container replica before stopping the old one. This can improve application availability.
    Valid values are `"start-first"` and `"stop-first"`. The latter is the default.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`--update-order`参数指示Docker在停止旧容器之前启动新的容器副本。这可以提高应用程序的可用性。有效值为`"start-first"`和`"stop-first"`。后者是默认值。
- en: 'Once we''re done, we can tear down the stack using the following command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以使用以下命令拆除堆栈：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Although using stack files to define and deploy applications is the recommended
    best practice, we can also define the update behavior in a service `create` statement.
    If we just want to deploy a single service, this might be the preferred way of
    doing things. Let''s look at such a `create` command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用堆栈文件来定义和部署应用程序是推荐的最佳实践，但我们也可以在服务`create`语句中定义更新行为。如果我们只想部署单个服务，这可能是做事情的首选方式。让我们看看这样一个`create`命令：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This command defines the same desired state as the preceding stack file. We
    want the service to run with `10` replicas and we want a rolling update to happen
    in batches of two tasks at a time, with a 10-second interval between consecutive
    batches.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令定义了与前面的堆栈文件相同的期望状态。我们希望服务以`10`个副本运行，并且我们希望滚动更新以每次两个任务的批次进行，并且在连续批次之间间隔10秒。
- en: Health checks
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: To make informed decisions, for example, during a rolling update of a Swarm
    service regarding whether or not the just-installed batch of new service instances
    is running OK or if a rollback is needed, the SwarmKit needs a way to know about
    the overall health of the system. On its own, SwarmKit (and Docker) can collect
    quite a bit of information. But there is a limit. Imagine a container containing
    an application. The container, as seen from the outside, can look absolutely healthy
    and carry on just fine. But that doesn't necessarily mean that the application
    running inside the container is also doing well. The application could, for example,
    be in an infinite loop or be in a corrupt state, yet still running. However, as
    long as the application runs, the container runs and from outside, everything
    looks perfect.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出明智的决定，例如在滚动更新Swarm服务期间，关于刚安装的新服务实例批次是否正常运行，或者是否需要回滚，SwarmKit需要一种了解系统整体健康状况的方式。SwarmKit（和Docker）本身可以收集相当多的信息。但是有限制。想象一个包含应用程序的容器。从外部看，容器可能看起来绝对健康，可以正常运行。但这并不一定意味着容器内部运行的应用程序也很好。例如，应用程序可能陷入无限循环或处于损坏状态，但仍在运行。但只要应用程序运行，容器就运行，并且从外部看，一切都看起来完美。
- en: Thus, SwarmKit provides a seam where we can provide it with some help. We, the
    authors of the application services running inside the containers in the swarm,
    know best as to whether or not our service is in a healthy state. SwarmKit gives
    us the opportunity to define a command that is executed against our application
    service to test its health. What exactly this command does is not important to
    Swarm; the command just needs to return `OK`, `NOT OK`, or `time out`. The latter
    two situations, namely `NOT OK` or `timeout`, will tell SwarmKit that the task
    it is investigating is potentially unhealthy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SwarmKit提供了一个接口，我们可以在其中提供一些帮助。我们，即运行在集群容器内部的应用程序服务的作者，最了解我们的服务是否处于健康状态。SwarmKit给了我们定义一个命令的机会，该命令针对我们的应用程序服务进行健康测试。这个命令具体做什么对Swarm来说并不重要；命令只需要返回`OK`、`NOT
    OK`或`超时`。后两种情况，即`NOT OK`或`超时`，将告诉SwarmKit正在调查的任务可能不健康。
- en: 'Here, I am writing potentially on purpose and later, we will see why:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我故意写了一些东西，稍后我们会看到原因：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the preceding snippet from a `Dockerfile`, we can see the keyword `HEALTHCHECK`.
    It has a few options or parameters and an actual command, that is, `CMD`. Let''s
    discuss the options:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在来自Dockerfile的前面的片段中，我们可以看到关键字HEALTHCHECK。它有一些选项或参数和一个实际的命令，即CMD。让我们讨论一下选项：
- en: '`--interval`: Defines the wait time between health checks. Thus, in our case,
    the orchestrator executes a check every `30` seconds.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --interval：定义健康检查之间的等待时间。因此，在我们的情况下，编排器每30秒执行一次检查。
- en: '`--timeout`: This parameter defines how long Docker should wait if the health
    check does not respond until it times out with an error. In our sample, this is
    `10` seconds. Now, if one health check fails, SwarmKit retries a couple of times
    until it gives up and declares the corresponding task as unhealthy and opens the
    door for Docker to kill this task and replace it with a new instance.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --timeout：此参数定义Docker在健康检查不响应时应等待多长时间，直到超时出现错误。在我们的示例中，这是10秒。现在，如果一个健康检查失败，SwarmKit会重试几次，直到放弃并声明相应的任务不健康，并打开Docker杀死该任务并用新实例替换的机会。
- en: The number of retries is defined with the `--retries` parameter. In the preceding
    code, we want to have three retries.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重试次数由--retries参数定义。在前面的代码中，我们希望有三次重试。
- en: Next, we have the start period. Some containers take some time to start up (not
    that this is a recommended pattern, but sometimes it is inevitable). During this
    startup time, the service instance might not be able to respond to health checks.
    With the start period, we can define how long SwarmKit should wait before it executes
    the very first health check and thus give the application time to initialize.
    To define the startup time, we use the `--start-period` parameter. In our case,
    we do the first check after `60` seconds. How long this start period needs to
    be depends on the application and its startup behavior. The recommendation is
    to start with a relatively low value and if you have a lot of false positives
    and tasks that are restarted many times, you might want to increase the time interval.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们有启动周期。有些容器需要一些时间来启动（虽然这不是一种推荐的模式，但有时是不可避免的）。在这个启动时间内，服务实例可能无法响应健康检查。有了启动周期，我们可以定义SwarmKit在执行第一次健康检查之前等待多长时间，从而给应用程序初始化的时间。为了定义启动时间，我们使用--start-period参数。在我们的情况下，我们在60秒后进行第一次检查。启动时间需要多长取决于应用程序及其启动行为。建议是从相对较低的值开始，如果有很多错误的阳性和任务被多次重启，可能需要增加时间间隔。
- en: 'Finally, we define the actual probing command on the last line with the `CMD`
    keyword. In our case, we are defining a request to the `/health` endpoint of `localhost`
    at port `3000` as a probing command. This call is expected to have three possible
    outcomes:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们在最后一行用CMD关键字定义了实际的探测命令。在我们的情况下，我们正在定义对端口3000的localhost的/health端点的请求作为探测命令。这个调用有三种可能的结果：
- en: The command succeeds.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令成功。
- en: The command fails.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令失败。
- en: The command times out.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令超时。
- en: The latter two are treated the same way by SwarmKit. This is the orchestrator
    telling us that the corresponding task might be unhealthy. I did say *might *with
    intent since SwarmKit does not immediately assume the worst-case scenario but
    assumes that this might just be a temporary fluke of the task and that it will
    recover from it. This is the reason why we have a `--retries` parameter. There,
    we can define how many times SwarmKit should retry before it can assume that the
    task is indeed unhealthy, and consequently kill it and reschedule another instance
    of this task on another free node to reconcile the desired state of the service.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: SwarmKit将后两者视为相同。这是编排器告诉我们相应的任务可能不健康。我故意说*可能*，因为SwarmKit并不立即假设最坏的情况，而是假设这可能只是任务的暂时故障，并且它将从中恢复。这就是为什么我们有一个`--retries`参数的原因。在那里，我们可以定义SwarmKit在可以假定任务确实不健康之前应重试多少次，因此杀死它并在另一个空闲节点上重新安排此任务的另一个实例以调和服务的期望状态。
- en: '*Why can we use localhost in our probing command?* This is a very good question,
    and the reason is because SwarmKit, when probing a container running in the Swarm,
    executes this `probing` command inside the container (that is, it does something
    like `docker container exec <containerID> <probing command>`). Thus, the command
    executes in the same network namespace as the application running inside the container.
    In the following diagram, we can see the life cycle of a service task from its
    beginning:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么我们可以在我们的探测命令中使用localhost？*这是一个非常好的问题，原因是因为当SwarmKit在Swarm中运行的容器进行探测时，它在容器内执行这个`探测`命令（也就是说，它做了类似`docker
    container exec <containerID> <probing command>`的事情）。因此，该命令在与容器内运行的应用程序相同的网络命名空间中执行。在下图中，我们可以看到服务任务的生命周期：'
- en: '![](assets/04607fa9-4a95-4188-9437-5db991b5d3b1.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/04607fa9-4a95-4188-9437-5db991b5d3b1.png)'
- en: Service task with transient health failure
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 具有瞬态健康失败的服务任务
- en: First, SwarmKit waits to probe until the start period is over. Then, we have
    our first health check. Shortly thereafter, the task fails when probed. It fails
    two consecutive times but then it recovers. Thus, **health check 4** is successful
    and SwarmKit leaves the task running.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，SwarmKit等待到启动期结束才进行探测。然后，我们进行第一次健康检查。不久之后，任务在探测时失败。它连续失败两次，但然后恢复。因此，**健康检查4**成功了，SwarmKit让任务继续运行。
- en: 'Here, we can see a task that is permanently failing:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到一个永久失败的任务：
- en: '![](assets/69eb3697-f61f-446c-b5cc-3c5d36bbe6d0.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/69eb3697-f61f-446c-b5cc-3c5d36bbe6d0.png)'
- en: Permanent failure of a task
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 任务的永久失败
- en: 'We have just learned how we can define a health check for a service in the
    `Dockerfile` of its image. But this is not the only way we can do this. We can
    also define the health check in the stack file that we use to deploy our application
    into Docker Swarm. Here is a short snippet of what such a stack file would look
    like:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了如何在服务的镜像的`Dockerfile`中定义健康检查。但这并不是我们可以做到这一点的唯一方式。我们还可以在用于将我们的应用程序部署到Docker
    Swarm中的堆栈文件中定义健康检查。以下是这样一个堆栈文件的简短片段：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding snippet, we can see how the health check-related information
    is defined in the stack file. First and foremost, it is important to realize that
    we have to define a health check for every service individually. There is no health
    check at an application or global level.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述片段中，我们可以看到健康检查相关信息是如何在堆栈文件中定义的。首先，首先要意识到的是，我们必须为每个服务单独定义健康检查。没有应用程序或全局级别的健康检查。
- en: Similar to what we defined previously in the `Dockerfile`, the command that
    is used to execute the health check by SwarmKit is `curl -f http://localhost:3000/health`.
    We also have definitions for `interval`, `timeout`, `retries`, and `start_period`.
    These four key-value pairs have the same meaning as the corresponding parameters
    we used in the `Dockerfile`. If there are health check-related settings defined
    in the image, then the ones defined in the stack file override the ones from the
    `Dockerfile`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前在 `Dockerfile` 中定义的类似，SwarmKit 用于执行健康检查的命令是 `curl -f http://localhost:3000/health`。我们还定义了
    `interval`、`timeout`、`retries` 和 `start_period`。这四个键值对的含义与我们在 `Dockerfile` 中使用的相应参数相同。如果镜像中定义了与健康检查相关的设置，那么堆栈文件中定义的设置将覆盖
    `Dockerfile` 中的设置。
- en: 'Now, let''s try to use a service that has a health check defined. In our `lab`
    folder, we have a file called `stack-health.yaml` with the following content:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用一个定义了健康检查的服务。在我们的 `lab` 文件夹中，有一个名为 `stack-health.yaml` 的文件，内容如下：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s deploy this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署这个：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can find out where the single task was deployed to using `docker stack ps
    myapp`. On that particular node, we can list all the containers to find one of
    our stacks. In my example, the task had been deployed to `node-3`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `docker stack ps myapp` 命令找出单个任务部署到了哪里。在特定的节点上，我们可以列出所有容器，找到我们的其中一个堆栈。在我的例子中，任务已经部署到了
    `node-3`：
- en: '![](assets/b39744ac-ae71-456f-b8d1-3b34c99837e2.png)Displaying the health status
    of a running task instance'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/b39744ac-ae71-456f-b8d1-3b34c99837e2.png)显示运行任务实例的健康状态'
- en: The interesting thing in this screenshot is the `STATUS` column. Docker, or
    more precisely, SwarmKit, has recognized that the service has a health check function
    defined and is using it to determine the health of each task of the service.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这张截图中有趣的地方是 `STATUS` 列。Docker，或者更准确地说，SwarmKit，已经识别出服务有一个健康检查函数定义，并且正在使用它来确定服务的每个任务的健康状况。
- en: Rollback
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚
- en: Sometimes, things don't go as expected. A last-minute fix in an application
    release may have inadvertently introduced a new bug, or the new version significantly
    decreases the throughput of the component, and so on. In such cases, we need to
    have a plan B, which in most cases means the ability to roll back the update to
    the previous good version.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，事情并不如预期。应用发布的最后一分钟修复可能无意中引入了一个新的 bug，或者新版本显著降低了组件的吞吐量，等等。在这种情况下，我们需要有一个备用计划，这在大多数情况下意味着能够将更新回滚到之前的良好版本。
- en: As with the update, the rollback has to happen in such a way that it does not
    cause any outages in terms of the application; it needs to cause zero-downtime.
    In that sense, a rollback can be looked at as a reverse update. We are installing
    a new version, yet this new version is actually the previous version.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与更新一样，回滚必须以不会导致应用中断的方式进行。它需要零停机时间。从这个意义上讲，回滚可以被看作是一个反向更新。我们正在安装一个新版本，但这个新版本实际上是之前的版本。
- en: 'As with the update behavior, we can declare, either in our stack files or in
    the Docker service `create` command, how the system should behave in case it needs
    to execute a rollback. Here, we have the stack file that we used previously, but
    this time with some rollback-relevant attributes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与更新行为一样，我们可以在堆栈文件或 Docker 服务 `create` 命令中声明系统在需要执行回滚时应该如何行为。在这里，我们有之前使用的堆栈文件，但这次有一些与回滚相关的属性：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this stack file, which is available in our lab as `stack-rollback.yaml`,
    we defined the details about the rolling update, the health checks, and the behavior
    during rollback. The health check is defined so that after an initial wait time
    of `2` seconds, the orchestrator starts to poll the service on `http://localhost`
    every `2` seconds and it retries `3` times before it considers a task as unhealthy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个堆栈文件中，我们定义了关于滚动更新、健康检查和回滚期间行为的详细信息。健康检查被定义为，在初始等待时间为`2`秒后，编排器开始每`2`秒在`http://localhost`上轮询服务，并在考虑任务不健康之前重试`3`次。
- en: If we do the math, then it takes at least 8 seconds until a task will be stopped
    if it is unhealthy due to a bug. So, now under deploy, we have a new entry called
    `monitor`. This entry defines how long newly deployed tasks should be monitored
    for health and whether or not to continue with the next batch in the rolling update.
    Here, in this sample, we have given it `10` seconds. This is slightly more than
    the 8 seconds we calculated it takes to discover that a defective service has
    been deployed, so this is good.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们做数学计算，那么如果由于错误而导致任务不健康，那么至少需要8秒才能停止任务。因此，现在在部署下，我们有一个名为`monitor`的新条目。该条目定义了新部署的任务应该被监视多长时间以确保其健康，并且是否继续进行滚动更新的下一批任务。在这个示例中，我们给了它`10`秒。这比我们计算出的8秒稍微长一些，可以发现已部署的有缺陷的服务，所以这很好。
- en: We also have a new entry, `failure_action`, which defines what the orchestrator
    will do if it encounters a failure during the rolling update, such as that the
    service is unhealthy. By default, the action is just to stop the whole update
    process and leave the system in an intermediate state. The system is not down
    since it is a rolling update and at least some healthy instances of the service
    are still operational, but an operations engineer would be better at taking a
    look and fixing the problem.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个新条目，`failure_action`，它定义了在滚动更新过程中遇到失败时编排器将采取的行动，例如服务不健康。默认情况下，动作只是停止整个更新过程，并使系统处于中间状态。系统并没有宕机，因为它是一个滚动更新，至少一些健康的服务实例仍然在运行，但运维工程师最好能够查看并解决问题。
- en: In our case, we have defined the action to be a `rollback`. Thus, in case of
    failure, SwarmKit will automatically revert all tasks that have been updated back
    to their previous version.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们已经定义了动作为`rollback`。因此，在失败的情况下，SwarmKit将自动将所有已更新的任务回滚到它们的先前版本。
- en: Blue–green deployments
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: In [Chapter 9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml), *Distributed Application
    Architecture*, we discussed what blue-green deployments are, in an abstract way.
    It turns out that, on Docker Swarm, we cannot really implement blue-green deployments
    for arbitrary services. The service discovery and load balancing between two services
    running in Docker Swarm are part of the Swarm routing mesh and cannot be (easily)
    customized.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml)中，*分布式应用架构*，我们以抽象的方式讨论了蓝绿部署是什么。事实证明，在Docker
    Swarm上，我们不能真正为任意服务实现蓝绿部署。在Docker Swarm中运行的两个服务之间的服务发现和负载均衡是Swarm路由网格的一部分，不能（轻松地）定制。
- en: 'If **Service A** wants to call **Service B**, then Docker does this implicitly.
    Docker, given the name of the target service, will use the Docker **DNS** service
    to resolve this name to a **virtual IP** (**VIP**) address. When the request is
    then targeted at the **VIP**, the Linux **IPVS** service will do another lookup
    in the Linux kernel IP tables with the **VIP** and load balance the request to
    one of the physical IP addresses of the tasks of the service represented by the
    **VIP**, as shown in the following diagram:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果**Service A**想要调用**Service B**，那么Docker会隐式地执行这个操作。给定目标服务的名称，Docker将使用Docker
    **DNS**服务将此名称解析为**虚拟IP**（**VIP**）地址。然后，当请求针对**VIP**时，Linux **IPVS**服务将在Linux内核IP表中使用**VIP**进行另一个查找，并将请求负载均衡到**VIP**所代表的服务的任务的物理IP地址之一，如下图所示：
- en: '![](assets/f7103312-f96e-4301-8f61-6a75c5c74a43.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f7103312-f96e-4301-8f61-6a75c5c74a43.png)'
- en: How service discovery and load balancing work in Docker Swarm
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm中的服务发现和负载均衡是如何工作的
- en: Unfortunately, there is no easy way to intercept this mechanism and replace
    it with a custom behavior. But this would be needed to allow for a true blue-green
    deployment of **Service B**, which is the target service in our example. As we
    will see in [Chapter 16](cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml), *Deploying,
    Updating, and Securing an Application with Kubernetes,* Kubernetes is more flexible
    in this area.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前还没有简单的方法来拦截这种机制并用自定义行为替换它。但这是需要的，以便允许对我们示例中的目标服务**Service B**进行真正的蓝绿部署。正如我们将在[第16章](cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml)中看到的那样，*使用Kubernetes部署、更新和保护应用程序*，Kubernetes在这个领域更加灵活。
- en: That being said, we can always deploy the public-facing services in a blue-green
    fashion. We can use interlock 2 and its layer 7 routing mechanism to allow for
    a true blue-green deployment.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们总是可以以蓝绿方式部署面向公众的服务。我们可以使用interlock 2及其第7层路由机制来实现真正的蓝绿部署。
- en: Canary releases
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: Technically speaking, rolling updates are a kind of canary release. But due
    to their lack of seams, where you could plug customized logic into the system,
    rolling updates are only a very limited version of canary releases.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，滚动更新是金丝雀发布的一种形式。但由于它们缺乏接口，无法将自定义逻辑插入系统中，滚动更新只是金丝雀发布的一个非常有限的版本。
- en: True canary releases require us to have more fine-grained control over the update
    process. Also, true canary releases do not take down the old version of the service
    until 100% of the traffic has been funneled through the new version. In that regard,
    they are treated like blue-green deployments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的金丝雀发布要求我们对更新过程有更精细的控制。此外，真正的金丝雀发布在将100%的流量引导到新版本之前不会关闭旧版本的服务。在这方面，它们被视为蓝绿部署。
- en: In a canary release scenario, we don't just want to use things such as health
    checks as deciding factors regarding whether or not to funnel more and more traffic
    through the new version of the service; we also want to consider external input
    in the decision-making process, such as metrics that are collected and aggregated
    by a log aggregator or tracing information. An example that could be used as a
    decision-maker includes conformance to **service-level agreements** (**SLAs**),
    namely if the new version of the service shows response times that are outside
    of the tolerance band. This can happen if we add new functionality to an existing
    service, yet this new functionality degrades the response time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀发布的情况下，我们不仅希望使用诸如健康检查之类的因素来决定是否将更多的流量引导到新版本的服务中；我们还希望考虑决策过程中的外部输入，例如由日志聚合器收集和聚合的指标或跟踪信息。可以作为决策者的一个示例是符合**服务级别协议**（**SLA**），即如果服务的新版本显示出超出容忍范围的响应时间。如果我们向现有服务添加新功能，但这些新功能降低了响应时间，就会发生这种情况。
- en: Storing configuration data in the swarm
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在swarm中存储配置数据
- en: If we want to store non-sensitive data such as configuration files in Docker
    Swarm, then we can use Docker configs. Docker configs are very similar to Docker
    secrets, which we will discuss in the next section. The main difference is that
    config values are not encrypted at rest, while secrets are. Docker configs can
    only be used in Docker Swarm, that is, they cannot be used in your non-Swarm development
    environment. Docker configs are mounted directly into the container's filesystem.
    Configuration values can either be strings or binary values up to a size of 500
    KB.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在Docker Swarm中存储诸如配置文件之类的非敏感数据，那么我们可以使用Docker配置。Docker配置与Docker秘密非常相似，我们将在下一节中讨论。主要区别在于配置值在静止状态下没有加密，而秘密有。Docker配置只能在Docker
    Swarm中使用，也就是说，它们不能在非Swarm开发环境中使用。Docker配置直接挂载到容器的文件系统中。配置值可以是字符串，也可以是二进制值，最大大小为500
    KB。
- en: With the use of Docker configs, you can separate the configuration from Docker
    images and containers. This way, your services can easily be configured with environment-specific
    values. The production swarm environment has different configuration values than
    the staging swarm, which in turn has different config values than the development
    or integration environment.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Docker配置，您可以将配置与Docker镜像和容器分离。这样，您的服务可以轻松地使用特定于环境的值进行配置。生产swarm环境的配置值与分期swarm的配置值不同，而后者又与开发或集成环境的配置值不同。
- en: We can add configs to services and also remove them from running services. Configs
    can even be shared among different services running in the swarm.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向服务添加配置，也可以从运行中的服务中删除配置。配置甚至可以在swarm中运行的不同服务之间共享。
- en: 'Now, let''s create some Docker configs:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一些Docker配置：
- en: 'First, we start with a simple string value:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从一个简单的字符串值开始：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding command creates the `Hello world` configuration value and uses
    it as input to the config named `hello-config`. The output of this command is
    the unique `ID` of this new config that's being stored in the swarm.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的命令创建了`Hello world`配置值，并将其用作名为`hello-config`的配置的输入。此命令的输出是存储在swarm中的这个新配置的唯一`ID`。
- en: 'Let''s see what we got and use the list command to do so:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么，并使用列表命令来这样做：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the list command shows the `ID` and the `NAME` of the config
    we just created, as well as its `CREATED` and (last) updated time. But since configs
    are non-confidential, we can do more and even output the content of a config,
    like so:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表命令的输出显示了我们刚刚创建的配置的`ID`和`NAME`，以及其`CREATED`和（最后）更新时间。但由于配置是非机密的，我们可以做更多的事情，甚至输出配置的内容，就像这样：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Hmmm, interesting. In the `Spec` subnode of the preceding JSON-formatted output,
    we have the `Data` key with a value of `SGVsbG8gd29ybGQK`. Didn''t we just say
    that the config data is not encrypted at rest? It turns out that the value is
    just our string encoded as `base64`, as we can easily verify:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，有趣。在前面的JSON格式输出的`Spec`子节点中，我们有一个`Data`键，其值为`SGVsbG8gd29ybGQK`。我们不是刚说过配置数据在静止状态下没有加密吗？原来这个值只是我们的字符串编码为`base64`，我们可以很容易地验证：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So far, so good.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很好。
- en: 'Now, let''s define a somewhat more complicated Docker config. Let''s assume
    we are developing a Java application. Java''s preferred way of passing configuration
    data to the application is the use of so-called `properties` files. A `properties`
    file is just a text file containing a list of key-value pairs. Let''s take a look:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个稍微复杂一些的Docker配置。假设我们正在开发一个Java应用程序。Java传递配置数据给应用程序的首选方式是使用所谓的“属性”文件。`属性`文件只是一个包含键值对列表的文本文件。让我们来看一下：
- en: 'Let''s create a file called `my-app.properties` and add the following content
    to it:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`my-app.properties`的文件，并将以下内容添加到其中：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Save the file and create a Docker config called `app.properties` from it:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存文件并从中创建一个名为`app.properties`的Docker配置：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can use this (somewhat contrived) command to get the clear text value
    of the config we just created:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个（有些牵强的）命令来获取我们刚刚创建的配置的明文值：
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is exactly what we expected.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们预期的。
- en: 'Now, let''s create a Docker service that uses the preceding config. For simplicity,
    we will be using the nginx image to do so:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个使用前述配置的Docker服务。为简单起见，我们将使用nginx镜像来实现：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The interesting part in the preceding service `create` command is the line that
    contains `--config`. With this line, we're telling Docker to use the config named
    `app.properties` and mount it as a file at `/etc/my-app/conf/app.properties` inside
    the container. Furthermore, we want that file to have the mode `0440` assigned
    to it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的服务`create`命令中有趣的部分是包含`--config`的那一行。通过这一行，我们告诉Docker使用名为`app.properties`的配置，并将其挂载为一个文件到容器内的`/etc/my-app/conf/app.properties`。此外，我们希望该文件具有`0440`的模式。
- en: 'Let''s see what we got:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们得到了什么：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the preceding output, we can see that the only instance of the service is
    running on node `node-1`. On this node, I can now list the containers to get the
    `ID` of the nginx instance:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，我们可以看到服务的唯一实例正在节点`node-1`上运行。在这个节点上，我现在可以列出容器以获取nginx实例的`ID`：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we can `exec` into that container and output the value of the `/etc/my-app/conf/app.properties`
    file:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以`exec`进入该容器并输出`/etc/my-app/conf/app.properties`文件的值：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: No surprise here; this is exactly what we expected.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无意外；这正是我们预期的。
- en: 'Docker configs can, of course, also be removed from the swarm, but only if
    they are not being used. If we try to remove the config we were just using previously,
    without first stopping and removing the service, we would get the following output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Docker配置也可以从集群中移除，但前提是它们没有被使用。如果我们尝试移除之前使用过的配置，而没有先停止和移除服务，我们会得到以下输出：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We get an error message in which Docker is nice enough to tell us that the config
    is being used by our service called `nginx`. This behavior is somewhat similar
    to what we are used to when working with Docker volumes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了一个错误消息，其中Docker友好地告诉我们该配置正在被我们称为`nginx`的服务使用。这种行为与我们在使用Docker卷时所习惯的有些相似。
- en: 'Thus, first, we need to remove the service and then we can remove the config:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先我们需要移除服务，然后我们可以移除配置：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It is important to note once more that Docker configs should never be used to
    store confidential data such as secrets, passwords, or access keys and key secrets.In
    the next section, we will discuss how to handle confidential data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 需要再次注意的是，Docker配置绝不应该用于存储诸如密码、秘钥或访问密钥等机密数据。在下一节中，我们将讨论如何处理机密数据。
- en: Protecting sensitive data with Docker secrets
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker secrets保护敏感数据
- en: Secrets are used to work with confidential data in a secure way. Swarm secrets
    are secure at rest and in transit. That is, when a new secret is created on a
    manager node, and it can only be created on a manager node, its value is encrypted
    and stored in the raft consensus storage. This is why it is secure at rest. If
    a service gets a secret assigned to it, then the manager reads the secret from
    storage, decrypts it, and forwards it to all the containers who are instances
    of the swarm service that requested the secret. Since node-to-node communication
    in Docker Swarm uses mutual **transport layer security** (**TLS**), the secret
    value, although decrypted, is still secure in transit. The manager forwards the
    secret only to the worker nodes that a service instance is running on. Secrets
    are then mounted as files into the target container. Each secret corresponds to
    a file. The name of the secret will be the name of the file inside the container,
    and the value of the secret is the content of the respective file. Secrets are
    never stored on the filesystem of a worker node and are instead mounted using
    `tmpFS` into the container. By default, secrets are mounted into the container
    at `/run/secrets`, but you can change that to any custom folder.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密用于以安全的方式处理机密数据。Swarm秘密在静态和传输中是安全的。也就是说，当在管理节点上创建新的秘密时，它只能在管理节点上创建，其值会被加密并存储在raft一致性存储中。这就是为什么它在静态时是安全的。如果一个服务被分配了一个秘密，那么管理节点会从存储中读取秘密，解密它，并将其转发给请求秘密的swarm服务的所有容器实例。由于Docker
    Swarm中的节点之间通信使用了**传输层安全**（**TLS**），即使解密了，秘密值在传输中仍然是安全的。管理节点只将秘密转发给服务实例正在运行的工作节点。然后，秘密被挂载为文件到目标容器中。每个秘密对应一个文件。秘密的名称将成为容器内文件的名称，秘密的值将成为相应文件的内容。秘密永远不会存储在工作节点的文件系统上，而是使用`tmpFS`挂载到容器中。默认情况下，秘密被挂载到容器的`/run/secrets`目录中，但您可以将其更改为任何自定义文件夹。
- en: It is important to note that secrets will not be encrypted on Windows nodes
    since there is no concept similar to `tmpfs`. To achieve the same level of security
    that you would get on a Linux node, the administrator should encrypt the disk
    of the respective Windows node.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在Windows节点上，秘密不会被加密，因为没有类似于`tmpfs`的概念。为了达到在Linux节点上获得的相同安全级别，管理员应该加密相应Windows节点的磁盘。
- en: Creating secrets
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建秘密
- en: 'First, let''s see how we can actually create a secret:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们实际上如何创建一个秘密：
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This command creates a secret called `sample-secret` with the `sample secret
    value` value. Please note the hyphen at the end of the `docker secret create`
    command. This means that Docker expects the value of the secret from standard
    input. This is exactly what we're doing by piping the `sample secret value` value
    into the `create` command.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令创建了一个名为`sample-secret`的秘密，其值为`sample secret value`。请注意`docker secret create`命令末尾的连字符。这意味着Docker期望从标准输入获取秘密的值。这正是我们通过将`sample
    secret value`值传输到`create`命令中所做的。
- en: 'Alternatively, we can use a file as the source for the secret value:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用文件作为秘密值的来源：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, the value of the secret with the name `other-secret` is read from a file
    called `~/my-secrets/secret-value.txt`. Once a secret has been created, there
    is no way to access the value of it. We can, for example, list all our secrets
    to get the following output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，具有名称`other-secret`的秘密的值是从名为`~/my-secrets/secret-value.txt`的文件中读取的。一旦创建了一个秘密，就没有办法访问它的值。例如，我们可以列出所有的秘密来获取以下输出：
- en: '![](assets/2b31dcdd-9f1d-44eb-ac20-1d8263bf8f1c.png)List of all secrets'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2b31dcdd-9f1d-44eb-ac20-1d8263bf8f1c.png)所有秘密的列表'
- en: 'In this list, we can only see the `ID` and `NAME` of the secret, plus some
    other metadata, but the actual value of the secret is not visible. We can also
    use `inspect` on a secret, for example, to get more information about `other-secret`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中，我们只能看到秘密的`ID`和`名称`，以及一些其他元数据，但秘密的实际值是不可见的。我们也可以对秘密使用`inspect`，例如，获取有关`other-secret`的更多信息：
- en: '![](assets/82cad794-8a35-44db-81d7-51a9bbd67b29.png)Inspecting a swarm secret'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/82cad794-8a35-44db-81d7-51a9bbd67b29.png)检查集群秘密'
- en: 'Even here, we do not get the value of the secret back. This is, of course,
    intentional: a secret is a secret and thus needs to remain confidential. We can
    assign labels to secrets if we want and we can even use a different driver to
    encrypt and decrypt the secret if we''re not happy with what Docker delivers out
    of the box.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在这里，我们也无法获取秘密的值。当然，这是有意的：秘密是秘密，因此需要保密。如果我们愿意，我们可以为秘密分配标签，甚至可以使用不同的驱动程序来加密和解密秘密，如果我们对Docker默认提供的不满意的话。
- en: Using a secret
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用秘密
- en: 'Secrets are used by services that run in the swarm. Usually, secrets are assigned
    to a service at creation time. Thus, if we want to run a service called `web`
    and assign it a secret, say, `api-secret-key`, the syntax would look as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 秘密被用于在集群中运行的服务。通常，秘密在创建服务时分配。因此，如果我们想要运行一个名为`web`的服务并分配一个名为`api-secret-key`的秘密，语法如下：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This command creates a service called `web` based on the `fundamentalsofdocker/whoami:latest`
    image, publishes the container port `8000` to port `8000` on all swarm nodes,
    and assigns it the secret called `api-secret-key`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令基于`fundamentalsofdocker/whoami:latest`镜像创建了一个名为`web`的服务，将容器端口`8000`发布到所有集群节点的端口`8000`，并分配了名为`api-secret-key`的秘密。
- en: 'This will only work if the secret called `api-secret-key` is defined in the
    swarm; otherwise, an error will be generated with the text `secret not found:
    api-secret-key`. Thus, let''s create this secret now:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '只有在集群中定义了名为`api-secret-key`的秘密时，这才有效；否则，将生成一个带有文本`secret not found: api-secret-key`的错误。因此，让我们现在创建这个秘密：'
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, if we rerun the service `create` command, it will succeed:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们重新运行服务`create`命令，它将成功：
- en: '![](assets/4dd798d2-4a33-41b6-ae25-39f71d55386e.png)Creating a service with
    a secret'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/4dd798d2-4a33-41b6-ae25-39f71d55386e.png)使用秘密创建服务'
- en: 'Now, we can use `docker service ps web` to find out on which node the sole
    service instance has been deployed, and then `exec` into this container. In my
    case, the instance has been deployed to `node-3`, so I need to `SSH` into that
    node:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`docker service ps web`来找出唯一服务实例部署在哪个节点上，然后`exec`进入这个容器。在我的情况下，该实例已部署到`node-3`，所以我需要`SSH`进入该节点：
- en: '[PRE29]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, I list all my containers on that node to find the one instance belonging
    to my service and copy its `container ID`. We can then run the following command
    to make sure that the secret is indeed available inside the container under the
    expected filename containing the secret value in clear text:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我列出该节点上的所有容器，找到属于我的服务的一个实例并复制其`容器ID`。然后，我们可以运行以下命令，确保秘密确实在容器内以明文形式的预期文件名中可用：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once again, in my case, this looks like this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，在我的情况下，这看起来是这样的：
- en: '![](assets/397cb02b-4760-4f21-9a10-0364294c20b9.png)A secret as a container
    sees it'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/397cb02b-4760-4f21-9a10-0364294c20b9.png)容器看到的秘密'
- en: 'If, for some reason, the default location where Docker mounts the secrets inside
    the container is not acceptable to you, you can define a custom location. In the
    following command, we mount the secret to `/app/my-secrets`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某种原因，Docker在容器内部挂载秘密的默认位置不可接受，您可以定义一个自定义位置。在下面的命令中，我们将秘密挂载到`/app/my-secrets`：
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In this command, we are using the extended syntax to define a secret that includes
    the destination folder.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，我们使用了扩展语法来定义一个包括目标文件夹的秘密。
- en: Simulating secrets in a development environment
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在开发环境中模拟秘密
- en: When working in development, we usually don't have a local swarm on our machine.
    But secrets only work in a swarm. So, *what can we do*? Well, luckily, this answer
    is really simple. Due to the fact that secrets are treated as files, we can easily
    mount a volume that contains the secrets into the container to the expected location,
    which by default is at `/run/secrets`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发中，我们通常在本地没有一个swarm。但是秘密只在swarm中起作用。*我们能做什么呢*？幸运的是，这个答案非常简单。由于秘密被视为文件，我们可以轻松地将包含秘密的卷挂载到容器中的预期位置，这个位置默认为`/run/secrets`。
- en: 'Let''s assume that we have a folder called `./dev-secrets` on our local workstation.
    For each secret, we have a file named the same as the secret name and with the
    unencrypted value of the secret as the content of the file. For example, we can
    simulate a secret called `demo-secret` with a secret value of `demo secret value`
    by executing the following command on our workstation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在本地工作站上有一个名为`./dev-secrets`的文件夹。对于每个秘密，我们都有一个与秘密名称相同且具有未加密值的文件作为文件内容。例如，我们可以通过在工作站上执行以下命令来模拟一个名为`demo-secret`的秘密，其秘密值为`demo
    secret value`：
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we can create a container that mounts this folder, like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建一个容器，挂载这个文件夹，就像这样：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The process running inside the container will be unable to distinguish these
    mounted files from the ones originating from a secret. So, for example, `demo-secret`
    is available as a file called `/run/secrets/demo-secret` inside the container
    and has the expected value `demo secret value`. Let''s take a look at this in
    more detail in the following steps:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 容器内运行的进程将无法区分这些挂载的文件和来自秘密的文件。因此，例如，`demo-secret`在容器内作为名为`/run/secrets/demo-secret`的文件可用，并具有预期值`demo
    secret value`。让我们在以下步骤中更详细地看一下这个情况：
- en: 'To test this, we can `exec` a shell inside the preceding container:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试这一点，我们可以在前面的容器中`exec`一个shell：
- en: '[PRE34]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we can navigate to the `/run/secrets` folder and display the content of
    the `demo-secret` file:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以导航到`/run/secrets`文件夹，并显示`demo-secret`文件的内容：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Next, we will be looking at secrets and legacy applications.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将研究秘密和遗留应用程序。
- en: Secrets and legacy applications
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 秘密和遗留应用程序
- en: Sometimes, we want to containerize a legacy application that we cannot easily,
    or do not want to, change. This legacy application might expect a secret value
    to be available as an environment variable. *How are we going to deal with this
    now?* Docker presents us with the secrets as files but the application is expecting
    them in the form of environment variables.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们希望将无法轻松或不想更改的遗留应用程序容器化。这个遗留应用程序可能希望将秘密值作为环境变量可用。*那么我们现在该怎么办呢？* Docker将秘密呈现为文件，但应用程序期望它们以环境变量的形式存在。
- en: 'In this situation, it is helpful to define a script that runs when the container
    is started (a so-called entry point or startup script). This script will read
    the secret value from the respective file and define an environment variable with
    the same name as the file, assigning the new variable the value read from the
    file. In the case of a secret called `demo-secret` whose value should be available
    in an environment variable called `DEMO_SECRET,` the necessary code snippet in
    this startup script could look like this:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，定义一个在容器启动时运行的脚本是有帮助的（称为入口点或启动脚本）。这个脚本将从相应的文件中读取秘密值，并定义一个与文件名相同的环境变量，将新变量赋予从文件中读取的值。对于一个名为`demo-secret`的秘密，其值应该在名为`DEMO_SECRET`的环境变量中可用，这个启动脚本中必要的代码片段可能如下所示：
- en: '[PRE36]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Similarly, let''s say we have a legacy application that expects the secret
    values to be present as an entry in, say, a YAML configuration file located in
    the `/app/bin` folder and called `app.config`, whose relevant part looks like
    this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，假设我们有一个旧应用程序，它期望将秘密值作为一个条目存在于位于`/app/bin`文件夹中的一个名为`app.config`的YAML配置文件中，其相关部分如下所示：
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Our initialization script now needs to read the secret value from the `secret`
    file and replace the corresponding placeholder in the config file with the `secret`
    value. For `demo-secret`, this could look like this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始化脚本现在需要从`secret`文件中读取秘密值，并用`secret`值替换配置文件中的相应占位符。对于`demo-secret`，这可能看起来像这样：
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In the preceding snippet, we're using the `sed` tool to replace a placeholder
    with a value in place. We can use the same technique for the other two secrets
    in the config file.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的片段中，我们使用`sed`工具来替换占位符为实际值。我们可以使用相同的技术来处理配置文件中的其他两个秘密。
- en: We put all the initialization logic into a file called `entrypoint.sh`, make
    this file executable and, for example, add it to the root of the container's filesystem.
    Then, we define this file as `ENTRYPOINT` in the `Dockerfile`, or we can override
    the existing `ENTRYPOINT` of an image in the `docker container run` command.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有的初始化逻辑放入一个名为`entrypoint.sh`的文件中，使该文件可执行，并将其添加到容器文件系统的根目录。然后，在`Dockerfile`中将此文件定义为`ENTRYPOINT`，或者我们可以在`docker
    container run`命令中覆盖镜像的现有`ENTRYPOINT`。
- en: 'Let''s make a sample. Let''s assume that we have a legacy application running
    inside a container defined by the `fundamentalsofdocker/whoami:latest` image that
    expects a secret called `db_password` to be defined in a file, `whoami.conf`,
    in the application folder. Let''s take a look at these steps:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个示例。假设我们有一个旧应用程序运行在由`fundamentalsofdocker/whoami:latest`镜像定义的容器中，该应用程序期望在应用程序文件夹中的一个名为`whoami.conf`的文件中定义一个名为`db_password`的秘密。让我们看看这些步骤：
- en: 'We can define a file, `whoami.conf`, on our local machine that contains the
    following content:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在本地机器上定义一个名为`whoami.conf`的文件，其中包含以下内容：
- en: '[PRE40]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The important part is line 3 of this snippet. It defines where the secret value
    has to be put by the startup script.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段的第3行是重要的部分。它定义了启动脚本必须放置秘密值的位置。
- en: 'Let''s add a file called `entrypoint.sh` to the local folder that contains
    the following content:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在本地文件夹中添加一个名为`entrypoint.sh`的文件，其中包含以下内容：
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The last line in the preceding script stems from the fact that this is the start
    command that was used in the original `Dockerfile`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本中的最后一行源自于原始`Dockerfile`中使用的启动命令。
- en: 'Now, change the mode of this file to an executable:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将此文件的模式更改为可执行：
- en: '[PRE42]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now, we define a `Dockerfile` that inherits from the `fundamentalsofdocker/whoami:latest`
    image.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个继承自`fundamentalsofdocker/whoami:latest`镜像的`Dockerfile`。
- en: 'Add a file called `Dockerfile` to the current folder that contains the following
    content:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在当前文件夹中添加一个名为`Dockerfile`的文件，其中包含以下内容：
- en: '[PRE43]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s build the image from this `Dockerfile`:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从这个`Dockerfile`构建镜像：
- en: '[PRE44]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Once the image has been built, we can run a service from it. But before we
    can do that, we need to define the secret in Swarm:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建完镜像后，我们可以从中运行一个服务。但在这之前，我们需要在Swarm中定义秘密：
- en: '[PRE45]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, we can create a service that uses the following secret:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个使用以下秘密的服务：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Updating secrets
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新秘密
- en: At times, we need to update a secret in a running service since secrets could
    be leaked out to the public or be stolen by malicious people, such as hackers.
    In this case, we need to change our confidential data since the moment it is leaked
    to a non-trusted entity, it has to be considered as insecure.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要更新运行中的服务中的秘密，因为秘密可能会泄露给公众，或者被恶意人士，如黑客，窃取。在这种情况下，我们需要更改我们的机密数据，因为一旦它泄露给不受信任的实体，它就必须被视为不安全。
- en: Updating secrets, like any other update, has to happen in a way that requires
    zero-downtime. Docker SwarmKit supports us in this regard.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 更新秘密，就像任何其他更新一样，必须以零停机的方式进行。Docker SwarmKit在这方面支持我们。
- en: 'First, we create a new secret in the swarm. It is recommended to use a versioning
    strategy when doing so. In our example, we use a version as a postfix of the secret
    name. We originally started with the secret named `db-password` and now the new
    version of this secret is called `db-password-v2`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在Swarm中创建一个新的秘密。建议在这样做时使用版本控制策略。在我们的例子中，我们使用版本作为秘密名称的后缀。我们最初使用名为`db-password`的秘密，现在这个秘密的新版本被称为`db-password-v2`：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let''s assume that the original service that used the secret had been created
    like this:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设使用该秘密的原始服务是这样创建的：
- en: '[PRE48]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The application running inside the container was able to access the secret
    at `/run/secrets/db-password`. Now, SwarmKit does not allow us to update an existing
    secret in a running service, so we have to remove the now obsolete version of
    the secret and then add the new one. Let''s start with removal with the following
    command:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 容器内运行的应用程序能够访问`/run/secrets/db-password`处的秘密。现在，SwarmKit不允许我们在运行中的服务中更新现有的秘密，因此我们必须删除现在过时的秘密版本，然后添加新的秘密。让我们从以下命令开始删除：
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, we can add the new secret with the following command:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令添加新的秘密：
- en: '[PRE50]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Please note the extended syntax of `--secret-add` with the `source` and `target`
    parameters.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`--secret-add`的扩展语法，其中包括`source`和`target`参数。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how SwarmKit allows us to update services without
    requiring downtime. We also discussed the current limits of SwarmKit in regard
    to zero-downtime deployments. In the second part of this chapter, we introduced
    secrets as a means to provide confidential data to services in a highly secure
    way.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了SwarmKit如何允许我们更新服务而不需要停机。我们还讨论了SwarmKit在零停机部署方面的当前限制。在本章的第二部分，我们介绍了秘密作为一种以高度安全的方式向服务提供机密数据的手段。
- en: In the next chapter, we will introduce the currently most popular container
    orchestrator, Kubernetes. We'll discuss the objects that are used to define and
    run a distributed, resilient, robust, and highly available application in a Kubernetes
    cluster. Furthermore, this chapter will familiarize us with MiniKube, a tool that's
    used to locally deploy a Kubernetes application, and also demonstrate the integration
    of Kubernetes with Docker for macOS and Docker for Windows.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍目前最流行的容器编排器Kubernetes。我们将讨论用于在Kubernetes集群中定义和运行分布式、弹性、健壮和高可用应用程序的对象。此外，本章还将使我们熟悉MiniKube，这是一个用于在本地部署Kubernetes应用程序的工具，并演示Kubernetes与Docker
    for macOS和Docker for Windows的集成。
- en: Questions
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'To assess your understanding of the topics that were discussed in this chapter,
    please answer the following questions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估您对本章讨论的主题的理解，请回答以下问题：
- en: In a few simple sentences, explain to an interested layman what zero-downtime
    deployment means.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用简洁的语句向一个感兴趣的外行解释什么是零停机部署。
- en: How does SwarmKit achieve zero-downtime deployments?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SwarmKit如何实现零停机部署？
- en: Contrary to traditional (non-containerized) systems, why does a rollback in
    Docker Swarm just work? Explain this in a few short sentences.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与传统的（非容器化）系统相反，为什么Docker Swarm中的回滚可以正常工作？用简短的句子解释一下。
- en: Describe two to three characteristics of a Docker secret.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述Docker秘密的两到三个特征。
- en: 'You need to roll out a new version of the `inventory` service. What does your
    command look like? Here is some more information:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要推出`inventory`服务的新版本。您的命令是什么样的？以下是更多信息：
- en: The new image is called `acme/inventory:2.1`.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新镜像名为`acme/inventory:2.1`。
- en: We want to use a rolling update strategy with a batch size of two tasks.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望使用批量大小为两个任务的滚动更新策略。
- en: We want the system to wait for one minute after each batch.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望系统在每个批次之后等待一分钟。
- en: You need to update an existing service named `inventory` with a new password
    that is provided through a Docker secret. The new secret is called `MYSQL_PASSWORD_V2`.
    The code in the service expects the secret to be called `MYSQL_PASSWORD`. What
    does the update command look like? (Note that we do not want the code of the service
    to be changed!)
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要更新名为`inventory`的现有服务的密码，该密码通过Docker secret提供。新的秘密称为`MYSQL_PASSWORD_V2`。服务中的代码期望秘密被称为`MYSQL_PASSWORD`。更新命令是什么样子？（请注意，我们不希望更改服务的代码！）
- en: Further reading
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多阅读
- en: 'Here are some links to external sources:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些外部来源的链接：
- en: Apply rolling updates to a service, at [https://dockr.ly/2HfGjlD](https://dockr.ly/2HfGjlD)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对服务应用滚动更新，网址为[https://dockr.ly/2HfGjlD](https://dockr.ly/2HfGjlD)
- en: Managing sensitive data with Docker secrets, at [https://dockr.ly/2vUNbuH](https://dockr.ly/2vUNbuH)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker secrets管理敏感数据，网址为[https://dockr.ly/2vUNbuH](https://dockr.ly/2vUNbuH)
- en: Introducing Docker secrets management, at [https://dockr.ly/2k7zwzE](https://dockr.ly/2k7zwzE)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Docker secrets管理，网址为[https://dockr.ly/2k7zwzE](https://dockr.ly/2k7zwzE)
- en: From env variables to Docker secrets, at [https://bit.ly/2GY3UUB](https://bit.ly/2GY3UUB)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从环境变量到Docker secrets，网址为[https://bit.ly/2GY3UUB](https://bit.ly/2GY3UUB)
