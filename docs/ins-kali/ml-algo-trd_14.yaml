- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Text Data for Trading – Sentiment Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交易的文本数据-情感分析
- en: This is the first of three chapters dedicated to extracting signals for algorithmic
    trading strategies from text data using **natural language processing** (**NLP**)
    and **machine learning** (**ML**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这是专门从文本数据中提取算法交易策略信号的三章之一，使用自然语言处理（NLP）和机器学习（ML）。
- en: Text data is very rich in content but highly unstructured, so it requires more
    preprocessing to enable an ML algorithm to extract relevant information. A key
    challenge consists of converting text into a numerical format without losing its
    meaning. We will cover several techniques capable of capturing the nuances of
    language so that they can be used as input for ML algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据在内容上非常丰富，但结构非常不规则，因此需要更多的预处理才能使ML算法提取相关信息。一个关键挑战在于将文本转换为数字格式而不丢失其含义。我们将介绍几种能够捕捉语言细微差别的技术，以便它们可以用作ML算法的输入。
- en: In this chapter, we will introduce fundamental **feature extraction** techniques
    that focus on individual semantic units, that is, words or short groups of words
    called **tokens**. We will show how to represent documents as vectors of token
    counts by creating a document-term matrix and then proceed to use it as input
    for **news classification** and **sentiment analysis**. We will also introduce
    the naive Bayes algorithm, which is popular for this purpose.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍基本的特征提取技术，重点放在个体语义单元上，即称为标记的单词或短语组。我们将展示如何通过创建文档-术语矩阵将文档表示为标记计数的向量，然后将其用作新闻分类和情感分析的输入。我们还将介绍朴素贝叶斯算法，这在这个目的上很受欢迎。
- en: In the following two chapters, we build on these techniques and use ML algorithms
    such as topic modeling and word-vector embeddings to capture the information contained
    in a broader context.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两章中，我们将基于这些技术，使用主题建模和词向量嵌入等ML算法来捕获更广泛上下文中包含的信息。
- en: 'In particular in this chapter, we will cover the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在本章中，我们将涵盖以下内容：
- en: What the fundamental NLP workflow looks like
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP的基本工作流程是什么样的
- en: How to build a multilingual feature extraction pipeline using spaCy and TextBlob
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用spaCy和TextBlob构建多语言特征提取管道
- en: Performing NLP tasks such as **part-of-speech** (**POS**) tagging or named entity
    recognition
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行NLP任务，如词性标注或命名实体识别
- en: Converting tokens to numbers using the document-term matrix
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文档-术语矩阵将标记转换为数字
- en: Classifying text using the naive Bayes model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯模型对文本进行分类
- en: How to perform sentiment analysis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何进行情感分析
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: ML with text data – from language to features
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据的机器学习-从语言到特征
- en: Text data can be extremely valuable given how much information humans communicate
    and store using natural language. The diverse set of data sources relevant to
    financial investments range from formal documents like company statements, contracts,
    and patents, to news, opinion, and analyst research or commentary, to various
    types of social media postings or messages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于人类使用自然语言进行沟通和存储信息的丰富程度，文本数据可能非常有价值。与金融投资相关的各种数据来源范围广泛，从公司声明、合同和专利等正式文件，到新闻、观点和分析研究或评论，再到各种类型的社交媒体帖子或消息。
- en: Numerous and diverse text data samples are available online to explore the use
    of NLP algorithms, many of which are listed among the resources included in this
    chapter's `README` file on GitHub. For a comprehensive introduction, see Jurafsky
    and Martin (2008).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在线提供了许多不同的文本数据样本，可以探索NLP算法的使用，其中许多列在本章的GitHub存储库的`README`文件中。有关全面介绍，请参阅Jurafsky和Martin（2008）。
- en: To realize the potential value of text data, we'll introduce the specialized
    NLP techniques and the most effective Python libraries, outline key challenges
    particular to working with language data, introduce critical elements of the NLP
    workflow, and highlight NLP applications relevant for algorithmic trading.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现文本数据的潜在价值，我们将介绍专门的NLP技术和最有效的Python库，概述处理语言数据的关键挑战，介绍NLP工作流程的关键要素，并强调与算法交易相关的NLP应用。
- en: Key challenges of working with text data
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理文本数据的关键挑战
- en: The conversion of unstructured text into a machine-readable format requires
    careful preprocessing to preserve the valuable semantic aspects of the data. How
    humans comprehend the content of language is not fully understood and improving
    machines' ability to understand language remains an area of very active research.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将非结构化文本转换为机器可读格式需要仔细的预处理，以保留数据宝贵的语义方面。人类如何理解语言的内容尚未完全理解，改进机器理解语言的能力仍然是一个非常活跃的研究领域。
- en: 'NLP is particularly challenging because the effective use of text data for
    ML requires an understanding of the inner workings of language as well as knowledge
    about the world to which it refers. Key challenges include the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: NLP特别具有挑战性，因为有效利用文本数据进行ML需要对语言的内部运作有所了解，以及对其所指的世界有所了解。主要挑战包括以下内容：
- en: Ambiguity due to **polysemy**, that is, a word or phrase having different meanings
    depending on context ("Local High School Dropouts Cut in Half")
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于多义性而产生的歧义，即一个词或短语根据上下文具有不同的含义（“本地高中辍学率减半”）
- en: The nonstandard and **evolving use** of language, especially on social media
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言的非标准和不断发展的使用，尤其是在社交媒体上
- en: The use of **idioms** like "throw in the towel"
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用“throw in the towel”等习语
- en: Tricky **entity names** like "Where is A Bug's Life playing?"
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像“A Bug's Life在哪里上映？”这样的棘手实体名称
- en: 'Knowledge of the world: "Mary and Sue are sisters" versus "Mary and Sue are
    mothers"'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 世界知识：“玛丽和苏是姐妹”与“玛丽和苏是母亲”
- en: The NLP workflow
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP工作流程
- en: A key goal for using ML from text data for algorithmic trading is to extract
    signals from documents. A document is an individual sample from a relevant text
    data source, for example, a company report, a headline, a news article, or a tweet.
    A corpus, in turn, is a collection of documents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 利用文本数据进行算法交易的ML的一个关键目标是从文档中提取信号。文档是来自相关文本数据源的单个样本，例如公司报告、标题、新闻文章或推文。而语料库则是文档的集合。
- en: '*Figure 14.1* lays out the **key steps** to convert documents into a dataset
    that can be used to train a supervised ML algorithm capable of making actionable
    predictions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.1*列出了将文档转换为可用于训练能够做出可操作预测的监督ML算法的数据集的**关键步骤**：'
- en: '![](img/B15439_14_01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_01.png)'
- en: 'Figure 14.1: The NLP workflow'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：NLP工作流程
- en: '**Fundamental techniques** extract text features as isolated semantic units
    called tokens and use rules and dictionaries to annotate them with linguistic
    and semantic information. The bag-of-words model uses token frequency to model
    documents as token vectors, which leads to the document-term matrix that is frequently
    used for text classification, retrieval, or summarization.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**基本技术**提取文本特征作为被称为令牌的孤立语义单元，并使用规则和字典对它们进行语言和语义信息的注释。词袋模型使用令牌频率来将文档建模为令牌向量，这导致了经常用于文本分类、检索或摘要的文档-术语矩阵。'
- en: '**Advanced approaches** rely on ML to refine basic features such as tokens
    and produce richer document models. These include topic models that reflect the
    joint usage of tokens across documents and word-vector models that aim to capture
    the context of token usage.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**高级方法**依赖于ML来完善诸如令牌之类的基本特征，并产生更丰富的文档模型。这些包括反映跨文档令牌联合使用的主题模型和旨在捕捉令牌使用上下文的词向量模型。'
- en: 'We will review key decisions at each step of the workflow and the related tradeoffs
    in more detail before illustrating their implementation using the spaCy library
    in the next section. The following table summarizes the key tasks of an NLP pipeline:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地审查工作流程的每个步骤和相关权衡，并使用spaCy库来说明它们的实施。以下表总结了NLP管道的关键任务：
- en: '| Feature | Description |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: 特征 | 描述
- en: '| Tokenization | Segment text into words, punctuation marks, and so on. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: 分词 | 将文本分割成单词、标点符号等。
- en: '| Part-of-speech tagging | Assign word types to tokens, such as a verb or noun.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: 词性标注 | 为标记分配词性，如动词或名词。
- en: '| Dependency parsing | Label syntactic token dependencies, like subject <=>
    object. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: 依赖解析 | 标记句法令牌依赖，如主语<=>宾语。
- en: '| Stemming and lemmatization | Assign the base forms of words: "was" => "be",
    "rats" => "rat". |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: 词干和词形还原 | 分配单词的基本形式：“was” => “be”，“rats” => “rat”。
- en: '| Sentence boundary detection | Find and segment individual sentences. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: 句子边界检测 | 查找并分割单个句子。
- en: '| Named entity recognition | Label "real-world" objects, such as people, companies,
    or locations. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: 命名实体识别 | 标记“现实世界”对象，如人、公司或地点。
- en: '| Similarity | Evaluate the similarity of words, text spans, and documents.
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: 相似性 | 评估单词、文本跨度和文档的相似性。
- en: Parsing and tokenizing text data – selecting the vocabulary
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析和标记文本数据 - 选择词汇表
- en: A token is an instance of a sequence of characters in a given document and is
    considered a semantic unit. The vocabulary is the set of tokens contained in a
    corpus deemed relevant for further processing; tokens not in the vocabulary will
    be ignored.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌是给定文档中字符序列的一个实例，并被视为语义单元。词汇是被认为对进一步处理相关的语料库中包含的令牌的集合；不在词汇表中的令牌将被忽略。
- en: The **goal**, of course, is to extract tokens that most accurately reflect the
    document's meaning. The **key tradeoff** at this step is the choice of a larger
    vocabulary to better reflect the text source at the expense of more features and
    higher model complexity (discussed as the *curse of dimensionality* in *Chapter
    13*, *Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning*).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，**目标**是提取最准确反映文档含义的令牌。在这一步的**关键权衡**是选择更大的词汇量以更好地反映文本来源，代价是更多的特征和更高的模型复杂性（在*第13章*，*使用无监督学习的数据驱动风险因素和资产配置*中讨论为*维度诅咒*）。
- en: Basic choices in this regard concern the treatment of punctuation and capitalization,
    the use of spelling correction, and whether to exclude very frequent so-called
    "stop words" (such as "and" or "the") as meaningless noise.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面的基本选择涉及标点和大写的处理、拼写校正的使用以及是否排除非常频繁的所谓“停用词”（如“and”或“the”）作为无意义的噪音。
- en: In addition, we need to decide whether to include groupings of *n* individual
    tokens called *n***-grams** as semantic units (an individual token is also called
    *unigram*). An example of a two-gram (or *bigram*) is "New York", whereas "New
    York City" is a three-gram (or *trigram*). The decision can rely on dictionaries
    or a comparison of the relative frequencies of the individual and joint usage.
    There are more unique combinations of tokens than unigrams, hence adding *n*-grams
    will drive up the number of features and risks adding noise unless filtered for
    by frequency.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要决定是否将称为*n*个单独令牌的组合称为语义单元（单个令牌也称为*unigram*）。两个令牌（或*bigram*）的例子是“New York”，而“New
    York City”是三个令牌（或*trigram*）。决定可以依赖于字典或对个体和联合使用的相对频率的比较。与单个令牌相比，令牌的独特组合更多，因此添加*n*个令牌将增加特征数量，并且除非按频率进行过滤，否则会增加噪音。
- en: Linguistic annotation – relationships among tokens
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言学注释 - 令牌之间的关系
- en: Linguistic annotations include the application of **syntactic and grammatical
    rules** to identify the boundary of a sentence despite ambiguous punctuation,
    and a token's role and relationships in a sentence for POS tagging and dependency
    parsing. It also permits the identification of common root forms for stemming
    and lemmatization to group together related words.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学注释包括将**句法和语法规则**应用于识别句子的边界，尽管标点符号不明确，并且标记句子中的令牌的角色和关系以进行词性标注和依赖解析。它还允许识别用于词干和词形还原的常见词根形式，以将相关单词分组在一起。
- en: 'The following are some key concepts related to annotations:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与注释相关的一些关键概念：
- en: '**Stemming** uses simple rules to remove common endings such as *s*, *ly*,
    *ing*, or *ed* from a token and reduce it to its stem or root form.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**使用简单的规则从标记中去除常见的结尾，例如*s*，*ly*，*ing*或*ed*，并将其减少到其词干或根形式。'
- en: '**Lemmatization** uses more sophisticated rules to derive the canonical root
    (**lemma**) of a word. It can detect irregular common roots such as "better" and
    "best" and more effectively condenses the vocabulary but is slower than stemming.
    Both approaches simplify the vocabulary at the expense of semantic nuances.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原**使用更复杂的规则来推导单词的规范根（**lemma**）。 它可以检测不规则的常见根，例如“better”和“best”，并更有效地压缩词汇，但比词干提取慢。
    这两种方法都简化了词汇，但牺牲了语义细微差别。'
- en: '**POS** annotations help disambiguate tokens based on their function (for example,
    when a verb and noun have the same form), which increases the vocabulary but may
    capture meaningful distinctions.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**POS**注释有助于根据其功能消除标记的歧义性（例如，当动词和名词具有相同形式时），这会增加词汇量，但可能捕捉有意义的区别。'
- en: '**Dependency parsing** identifies hierarchical relationships among tokens and
    is commonly used for translation. It is important for interactive applications
    that require more advanced language understanding, such as chatbots.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖解析**识别标记之间的层次关系，通常用于翻译。 对于需要更高级语言理解的交互式应用程序（例如聊天机器人），这是很重要的。'
- en: Semantic annotation – from entities to knowledge graphs
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义标注 - 从实体到知识图谱
- en: '**Named-entity recognition** (**NER**) aims at identifying tokens that represent
    objects of interest, such as persons, countries, or companies. It can be further
    developed into a **knowledge graph** that captures semantic and hierarchical relationships
    among such entities. It is a critical ingredient for applications that, for example,
    aim at predicting the impact of news events on sentiment.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）旨在识别代表感兴趣对象的标记，例如人物，国家或公司。 它可以进一步发展为捕捉这些实体之间的语义和层次关系的**知识图谱**。
    这对于例如旨在预测新闻事件对情绪的影响的应用至关重要。'
- en: Labeling – assigning outcomes for predictive modeling
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签 - 为预测建模分配结果
- en: Many NLP applications learn to predict outcomes based on meaningful information
    extracted from the text. Supervised learning requires labels to teach the algorithm
    the true input-output relationship. With text data, establishing this relationship
    may be challenging and require explicit data modeling and collection.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 许多NLP应用程序学习根据从文本中提取的有意义的信息来预测结果。 监督学习需要标签来教算法真实的输入输出关系。 对于文本数据，建立这种关系可能具有挑战性，并且可能需要明确的数据建模和收集。
- en: Examples include decisions on how to quantify the sentiment implicit in a text
    document such as an email, transcribed interview, or tweet with respect to a new
    domain, or which aspects of a research document or news report should be assigned
    a specific outcome.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，决定如何量化文本文档（例如电子邮件，转录的采访或推文）中隐含的情绪，以及应该为研究文档或新闻报道的哪些方面分配特定结果。
- en: Applications
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用
- en: 'The use of ML with text data for trading relies on extracting meaningful information
    in the form of features that help predict future price movements. Applications
    range from the exploitation of the short-term market impact of news to the longer-term
    fundamental analysis of the drivers of asset valuation. Examples include the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 利用文本数据进行交易的ML依赖于提取有意义的信息，以特征的形式帮助预测未来的价格走势。 应用范围从利用新闻的短期市场影响到资产估值驱动因素的长期基本分析。
    例如：
- en: The evaluation of product review sentiment to assess a company's competitive
    position or industry trends
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估产品评论情绪以评估公司的竞争地位或行业趋势
- en: The detection of anomalies in credit contracts to predict the probability or
    impact of a default
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测信用合同中的异常以预测违约的概率或影响
- en: The prediction of news impact in terms of direction, magnitude, and affected
    entities
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测新闻影响的方向，幅度和受影响实体
- en: JP Morgan, for instance, developed a predictive model based on 250,000 analyst
    reports that outperformed several benchmark indices and produced uncorrelated
    signals relative to sentiment factors formed from consensus EPS and recommendation
    changes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，摩根大通开发了一个基于25万份分析师报告的预测模型，该模型的表现优于几个基准指数，并且相对于从共识EPS和推荐变化形成的情绪因素产生了不相关的信号。
- en: From text to tokens – the NLP pipeline
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本到标记 - NLP流水线
- en: In this section, we will demonstrate how to construct an NLP pipeline using
    the open-source Python library spaCy. The textacy library builds on spaCy and
    provides easy access to spaCy attributes and additional functionality.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用开源Python库spaCy构建NLP流水线。 textacy库基于spaCy构建，并提供对spaCy属性和附加功能的便捷访问。
- en: Refer to the notebook `nlp_pipeline_with_spaCy` for the following code samples,
    installation instruction, and additional details.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有关以下代码示例，安装说明和其他详细信息，请参阅笔记本`nlp_pipeline_with_spaCy`。
- en: NLP pipeline with spaCy and textacy
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用spaCy和textacy的NLP流水线
- en: spaCy is a widely used Python library with a comprehensive feature set for fast
    text processing in multiple languages. The usage of the tokenization and annotation
    engines requires the installation of language models. The features we will use
    in this chapter only require the small models; the larger models also include
    word vectors that we will cover in *Chapter 16*, *Word Embeddings for Earnings
    Calls and SEC Filings*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy是一个广泛使用的Python库，具有多语言快速文本处理的全面功能集。 使用标记化和注释引擎需要安装语言模型。 我们在本章中将使用的功能仅需要小型模型；
    更大的模型还包括我们将在*第16章*中介绍的单词向量。
- en: 'With the library installed and linked, we can instantiate a spaCy language
    model and then apply it to the document. The result is a `Doc` object that tokenizes
    the text and processes it according to configurable pipeline components that by
    default consist of a tagger, a parser, and a named-entity recognizer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 安装并链接了库之后，我们可以实例化一个spaCy语言模型，然后将其应用于文档。结果是一个`Doc`对象，它对文本进行标记化并根据可配置的管道组件进行处理，默认情况下包括标签器、解析器和命名实体识别器：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s illustrate the pipeline using a simple sentence:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个简单的句子来说明管道：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parsing, tokenizing, and annotating a sentence
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析、标记和注释句子
- en: 'The parsed document content is iterable, and each element has numerous attributes
    produced by the processing pipeline. The next sample illustrates how to access
    the following attributes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 解析后的文档内容是可迭代的，每个元素都有处理管道生成的许多属性。下一个示例说明了如何访问以下属性：
- en: '`.text`: The original word text'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.text`: 原始单词文本'
- en: '`.lemma_`: The word root'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.lemma_`: 单词的词根'
- en: '`.pos_`: A basic POS tag'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.pos_`: 基本的词性标记'
- en: '`.tag_`: The detailed POS tag'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.tag_`: 详细的词性标记'
- en: '`.dep_`: The syntactic relationship or dependency between tokens'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.dep_`: 标记之间的句法关系或依赖关系'
- en: '`.shape_`: The shape of the word, in terms of capitalization, punctuation,
    and digits'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.shape_`: 单词的形状，包括大小写、标点和数字'
- en: '`.is alpha`: Checks whether the token is alphanumeric'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.is alpha`: 检查标记是否是字母数字字符'
- en: '`.is stop`: Checks whether the token is on a list of common words for the given
    language'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.is stop`: 检查标记是否在给定语言的常用词列表中'
- en: 'We iterate over each token and assign its attributes to a `pd.DataFrame`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以遍历每个标记，并将其属性分配给`pd.DataFrame`：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following result:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '| text | lemma | pos | tag | dep | shape | is_alpha | is_stop |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| text | lemma | pos | tag | dep | shape | is_alpha | is_stop |'
- en: '| Apple | apple | PROPN | NNP | nsubj | Xxxxx | TRUE | FALSE |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Apple | apple | PROPN | NNP | nsubj | Xxxxx | TRUE | FALSE |'
- en: '| is | be | VERB | VBZ | aux | xx | TRUE | TRUE |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| is | be | VERB | VBZ | aux | xx | TRUE | TRUE |'
- en: '| looking | look | VERB | VBG | ROOT | xxxx | TRUE | FALSE |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| looking | look | VERB | VBG | ROOT | xxxx | TRUE | FALSE |'
- en: '| at | at | ADP | IN | prep | xx | TRUE | TRUE |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| at | at | ADP | IN | prep | xx | TRUE | TRUE |'
- en: '| buying | buy | VERB | VBG | pcomp | xxxx | TRUE | FALSE |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| buying | buy | VERB | VBG | pcomp | xxxx | TRUE | FALSE |'
- en: '| U.K. | u.k. | PROPN | NNP | compound | X.X. | FALSE | FALSE |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| U.K. | u.k. | PROPN | NNP | compound | X.X. | FALSE | FALSE |'
- en: '| startup | startup | NOUN | NN | dobj | xxxx | TRUE | FALSE |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| startup | startup | NOUN | NN | dobj | xxxx | TRUE | FALSE |'
- en: '| for | for | ADP | IN | prep | xxx | TRUE | TRUE |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| for | for | ADP | IN | prep | xxx | TRUE | TRUE |'
- en: '| $ | $ | SYM | $ | quantmod | $ | FALSE | FALSE |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $ | $ | SYM | $ | quantmod | $ | FALSE | FALSE |'
- en: '| 1 | 1 | NUM | CD | compound | d | FALSE | FALSE |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | NUM | CD | compound | d | FALSE | FALSE |'
- en: '| billion | billion | NUM | CD | pobj | xxxx | TRUE | FALSE |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| billion | billion | NUM | CD | pobj | xxxx | TRUE | FALSE |'
- en: 'We can visualize the syntactic dependency in a browser or notebook using the
    following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方法在浏览器或笔记本中可视化句法依赖：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code allows us to obtain a dependency tree like the following
    one:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码使我们可以获得以下依赖树：
- en: '![](img/B15439_14_02.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_02.png)'
- en: 'Figure 14.2: spaCy dependency tree'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '图14.2: spaCy依赖树'
- en: 'We can get additional insight into the meaning of attributes using `spacy.explain()`,
    such as the following, for example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`spacy.explain()`来获得有关属性含义的额外见解，例如：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Batch-processing documents
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理文档
- en: 'We will now read a larger set of 2,225 BBC News articles (see GitHub for the
    data source details) that belong to five categories and are stored in individual
    text files. We do the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将阅读一组更大的2,225篇BBC新闻文章（有关数据来源的详细信息，请参见GitHub），这些文章属于五个类别，并存储在单独的文本文件中。我们进行以下操作：
- en: Call the `.glob()` method of the `pathlib` module's `Path` object.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`pathlib`模块的`Path`对象的`.glob()`方法。
- en: Iterate over the resulting list of paths.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历结果路径列表。
- en: Read all lines of the news article excluding the heading in the first line.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取新闻文章的所有行，不包括第一行的标题。
- en: 'Append the cleaned result to a list:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将清理后的结果附加到列表中：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sentence boundary detection
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 句子边界检测
- en: 'We will illustrate sentence detection by calling the NLP object on the first
    of the articles:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过调用NLP对象来说明句子检测：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: spaCy computes sentence boundaries from the syntactic parse tree so that punctuation
    and capitalization play an important but not decisive role. As a result, boundaries
    will coincide with clause boundaries, even for poorly punctuated text.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy从句法分析树中计算句子边界，因此标点符号和大写字母起着重要但并非决定性的作用。因此，边界将与从句边界重合，即使是标点不正确的文本也是如此。
- en: 'We can access the parsed sentences using the `.sents` attribute:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`.sents`属性访问解析后的句子：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Named entity recognition
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'spaCy enables named entity recognition using the `.ent_type_` attribute:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy使用`.ent_type_`属性进行命名实体识别：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Textacy makes access to the named entities that appear in the first article
    easy:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Textacy使得轻松查看出现在第一篇文章中的命名实体变得容易：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: N-grams
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N-gram
- en: N-grams combine *n* consecutive tokens. This can be useful for the bag-of-words
    model because, depending on the textual context, treating (for example) "data
    scientist" as a single token may be more meaningful than the two distinct tokens
    "data" and "scientist".
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: N-grams结合了*n*个连续的标记。这对于词袋模型可能很有用，因为根据文本上下文，将（例如）"数据科学家"视为单个标记可能比将"数据"和"科学家"视为两个不同的标记更有意义。
- en: 'Textacy makes it easy to view the `ngrams` of a given length `n` occurring
    at least `min_freq` times:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Textacy可以轻松查看给定长度`n`且至少出现`min_freq`次的`ngrams`：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: spaCy's streaming API
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: spaCy的流式API
- en: 'To pass a larger number of documents through the processing pipeline, we can
    use spaCy''s streaming API as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过处理管道传递更多的文档，我们可以使用spaCy的流式API，如下所示：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Multi-language NLP
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多语言NLP
- en: spaCy includes trained language models for English, German, Spanish, Portuguese,
    French, Italian, and Dutch, as well as a multi-language model for named-entity
    recognition. Cross-language usage is straightforward since the API does not change.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy包括针对英语、德语、西班牙语、葡萄牙语、法语、意大利语和荷兰语的训练语言模型，以及用于命名实体识别的多语言模型。由于API不会改变，因此跨语言使用非常简单。
- en: 'We will illustrate the Spanish language model using a parallel corpus of TED
    talk subtitles (see the GitHub repo for data source references). For this purpose,
    we instantiate both language models:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TED演讲字幕的平行语料库来说明西班牙语语言模型（请参阅数据来源参考的GitHub存储库）。为此，我们实例化两种语言模型：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We read small corresponding text samples in each model:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个模型中读取小的对应文本样本：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Sentence boundary detection uses the same logic but finds a different breakdown:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 句子边界检测使用相同的逻辑，但找到不同的分解：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'POS tagging also works in the same way:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注也以相同的方式工作：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces the following table:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下表格：
- en: '| Token | POS Tag | Meaning | Token | POS Tag | Meaning |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Token | POS Tag | 意义 | Token | POS Tag | 意义 |'
- en: '| There | ADV | adverb | Existe | VERB | verb |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| There | ADV | 副词 | Existe | VERB | 动词 |'
- en: '| s | VERB | verb | una | DET | determiner |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| s | VERB | 动词 | una | DET | 限定词 |'
- en: '| a | DET | determiner | estrecha | ADJ | adjective |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| a | DET | 限定词 | estrecha | ADJ | 形容词 |'
- en: '| tight | ADJ | adjective | y | CONJ | conjunction |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| tight | ADJ | 形容词 | y | CONJ | 连词 |'
- en: '| and | CCONJ | coordinating conjunction | sorprendente | ADJ | adjective |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| and | CCONJ | 并列连词 | sorprendente | ADJ | 形容词 |'
- en: The next section illustrates how to use the parsed and annotated tokens to build
    a document-term matrix that can be used for text classification.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将说明如何使用解析和注释的标记来构建可用于文本分类的文档-术语矩阵。
- en: NLP with TextBlob
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TextBlob进行自然语言处理
- en: TextBlob is a Python library that provides a simple API for common NLP tasks
    and builds on the  **Natural Language Toolkit** (**NLTK**) and the Pattern web
    mining libraries. TextBlob facilitates POS tagging, noun phrase extraction, sentiment
    analysis, classification, and translation, among others.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: TextBlob是一个Python库，提供了一个简单的API用于常见的NLP任务，并构建在**自然语言工具包**（**NLTK**）和Pattern网络挖掘库之上。TextBlob便于进行词性标注、名词短语提取、情感分析、分类和翻译等任务。
- en: 'To illustrate the use of TextBlob, we sample a BBC Sport article with the headline
    "Robinson ready for difficult task". Similar to spaCy and other libraries, the
    first step is to pass the document through a pipeline represented by the `TextBlob`
    object to assign the annotations required for various tasks (see the notebook
    `nlp_with_textblob`):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明TextBlob的使用，我们抽样了一篇BBC体育文章，标题为“罗宾逊准备应对艰巨任务”。与spaCy和其他库类似，第一步是通过`TextBlob`对象将文档传递到管道中，以分配各种任务所需的注释（请参阅笔记本`nlp_with_textblob`）：
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Stemming
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干提取
- en: 'To perform stemming, we instantiate the `SnowballStemmer` from the NTLK library,
    call its `.stem()` method on each token, and display tokens that were modified
    as a result:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行词干提取，我们从NTLK库中实例化`SnowballStemmer`，对每个标记调用其`.stem()`方法，并显示因此被修改的标记：
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Sentiment polarity and subjectivity
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情感极性和主观性
- en: TextBlob provides polarity and subjectivity estimates for parsed documents using
    dictionaries provided by the Pattern library. These dictionaries lexicon-map adjectives
    frequently found in product reviews to sentiment polarity scores, ranging from
    -1 to +1 (negative ↔ positive) and a similar subjectivity score (objective ↔ subjective).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: TextBlob使用Pattern库提供的字典为解析的文档提供极性和主观性估计。这些字典将产品评论中经常出现的形容词映射到情感极性分数，范围从-1到+1（负面↔正面），以及类似的主观性分数（客观↔主观）。
- en: 'The `.sentiment` attribute provides the average for each score over the relevant
    tokens, whereas the `.sentiment_assessments` attribute lists the underlying values
    for each token (see the notebook):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`.sentiment`属性提供了相关标记的每个分数的平均值，而`.sentiment_assessments`属性列出了每个标记的基础值（请参阅笔记本）：'
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Counting tokens – the document-term matrix
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数标记-文档-术语矩阵
- en: In this section, we first introduce how the bag-of-words model converts text
    data into a numeric vector space representations. The goal is to approximate document
    similarity by their distance in that space. We then proceed to illustrate how
    to create a document-term matrix using the sklearn library.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍词袋模型如何将文本数据转换为数值向量空间表示。目标是通过它们在该空间中的距离来近似文档的相似性。然后我们继续说明如何使用sklearn库创建文档-术语矩阵。
- en: The bag-of-words model
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: The bag-of-words model represents a document based on the frequency of the terms
    or tokens it contains. Each document becomes a vector with one entry for each
    token in the vocabulary that reflects the token's relevance to the document.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型基于文档中包含的术语或标记的频率来表示文档。每个文档都变成了一个向量，其中每个词汇表中的标记都有一个条目，反映了该标记对文档的相关性。
- en: Creating the document-term matrix
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建文档-术语矩阵
- en: The document-term matrix is straightforward to compute given the vocabulary.
    However, it is also a crude simplification because it abstracts from word order
    and grammatical relationships. Nonetheless, it often achieves good results in
    text classification quickly and, thus, provides a very useful starting point.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 文档-术语矩阵在给定词汇表的情况下很容易计算。然而，它也是一种粗糙的简化，因为它抽象出了词序和语法关系。尽管如此，它通常能够快速实现良好的文本分类结果，因此提供了一个非常有用的起点。
- en: The left panel of *Figure 14.3* illustrates how this document model converts
    text data into a matrix with numerical entries where each row corresponds to a
    document and each column to a token in the vocabulary. The resulting matrix is
    usually both very high-dimensional and sparse, that is, it contains many zero
    entries because most documents only contain a small fraction of the overall vocabulary.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.3*的左侧面板说明了这个文档模型如何将文本数据转换为具有数字条目的矩阵，其中每行对应一个文档，每列对应词汇表中的一个标记。结果矩阵通常是非常高维且稀疏的，即它包含许多零条目，因为大多数文档只包含整体词汇表的一小部分。'
- en: '![](img/B15439_14_03.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_03.png)'
- en: 'Figure 14.3: Document-term matrix and cosine similarity'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：文档-术语矩阵和余弦相似度
- en: There are several ways to weigh a token's vector entry to capture its relevance
    to the document. We will illustrate how to use sklearn to use binary flags that
    indicate presence or absence, counts, and weighted counts that account for differences
    in term frequencies across all documents in the corpus.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以对标记的向量条目进行加权，以捕捉其与文档的相关性。我们将说明如何使用sklearn使用指示存在或不存在的二进制标志，计数和加权计数，以考虑语料库中所有文档中术语频率的差异。
- en: Measuring the similarity of documents
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量文档的相似性
- en: The representation of documents as word vectors assigns to each document a location
    in the vector space created by the vocabulary. Interpreting the vector entries
    as Cartesian coordinates in this space, we can use the angle between two vectors
    to measure their similarity because vectors that point in the same direction contain
    the same terms with the same frequency weights.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档表示为单词向量将为每个文档分配一个位置，该位置位于词汇表创建的向量空间中。将向量条目解释为此空间中的笛卡尔坐标，我们可以使用两个向量之间的角度来衡量它们的相似性，因为指向相同方向的向量包含相同频率权重的相同术语。
- en: The right panel of the preceding figure illustrates—simplified in two dimensions—the
    calculation of the distance between a document represented by a vector *d*[1]
    and a query vector (either a set of search terms or another document) represented
    by the vector *q*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图的右侧简化了两个维度，计算了由向量*d*[1]表示的文档与由向量*q*表示的查询向量(可以是一组搜索词或另一个文档)之间的距离。
- en: The **cosine similarity** equals the cosine of the angle between the two vectors.
    It translates the size of the angle into a number in the range [0, 1] since all
    vector entries are non-negative token weights. A value of 1 implies that both
    documents are identical with respect to their token weights, whereas a value of
    0 implies that the two documents only contain distinct tokens.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**余弦相似度**等于两个向量之间角度的余弦。它将角度的大小转换为[0,1]范围内的数字，因为所有向量条目都是非负的标记权重。值为1意味着两个文档在其标记权重方面是相同的，而值为0意味着两个文档只包含不同的标记。'
- en: As shown in the figure, the cosine of the angle is equal to the dot product
    of the vectors, that is, the sum product of their coordinates, divided by the
    product of the lengths, measured by the Euclidean norms, of each vector.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，角度的余弦等于向量的点积，即它们坐标的和乘积，除以长度的乘积，由欧几里得范数测量，每个向量的长度。
- en: Document-term matrix with scikit-learn
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn的文档-术语矩阵
- en: The scikit-learn preprocessing module offers two tools to create a document-term
    matrix. `CountVectorizer` uses binary or absolute counts to measure the **term
    frequency** (**TF**) *tf*(*d, t*) for each document *d* and token *t*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn预处理模块提供了两种工具来创建文档-术语矩阵。 `CountVectorizer`使用二进制或绝对计数来衡量每个文档*d*和标记*t*的**术语频率**(**TF**)
    *tf*(*d, t*)。
- en: '`TfidfVectorizer`, by contrast, weighs the (absolute) term frequency by the
    **inverse document frequency** (**IDF**). As a result, a term that appears in
    more documents will receive a lower weight than a token with the same frequency
    for a given document but with a lower frequency across all documents. More specifically,
    using the default settings, the *tf-idf*(*d*, *t*) entries for the document-term
    matrix are computed as *tf-idf(d, t) = tf(d, t)* x *idf(t)* with:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，`TfidfVectorizer`通过**逆文档频率**(**IDF**)对(绝对)术语频率进行加权。因此，出现在更多文档中的术语将比在给定文档中具有相同频率但在所有文档中频率较低的标记获得较低的权重。更具体地说，使用默认设置，文档-术语矩阵的*tf-idf*(*d*,
    *t*)条目计算为*tf-idf(d, t) = tf(d, t)* x *idf(t)*，其中：
- en: '![](img/B15439_14_001.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_001.png)'
- en: where *n*[d] is the number of documents and *df*(*d*, *t*) the document frequency
    of term *t*. The resulting TF-IDF vectors for each document are normalized with
    respect to their absolute or squared totals (see the sklearn documentation for
    details). The TF-IDF measure was originally used in information retrieval to rank
    search engine results and has subsequently proven useful for text classification
    and clustering.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*n*[d]是文档的数量，*df*(*d*, *t*)是术语*t*的文档频率。每个文档的TF-IDF向量都相对于它们的绝对或平方总数进行了归一化(有关详细信息，请参阅sklearn文档)。TF-IDF度量最初用于信息检索以对搜索引擎结果进行排名，并且随后已被证明对文本分类和聚类非常有用。
- en: Both tools use the same interface and perform tokenization and further optional
    preprocessing of a list of documents before vectorizing the text by generating
    token counts to populate the document-term matrix.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个工具使用相同的接口，在向量化文本之前对文档列表进行标记和进一步的可选预处理，以生成标记计数以填充文档-术语矩阵。
- en: 'Key parameters that affect the size of the vocabulary include the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 影响词汇量大小的关键参数包括以下内容：
- en: '`stop_words`: Uses a built-in or user-provided list of (frequent) words to
    exclude'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop_words`:使用内置或用户提供的(常见)单词列表来排除'
- en: '`ngram_range`: Includes *n*-grams in a range of *n* defined by a tuple of (*n*[min],
    *n*[max])'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_range`:包括由(*n*[min], *n*[max])元组定义的*n*-gram范围'
- en: '`lowercase`: Converts characters accordingly (the default is `True`)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lowercase`:相应地转换字符(默认为`True`)'
- en: '`min_df`/`max_df`: Ignores words that appear in less/more (`int`) or are present
    in a smaller/larger share of documents (if `float` [0.0,1.0])'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_df`/`max_df`:忽略出现在较少/更多(`int`)或出现在较小/较大份额文档中的单词(如果是`float` [0.0,1.0])'
- en: '`max_features`: Limits the number of tokens in vocabulary accordingly'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`:相应地限制词汇表中的标记数量。'
- en: '`binary`: Sets non-zero counts to 1 (`True`)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary`:将非零计数设置为1(`True`)'
- en: See the notebook `document_term_matrix` for the following code samples and additional
    details. We are again using the 2,225 BBC news articles for illustration.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 有关以下代码示例和其他详细信息，请参阅笔记本`document_term_matrix`。我们再次使用2,225篇BBC新闻文章进行说明。
- en: Using CountVectorizer
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用CountVectorizer
- en: 'The notebook contains an interactive visualization that explores the impact
    of the `min_df` and `max_df` settings on the size of the vocabulary. We read the
    articles into a DataFrame, set `CountVectorizer` to produce binary flags and use
    all tokens, and call its `.fit_transform()` method to produce a document-term
    matrix:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中包含一个交互式可视化，探索`min_df`和`max_df`设置对词汇量大小的影响。我们将文章读入DataFrame，设置`CountVectorizer`以生成二进制标志并使用所有标记，并调用其`.fit_transform()`方法生成文档-词汇矩阵：
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The output is a `scipy.sparse` matrix in row format that efficiently stores
    a small share (<0.7 percent) of the 445,870 non-zero entries in the 2,225 (document)
    rows and 29,275 (token) columns.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是以行格式存储的`scipy.sparse`矩阵，它有效地存储了2,225（文档）行和29,275（标记）列中445,870个非零条目的一小部分（<0.7%）。
- en: Visualizing the vocabulary distribution
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化词汇分布
- en: The visualization in *Figure 14.4* shows that requiring tokens to appear in
    at least 1 percent and less than 50 percent of documents restricts the vocabulary
    to around 10 percent of the almost 30,000 tokens.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.4*中的可视化显示，要求标记至少出现在1%至50%的文档中，将词汇限制在几乎30,000个标记中的约10%。'
- en: 'This leaves a mode of slightly over 100 unique tokens per document, as shown
    in the left panel of the following plot. The right panel shows the document frequency
    histogram for the remaining tokens:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这留下了每个文档略多于100个独特标记的模式，如下图左侧面板所示。右侧面板显示了剩余标记的文档频率直方图：
- en: '![](img/B15439_14_04.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_04.png)'
- en: 'Figure 14.4: The distributions of unique tokens and number of tokens per document'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：独特标记和每个文档标记数的分布
- en: Finding the most similar documents
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查找最相似的文档
- en: '`CountVectorizer` result lets us find the most similar documents using the
    `pdist()` functions for pairwise distances provided by the `scipy.spatial.distance`
    module. It returns a condensed distance matrix with entries corresponding to the
    upper triangle of a square matrix. We use `np.triu_indices()` to translate the
    index that minimizes the distance to the row and column indices that in turn correspond
    to the closest token vectors:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`的结果让我们使用`scipy.spatial.distance`模块提供的`pdist()`函数来找到最相似的文档。它返回一个压缩的距离矩阵，其中的条目对应于方阵的上三角。我们使用`np.triu_indices()`将最小距离的索引转换为相应的最接近标记向量的行和列索引：'
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Articles 6 and 245 are closest by cosine similarity, due to the fact that they
    share 38 tokens out of a combined vocabulary of 303 (see notebook). The following
    table summarizes these two articles and demonstrates the limited ability of similarity
    measures based on word counts to identify deeper semantic similarity:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们共享303个词汇表中的38个标记（请参阅笔记本），文章6和245在余弦相似性上最接近。以下表格总结了这两篇文章，并展示了基于单词计数的相似性度量识别更深层语义相似性的有限能力：
- en: '|  | Article 6 | Article 245 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | 文章6 | 文章245 |'
- en: '| Topic | Business | Business |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 商业 | 商业 |'
- en: '| Heading | Jobs growth still slow in the US | Ebbers ''aware'' of WorldCom
    fraud |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 美国的就业增长仍然缓慢 | 埃伯斯知晓世界通的欺诈行为 |'
- en: '| Body | The US created fewer jobs than expected in January, but a fall in
    jobseekers pushed the unemployment rate to its lowest level in three years. According
    to Labor Department figures, US firms added only 146,000 jobs in January. | Former
    WorldCom boss Bernie Ebbers was directly involved in the $11bn financial fraud
    at the firm, his closest associate has told a US court. Giving evidence in the
    criminal trial of Mr Ebbers, ex-finance chief Scott Sullivan implicated his colleague
    in the accounting scandal at the firm. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 正文 | 美国1月份创造的工作岗位少于预期，但求职者减少使失业率降至三年来的最低水平。根据劳工部的数据，美国企业仅在1月份增加了146,000个工作岗位。
    | 前世界通总裁伯尼·埃伯斯直接参与了该公司110亿美元的财务欺诈，他最亲密的同事在美国法庭上作证。在埃伯斯先生的刑事审判中，前财务主管斯科特·沙利文指控他的同事参与了该公司的会计丑闻。
    |'
- en: 'Both `CountVectorizer` and `TfidfVectorizer` can be used with spaCy, for example,
    to perform lemmatization and exclude certain characters during tokenization:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`和`TfidfVectorizer`都可以与spaCy一起使用，例如进行词形还原和在标记化过程中排除某些字符：'
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: See the notebook for additional detail and more examples.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多细节和更多示例，请参阅笔记本。
- en: TfidfTransformer and TfidfVectorizer
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TfidfTransformer和TfidfVectorizer
- en: '`TfidfTransformer` computes the TF-IDF weights from a document-term matrix
    of token counts like the one produced by `CountVectorizer`.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfTransformer`从类似`CountVectorizer`生成的文档-词汇矩阵中计算TF-IDF权重。'
- en: '`TfidfVectorizer` performs both computations in a single step. It adds a few
    parameters to the `CountVectorizer` API that controls the smoothing behavior.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer`在一个步骤中执行这两个计算。它向`CountVectorizer`API添加了一些参数，用于控制平滑行为。'
- en: 'The TFIDF computation works as follows for a small text sample:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小文本样本，TFIDF计算如下：
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We compute the term frequency as before:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样计算词频：
- en: '[PRE23]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The document frequency is the number of documents containing the token:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 文档频率是包含该标记的文档数：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The TF-IDF weights are the ratio of these values:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF权重是这些值的比值：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The effect of smoothing
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平滑的效果
- en: 'To avoid zero division, `TfidfVectorizer` uses smoothing for document and term
    frequencies:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免零除法，`TfidfVectorizer`对文档和词频使用平滑处理：
- en: '`smooth_idf`: Adds one to document frequency, as if an extra document contained
    every token in the vocabulary, to prevent zero divisions'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`smooth_idf`：增加一次文档频率，就好像额外的文档包含了词汇表中的每个标记，以防止零除法'
- en: '`sublinear_tf`: Applies sublinear tf scaling, that is, replaces tf with 1 +
    log(tf)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sublinear_tf`：应用次线性tf缩放，即用1 + log(tf)替换tf'
- en: 'In combination with normed weights, the results differ slightly:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与归一化权重结合，结果略有不同：
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Summarizing news articles using TfidfVectorizer
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用TfidfVectorizer总结新闻文章
- en: Due to their ability to assign meaningful token weights, TF-IDF vectors are
    also used to summarize text data. For example, Reddit's `autotldr` function is
    based on a similar algorithm. See the notebook for an example using the BBC articles.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们能够分配有意义的标记权重，TF-IDF向量也用于总结文本数据。例如，Reddit的`autotldr`功能就是基于类似的算法。请参阅笔记本，了解使用BBC文章的示例。
- en: Key lessons instead of lessons learned
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键教训而不是已学到的教训
- en: The large number of techniques and options to process natural language for use
    in ML models corresponds to the complex nature of this highly unstructured data
    source. The engineering of good language features is both challenging and rewarding,
    and arguably the most important step in unlocking the semantic value hidden in
    text data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在ML模型中处理自然语言的技术和选项的数量之多，对应于这种高度非结构化数据源的复杂性。构建良好的语言特征既具有挑战性又具有回报，并且可以说是解锁文本数据中隐藏的语义价值的最重要步骤。
- en: In practice, experience helps to select transformations that remove the noise
    rather than the signal, but it will likely remain necessary to cross-validate
    and compare the performance of different combinations of preprocessing choices.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，经验有助于选择去除噪音而不是信号的转换，但很可能仍然需要交叉验证和比较不同预处理选择的性能。
- en: NLP for trading
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于交易的NLP
- en: Once text data has been converted into numerical features using the NLP techniques
    discussed in the previous sections, text classification works just like any other
    classification task.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本数据使用前面讨论的NLP技术转换为数值特征，文本分类就像任何其他分类任务一样。
- en: In this section, we will apply these preprocessing techniques to news articles,
    product reviews, and Twitter data and teach various classifiers to predict discrete
    news categories, review scores, and sentiment polarity.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用这些预处理技术到新闻文章、产品评论和Twitter数据，并教授各种分类器来预测离散的新闻类别、评论分数和情感极性。
- en: First, we will introduce the naive Bayes model, a probabilistic classification
    algorithm that works well with the text features produced by a bag-of-words model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍朴素贝叶斯模型，这是一种概率分类算法，适用于由词袋模型产生的文本特征。
- en: The code samples for this section are in the notebook `news_text_classification`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码示例在笔记本`news_text_classification`中。
- en: The naive Bayes classifier
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: The naive Bayes algorithm is very popular for text classification because its
    low computational cost and memory requirements facilitate training on very large,
    high-dimensional datasets. Its predictive performance can compete with more complex
    models, provides a good baseline, and is best known for successful spam detection.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法在文本分类中非常受欢迎，因为其低计算成本和内存需求有助于在非常大的高维数据集上进行训练。它的预测性能可以与更复杂的模型竞争，提供了一个很好的基准，并且以成功的垃圾邮件检测而闻名。
- en: The model relies on Bayes' theorem and the assumption that the various features
    are independent of each other given the outcome class. In other words, for a given
    outcome, knowing the value of one feature (for example, the presence of a token
    in a document) does not provide any information about the value of another feature.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型依赖于贝叶斯定理和各种特征在给定结果类别的情况下相互独立的假设。换句话说，对于给定的结果，知道一个特征的值（例如，文档中标记的存在）并不提供关于另一个特征值的任何信息。
- en: Bayes' theorem refresher
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯定理复习
- en: 'Bayes'' theorem expresses the conditional probability of one event (for example,
    that an email is spam as opposed to benign "ham") given another event (for example,
    that the email contains certain words) as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理表达了一个事件的条件概率（例如，一封电子邮件是垃圾邮件而不是良性的“非垃圾邮件”）在另一个事件（例如，电子邮件包含某些词）给定的情况下的概率，如下所示：
- en: '![](img/B15439_14_002.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_002.png)'
- en: 'The **posterior** probability that an email is in fact spam given that it contains
    certain words depends on the interplay of three factors:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**后验**概率，即包含某些词的电子邮件实际上是垃圾邮件的概率，取决于三个因素的相互作用：'
- en: The **prior** probability that an email is spam
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件是垃圾邮件的**先验**概率
- en: The **likelihood** of encountering these words in a spam email
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在垃圾邮件中遇到这些词的**可能性**
- en: The **evidence**, that is, the probability of seeing these words in an email
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**证据**，即在电子邮件中看到这些词的概率'
- en: To compute the posterior, we can ignore the evidence because it is the same
    for all outcomes (spam versus ham), and the unconditional prior may be easy to
    compute.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算后验概率，我们可以忽略证据，因为它对所有结果（垃圾邮件与非垃圾邮件）都是相同的，而且无条件先验可能很容易计算。
- en: However, the likelihood poses insurmountable challenges for a reasonably sized
    vocabulary and a real-world corpus of emails. The reason is the combinatorial
    explosion of the words that did or did not appear jointly in different documents
    that prevent the evaluation required to compute a probability table and assign
    a value to the likelihood.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于合理大小的词汇表和真实世界的电子邮件语料库，可能会遇到无法克服的挑战。原因是单词在不同文档中联合出现或未出现的组合爆炸，这阻止了计算概率表和为可能性分配值所需的评估。
- en: The conditional independence assumption
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件独立假设
- en: 'The key assumption to make the model both tractable and earn it the name *naive*
    is that the features are independent conditional on the outcome. To illustrate,
    let''s classify an email with the three words "Send money now" so that Bayes''
    theorem becomes the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使模型既易于处理又赋予其“朴素”名称的关键假设是，特征在给定结果的条件下是独立的。举例说明，让我们对包含三个词“现在发送钱”的电子邮件进行分类，这样贝叶斯定理就变成了以下形式：
- en: '![](img/B15439_14_003.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_003.png)'
- en: 'Formally, the assumption that the three words are conditionally independent
    means that the probability of observing "send" is not affected by the presence
    of the other terms given that the mail is spam, that is, *P*(send | money, now,
    spam) = *P*(send | spam). As a result, we can simplify the likelihood function:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，三个词的条件独立假设意味着观察到“发送”的概率不受其他术语的存在的影响，假设邮件是垃圾邮件，即*P*(send | money, now, spam)
    = *P*(send | spam)。因此，我们可以简化似然函数：
- en: '![](img/B15439_14_004.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_004.png)'
- en: Using the "naive" conditional independence assumption, each term in the numerator
    is straightforward to compute as relative frequencies from the training data.
    The denominator is constant across classes and can be ignored when posterior probabilities
    need to be compared rather than calibrated. The prior probability becomes less
    relevant as the number of factors, that is, features, increases.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用“朴素”的条件独立假设，分子中的每个术语都可以从训练数据的相对频率中直接计算。分母在各类别之间是恒定的，在需要比较后验概率而不是校准时可以忽略。先验概率在因素数量（即特征）增加时变得不那么相关。
- en: In sum, the advantages of the naive Bayes model are fast training and prediction
    because the number of parameters is proportional to the number of features, and
    their estimation has a closed-form solution (based on training data frequencies)
    rather than expensive iterative optimization. It is also intuitive and somewhat
    interpretable, does not require hyperparameter tuning and is relatively robust
    to irrelevant features given sufficient signal.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，朴素贝叶斯模型的优势在于训练和预测速度快，因为参数数量与特征数量成正比，并且它们的估计具有封闭形式的解决方案（基于训练数据频率），而不是昂贵的迭代优化。它也直观且有一定的可解释性，不需要超参数调整，并且在信号充分的情况下相对不太受无关特征的影响。
- en: However, when the independence assumption does not hold and text classification
    depends on combinations of features, or features are correlated, the model will
    perform poorly.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当独立假设不成立且文本分类依赖于特征的组合，或者特征之间存在相关性时，模型的性能将较差。
- en: Classifying news articles
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类新闻文章
- en: 'We start with an illustration of the naive Bayes model for news article classification
    using the BBC articles that we read before to obtain a `DataFrame` with 2,225
    articles from five categories:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先以BBC文章为例，使用朴素贝叶斯模型进行新闻文章分类，以获得一个包含来自五个类别的2,225篇文章的`DataFrame`：
- en: '[PRE27]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To train and evaluate a multinomial naive Bayes classifier, we split the data
    into the default 75:25 train-test set ratio, ensuring that the test set classes
    closely mirror the train set:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练和评估多项式朴素贝叶斯分类器，我们将数据按默认的75:25训练-测试集比例进行拆分，确保测试集类别与训练集类别紧密匹配：
- en: '[PRE28]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We proceed to learn the vocabulary from the training set and transform both
    datasets using `CountVectorizer` with default settings to obtain almost 26,000
    features:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续从训练集中学习词汇，并使用默认设置的`CountVectorizer`转换两个数据集，以获得近26,000个特征：
- en: '[PRE29]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Training and prediction follow the standard sklearn fit/predict interface:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和预测遵循标准的sklearn拟合/预测接口：
- en: '[PRE30]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We evaluate the multiclass predictions using `accuracy` to find the default
    classifier achieved an accuracy of almost 98 percent:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`accuracy`评估多类预测，发现默认分类器的准确率接近98％：
- en: '[PRE31]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Sentiment analysis with Twitter and Yelp data
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Twitter和Yelp数据进行情感分析
- en: Sentiment analysis is one of the most popular uses of NLP and ML for trading
    because positive or negative perspectives on assets or other price drivers are
    likely to impact returns.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是自然语言处理和机器学习在交易中最常见的用途之一，因为对资产或其他价格驱动因素的积极或消极观点可能会影响回报。
- en: Generally, modeling approaches to sentiment analysis rely on dictionaries (as
    does the TextBlob library) or models trained on outcomes for a specific domain.
    The latter is often preferable because it permits more targeted labeling, for
    example, by tying text features to subsequent price changes rather than indirect
    sentiment scores.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，情感分析的建模方法依赖于词典（就像TextBlob库一样）或在特定领域的结果上训练的模型。后者通常更可取，因为它允许更有针对性的标记，例如，将文本特征与后续价格变化联系起来，而不是间接的情感分数。
- en: We will illustrate ML for sentiment analysis using a Twitter dataset with binary
    polarity labels and a large Yelp business review dataset with a five-point outcome
    scale.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Twitter数据集进行情感分析的机器学习，该数据集具有二元极性标签，还有一个包含五个等级结果的大型Yelp商业评论数据集。
- en: Binary sentiment classification with Twitter data
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Twitter数据的二元情感分类
- en: We use a dataset that contains 1.6 million training and 350 test tweets from
    2009 with algorithmically assigned binary positive and negative sentiment scores
    that are fairly evenly split (see the notebook for more detailed data exploration).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用包含2009年1.6百万条训练推文和350条测试推文的数据集，这些推文具有算法分配的二元正面和负面情感分数，分布相对均匀（有关更详细的数据探索，请参阅笔记本）。
- en: Multinomial naive Bayes
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: 'We create a document-term matrix with 934 tokens as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含934个标记的文档-术语矩阵，如下所示：
- en: '[PRE32]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then train the `MultinomialNB` classifier as before and predict the test
    set:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们像以前一样训练`MultinomialNB`分类器并预测测试集：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The result has over 77.5 percent accuracy:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的准确率超过了77.5％：
- en: '[PRE34]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Comparison with TextBlob sentiment scores
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与TextBlob情感分数的比较
- en: We also obtain TextBlob sentiment scores for the tweets and note (see the left
    panel in *Figure 14.5*) that positive test tweets receive a significantly higher
    sentiment estimate. We then use the `MultinomialNB` model's `.predict_proba()`
    method to compute predicted probabilities and compare both models using the respective
    area **under the curve**, or **AUC**, that we introduced in *Chapter 6*, *The
    Machine Learning Process* (see the right panel in *Figure 14.5*).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为推文获取TextBlob情感分数，并注意到（参见*图14.5*中的左面板），积极的测试推文获得了显著更高的情感估计。然后，我们使用`MultinomialNB`模型的`.predict_proba()`方法来计算预测概率，并使用我们在*第6章*“机器学习过程”中介绍的各自曲线下面积**AUC**来比较两个模型（参见*图14.5*中的右面板）。
- en: '![](img/B15439_14_05.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_05.png)'
- en: 'Figure 14.5: Accuracy of custom versus generic sentiment scores'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：自定义与通用情感分数的准确性
- en: The custom naive Bayes model outperforms TextBlob in this case, achieving a
    test AUC of 0.848 compared to 0.825 for TextBlob.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，自定义朴素贝叶斯模型的性能优于TextBlob，测试AUC为0.848，而TextBlob为0.825。
- en: Multiclass sentiment analysis with Yelp business reviews
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Yelp商业评论的多类情感分析
- en: Finally, we apply sentiment analysis to the significantly larger Yelp business
    review dataset with five outcome classes (see the notebook `sentiment_analysis_yelp`
    for code and additional details).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对Yelp商业评论数据集应用情感分析，其中有五个结果类别（有关代码和其他详细信息，请参阅笔记本`sentiment_analysis_yelp`）。
- en: The data consists of several files with information on the business, the user,
    the review, and other aspects that Yelp provides to encourage data science innovation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含有关企业、用户、评论和Yelp提供的其他方面的多个文件，以鼓励数据科学创新。
- en: We will use around six million reviews produced over the 2010-2018 period (see
    the notebook for details). The following figure shows the number of reviews and
    the average number of stars per year, as well as the star distribution across
    all reviews.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在2010-2018年期间产生的约600万条评论（有关详细信息，请参阅笔记本）。下图显示了每年的评论数量和平均星级数，以及所有评论的星级分布。
- en: '![](img/B15439_14_06.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_06.png)'
- en: 'Figure 14.6: Basic exploratory analysis of Yelp reviews'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：Yelp评论的基本探索性分析
- en: We will train various models on a 10 percent sample of the data through 2017
    and use the 2018 reviews as the test set. In addition to the text features resulting
    from the review texts, we will also use other information submitted with the review
    about the given user.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在2017年的数据的10%样本上训练各种模型，并将2018年的评论作为测试集。除了评论文本产生的文本特征外，我们还将使用有关给定用户的评论提交的其他信息。
- en: Combining text and numerical features
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结合文本和数值特征
- en: The dataset contains various numerical features (see notebook for implementation
    details).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含各种数值特征（有关实施细节，请参阅笔记本）。
- en: The vectorizers produce `scipy.sparse` matrices. To combine the vectorized text
    data with other features, we need to first convert these to sparse matrices as
    well; many sklearn objects and other libraries such as LightGBM can handle these
    very memory-efficient data structures. Converting the sparse matrix to a dense
    NumPy array risks memory overflow.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化器生成`scipy.sparse`矩阵。为了将向量化的文本数据与其他特征结合起来，我们需要首先将它们转换为稀疏矩阵；许多sklearn对象和其他库（如LightGBM）可以处理这些非常节省内存的数据结构。将稀疏矩阵转换为密集的NumPy数组会有内存溢出的风险。
- en: Most variables are categorical, so we use one-hot encoding since we have a fairly
    large dataset to accommodate the increase in features.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数变量都是分类的，因此我们使用独热编码，因为我们有一个相当大的数据集来容纳特征的增加。
- en: 'We convert the encoded numerical features and combine them with the document-term
    matrix:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编码的数值特征转换并与文档-术语矩阵结合：
- en: '[PRE35]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Benchmark accuracy
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准准确性
- en: 'Using the most frequent number of stars (=5) to predict the test set achieves
    an accuracy close to 52 percent:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最频繁的星级数（=5）来预测测试集，准确性接近52%：
- en: '[PRE36]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Multinomial naive Bayes model
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯模型
- en: Next, we train a naive Bayes classifier using a document-term matrix produced
    by `CountVectorizer` with default settings.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用由`CountVectorizer`生成的文档-术语矩阵训练一个朴素贝叶斯分类器，使用默认设置。
- en: '[PRE37]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The prediction produces 64.7 percent accuracy on the test set, a 24.4 percent
    improvement over the benchmark:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 预测在测试集上产生了64.7%的准确性，比基准提高了24.4%：
- en: '[PRE38]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Training with the combination of text and other features improves the test accuracy
    to 0.671.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本和其他特征的组合训练提高了测试准确性至0.671。
- en: Logistic regression
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: In *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*, we
    introduced binary logistic regression. sklearn also implements a multiclass model
    with a multinomial and a one-versus-all training option, where the latter trains
    a binary model for each class while considering all other classes as the negative
    class. The multinomial option is much faster and more accurate than the one-versus-all
    implementation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*中，*线性模型-从风险因素到回报预测*，我们介绍了二元逻辑回归。sklearn还实现了一个多类模型，具有多项式和一对所有的训练选项，后者为每个类训练一个二元模型，同时将所有其他类视为负类。多项式选项比一对所有的实现更快更准确。
- en: 'We evaluate a range of values for the regularization parameter `C` to identify
    the best performing model, using the `lbfgs` solver as follows (see the sklearn
    documentation for details):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估正则化参数`C`的一系列值，以确定表现最佳的模型，使用`lbfgs`求解器如下（有关详细信息，请参阅sklearn文档）：
- en: '[PRE39]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*Figure 14.7* shows the plots of the validation results.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.7*显示了验证结果的图表。'
- en: Multiclass gradient boosting with LightGBM
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LightGBM多类梯度提升
- en: 'For comparison, we also train a LightGBM gradient boosting tree ensemble with
    default settings and a `multiclass` objective:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们还使用默认设置训练了一个LightGBM梯度提升树集成，使用`multiclass`目标：
- en: '[PRE40]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Predictive performance
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预测性能
- en: '*Figure 14.7* displays the accuracy of each model for the combined data. The
    right panel plots the validation performance for the logistic regression models
    for both datasets and different levels of regularization.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.7*显示了每个模型对组合数据的准确性。右侧面板绘制了逻辑回归模型的验证性能，适用于两个数据集和不同的正则化水平。'
- en: Multinomial logistic regression performs best with a test accuracy slightly
    above 74 percent. Naive Bayes performs significantly worse. The default LightGBM
    settings did not improve over the linear model with an accuracy of 0.736\. However,
    we could tune the hyperparameters of the gradient boosting model and may well
    see performance improvements that put it at least on par with logistic regression.
    Either way, the result serves as a reminder not to discount simple, regularized
    models as they may deliver not only good results, but also do so quickly.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式逻辑回归在测试准确性略高于74%时表现最佳。朴素贝叶斯表现明显较差。默认的LightGBM设置并未提高到线性模型的准确性0.736。然而，我们可以调整梯度提升模型的超参数，很可能会看到性能改进，使其至少与逻辑回归相媲美。无论如何，结果提醒我们不要忽视简单的正则化模型，因为它们不仅可能产生良好的结果，而且速度快。
- en: '![](img/B15439_14_07.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_14_07.png)'
- en: 'Figure 14.7: Test performance on combined data (all models, left) and for logistic
    regression with varying regularization'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：组合数据的测试性能（所有模型，左）和逻辑回归的正则化变化
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored numerous techniques and options to process unstructured
    data with the goal of extracting semantically meaningful numerical features for
    use in ML models.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了许多技术和选项，以处理非结构化数据，目的是提取语义上有意义的数值特征，以供机器学习模型使用。
- en: We covered the basic tokenization and annotation pipeline and illustrated its
    implementation for multiple languages using spaCy and TextBlob. We built on these
    results to build a document model based on the bag-of-words model to represent
    documents as numerical vectors. We learned how to refine the preprocessing pipeline
    and then used the vectorized text data for classification and sentiment analysis.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们涵盖了基本的标记化和注释流程，并使用spaCy和TextBlob展示了其在多种语言中的实现。我们基于这些结果构建了基于词袋模型的文档模型，以将文档表示为数值向量。我们学习了如何完善预处理流程，然后使用向量化的文本数据进行分类和情感分析。
- en: We have two more chapters on alternative text data. In the next chapter, we
    will learn how to summarize texts using unsupervised learning to identify latent
    topics. Then, in *Chapter 16*, *Word Embeddings for Earnings Calls and SEC Filings*,
    we will learn how to represent words as vectors that reflect the context of word
    usage, a technique that has been used very successfully to provide richer text
    features for various classification tasks.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有两章关于替代文本数据。在下一章中，我们将学习如何使用无监督学习来总结文本，以识别潜在主题。然后，在第16章《用于收益电话和SEC备案的词嵌入》，我们将学习如何将单词表示为反映单词使用上下文的向量，这种技术已被非常成功地用于为各种分类任务提供更丰富的文本特征。
