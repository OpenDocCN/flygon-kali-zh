- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: RNNs for Multivariate Time Series and Sentiment Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNNs用于多变量时间序列和情感分析
- en: The previous chapter showed how **convolutional neural networks** (**CNNs**)
    are designed to learn features that represent the spatial structure of grid-like
    data, especially images, but also time series. This chapter introduces **recurrent
    neural networks** (**RNNs**) that specialize in sequential data where patterns
    evolve over time and learning typically requires memory of preceding data points.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章展示了**卷积神经网络**（**CNNs**）是如何设计来学习代表网格数据的空间结构的特征，特别是图像，但也包括时间序列。本章介绍了**循环神经网络**（**RNNs**），它专门用于序列数据，其中模式随时间演变，学习通常需要记忆先前的数据点。
- en: '**Feedforward neural networks** (**FFNNs**) treat the feature vectors for each
    sample as independent and identically distributed. Consequently, they do not take
    prior data points into account when evaluating the current observation. In other
    words, they have no memory.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈神经网络**（**FFNNs**）将每个样本的特征向量视为独立且相同分布。因此，在评估当前观察时，它们不考虑先前的数据点。换句话说，它们没有记忆。'
- en: 'The one- and two-dimensional convolutional filters used by CNNs can extract
    features that are a function of what is typically a small number of neighboring
    data points. However, they only allow shallow parameter-sharing: each output results
    from applying the same filter to the relevant time steps and features.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: CNN使用的一维和二维卷积滤波器可以提取通常是少数相邻数据点的特征。但是，它们只允许浅层参数共享：每个输出都是通过将相同的滤波器应用于相关的时间步和特征而得到的。
- en: The major innovation of the RNN model is that each output is a function of both
    the previous output and new information. RNNs can thus incorporate information
    on prior observations into the computation they perform using the current feature
    vector. This recurrent formulation enables parameter-sharing across a much deeper
    computational graph (Goodfellow, Bengio, and Courville, 2016). In this chapter,
    you will encounter **long short-term memory** (**LSTM**) units and **gated recurrent
    units** (**GRUs**), which aim to overcome the challenge of vanishing gradients
    associated with learning long-range dependencies, where errors need to be propagated
    over many connections.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: RNN模型的主要创新在于每个输出都是先前输出和新信息的函数。因此，RNN可以将先前观察的信息合并到使用当前特征向量进行的计算中。这种递归公式使得参数共享跨越更深的计算图（Goodfellow，Bengio和Courville，2016）。在本章中，您将遇到**长短期记忆**（**LSTM**）单元和**门控循环单元**（**GRUs**），它们旨在克服与学习长程依赖性相关的梯度消失的挑战，其中错误需要在许多连接上传播。
- en: Successful RNN use cases include various tasks that require mapping one or more
    input sequences to one or more output sequences and prominently feature natural
    language applications. We will explore how RNNs can be applied to univariate and
    multivariate time series to predict asset prices using market or fundamental data.
    We will also cover how RNNs can leverage alternative text data using word embeddings,
    which we covered in *Chapter 16*, *Word Embeddings for Earnings Calls and SEC
    Filings*, to classify the sentiment expressed in documents. Finally, we will use
    the most informative sections of SEC filings to learn word embeddings and predict
    returns around filing dates.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的RNN用例包括需要将一个或多个输入序列映射到一个或多个输出序列的各种任务，并且主要涉及自然语言应用。我们将探讨RNN如何应用于单变量和多变量时间序列，以使用市场或基本数据预测资产价格。我们还将介绍RNN如何利用替代文本数据使用词嵌入，我们在*第16章*中介绍了*用于盈利电话和SEC备案的词嵌入*，以对文件中表达的情感进行分类。最后，我们将使用SEC备案的最具信息量的部分来学习词嵌入，并预测备案日期周围的回报。
- en: 'More specifically, in this chapter, you will learn about the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，您将学习以下内容：
- en: How recurrent connections allow RNNs to memorize patterns and model a hidden state
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归连接如何使RNN能够记忆模式并建模隐藏状态
- en: Unrolling and analyzing the computational graph of RNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展开和分析RNN的计算图
- en: How gated units learn to regulate RNN memory from data to enable long-range
    dependencies
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 门控单元如何学习调节RNN记忆以从数据中实现长程依赖
- en: Designing and training RNNs for univariate and multivariate time series in Python
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中设计和训练单变量和多变量时间序列的RNN
- en: How to learn word embeddings or use pretrained word vectors for sentiment analysis
    with RNNs
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何学习词嵌入或使用预训练的词向量进行情感分析与RNNs
- en: Building a bidirectional RNN to predict stock returns using custom word embeddings
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义词嵌入构建双向RNN以预测股票回报
- en: You can find the code examples and additional resources in the GitHub repository's
    directory for this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的本章目录中找到代码示例和其他资源。
- en: How recurrent neural nets work
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络的工作原理
- en: RNNs assume that the input data has been generated as a sequence such that previous
    data points impact the current observation and are relevant for predicting subsequent
    values. Thus, they allow more complex input-output relationships than FFNNs and
    CNNs, which are designed to map one input vector to one output vector using a
    given number of computational steps. RNNs, in contrast, can model data for tasks
    where the input, the output, or both, are best represented as a sequence of vectors.
    For a good overview, refer to *Chapter 10* in Goodfellow, Bengio, and Courville
    (2016).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RNN假设输入数据已生成为序列，以便先前的数据点影响当前观察并且对于预测后续值是相关的。因此，它们允许比FFNNs和CNNs更复杂的输入输出关系，后者旨在使用给定数量的计算步骤将一个输入向量映射到一个输出向量。相比之下，RNN可以为输入、输出或两者都最好表示为向量序列的任务建模。有关良好的概述，请参阅Goodfellow，Bengio和Courville（2016）中的*第10章*。
- en: 'The diagram in *Figure 19.1*, inspired by Andrew Karpathy''s 2015 blog post
    *The Unreasonable Effectiveness of Recurrent Neural Networks* (see GitHub for
    a link), illustrates mappings from input to output vectors using nonlinear transformations
    carried out by one or more neural network layers:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.1*中的图表受Andrew Karpathy 2015年的博客文章*循环神经网络的非理性有效性*（请参见GitHub获取链接）的启发，通过一个或多个神经网络层进行非线性变换，说明了从输入到输出向量的映射：'
- en: '![](img/B15439_19_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_01.png)'
- en: 'Figure 19.1: Various types of sequence-to-sequence models'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.1：各种序列到序列模型
- en: The left panel shows a one-to-one mapping between vectors of fixed sizes, typical
    for FFNs and CNNs covered in the last two chapters. The other three panels show
    various RNN applications that map input vectors to output vectors by applying
    a recurrent transformation to the new input and the state produced by the previous
    iteration. The *x* input vectors to an RNN are also called **context**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧面板显示了固定大小向量之间的一对一映射，这在前两章中介绍的前馈神经网络和卷积神经网络中很典型。其他三个面板展示了各种RNN应用，通过对新输入和上一次迭代产生的状态应用循环转换，将输入向量映射到输出向量。RNN的*x*输入向量也被称为**上下文**。
- en: 'The vectors are time-indexed, as usually required by trading-related applications,
    but they could also be labeled by a different set of sequential values. Generic
    sequence-to-sequence mapping tasks and sample applications include:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是按时间索引的，通常是交易相关应用所需的，但它们也可以由不同的顺序值标记。通用的序列到序列映射任务和示例应用包括：
- en: '**One-to-many**: Image captioning, for example, takes a single vector of pixels
    (as in the previous chapter) and maps it to a sequence of words.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：例如，图像字幕接受单个像素向量（如前一章中）并将其映射到一系列单词。'
- en: '**Many-to-one**: Sentiment analysis takes a sequence of words or tokens (see
    *Chapter 14*, *Text Data for Trading – Sentiment Analysis*) and maps it to an
    output scalar or vector.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一**：情感分析接受一系列单词或标记（见*第14章*，*交易文本数据-情感分析*）并将其映射到一个输出标量或向量。'
- en: '**Many-to-many**: Machine translation or labeling of video frame map sequences
    of input vectors to sequences of output vectors, either in a synchronized (as
    shown) or asynchronous fashion. Multistep prediction of multivariate time series
    also maps several input vectors to several output vectors.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多**：机器翻译或视频帧标记将输入向量序列映射到输出向量序列，可以是同步的（如所示）或异步的。多变量时间序列的多步预测也将多个输入向量映射到多个输出向量。'
- en: Note that input and output sequences can be of arbitrary lengths because the
    recurrent transformation that is fixed but learned from the data can be applied
    as many times as needed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入和输出序列可以是任意长度的，因为可以根据数据学习到的固定但可应用多次的循环转换。
- en: Just as CNNs easily scale to large images and some CNNs can process images of
    variable size, RNNs scale to much longer sequences than networks not tailored
    to sequence-based tasks. Most RNNs can also process sequences of variable length.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就像CNN可以轻松扩展到大图像，一些CNN可以处理可变大小的图像一样，RNN可以扩展到比不适用于基于序列的任务的网络长得多的序列。大多数RNN也可以处理可变长度的序列。
- en: Unfolding a computational graph with cycles
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展开具有循环的计算图
- en: RNNs are called recurrent because they apply the same transformations to every
    element of a sequence in a way that the RNN's output depends on the outcomes of
    prior iterations. As a result, RNNs maintain an **internal state** that captures
    information about previous elements in the sequence, just like memory.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RNN被称为循环神经网络，因为它们以一种使RNN的输出取决于先前迭代结果的方式对序列的每个元素应用相同的转换。因此，RNN保持着一个内部状态，它捕捉了序列中先前元素的信息，就像记忆一样。
- en: '*Figure 19.2* shows the **computational graph** implied by a single hidden
    RNN unit that learns two weight matrices during training:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.2*显示了在训练期间学习两个权重矩阵的单隐藏RNN单元所暗示的**计算图**：'
- en: '*W*[hh]: applied to the previous hidden state, *h*[t-1]'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*[hh]：应用于先前的隐藏状态，*h*[t-1]'
- en: '*W*[hx]: applied to the current input, *x*[t]'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W*[hx]：应用于当前输入，*x*[t]'
- en: 'The RNN''s output, *y*[t], is a nonlinear transformation of the sum of the
    two matrix multiplications using, for example, the tanh or ReLU activation functions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的输出*y*[t]是两个矩阵乘法的和的非线性变换，例如使用tanh或ReLU激活函数：
- en: '![](img/B15439_19_001.png)![](img/B15439_19_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_001.png)![](img/B15439_19_02.png)'
- en: 'Figure 19.2: Recurrent and unrolled view of the computational graph of an RNN
    with a single hidden unit'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.2：具有单个隐藏单元的RNN的计算图的循环和展开视图
- en: The right side of the equation shows the effect of unrolling the recurrent relationship
    depicted in the right panel of the figure. It highlights the repeated linear algebra
    transformations and the resulting hidden state that combines information from
    past sequence elements with the current input, or context. An alternative formulation
    connects the context vector to the first hidden state only; we will outline additional
    options to modify this baseline architecture in the subsequent section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 方程的右侧显示了在图中右侧面板所示的循环关系展开的效果。它突出了重复的线性代数变换和由此产生的隐藏状态，该隐藏状态将过去序列元素的信息与当前输入或上下文结合起来。另一种表述将上下文向量连接到第一个隐藏状态；我们将在后续部分概述修改这种基线架构的其他选项。
- en: Backpropagation through time
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: The unrolled computational graph in the preceding figure highlights that the
    learning process necessarily encompasses all time steps of the given input sequence.
    The backpropagation algorithm that updates the weights during training involves
    a forward pass from left to right along with the unrolled computational graph,
    followed by a backward pass in the opposite direction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图中展开的计算图突出了学习过程必然包括给定输入序列的所有时间步。在训练期间更新权重的反向传播算法涉及沿着展开的计算图从左到右的前向传播，然后是相反方向的反向传播。
- en: As discussed in *Chapter 17*, *Deep Learning for Trading*, the backpropagation
    algorithm evaluates a loss function and computes its gradient with respect to
    the parameters to update the weights accordingly. In the RNN context, backpropagation
    runs from right to left in the computational graph, updating the parameters from
    the final time step all the way to the initial time step. Therefore, the algorithm
    is called **backpropagation through time** (Werbos 1990).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*第17章*，*交易的深度学习*中讨论的，反向传播算法评估损失函数并计算其相对于参数的梯度，以相应地更新权重。在RNN上下文中，反向传播从计算图的右侧向左侧运行，从最终时间步更新参数一直到初始时间步。因此，该算法被称为**时间反向传播**（Werbos
    1990）。
- en: It highlights both the power of an RNN to model long-range dependencies by sharing
    parameters across an arbitrary number of sequence elements while maintaining a
    corresponding state. On the other hand, it is computationally quite expensive,
    and the computations for each time step cannot be parallelized due to its inherently
    sequential nature.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它突出了RNN模拟长程依赖关系的能力，通过在任意数量的序列元素之间共享参数来保持相应的状态。另一方面，它在计算上相当昂贵，由于其固有的顺序性质，每个时间步的计算不能并行化。
- en: Alternative RNN architectures
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代RNN架构
- en: Just like the FFNN and CNN architectures we covered in the previous two chapters,
    RNNs can be optimized in a variety of ways to capture the dynamic relationship
    between input and output data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在前两章中介绍的FFNN和CNN架构一样，RNN可以通过各种方式进行优化，以捕捉输入和输出数据之间的动态关系。
- en: In addition to modifying the recurrent connections between the hidden states,
    alternative approaches include recurrent output relationships, bidirectional RNNs,
    and encoder-decoder architectures. Refer to GitHub for background references to
    complement this brief summary.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了修改隐藏状态之间的重复连接，替代方法还包括重复输出关系，双向RNN和编码器-解码器架构。请参考GitHub以获取背景参考资料，以补充本简要摘要。
- en: Output recurrence and teacher forcing
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出重复和教师强迫
- en: One way to reduce the computational complexity of hidden state recurrences is
    to connect a unit's hidden state to the prior unit's output rather than its hidden
    state. The resulting RNN has a lower capacity than the architecture discussed
    previously, but different time steps are now decoupled and can be trained in parallel.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 减少隐藏状态重复计算的计算复杂性的一种方法是将一个单元的隐藏状态连接到先前单元的输出，而不是其隐藏状态。结果RNN的容量低于先前讨论的架构，但不同的时间步骤现在是解耦的，并且可以并行训练。
- en: However, to successfully learn relevant past information, the training output
    samples need to reflect this information so that backpropagation can adjust the
    network parameters accordingly. To the extent that asset returns are independent
    of their lagged values, financial data may not meet this requirement. The use
    of previous outcome values alongside the input vectors is called **teacher forcing**
    (Williams and Zipser, 1989).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要成功地学习相关的过去信息，训练输出样本需要反映这些信息，以便反向传播可以相应地调整网络参数。在资产回报与它们的滞后值无关的程度上，金融数据可能不符合这一要求。在输入向量旁边使用先前的结果值称为**教师强迫**（Williams和Zipser，1989）。
- en: Connections from the output to the subsequent hidden state can also be used
    in combination with hidden recurrence. However, training requires backpropagation
    through time and cannot be run in parallel.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出到后续隐藏状态的连接也可以与隐藏重复结合使用。然而，训练需要通过时间进行反向传播，不能并行运行。
- en: Bidirectional RNNs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双向RNN
- en: For some tasks, it can be realistic and beneficial for the output to depend
    not only on past sequence elements, but also on future elements (Schuster and
    Paliwal, 1997). Machine translation or speech and handwriting recognition are
    examples where subsequent sequence elements are both informative and realistically
    available to disambiguate competing outputs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些任务，输出不仅依赖于过去的序列元素，而且还依赖于未来的元素（Schuster和Paliwal，1997）可能是现实和有益的。机器翻译或语音和手写识别是例子，其中后续序列元素既具有信息性又现实地可用于消除竞争输出的歧义。
- en: For a one-dimensional sequence, **bidirectional RNNs** combine an RNN that moves
    forward with another RNN that scans the sequence in the opposite direction. As
    a result, the output comes to depend on both the future and the past of the sequence.
    Applications in the natural language and music domains (Sigtia et al., 2014) have
    been very successful (see *Chapter 16*, *Word Embeddings for Earnings Calls and
    SEC Filings*, and the last example in this chapter using SEC filings).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一维序列，**双向RNN**结合了一个向前移动的RNN和另一个以相反方向扫描序列的RNN。因此，输出取决于序列的未来和过去。在自然语言和音乐领域（Sigtia等，2014）的应用非常成功（参见*第16章*，*用于收益电话和SEC备案的词嵌入*，以及本章中使用SEC备案的最后一个示例）。
- en: Bidirectional RNNs can also be used with two-dimensional image data. In this
    case, one pair of RNNs performs the forward and backward processing of the sequence
    in each dimension.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 双向RNN也可以用于二维图像数据。在这种情况下，一对RNN执行每个维度中序列的前向和后向处理。
- en: Encoder-decoder architectures, attention, and transformers
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器-解码器架构，注意力和变压器
- en: The architectures discussed so far assumed that the input and output sequences
    have equal length. Encoder-decoder architectures, also called **sequence-to-sequence**
    (**seq2seq**) architectures, relax this assumption and have become very popular
    for machine translation and other applications with this characteristic (Prabhavalkar
    et al., 2017).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的架构假设输入和输出序列具有相等的长度。编码器-解码器架构，也称为**序列到序列**（**seq2seq**）架构，放宽了这一假设，并且已经在具有这种特征的机器翻译和其他应用中变得非常流行（Prabhavalkar等，2017）。
- en: The **encoder** is an RNN that maps the input space to a different space, also
    called **latent space**, whereas the **decoder** function is a complementary RNN
    that maps the encoded input to the target space (Cho et al., 2014). In the next
    chapter, we will cover autoencoders that learn a feature representation in an
    unsupervised setting using a variety of deep learning architectures.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器**是将输入空间映射到不同空间的RNN，也称为**潜在空间**，而**解码器**函数是将编码输入映射到目标空间的互补RNN（Cho等，2014）。在下一章中，我们将介绍使用各种深度学习架构在无监督设置中学习特征表示的自动编码器。'
- en: Encoder and decoder RNNs are trained jointly so that the input of the final
    encoder hidden state becomes the input to the decoder, which, in turn, learns
    to match the training samples.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器RNN是联合训练的，以便最终编码器隐藏状态的输入成为解码器的输入，解码器反过来学习匹配训练样本。
- en: The **attention mechanism** addresses a limitation of using fixed-size encoder
    inputs when input sequences themselves vary in size. The mechanism converts raw
    text data into a distributed representation (see *Chapter 16*, *Word Embeddings
    for Earnings Calls and SEC Filings*), stores the result, and uses a weighted average
    of these feature vectors as context. The weights are learned by the model and
    alternate between putting more weight or attention to different elements of the input.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制解决了使用固定大小的编码器输入的局限性，当输入序列本身大小不同时。该机制将原始文本数据转换为分布式表示（见第16章，“用于盈利电话和SEC备案的词嵌入”），存储结果，并使用这些特征向量的加权平均作为上下文。权重由模型学习，并在不同元素的输入之间交替放置更多的权重或关注。
- en: A recent **transformer** architecture dispenses with recurrence and convolutions
    and exclusively relies on this attention mechanism to learn input-output mappings.
    It has achieved superior quality on machine translation tasks while requiring
    much less time for training, not least because it can be parallelized (Vaswani
    et al., 2017).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的**transformer**架构摒弃了循环和卷积，完全依赖于这种注意机制来学习输入输出映射。它在机器翻译任务上取得了更高的质量，同时需要更少的训练时间，这不仅因为它可以并行化（Vaswani等人，2017）。
- en: How to design deep RNNs
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何设计深度RNN
- en: The **unrolled computational graph** in *Figure 19.2* shows that each transformation
    involves a linear matrix operation followed by a nonlinear transformation that
    could be jointly represented by a single network layer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.2*中的展开计算图显示，每个转换都涉及线性矩阵操作，然后是一个非线性转换，这可以由单个网络层共同表示。'
- en: 'In the two preceding chapters, we saw how adding depth allows FFNNs, and CNNs
    in particular, to learn more useful hierarchical representations. RNNs also benefit
    from decomposing the input-output mapping into multiple layers. For RNNs, this
    mapping typically transforms:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们看到增加深度允许FFNNs，特别是CNNs，学习更有用的分层表示。RNNs也受益于将输入输出映射分解为多个层。对于RNNs，这种映射通常转换为：
- en: The input and the prior hidden state into the current hidden state
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和先前的隐藏状态进入当前的隐藏状态
- en: The hidden state into the output
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏状态到输出
- en: A common approach is to **stack recurrent layers** on top of each other so that
    they learn a hierarchical temporal representation of the input data. This means
    that a lower layer may capture higher-frequency patterns, synthesized by a higher
    layer into lower-frequency characteristics that prove useful for the classification
    or regression task. We will demonstrate this approach in the next section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是将递归层**堆叠**在一起，以便它们学习输入数据的分层时间表示。这意味着较低层可能捕获更高频率的模式，由较高层合成为对分类或回归任务有用的较低频率特征。我们将在下一节中演示这种方法。
- en: Less popular alternatives include adding layers to the connections from input
    to the hidden state, between hidden states, or from the hidden state to the output.
    These designs employ skip connections to avoid a situation where the shortest
    path between time steps increases and training becomes more difficult.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不太受欢迎的替代方案包括在从输入到隐藏状态的连接中添加层，隐藏状态之间的连接，或者从隐藏状态到输出的连接。这些设计利用跳过连接来避免时间步长之间的最短路径增加并且训练变得更加困难。
- en: The challenge of learning long-range dependencies
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习长期依赖的挑战
- en: In theory, RNNs can make use of information in arbitrarily long sequences. However,
    in practice, they are limited to looking back only a few steps. More specifically,
    RNNs struggle to derive useful context information from time steps far apart from
    the current observation (Hochreiter et al., 2001).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，RNNs可以利用任意长的序列中的信息。然而，在实践中，它们只能回望几个步骤。更具体地说，RNNs难以从当前观察到的时间步长远离的地方获得有用的上下文信息（Hochreiter等人，2001）。
- en: The fundamental problem is the impact of repeated multiplication on gradients
    during backpropagation over many time steps. As a result, the **gradients tend
    to either vanish** and decrease toward zero (the typical case), **or explode**
    and grow toward infinity (less frequent, but rendering optimization very difficult).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基本问题是在多个时间步长上反向传播期间重复乘法对梯度的影响。结果，梯度往往要么消失并趋向于零（典型情况），要么爆炸并趋向于无穷大（不太频繁，但使优化非常困难）。
- en: Even if parameters allow stability and the network is able to store memories,
    long-term interactions will receive exponentially smaller weights due to the multiplication
    of many Jacobians, the matrices containing the gradient information. Experiments
    have shown that stochastic gradient descent faces serious challenges in training
    RNNs for sequences with only 10 or 20 elements.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 即使参数允许稳定性并且网络能够存储记忆，由于许多雅可比矩阵的乘积，长期交互将获得指数级较小的权重，这些矩阵包含梯度信息。实验证明，随机梯度下降在训练仅有10或20个元素的序列的RNN时面临严重挑战。
- en: Several RNN design techniques have been introduced to address this challenge,
    including **echo state networks** (Jaeger, 2001) and **leaky units** (Hihi and
    Bengio, 1996). The latter operate at different time scales, focusing part of the
    model on higher-frequency and other parts on lower-frequency representations to
    deliberately learn and combine different aspects from the data. Other strategies
    include connections that skip time steps or units that integrate signals from
    different frequencies.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 已经引入了几种RNN设计技术来解决这一挑战，包括**回声状态网络**（Jaeger，2001）和**泄漏单元**（Hihi和Bengio，1996）。后者在不同的时间尺度上运行，将模型的一部分集中在更高频率，另一部分集中在较低频率的表示上，有意地学习和结合数据的不同方面。其他策略包括跳过时间步长的连接或集成来自不同频率的信号的单元。
- en: The most successful approaches use gated units that are trained to regulate
    how much past information a unit maintains in its current state and when to reset
    or forget this information. As a result, they are able to learn dependencies over
    hundreds of time steps. The most popular examples include **long short-term memory**
    (**LSTM**) units and **gated recurrent units** (**GRUs**). An empirical comparison
    by Chung et al. (2014) finds both units superior to simpler recurrent units such
    as tanh units, while performing equally well on various speech and music modeling
    tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最成功的方法使用经过训练的门控单元，以调节单元在当前状态中保持多少过去信息，以及何时重置或遗忘这些信息。因此，它们能够学习数百个时间步长的依赖关系。最流行的例子包括**长短期记忆**（**LSTM**）单元和**门控循环单元**（**GRUs**）。Chung等人（2014）的实证比较发现，这两种单元都优于简单的循环单元，如tanh单元，在各种语音和音乐建模任务上表现同样出色。
- en: Long short-term memory – learning how much to forget
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆-学习如何遗忘
- en: RNNs with an LSTM architecture have more complex units that maintain an internal
    state. They contain gates to keep track of dependencies between elements of the
    input sequence and regulate the cell's state accordingly. These gates recurrently
    connect to each other instead of the hidden units we encountered earlier. They
    aim to address the problem of vanishing and exploding gradients due to the repeated
    multiplication of possibly very small or very large values by letting gradients
    pass through unchanged (Hochreiter and Schmidhuber, 1996).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 具有LSTM架构的RNN具有更复杂的单元，可以保持内部状态。它们包含门，用于跟踪输入序列元素之间的依赖关系，并相应地调节细胞的状态。这些门相互连接，而不是我们之前遇到的隐藏单元。它们旨在解决由于重复相乘可能非常小或非常大的值而导致的梯度消失和爆炸问题，通过让梯度保持不变来让梯度通过（Hochreiter和Schmidhuber，1996）。
- en: 'The diagram in *Figure 19.3* shows the information flow for an unrolled LSTM
    unit and outlines its typical gating mechanism:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.3*中的图表显示了展开的LSTM单元的信息流，并概述了其典型的门控机制。'
- en: '![](img/B15439_19_03.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_03.png)'
- en: 'Figure 19.3: Information flow through an unrolled LSTM cell'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.3：通过展开的LSTM单元的信息流
- en: 'A typical LSTM unit combines **four parameterized layers** that interact with
    each other and the cell state by transforming and passing along vectors. These
    layers usually involve an input gate, an output gate, and a forget gate, but there
    are variations that may have additional gates or lack some of these mechanisms.
    The white nodes in *Figure 19.4* identify element-wise operations, and the gray
    elements represent layers with weight and bias parameters learned during training:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的LSTM单元结合了**四个参数化层**，它们相互作用并通过转换和传递向量与细胞状态交互。这些层通常包括一个输入门、一个输出门和一个遗忘门，但也有一些变体可能有额外的门或缺少其中一些机制。*图19.4*中的白色节点标识逐元素操作，灰色元素代表在训练期间学习的具有权重和偏差参数的层。
- en: '![](img/B15439_19_04.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_04.png)'
- en: 'Figure 19.4: The logic of, and math behind, an LSTM cell'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.4：LSTM单元的逻辑和数学背后的原理
- en: 'The **cell state**, *c*, passes along the horizontal connection at the top
    of the cell. The cell state''s interaction with the various gates leads to a series
    of recurrent decisions:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**细胞状态** *c* 沿着细胞顶部的水平连接传递。细胞状态与各种门的交互导致一系列递归决策：'
- en: The **forget gate** controls how much of the cell's state should be voided to
    regulate the network's memory. It receives the prior hidden state, *h*[t-1], and
    the current input, *x*[t], as inputs, computes a sigmoid activation, and multiplies
    the resulting value, *f*[t], which has been normalized to the [0, 1] range, by
    the cell state, reducing or keeping it accordingly.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**遗忘门**控制细胞状态的多少应该被清空以调节网络的记忆。它接收先前的隐藏状态*h*[t-1]和当前的输入*x*[t]，计算一个sigmoid激活，并将结果值*f*[t]乘以细胞状态，相应地减少或保持它。'
- en: The **input gate** also computes a sigmoid activation from *h*[t-1] and *x*[t]
    that produces update candidates. A *tan*[h] activation in the range from [-1,
    1] multiplies the update candidates, *u*[t], and, depending on the resulting sign,
    adds or subtracts the result from the cell state.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入门还从*h*[t-1]和*x*[t]计算出一个sigmoid激活，产生更新候选。范围在[-1, 1]的*tan*[h]激活乘以更新候选*u*[t]，并根据结果的符号，将结果加或减到细胞状态中。
- en: The **output gate** filters the updated cell state using a sigmoid activation,
    *o*[t], and multiplies it by the cell state normalized to the range [-1, 1] using
    a *tan*[h] activation.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出门**使用sigmoid激活*o*[t]过滤更新的细胞状态，并将其乘以范围在[-1, 1]之间的细胞状态，使用*tan*[h]激活进行归一化。'
- en: Gated recurrent units
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: GRUs simplify LSTM units by omitting the output gate. They have been shown to
    achieve similar performance on certain language modeling tasks, but do better
    on smaller datasets.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: GRU通过省略输出门简化了LSTM单元。已经证明它们在某些语言建模任务上可以达到类似的性能，但在较小的数据集上表现更好。
- en: GRUs aim for each recurrent unit to adaptively capture dependencies of different
    time scales. Similar to the LSTM unit, the GRU has gating units that modulate
    the flow of information inside the unit but discard separate memory cells (see
    references on GitHub for additional details).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GRUs旨在使每个循环单元自适应地捕获不同时间尺度的依赖关系。与LSTM单元类似，GRU具有调节信息流的门控单元，但丢弃了单独的记忆单元（有关更多细节，请参阅GitHub上的参考资料）。
- en: RNNs for time series with TensorFlow 2
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2进行时间序列的RNN
- en: In this section, we illustrate how to build recurrent neural nets using the
    TensorFlow 2 library for various scenarios. The first set of models includes the
    regression and classification of univariate and multivariate time series. The
    second set of tasks focuses on text data for sentiment analysis using text data
    converted to word embeddings (see *Chapter 16*, *Word Embeddings for Earnings
    Calls and SEC Filings*).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用TensorFlow 2库构建递归神经网络以应对各种情况。第一组模型包括对一元和多元时间序列的回归和分类。第二组任务集中在文本数据上，用于情感分析，使用将文本数据转换为词嵌入（参见*第16章*，*收益电话和SEC备案的词嵌入*）。
- en: More specifically, we'll first demonstrate how to prepare time-series data to
    predict the next value for **univariate time series** with a single LSTM layer
    to predict stock index values.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将首先演示如何准备时间序列数据，以预测**单变量时间序列**的下一个值，使用单个LSTM层来预测股票指数值。
- en: Next, we'll build a **deep RNN** with three distinct inputs to classify asset
    price movements. To this end, we'll combine a two-layer, **stacked LSTM** with
    learned **embeddings** and one-hot encoded categorical data. Finally, we will
    demonstrate how to model **multivariate time series** using an RNN.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个具有三个不同输入的**深度RNN**，以分类资产价格的变动。为此，我们将结合两层**堆叠LSTM**，学习的**嵌入**和独热编码的分类数据。最后，我们将演示如何使用RNN对**多变量时间序列**进行建模。
- en: Univariate regression – predicting the S&P 500
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单变量回归-预测S&P 500
- en: In this subsection, we will forecast the S&P 500 index values (refer to the
    `univariate_time_series_regression` notebook for implementation details).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将预测S&P 500指数值（有关实现细节，请参阅`univariate_time_series_regression`笔记本）。
- en: 'We''ll obtain data for 2010-2019 from the Federal Reserve Bank''s Data Service
    (FRED; see *Chapter 2*, *Market and Fundamental Data – Sources and Techniques*):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从美联储银行的数据服务（FRED）获取2010-2019年的数据（请参阅*第2章*，*市场和基本数据-来源和技术*）：
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We preprocess the data by scaling it to the [0, 1] interval using scikit-learn''s
    `MinMaxScaler()` class:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用scikit-learn的`MinMaxScaler()`类将数据缩放到[0,1]区间来预处理数据：
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to get time series data into shape for an RNN
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何将时间序列数据整理成RNN
- en: We generate sequences of 63 consecutive trading days, approximately three months,
    and use a single LSTM layer with 20 hidden units to predict the scaled index value
    one time step ahead.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成63个连续交易日的序列，大约三个月，并使用具有20个隐藏单元的单个LSTM层来预测缩放后的指数值提前一个时间步。
- en: 'The input to every LSTM layer must have three dimensions, namely:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每个LSTM层的输入必须具有三个维度，即：
- en: '**Batch size**: One sequence is one sample. A batch contains one or more samples.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理大小**：一个序列是一个样本。一个批次包含一个或多个样本。'
- en: '**Time steps**: One time step is a single observation in the sample.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间步长**：一个时间步是样本中的单个观察。'
- en: '**Features**: One feature is one observation at a time step.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**：一个特征是一个时间步的一个观察。'
- en: 'The following figure visualizes the shape of the input tensor:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图形可视化了输入张量的形状：
- en: '![](img/B15439_19_05.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/B15439_19_05.png)
- en: 'Figure 19.5: The three dimensions of an RNN input tensor'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.5：RNN输入张量的三个维度
- en: 'Our S&P 500 sample has 2,463 observations or time steps. We will create overlapping
    sequences using a window of 63 observations each. Using a simpler window of size
    *T* = 5 to illustrate this autoregressive sequence pattern, we obtain input-output
    pairs where each output is associated with its first five lags, as shown in the
    following table:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的S&P 500样本有2,463个观察或时间步。我们将使用每个窗口63个观察的重叠序列。使用大小为*T*=5的简单窗口来说明这种自回归序列模式，我们获得输入-输出对，其中每个输出与其前五个滞后相关联，如下表所示：
- en: '![](img/B15439_19_06.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/B15439_19_06.png)
- en: 'Figure 19.6: Input-output pairs with a T=5 size window'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.6：具有T=5大小窗口的输入-输出对
- en: 'We can use the `create_univariate_rnn_data()` function to stack the overlapping
    sequences that we select using a rolling window:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`create_univariate_rnn_data()`函数来堆叠我们使用滚动窗口选择的重叠序列：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We apply this function to the rescaled stock index using `window_size=63` to
    obtain a two-dimensional dataset with a shape of the number of samples x the number
    of time steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`window_size=63`将这个函数应用于重新缩放的股票指数，以获得一个形状为样本数x时间步数的二维数据集：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will use data from 2019 as our test set and reshape the features to add
    a requisite third dimension:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用2019年的数据作为我们的测试集，并重塑特征以添加必需的第三维度：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How to define a two-layer RNN with a single LSTM layer
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何定义具有单个LSTM层的两层RNN
- en: 'Now that we have created autoregressive input/output pairs from our time series
    and split the pairs into training and test sets, we can define our RNN architecture.
    The Keras interface of TensorFlow 2 makes it very straightforward to build an
    RNN with two hidden layers with the following specifications:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从时间序列中创建了自回归的输入/输出对，并将这些对分成训练集和测试集，我们可以定义我们的RNN架构。 TensorFlow 2的Keras接口使得构建具有以下规格的两个隐藏层的RNN非常简单：
- en: '**Layer 1**: An LSTM module with 10 hidden units (with `input_shape = (window_size,1)`;
    we will define `batch_size` in the omitted first dimension during training)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层1**：具有10个隐藏单元的LSTM模块（具有`input_shape = (window_size,1)`；我们将在训练期间定义省略的第一维`batch_size`）'
- en: '**Layer 2**: A fully connected module with a single unit and linear activation'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层2**：具有单个单元和线性激活的全连接模块'
- en: '**Loss**: `mean_squared_error` to match the regression objective'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失**：`mean_squared_error`以匹配回归目标'
- en: 'Just a few lines of code create the computational graph:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几行代码即可创建计算图：
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The summary shows that the model has 491 parameters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要显示模型有491个参数：
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Training and evaluating the model
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: 'We train the model using the RMSProp optimizer recommended for RNN with default
    settings and compile the model with `mean_squared_error` for this regression problem:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用RMSProp优化器进行模型训练，该优化器适用于RNN，并使用默认设置对模型进行编译，对于这个回归问题使用`mean_squared_error`：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We define an `EarlyStopping` callback and train the model for 500 episodes:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个`EarlyStopping`回调并训练模型进行500个周期：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Training stops after 138 epochs. The loss history in *Figure 19.7* shows the
    5-epoch rolling average of the training and validation RMSE, highlights the best
    epoch, and shows that the loss is 0.998 percent:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在经过138个时期后停止。 *图19.7*中的损失历史显示了训练和验证RMSE的5个时期滚动平均值，突出显示了最佳时期，并显示损失为0.998％：
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B15439_19_07.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/B15439_19_07.png)
- en: 'Figure 19.7: Cross-validation performance'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.7：交叉验证性能
- en: Re-scaling the predictions
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新缩放预测
- en: 'We use the `inverse_transform()` method of `MinMaxScaler()` to rescale the
    model predictions to the original S&P 500 range of values:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`MinMaxScaler()`的`inverse_transform()`方法将模型预测重新调整为原始S&P 500值范围：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The four plots in *Figure 19.8* illustrate the forecast performance based on
    the rescaled predictions that track the 2019 out-of-sample S&P 500 data with a
    test **information coefficient** (**IC**) of 0.9889:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.8*中的四个图表说明了基于重新缩放的预测的预测性能，这些预测性能跟踪了2019年的S&P 500数据，测试**信息系数**（**IC**）为0.9889：'
- en: '![](img/B15439_19_08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_08.png)'
- en: 'Figure 19.8: RNN performance on S&P 500 predictions'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.8：S&P 500预测的RNN性能
- en: Stacked LSTM – predicting price moves and returns
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠的LSTM - 预测价格变动和回报
- en: We'll now build a deeper model by stacking two LSTM layers using the `Quandl`
    stock price data (see the `stacked_lstm_with_feature_embeddings.ipynb` notebook
    for implementation details). Furthermore, we will include features that are not
    sequential in nature, namely, indicator variables identifying the equity and the
    month.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`Quandl`股票价格数据构建一个更深层的模型，通过堆叠两个LSTM层（有关实现细节，请参阅`stacked_lstm_with_feature_embeddings.ipynb`笔记本）。此外，我们将包括一些不是顺序性质的特征，即标识股票和月份的指示变量。
- en: '*Figure 19.9* outlines the architecture that illustrates how to combine different
    data sources in a single deep neural network. For example, instead of, or in addition
    to, one-hot encoded months, you could add technical or fundamental features:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.9*概述了如何在单个深度神经网络中结合不同的数据源的架构。例如，除了独热编码的月份之外，您还可以添加技术或基本特征：'
- en: '![](img/B15439_19_09.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_09.png)'
- en: 'Figure 19.9: Stacked LSTM architecture with additional features'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.9：具有额外特征的堆叠LSTM架构
- en: Preparing the data – how to create weekly stock returns
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据 - 如何创建每周股票回报
- en: 'We load the Quandl adjusted stock price data (see instructions on GitHub on
    how to obtain the source data) as follows (refer to the `build_dataset.ipynb`
    notebook):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了Quandl调整后的股票价格数据（请参考GitHub上如何获取源数据的说明）如下（参考`build_dataset.ipynb`笔记本）：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We start by generating weekly returns for close to 2,500 stocks with complete
    data for the 2008-17 period:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先生成了2008-17年完整数据的近2500支股票的周回报：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We create and stack rolling sequences of 52 weekly returns for each ticker
    and week as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建并堆叠了每个股票和每周52周回报的滚动序列，如下所示：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We winsorize outliers at the 1 and 99 percentile level and create a binary
    label that indicates whether the weekly return was positive:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对1和99百分位水平的异常值进行了winsorize处理，并创建了一个二进制标签，指示周回报是否为正：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As a result, we obtain 1.16 million observations on over 2,400 stocks with
    52 weeks of lagged returns each (plus the label):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们获得了超过2400支股票的116万条观察数据，每支股票有52周的滞后回报（加上标签）：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now we are ready to create the additional features, split the data into training
    and test sets, and bring them into the three-dimensional format required for the
    LSTM.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备创建额外的特征，将数据分割成训练和测试集，并将它们带入LSTM所需的三维格式。
- en: How to create multiple inputs in RNN format
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何以RNN格式创建多个输入
- en: 'This example illustrates how to combine several input data sources, namely:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明了如何结合几个输入数据源，即：
- en: Rolling sequences of 52 weeks of lagged returns
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 52周滞后回报的滚动序列
- en: One-hot encoded indicator variables for each of the 12 months
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个月的独热编码指示变量
- en: Integer-encoded values for the tickers
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股票代码的整数编码值
- en: 'The following code generates the two additional features:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成了两个额外的特征：
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we create a training set covering the 2009-2016 period and a separate
    test set with data for 2017, the last full year with data:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建了一个覆盖2009-2016年期间的训练集，以及一个包含2017年数据的单独测试集，这是最后一个完整年度的数据：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For training and test datasets, we generate a list containing the three input
    arrays as shown in *Figure 19.9*:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练和测试数据集，我们生成一个包含三个输入数组的列表，如*图19.9*所示：
- en: The lagged return series (using the format described in *Figure 19.5*)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滞后回报系列（使用*图19.5*中描述的格式）
- en: The integer-encoded stock ticker as a one-dimensional array
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整数编码的股票代码作为一个一维数组
- en: The month dummies as a two-dimensional array with one column per month
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份虚拟变量作为一个二维数组，每个月一列
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How to define the architecture using Keras' Functional API
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用Keras的Functional API定义架构
- en: 'Keras'' Functional API makes it easy to design an architecture like the one
    outlined at the beginning of this section with multiple inputs (or several outputs,
    as in the SVHN example in *Chapter 18*, *CNNs for Financial Time Series and Satellite
    Images*). This example illustrates a network with three inputs:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的Functional API使得设计像本节开头概述的具有多个输入（或多个输出，如*第18章*中的SVHN示例中）的架构变得容易。这个例子说明了一个具有三个输入的网络：
- en: '**Two stacked LSTM layers** with 25 and 10 units, respectively'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别为25和10个单元的**两个堆叠的LSTM层**
- en: An **embedding layer** that learns a 10-dimensional real-valued representation
    of the equities
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个学习股票的10维实值表示的**嵌入层**
- en: A **one-hot encoded** representation of the month
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 月份的**独热编码**表示
- en: 'We begin by defining the three inputs with their respective shapes:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了具有各自形状的三个输入：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To define **stacked LSTM layers**, we set the `return_sequences` keyword for
    the first layer to `True`. This ensures that the first layer produces an output
    in the expected three-dimensional input format. Note that we also use dropout
    regularization and how the Functional API passes the tensor outputs from one layer
    to the subsequent layer''s input:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义**堆叠的LSTM层**，我们将第一层的`return_sequences`关键字设置为`True`。这确保第一层以预期的三维输入格式产生输出。请注意，我们还使用了辍学正则化以及Functional
    API如何将张量输出从一层传递到后续层的输入：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The TensorFlow 2 guide for RNNs highlights the fact that GPU support is only
    available when using the default values for most LSTM settings ([https://www.tensorflow.org/guide/keras/rnn](https://www.tensorflow.org/guide/keras/rnn)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2中关于RNN的指南强调了只有在使用大多数LSTM设置的默认值时才支持GPU（[https://www.tensorflow.org/guide/keras/rnn](https://www.tensorflow.org/guide/keras/rnn)）。
- en: 'The **embedding layer** requires:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入层**需要：'
- en: The `input_dim` keyword, which defines how many embeddings the layer will learn
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_dim`关键字，定义了层将学习多少个嵌入'
- en: The `output_dim` keyword, which defines the size of the embedding
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dim`关键字，定义了嵌入的大小'
- en: The `input_length` parameter, which sets the number of elements passed to the
    layer (here, only one ticker per sample)
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_length`参数，设置传递给层的元素数量（这里每个样本只有一个标记）'
- en: 'The goal of the embedding layer is to learn vector representations that capture
    the relative locations of the feature values to one another with respect to the
    outcome. We''ll choose a five-dimensional embedding for the roughly 2,500 ticker
    values to combine the embedding layer with the LSTM layer and the month dummies
    we need to reshape (or flatten) it:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的目标是学习向量表示，捕捉特征值相对于结果的相对位置。我们将选择一个五维嵌入，用于大约2500个标记值，将嵌入层与LSTM层和我们需要重塑（或扁平化）的月份虚拟变量相结合：
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can concatenate the three tensors, followed by `BatchNormalization`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以连接三个张量，然后是`BatchNormalization`：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The fully connected final layers learn a mapping from these stacked LSTM layers,
    ticker embeddings, and month indicators to the binary outcome that reflects a
    positive or negative return over the following week. We formulate the complete
    RNN by defining its inputs and outputs with the implicit data flow we just defined:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接的最终层学习了从这些堆叠的LSTM层、标记嵌入和月份指示符到反映以下周正面或负面收益的二进制结果的映射。我们通过定义其输入和输出来构建完整的RNN，使用我们刚刚定义的隐式数据流：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The summary lays out this slightly more sophisticated architecture with 16,984
    parameters:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 总结列出了这个略微复杂的架构，其中包含16,984个参数：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We compile the model using the recommended RMSProp optimizer with default settings
    and compute the AUC metric that we''ll use for early stopping:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用推荐的默认设置的RMSProp优化器对模型进行编译，并计算AUC指标，我们将用于早停：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We train the model for 50 epochs by using early stopping:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用早停法对模型进行了50个时期的训练：
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following plots show that training stops after 8 epochs, each of which
    takes around three minutes on a single GPU. It results in a test AUC of 0.6816
    and a test accuracy of 0.6193 for the best model:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示，训练在8个时期后停止，每个时期在单个GPU上大约需要三分钟。最佳模型的测试AUC为0.6816，测试准确度为0.6193：
- en: '![](img/B15439_19_10.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_10.png)'
- en: 'Figure 19.10: Stacked LSTM classification—cross-validation performance'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.10：堆叠LSTM分类-交叉验证性能
- en: The IC for the test prediction and actual weekly returns is 0.32.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 测试预测和实际每周收益率的IC为0.32。
- en: Predicting returns instead of directional price moves
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测收益率而不是价格方向性变动
- en: The `stacked_lstm_with_feature_embeddings_regression.ipynb` notebook illustrates
    how to adapt the model to the regression task of predicting returns rather than
    binary price changes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`stacked_lstm_with_feature_embeddings_regression.ipynb`笔记本演示了如何将模型调整为预测收益率而不是二进制价格变动的回归任务。'
- en: 'The required changes are minor; just do the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的更改很小；只需执行以下操作：
- en: Select the `fwd_returns` outcome instead of the binary `label`.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`fwd_returns`结果，而不是二进制的`label`。
- en: Convert the model output to linear (the default) instead of `sigmoid`.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型输出转换为线性（默认）而不是`sigmoid`。
- en: Update the loss to mean squared error (and early stopping references).
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新损失为均方误差（和早停引用）。
- en: Remove or update optional metrics to match the regression task.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除或更新可选指标以匹配回归任务。
- en: 'Using otherwise the same training parameters (except that the Adam optimizer
    with default settings yields a better result in this case), the validation loss
    improves for nine epochs. The average weekly IC is 3.32, and 6.68 for the entire
    period while significant at the 1 percent level. The average weekly return differential
    between the equities in the top and bottom quintiles of predicted returns is slightly
    above 20 basis points:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Adam优化器使用默认设置在这种情况下效果更好之外，使用相同的训练参数，验证损失在九个时期内有所改善。平均每周IC为3.32，整个时期为6.68，显著水平为1%。预测收益率前五分位和后五分位之间的平均每周收益率差略高于20个基点：
- en: '![](img/B15439_19_11.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_11.png)'
- en: 'Figure 19.11: Stacked LSTM regression—out-of-sample performance'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.11：堆叠LSTM回归-样本外表现
- en: Multivariate time-series regression for macro data
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宏观数据的多变量时间序列回归
- en: So far, we have limited our modeling efforts to a single time series. RNNs are
    well-suited to multivariate time series and represent a nonlinear alternative
    to the **vector autoregressive** (**VAR**) models we covered in *Chapter 9*, *Time-Series
    Models for Volatility Forecasts and Statistical Arbitrage*. Refer to the `multivariate_timeseries`
    notebook for implementation details.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的建模工作局限于单个时间序列。RNN非常适合多变量时间序列，并且是**向量自回归**（**VAR**）模型的非线性替代方法，我们在*第9章*，*波动率预测和统计套利的时间序列模型*中介绍过。有关实现细节，请参阅`multivariate_timeseries`笔记本。
- en: Loading sentiment and industrial production data
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载情绪和工业生产数据
- en: 'We''ll show how to model and forecast multiple time series using RNNs with
    the same dataset we used for the VAR example. It has monthly observations over
    40 years on consumer sentiment and industrial production from the Federal Reserve''s
    FRED service:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何使用相同数据集对多个时间序列进行建模和预测，这与我们用于VAR示例的数据集相同。该数据集包括美联储FRED服务上40年来每月的消费者情绪和工业生产观察值：
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Making the data stationary and adjusting the scale
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使数据稳定并调整比例
- en: 'We apply the same transformation—annual difference for both series, prior log-transform
    for industrial production—to achieve stationarity (see *Chapter 9,* *Time-Series
    Models for Volatility Forecasts and Statistical Arbitrage* for details). We also
    rescale it to the [0, 1] range to ensure that the network gives both series equal
    weight during training:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用相同的转换-对两个系列进行年度差分，对工业生产进行对数转换，以实现平稳性（有关详细信息，请参见*第9章*，*波动率预测和统计套利的时间序列模型*）。我们还将其重新缩放到[0,1]范围，以确保网络在训练期间给予两个系列相等的权重：
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Figure 19.12* displays the original and transformed macro time series:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.12*显示了原始和转换后的宏观时间序列：'
- en: '![](img/B15439_19_12.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_12.png)'
- en: 'Figure 19.12: Original and transformed time series'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.12：原始和转换后的时间序列
- en: Creating multivariate RNN inputs
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建多变量RNN输入
- en: 'The `create_multivariate_rnn_data()` function transforms a dataset of several
    time series into the three-dimensional shape required by TensorFlow''s RNN layers,
    formed as `n_samples × window_size × n_series`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_multivariate_rnn_data()`函数将多个时间序列数据集转换为TensorFlow RNN层所需的三维形状，形成`n_samples
    × window_size × n_series`：'
- en: '[PRE29]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A `window_size` value of 18 ensures that the entries in the second dimension
    are the lagged 18 months of the respective output variable. We thus obtain the
    RNN model inputs for each of the two features as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`window_size`值为18确保第二维中的条目是各自输出变量的滞后18个月。因此，我们得到了每个特征的RNN模型输入如下：'
- en: '[PRE30]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we split our data into a training and a test set, using the last 24
    months to test the out-of-sample performance:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据分成训练集和测试集，使用最后24个月来测试样本外表现：
- en: '[PRE31]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Defining and training the model
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义和训练模型
- en: Given the relatively small dataset, we use a simpler RNN architecture than in
    the previous example. It has a single LSTM layer with 12 units, followed by a
    fully connected layer with 6 units. The output layer has two units, one for each
    time series.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于数据集相对较小，我们使用比前一个示例中更简单的RNN架构。它有一个具有12个单元的单个LSTM层，后面是一个具有6个单元的全连接层。输出层有两个单元，分别用于每个时间序列。
- en: 'We compile using mean absolute loss and the recommended RMSProp optimizer:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用平均绝对损失和推荐的RMSProp优化器进行编译：
- en: '[PRE32]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The model still has 812 parameters, compared to 10 for the `VAR(1, 1)` model
    from *Chapter 9**, Time-Series Models for Volatility Forecasts and Statistical
    Arbitrage*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与*第9章*中的`VAR(1, 1)`模型相比，该模型仍具有812个参数，而`VAR(1, 1)`模型只有10个参数，*时间序列模型用于波动率预测和统计套利*：
- en: '[PRE33]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We train for 100 epochs with a `batch_size` of 20 using early stopping:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用批量大小为20进行100个时期的训练，并使用提前停止：
- en: '[PRE34]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Training stops early after 62 epochs, yielding a test MAE of 0.034, an almost
    25 percent improvement over the test MAE for the VAR model of 0.043 on the same
    task.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在62个时期后，训练提前停止，测试MAE为0.034，比VAR模型在相同任务上的测试MAE 0.043提高了近25％。
- en: However, the two results are not fully comparable because the RNN produces 18
    1-step-ahead forecasts whereas the VAR model uses its own predictions as input
    for its out-of-sample forecast. You may want to tweak the VAR setup to obtain
    comparable forecasts and compare the performance.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两个结果并不完全可比，因为RNN生成了18个1步预测，而VAR模型使用其自己的预测作为其样本外预测的输入。您可能希望调整VAR设置以获得可比较的预测并进行比较性能。
- en: '*Figure 19.13* highlights training and validation errors, and the out-of-sample
    predictions for both series:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.13*突出显示了多个宏观系列的训练和验证错误，以及样本外预测：'
- en: '![](img/B15439_19_13.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_13.png)'
- en: 'Figure 19.13: Cross-validation and test results for RNNs with multiple macro
    series'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.13：具有多个宏观系列的RNN的交叉验证和测试结果
- en: RNNs for text data
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据的RNN
- en: RNNs are commonly applied to various natural language processing tasks, from
    machine translation to sentiment analysis, that we already encountered in Part
    3 of this book. In this section, we will illustrate how to apply an RNN to text
    data to detect positive or negative sentiment (easily extensible to a finer-grained
    sentiment scale) and to predict stock returns.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: RNN通常应用于各种自然语言处理任务，从机器翻译到情感分析，我们在本书的第3部分已经遇到过。在本节中，我们将说明如何将RNN应用于文本数据以检测积极或消极情绪（可以轻松扩展到更精细的情绪规模），并预测股票回报。
- en: More specifically, we'll use word embeddings to represent the tokens in the
    documents. We covered word embeddings in *Chapter 16*, *Word Embeddings for Earnings
    Calls and SEC Filings*. They are an excellent technique for converting a token
    into a dense, real-value vector because the relative location of words in the
    embedding space encodes useful semantic aspects of how they are used in the training
    documents.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将使用单词嵌入来表示文档中的标记。我们在*第16章*，*用于盈利电话和SEC备案的单词嵌入*中介绍了单词嵌入。它们是将标记转换为密集的实值向量的优秀技术，因为单词在嵌入空间中的相对位置编码了它们在训练文档中的有用语义方面。
- en: We saw in the previous stacked RNN example that TensorFlow has a built-in embedding
    layer that allows us to train vector representations specific to the task at hand.
    Alternatively, we can use pretrained vectors. We'll demonstrate both approaches
    in the following three sections.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的堆叠RNN示例中看到，TensorFlow有一个内置的嵌入层，允许我们训练特定于手头任务的向量表示。或者，我们可以使用预训练的向量。我们将在以下三个部分中演示这两种方法。
- en: LSTM with embeddings for sentiment classification
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有情感分类的嵌入LSTM
- en: This example shows how to learn custom embedding vectors while training an RNN
    on the classification task. This differs from the word2vec model that learns vectors
    while optimizing predictions of neighboring tokens, resulting in their ability
    to capture certain semantic relationships among words (see *Chapter 16*, *Word
    Embeddings for Earnings Calls and SEC Filings*). Learning word vectors with the
    goal of predicting sentiment implies that embeddings will reflect how a token
    relates to the outcomes it is associated with.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了如何在对分类任务进行RNN训练时学习自定义嵌入向量。这与word2vec模型不同，后者在优化相邻标记的预测时学习向量，从而能够捕捉单词之间的某些语义关系（参见*第16章*，*用于盈利电话和SEC备案的单词嵌入*）。学习旨在预测情感的单词向量意味着嵌入将反映标记与其关联的结果的关系。
- en: Loading the IMDB movie review data
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载IMDB电影评论数据
- en: To keep the data manageable, we will illustrate this use case with the IMDB
    reviews dataset, which contains 50,000 positive and negative movie reviews, evenly
    split into a training set and a test set, with balanced labels in each dataset.
    The vocabulary consists of 88,586 tokens. Alternatively, you could use the much
    larger Yelp review data (after converting the text into numerical sequences; see
    the next section on using pretrained embeddings or TensorFlow 2 docs).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据易于管理，我们将使用IMDB评论数据集来说明这种用例，该数据集包含50,000条正面和负面电影评论，均匀分为训练集和测试集，并且每个数据集中的标签均衡。词汇量包括88,586个标记。或者，您可以使用更大的Yelp评论数据（在将文本转换为数字序列后；请参阅下一节关于使用预训练嵌入或TensorFlow
    2文档）。
- en: 'The dataset is bundled into TensorFlow and can be loaded so that each review
    is represented as an integer-encoded sequence. We can limit the vocabulary to
    `num_words` while filtering out frequent and likely less informative words using
    `skip_top` as well as sentences longer than `maxlen`. We can also choose the `oov_char`
    value, which represents tokens we chose to exclude from the vocabulary on frequency
    grounds:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被捆绑到TensorFlow中，并且可以被加载，以便每个评论都表示为整数编码的序列。我们可以限制词汇量为`num_words`，同时使用`skip_top`过滤频繁和可能不太信息丰富的词，以及超过`maxlen`的句子。我们还可以选择`oov_char`值，它表示我们选择根据频率排除词汇的标记：
- en: '[PRE35]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the second step, convert the lists of integers into fixed-size arrays that
    we can stack and provide as an input to our RNN. The `pad_sequence` function produces
    arrays of equal length, truncated and padded to conform to `maxlen`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，将整数列表转换为固定大小的数组，我们可以堆叠并提供作为输入给我们的RNN。`pad_sequence`函数生成相等长度的数组，截断和填充以符合`maxlen`：
- en: '[PRE36]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Defining embedding and the RNN architecture
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义嵌入和RNN架构
- en: 'Now we can set up our RNN architecture. The first layer learns the word embeddings.
    We define the embedding dimensions as before, using the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以设置我们的RNN架构。第一层学习单词嵌入。我们像以前一样定义嵌入维度，使用以下内容：
- en: The `input_dim` keyword, which sets the number of tokens that we need to embed
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_dim`关键字，设置我们需要嵌入的标记数目'
- en: The `output_dim` keyword, which defines the size of each embedding
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dim`关键字，定义每个嵌入的大小'
- en: The `input_len` parameter, which specifies how long each input sequence is going
    to be
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_len`参数，指定每个输入序列的长度'
- en: 'Note that we are using GRU units this time that train faster and perform better
    on smaller amounts of data. We are also using recurrent dropout for regularization:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这次我们使用GRU单元进行训练，它在较小的数据量上训练更快并且表现更好。我们还使用循环丢失进行正则化：
- en: '[PRE37]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The resulting model has over 2 million trainable parameters:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型有超过200万个可训练参数：
- en: '[PRE38]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We compile the model to use the AUC metric and train with early stopping:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编译模型以使用AUC指标，并使用早期停止进行训练：
- en: '[PRE39]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Training stops after 12 epochs, and we recover the weights for the best models
    to find a high test AUC of 0.9393:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在12个时期后停止，我们恢复最佳模型的权重，找到高达0.9393的测试AUC：
- en: '[PRE40]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '*Figure 19.14* displays the cross-validation performance in terms of accuracy
    and AUC:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.14*显示了准确性和AUC方面的交叉验证性能：'
- en: '![](img/B15439_19_14.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_14.png)'
- en: 'Figure 19.14: Cross-validation for RNN using IMDB data with custom embeddings'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.14：使用IMDB数据进行RNN的交叉验证，使用自定义嵌入
- en: Sentiment analysis with pretrained word vectors
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的单词向量进行情感分析
- en: In *Chapter 16*, *Word Embeddings for Earnings Calls and SEC Filings*, we discussed
    how to learn domain-specific word embeddings. Word2vec and related learning algorithms
    produce high-quality word vectors but require large datasets. Hence, it is common
    that research groups share word vectors trained on large datasets, similar to
    the weights for pretrained deep learning models that we encountered in the section
    on transfer learning in the previous chapter.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第16章*中，*用于盈利电话和SEC备案的词嵌入*，我们讨论了如何学习特定领域的词嵌入。Word2vec和相关的学习算法产生高质量的单词向量，但需要大型数据集。因此，研究小组通常共享在大型数据集上训练的单词向量，类似于我们在上一章的迁移学习部分遇到的预训练深度学习模型的权重。
- en: We are now going to illustrate how to use pretrained **global vectors for word
    representation** (**GloVe**) provided by the Stanford NLP group with the IMDB
    review dataset (refer to GitHub for references and the `sentiment_analysis_pretrained_embeddings`
    notebook for implementation details).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将说明如何使用由斯坦福NLP组提供的IMDB评论数据集的预训练**全局词向量表示**（**GloVe**）（有关参考和`sentiment_analysis_pretrained_embeddings`笔记本的实现细节，请参考GitHub）。
- en: Preprocessing the text data
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理文本数据
- en: 'We are going to load the IMDB dataset from the source to manually preprocess
    it (see the notebook). TensorFlow provides a `Tokenizer`, which we''ll use to
    convert the text documents to integer-encoded sequences:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从源加载IMDB数据集，手动预处理它（请参阅笔记本）。TensorFlow提供了一个`Tokenizer`，我们将使用它将文本文档转换为整数编码序列：
- en: '[PRE41]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We also use the `pad_sequences` function to convert the list of lists (of unequal
    length) to stacked sets of padded and truncated arrays for both the training and
    test data:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用`pad_sequences`函数将不等长的列表（的列表）转换为填充和截断数组的堆叠集，用于训练和测试数据：
- en: '[PRE42]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Loading the pretrained GloVe embeddings
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载预训练的GloVe嵌入
- en: 'We downloaded and unzipped the GloVe data to the location indicated in the
    code and will now create a dictionary that maps GloVe tokens to 100-dimensional,
    real-valued vectors:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载并解压了GloVe数据到代码中指示的位置，现在将创建一个将GloVe标记映射到100维实值向量的字典：
- en: '[PRE43]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'There are around 340,000 word vectors that we use to create an embedding matrix
    that matches the vocabulary so that the RNN can access embeddings by the token
    index:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用大约340,000个单词向量来创建一个与词汇表匹配的嵌入矩阵，以便RNN可以通过标记索引访问嵌入：
- en: '[PRE44]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Defining the architecture with frozen weights
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用冻结权重定义架构
- en: 'The difference with the RNN setup in the previous example is that we are going
    to pass the embedding matrix to the embedding layer and set it to *not trainable*
    so that the weights remain fixed during training:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个示例中的RNN设置不同的是，我们将嵌入矩阵传递给嵌入层，并将其设置为*不可训练*，以便在训练期间保持权重不变：
- en: '[PRE45]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'From here on, we proceed as before. Training continues for 32 epochs, as shown
    in *Figure 19.15*, and we obtain a test AUC score of 0.9106\. This is slightly
    worse than our result in the previous sections where we learned custom embedding
    for this domain, underscoring the value of training your own word embeddings:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们继续进行。训练持续32个时期，如*图19.15*所示，我们获得了测试AUC分数为0.9106。这比我们在先前部分学习此领域的自定义嵌入的结果稍差，强调了训练自己的词嵌入的价值：
- en: '![](img/B15439_19_15.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_15.png)'
- en: 'Figure 19.15: Cross-validation and test results for RNNs with multiple macro
    series'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.15：具有多个宏观系列的RNN的交叉验证和测试结果
- en: You may want to apply these techniques to the larger financial text datasets
    that we used in Part 3.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望将这些技术应用于我们在第3部分中使用的更大的财务文本数据集。
- en: Predicting returns from SEC filing embeddings
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从SEC备案嵌入预测回报
- en: In *Chapter 16*, *Word Embeddings for Earnings Calls and SEC Filings*, we discussed
    important differences between product reviews and financial text data. While the
    former was useful to illustrate important workflows, in this section, we will
    tackle more challenging but also more relevant financial documents. More specifically,
    we will use the SEC filings data introduced in *Chapter 16*, *Word Embeddings
    for Earnings Calls and SEC Filings*, to learn word embeddings tailored to predicting
    the return of the ticker associated with the disclosures from before publication
    to one week after.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第16章*，*收益电话和SEC备案的词嵌入*中，我们讨论了产品评论和财务文本数据之间的重要差异。虽然前者对于说明重要的工作流程很有用，但在本节中，我们将处理更具挑战性但也更相关的财务文件。更具体地说，我们将使用*第16章*，*收益电话和SEC备案的词嵌入*中介绍的SEC备案数据，学习适合于预测与披露相关的股票代码的回报的词嵌入，从发布前到一周后。
- en: The `sec_filings_return_prediction` notebook contains the code examples for
    this section. See the `sec_preprocessing` notebook in *Chapter 16*, *Word Embeddings
    for Earnings Calls and SEC Filings*, and instructions in the data folder on GitHub
    on how to obtain the data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`sec_filings_return_prediction`笔记本包含了本节的代码示例。请参阅GitHub上的数据文件夹中的`sec_preprocessing`笔记本，了解如何获取数据的说明。'
- en: Source stock price data using yfinance
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用yfinance获取股票价格数据
- en: 'There are 22,631 filings for the period 2013-16\. We use yfinance to obtain
    stock price data for the related 6,630 tickers because it achieves higher coverage
    than Quandl''s WIKI Data. We use the ticker symbol and filing date from the filing
    index (see *Chapter 16*, *Word Embeddings for Earnings Calls and SEC Filings*)
    to download daily adjusted stock prices for three months before and one month
    after the filing data as follows, capturing both the price data and unsuccessful
    tickers in the process:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 2013-16年期间有22,631份备案。我们使用yfinance获取相关的6,630个股票代码的股价数据，因为它的覆盖范围比Quandl的WIKI数据更广。我们使用备案索引（请参阅*第16章*，*收益电话和SEC备案的词嵌入*）中的股票代码和备案日期下载备案数据之前三个月和之后一个月的每日调整股价数据，捕获了价格数据和不成功的股票代码的过程：
- en: '[PRE46]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We obtain data on 3,954 tickers and source prices for a few hundred missing
    tickers using the Quandl Wiki data (see the notebook) and end up with 16,758 filings
    for 4,762 symbols.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了3,954个股票代码的数据，并使用Quandl Wiki数据获取了几百个缺失的股票代码的价格（请参阅笔记本），最终得到了4,762个符号的16,758份备案。
- en: Preprocessing SEC filing data
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理SEC备案数据
- en: 'Compared to product reviews, financial text documents tend to be longer and
    have a more formal structure. In addition, in this case, we rely on data sourced
    from EDGAR that requires parsing of the XBRL source (see *Chapter 2*, *Market
    and Fundamental Data – Sources and Techniques*) and may have errors such as including
    material other than the desired sections. We take several steps during preprocessing
    to address outliers and format the text data as integer sequences of equal length,
    as required by the model that we will build in the next section:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 与产品评论相比，财务文本文件往往更长，结构更正式。此外，在这种情况下，我们依赖于从EDGAR获取的数据，这需要解析XBRL源（请参阅*第2章*，*市场和基本数据-来源和技术*），并且可能存在错误，例如包括除所需部分以外的其他材料。在预处理期间，我们采取了几个步骤来处理异常值，并将文本数据格式化为等长的整数序列，这是下一节中我们将构建的模型所要求的：
- en: Remove all sentences that contain fewer than 5 or more than 50 tokens; this
    affects approximately. 5 percent of sentences.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除包含少于5个或多于50个标记的所有句子；这影响了大约5%的句子。
- en: Create 28,599 bigrams, 10,032 trigrams, and 2,372 n-grams with 4 elements.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建28,599个二元组，10,032个三元组和2,372个具有4个元素的n元组。
- en: Convert filings to a sequence of integers that represent the token frequency
    rank, removing filings with fewer than 100 tokens and truncating sequences at
    20,000 elements.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将备案转换为代表标记频率排名的整数序列，删除少于100个标记的备案，并在20,000个元素处截断序列。
- en: '*Figure 19.16* highlights some corpus statistics for the remaining 16,538 filings
    with 179,214,369 tokens, around 204,206 of which are unique. The left panel shows
    the token frequency distribution on a log-log scale; the most frequent terms,
    "million," "business," "company," and "products" occur more than 1 million times
    each. As usual, there is a very long tail, with 60 percent of tokens occurring
    fewer than 25 times.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.16*突出了剩下的16,538份备案的一些语料库统计信息，共有179,214,369个标记，其中大约有204,206个是唯一的。左侧面板显示了标记频率分布的对数对数尺度；最常见的术语“百万”，“业务”，“公司”和“产品”每个出现超过100万次。与往常一样，有一个非常长的尾部，60%的标记出现次数少于25次。'
- en: 'The central panel shows the distribution of the sentence lengths with a mode
    of around 10 tokens. Finally, the right panel shows the distribution of the filing
    length with a peak at 20,000 due to truncation:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 中央面板显示了句子长度的分布，众数约为10个标记。最后，右侧面板显示了备案长度的分布，由于截断而在20,000处达到峰值：
- en: '![](img/B15439_19_16.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_16.png)'
- en: 'Figure 19.16: Cross-validation and test results for RNNs with multiple macro
    series'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.16：具有多个宏观系列的RNN的交叉验证和测试结果
- en: Preparing data for the RNN model
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为RNN模型准备数据
- en: Now we need an outcome for our model to predict. We'll compute (somewhat arbitrarily)
    five-day forward returns for the day of filing (or the day before if there are
    no prices for that date), assuming that filing occurred after market hours. Clearly,
    this assumption could be wrong, underscoring the need for **point-in-time data**
    emphasized in *Chapter 2*, *Market and Fundamental Data – Sources and Techniques*,
    and *Chapter 3*, *Alternative Data for Finance – Categories and Use Cases*. We'll
    ignore this issue as the hidden cost of using free data.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个模型预测的结果。我们将计算（在某种程度上是任意的）申报当天（如果当天没有价格，则为前一天）的五天正向回报，假设申报发生在市场闭市后。显然，这种假设可能是错误的，强调了*第2章*，*市场和基本数据-来源和技术*，以及*第3章*，*金融替代数据-类别和用例*中强调的**即时数据**的需求。我们将忽略使用免费数据的隐藏成本问题。
- en: 'We compute the forward returns as follows, removing outliers with weekly returns
    below 50 or above 100 percent:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算正向回报如下，去除每周回报低于50%或高于100%的异常值：
- en: '[PRE47]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This leaves us with 16,355 data points. Now we combine these outcomes with
    their matching filing sequences and convert the list of returns to a NumPy array:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们留下了16,355个数据点。现在我们将这些结果与它们匹配的申报序列相结合，并将回报列表转换为NumPy数组：
- en: '[PRE48]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, we create a 90:10 training/test split and use the `pad_sequences`
    function introduced in the first example in this section to generate fixed-length
    sequences of 20,000 elements each:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建了一个90:10的训练/测试分割，并使用本节中第一个示例中介绍的`pad_sequences`函数生成每个长度为20,000的固定长度序列：
- en: '[PRE49]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Building, training, and evaluating the RNN model
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建、训练和评估RNN模型
- en: 'Now we can define our RNN architecture. The first layer learns the word embeddings.
    We define the embedding dimensions as previously, setting the following:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义我们的RNN架构。第一层学习单词嵌入。我们将嵌入维度设置为以前的值，设置如下：
- en: The `input_dim` keyword to the size of the vocabulary
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_dim`关键字用于词汇量的大小'
- en: The `output_dim` keyword to the size of each embedding
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dim`关键字用于每个嵌入的大小'
- en: The `input_length` parameter to how long each input sequence is going to be
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_length`参数用于每个输入序列的长度'
- en: 'For the recurrent layer, we use a bidirectional GRU unit that scans the text
    both forward and backward and concatenates the resulting output. We also add batch
    normalization and dropout for regularization with a five-unit dense layer before
    the linear output:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 对于循环层，我们使用了一个双向GRU单元，它可以同时向前和向后扫描文本，并连接生成的输出。我们还在线性输出之前添加了批量归一化和正则化的dropout，以及一个五单元的密集层：
- en: '[PRE50]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The resulting model has over 2.5 million trainable parameters:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的模型具有超过250万个可训练参数：
- en: '[PRE51]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We compile using the Adam optimizer, targeting the mean squared loss for this
    regression task while also tracking the square root of the loss and the mean absolute
    error as optional metrics:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Adam优化器进行编译，针对这个回归任务的均方损失，并选择跟踪损失的平方根和平均绝对误差作为可选指标：
- en: '[PRE52]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'With early stopping, we train for up to 100 epochs on batches of 32 observations
    each:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提前停止，我们对每批32个观察值进行最多100个时期的训练：
- en: '[PRE53]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The mean absolute error improves for only 4 epochs, as shown in the left panel
    of *Figure 19.17:*
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 平均绝对误差仅在4个时期内有所改善，如*图19.17*左侧面板所示：
- en: '![](img/B15439_19_17.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_19_17.png)'
- en: 'Figure 19.17: Cross-validation test results for RNNs using SEC filings to predict
    weekly returns'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.17：使用SEC申报文件预测每周回报的RNN交叉验证测试结果
- en: 'On the test set, the best model achieves a highly significant IC of 6.02:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上，最佳模型实现了高度显著的IC值为6.02：
- en: '[PRE54]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Lessons learned and next steps
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学到的教训和下一步
- en: The model is capable of generating return predictions that are significantly
    better than chance using only text data. There are both caveats that suggest taking
    the results with a grain of salt and reasons to believe we could improve on the
    result of this experiment.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型能够仅使用文本数据生成明显优于随机的回报预测。有一些警告表明应该对结果持保留态度，也有理由相信我们可以改进这个实验的结果。
- en: On the one hand, the quality of both the stock price data and the parsed SEC
    filings is far from perfect. It's unclear whether price data issues bias the results
    positively or negatively, but they certainly increase the margin of error. More
    careful parsing and cleaning of the SEC filings would most likely improve the
    results by removing noise.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，股价数据和解析的SEC申报文件的质量都远非完美。目前尚不清楚价格数据问题是否会对结果产生积极或消极的偏见，但它们肯定会增加误差。更加仔细地解析和清理SEC申报文件很可能会通过消除噪音来改善结果。
- en: On the other hand, there are numerous optimizations that may well improve the
    result. Starting with the text input, we did not attempt to parse the filing content
    beyond selecting certain sections; there may be value in removing boilerplate
    language or otherwise trying to pick the most meaningful statements. We also made
    somewhat arbitrary choices about the maximum length of filings and the size of
    the vocabulary that we could revisit. We could also shorten or lengthen the weekly
    prediction horizon. Furthermore, there are multiple aspects of the model architecture
    that we could refine, from the size of the embeddings to the number and size of
    layers and the degree of regularization.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有许多优化可能会改善结果。从文本输入开始，我们并没有尝试解析申报内容，而是仅选择了某些部分；去除样板语言或尝试选择最有意义的陈述可能是有价值的。我们还对申报的最大长度和词汇量的大小做出了一些任意选择，我们可以重新审视这些选择。我们还可以缩短或延长每周的预测时间跨度。此外，模型架构的多个方面都可以进行改进，从嵌入的大小到层数和大小以及正则化程度。
- en: Most fundamentally, we could combine the text input with a richer set of complementary
    features, as demonstrated in the previous section, using stacked LSTM with multiple
    inputs. Finally, we would certainly want a larger set of filings.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的是，我们可以将文本输入与更丰富的互补特征集合结合起来，正如在前一节中所演示的，使用多输入的堆叠LSTM。最后，我们肯定需要更多的申报文件。
- en: Summary
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we presented the specialized RNN architecture that is tailored
    to sequential data. We covered how RNNs work, analyzed the computational graph,
    and saw how RNNs enable parameter-sharing over numerous steps to capture long-range
    dependencies that FFNNs and CNNs are not well suited for.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了专门针对序列数据的RNN架构。我们讨论了RNN的工作原理，分析了计算图，并看到RNN如何能够在许多步骤上实现参数共享，以捕捉长期依赖关系，而这是FFNN和CNN不太适合的。
- en: We also reviewed the challenges of vanishing and exploding gradients and saw
    how gated units like long short-term memory cells enable RNNs to learn dependencies
    over hundreds of time steps. Finally, we applied RNNs to challenges common in
    algorithmic trading, such as predicting univariate and multivariate time series
    and sentiment analysis using SEC filings.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还回顾了梯度消失和梯度爆炸的挑战，并看到像长短期记忆单元这样的门控单元如何使RNN能够在数百个时间步上学习依赖关系。最后，我们将RNN应用于算法交易中常见的挑战，比如预测单变量和多变量时间序列，以及使用SEC文件进行情绪分析。
- en: In the next chapter, we will introduce unsupervised deep learning techniques
    like autoencoders and generative adversarial networks and their applications to
    investment and trading strategies.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍无监督的深度学习技术，如自动编码器和生成对抗网络，以及它们在投资和交易策略中的应用。
