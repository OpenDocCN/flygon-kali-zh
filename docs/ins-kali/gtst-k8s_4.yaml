- en: Chapter 4. Updates and Gradual Rollouts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。更新和渐进式部署
- en: This chapter will expand upon the core concepts, which show the reader how to
    roll out updates and test new features of their application with minimal disruption
    to uptime. It will cover the basics of doing application updates, gradual rollouts,
    and A/B testing. In addition, we will look at scaling the Kubernetes cluster itself.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将扩展核心概念，向读者展示如何在最小干扰应用程序的正常运行时间内推出更新并测试其新功能。它将涵盖进行应用程序更新、渐进式部署和A/B测试的基础知识。此外，我们还将研究如何扩展Kubernetes集群本身。
- en: 'This chapter will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主题：
- en: Application scaling
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序扩展
- en: Rolling updates
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新
- en: A/B testing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A/B测试
- en: Scaling up your cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展您的集群
- en: Example set up
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例设置
- en: 'Before we start exploring the various capabilities built into Kubernetes for
    scaling and updates, we will need a new example environment. We are going to use
    a variation of our previous container image with a blue background (refer to Figure
    4.2 for a comparison). We have the following code:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探索Kubernetes中用于扩展和更新的各种功能之前，我们需要一个新的示例环境。我们将使用我们之前的带有蓝色背景的容器图像的变体（参见图4.2进行比较）。我们有以下代码：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 4-1*: `pod-scaling-controller.yaml`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单4-1*：`pod-scaling-controller.yaml`'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 4-2*: `pod-scaling-service.yaml`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单4-2*：`pod-scaling-service.yaml`'
- en: 'Create these services with the following commands:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建这些服务：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Scaling up
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: Over time, as you run your applications in the Kubernetes cluster, you will
    find that some applications need more resources, whereas others can manage with
    fewer resources. Instead of removing the entire RC (and associated pods), we want
    a more seamless way to scale our application up and down.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您在Kubernetes集群中运行应用程序的时间推移，您会发现一些应用程序需要更多资源，而其他应用程序可以使用更少的资源。我们希望以更无缝的方式来扩展和缩小应用程序，而不是删除整个RC（和相关的pod）。
- en: Thankfully, Kubernetes includes a `scale` command, which is suited specifically
    to this purpose. In our new example, we have only one replica running. You can
    check this with a `get pods` command.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes包含一个专门用于此目的的`scale`命令。在我们的新示例中，只有一个副本正在运行。您可以使用`get pods`命令来检查这一点。
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s try scaling that up to three with the following command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用以下命令将其扩展到三个：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If all goes well, you'll simply see the word **scaled** on the output of your
    terminal window.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您将在终端窗口的输出中看到**scaled**这个词。
- en: Tip
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Optionally, you can specify the `--current-replicas` flag as a verification
    step. The scaling will only occur if the actual number of replicas currently running
    matches this count.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为验证步骤，您还可以选择指定`--current-replicas`标志。只有在当前运行的副本实际数量与此计数匹配时，才会发生扩展。
- en: After listing our pods once again, we should now see three pods running with
    a name similar to `node-js-scale-``**XXXXX**`, where the `X`s are a random string.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 再次列出我们的pod后，现在应该看到三个运行中的pod，名称类似于`node-js-scale-``**XXXXX**`，其中`X`是一个随机字符串。
- en: You can also use the `scale` command to reduce the number of replicas. In either
    case, the `scale` command adds or removes the necessary pod replicas, and the
    service automatically updates and balances across new or remaining replicas.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`scale`命令来减少副本的数量。在任何情况下，`scale`命令都会添加或删除必要的pod副本，并且服务会自动更新和在新的或剩余的副本之间平衡。
- en: Smooth updates
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平滑更新
- en: The scaling of our application up and down as our resource demands change is
    useful for many production scenarios, but what about simple application updates?
    Any production system will have code updates, patches, and feature additions.
    These could be occurring monthly, weekly, or even daily. Making sure that we have
    a reliable way to push out these changes without interruption to our users is
    a paramount consideration.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的资源需求的变化，我们的应用程序的扩展和缩减对许多生产场景都是有用的，但是简单的应用程序更新呢？任何生产系统都会有代码更新、补丁和功能添加。这些可能每月、每周甚至每天都在发生。确保我们有一种可靠的方式来推送这些更改，而不会中断我们的用户，这是一个至关重要的考虑因素。
- en: Once again, we benefit from the years of experience the Kubernetes system is
    built on. There is a built-in support for rolling updates with the 1.0 version.
    The `rolling-update` command allows us to update entire RCs or just the underlying
    Docker image used by each replica. We can also specify an update interval, which
    will allow us to update one pod at a time and wait until proceeding to the next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们受益于Kubernetes系统建立在多年经验之上。1.0版本内置了对滚动更新的支持。`rolling-update`命令允许我们更新整个RC或只是每个副本使用的底层Docker映像。我们还可以指定更新间隔，这将允许我们逐个更新一个pod，并等待继续下一个。
- en: 'Let''s take our scaling example and perform a rolling update to the 0.2 version
    of our container image. We will use an update interval of 2 minutes, so we can
    watch the process as it happens in the following way:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以扩展示例为例，并对我们的容器映像进行滚动更新到0.2版本。我们将使用2分钟的更新间隔，这样我们就可以观察到以下过程：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should see some text about creating a new RC named `node-js-scale-XXXXX`,
    where the `X`s will be a random string of numbers and letters. In addition, you
    will see the beginning of a loop that is starting one replica of the new version
    and removing one from the existing RC. This process will continue until the new
    RC has the full count of replicas running.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到一些关于创建一个名为`node-js-scale-XXXXX`的新RC的文本，其中`X`将是一串随机的数字和字母。此外，您将看到一个循环的开始，它开始一个新版本的副本，并从现有RC中删除一个。这个过程将持续进行，直到新的RC有完整数量的副本在运行。
- en: If we want to follow along in real time, we can open another terminal window
    and use the `get pods` command, along with a label filter, to see what's happening.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想实时跟踪，我们可以打开另一个终端窗口，并使用`get pods`命令，以及一个标签过滤器，来查看发生了什么。
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This command will filter for pods with `node-js-scale` in the name. If you run
    this after issuing the `rolling-update` command, you should see several pods running
    as it creates new versions and removes the old ones one by one.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将过滤出名称中带有`node-js-scale`的pod。如果您在发出`rolling-update`命令后运行此命令，您应该会看到几个正在运行的pod，因为它逐个创建新版本并逐个删除旧版本。
- en: 'The full output of the previous `rolling-update` command should look something
    like Figure 4.1, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 先前`rolling-update`命令的完整输出应该类似于图4.1，如下所示：
- en: '![Smooth updates](../images/00045.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![平滑更新](../images/00045.jpeg)'
- en: Figure 4.1\. The scaling output
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1。扩展输出
- en: As we can see here, Kubernetes is first creating a new RC named `node-js-scale-10ea08ff9a118ac6a93f85547ed28f6`.
    K8s then loops through one by one. Creating a new pod in the new controller and
    removing one from the old. This continues until the new controller has the full
    replica count and the old one is at zero. After this, the old controller is deleted
    and the new one is renamed to the original controller name.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里所看到的，Kubernetes首先创建了一个名为`node-js-scale-10ea08ff9a118ac6a93f85547ed28f6`的新RC。然后K8s逐个循环。在新控制器中创建一个新的pod，并从旧控制器中删除一个。这将持续进行，直到新控制器达到完整的副本计数，旧控制器为零。之后，旧控制器被删除，新控制器被重命名为原始控制器名称。
- en: 'If you run a `get pods` command now, you''ll note that the pods still all have
    a longer name. Alternatively, we could have specified the name of a new controller
    in the command, and Kubernetes will create a new RC and pods using that name.
    Once again, the controller of the old name simply disappears after updating is
    complete. I recommend specifying a new name for the updated controller to avoid
    confusion in your pod naming down the line. The same `update` command with this
    method would look like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在运行`get pods`命令，您会注意到所有的pod仍然具有较长的名称。或者，我们可以在命令中指定一个新控制器的名称，Kubernetes将会创建一个新的RC和使用该名称的pod。更新完成后，旧名称的控制器将会消失。我建议为更新后的控制器指定一个新名称，以避免在以后的pod命名中造成混淆。使用这种方法的相同`update`命令将如下所示：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using the static external IP address from the service we created in the first
    section, we can open the service in a browser. We should see our standard container
    information page. However, you'll note that the title now says **Pod Scaling v0.2**
    and the background is light yellow.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在第一节创建的服务的静态外部IP地址，我们可以在浏览器中打开该服务。我们应该看到我们的标准容器信息页面。但是，您会注意到标题现在说**Pod Scaling
    v0.2**，背景是浅黄色的。
- en: '![Smooth updates](../images/00046.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![平滑更新](../images/00046.jpeg)'
- en: Figure 4.2\. v0.1 and v0.2 (side by side)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2. v0.1和v0.2（并排）
- en: It's worth noting that during the entire update process, we've only been looking
    at pods and RCs. We didn't do anything with our service, but the service is still
    running fine and now directing to the new version of our pods. This is because
    our service is using label selectors for membership. Because both our old and
    new replicas use the same labels, the service has no problem using the new pods
    to service requests. The updates are done on the pods one by one, so it's seamless
    for the users of the service.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在整个更新过程中，我们只关注了pod和RC。我们没有对服务做任何操作，但服务仍然正常运行，并且现在指向我们pod的新版本。这是因为我们的服务使用标签选择器进行成员资格。因为我们的旧和新副本都使用相同的标签，所以服务可以毫无问题地使用新的pod来处理请求。更新是逐个对pod进行的，因此对于服务的用户来说是无缝的。
- en: Testing, releases, and cutovers
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试、发布和切换
- en: The rolling update feature can work well for a simple blue-green deployment
    scenario. However, in a real-world blue-green deployment with a stack of multiple
    applications, there can be a variety of interdependencies that require in-depth
    testing. The `update-period` command allows us to add a `timeout` flag where some
    testing can be done, but this will not always be satisfactory for testing purposes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新功能对于简单的蓝绿部署场景可以很好地工作。然而，在一个真实的蓝绿部署中，涉及多个应用程序的堆栈，可能存在各种相互依赖关系，需要进行深入测试。`update-period`命令允许我们添加一个`timeout`标志，可以进行一些测试，但这并不总是令人满意的测试目的。
- en: Similarly, you may want partial changes to persist for a longer time and all
    the way up to the load balancer or service level. For example, you wish to A/B
    test a new user interface feature with a portion of your users. Another example
    is running a canary release (a replica in this case) of your application on new
    infrastructure like a newly added cluster node.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可能希望部分更改能够持续更长时间，一直到负载均衡器或服务级别。例如，您希望对部分用户进行新用户界面功能的A/B测试。另一个例子是在新基础设施上（比如新添加的集群节点）运行应用程序的金丝雀发布（在这种情况下是一个副本）。
- en: 'Let''s take a look at an A/B testing example. For this example, we will need
    to create a new service that uses `sessionAffinity`. We will set the affinity
    to `ClientIP`, which will allow us to forward clients to the same backend pod.
    This is a key if we want a portion of our users to see one version while others
    see another:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个A/B测试的例子。对于这个例子，我们需要创建一个使用`sessionAffinity`的新服务。我们将将亲和性设置为`ClientIP`，这将允许我们将客户端转发到相同的后端pod。如果我们希望部分用户看到一个版本，而其他用户看到另一个版本，这是关键的：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 4-3*: `pod-AB-service.yaml`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单4-3*：`pod-AB-service.yaml`'
- en: 'Create this service as usual with the `create` command as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样使用`create`命令创建此服务，如下所示：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will create a service that will point to our pods running both version
    0.2 and 0.3 of the application. Next, we will create the two RCs which create
    two replicas of the application. One set will have version 0.2 of the application,
    and the other will have version 0.3, as shown here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个服务，指向我们运行应用程序版本0.2和0.3的pod。接下来，我们将创建两个RCs，它们将创建应用程序的两个副本。一个集合将具有应用程序版本0.2，另一个将具有版本0.3，如下所示：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Listing 4-4*: `pod-A-controller.yaml`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单4-4*：`pod-A-controller.yaml`'
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 4-5:* `pod-B-controller.yaml`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单4-5*：`pod-B-controller.yaml`'
- en: 'Note that we have the same service label, so these replicas will also be added
    to the service pool based on this selector. We also have `livenessProbe` and `readinessProbe`
    defined to make sure that our new version is working as expected. Again, use the
    `create` command to spin up the controller:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有相同的服务标签，因此这些副本也将根据此选择器添加到服务池中。我们还定义了`livenessProbe`和`readinessProbe`，以确保我们的新版本按预期工作。再次使用`create`命令来启动控制器：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now we have a service balancing to both versions of our app. In a true A/B test,
    we would now want to start collecting metrics on the visit to each version. Again,
    we have the `sessionAffinity` set to `ClientIP`, so all requests will go to the
    same pod. Some users will see v0.2, and some will see v0.3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个服务平衡到我们应用程序的两个版本。在真正的A/B测试中，我们现在希望开始收集每个版本访问的指标。再次提醒，我们将`sessionAffinity`设置为`ClientIP`，因此所有请求将转到同一个pod。一些用户将看到v0.2，而另一些将看到v0.3。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Because we have `sessionAffinity` turned on, your test will likely show the
    same version every time. This is expected, and you would need to attempt a connection
    from multiple IP addresses to see both user experiences with each version.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经打开了`sessionAffinity`，所以你的测试很可能每次都会显示相同的版本。这是预期的，你需要尝试从多个IP地址连接，以查看每个版本的用户体验。
- en: Since the versions are each on their own pod, one can easily separate logging
    and even add a logging container to the pod definition for a sidecar logging pattern.
    For brevity, we will not cover that setup in this book, but we will look at some
    of the logging tools in [Chapter 6](part0046_split_000.html#1BRPS1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 6. Monitoring and Logging"), *Monitoring and Logging*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个版本都在自己的pod上，可以很容易地分开日志记录，甚至可以向pod定义添加一个日志记录容器，以实现sidecar日志记录模式。为简洁起见，本书不涵盖该设置，但我们将在[第6章](part0046_split_000.html#1BRPS1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第6章。监控和日志记录")中查看一些日志记录工具，*监控和日志记录*。
- en: We can start to see how this process would be useful for a canary release or
    a manual blue-green deployment. We can also see how easy it is to launch a new
    version and slowly transition over to the new release.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始看到这个过程对于金丝雀发布或手动蓝绿部署是如何有用的。我们还可以看到启动新版本并逐渐过渡到新版本有多么容易。
- en: 'Let''s look at a basic transition quickly. It''s really as simple as a few
    `scale` commands, which are as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下基本的过渡。这实际上就是一些`scale`命令，如下所示：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Use the `get pods` command combined with `–l` filter in between `scale` commands
    to watch the transition as it happens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scale`命令之间使用`get pods`命令结合`-l`过滤器来观察过渡的发生。
- en: 'Now we have fully transitioned over to version 0.3 (`node-js-scale-b`). All
    users will now see the version 0.3 of the site. We have four replicas of version
    0.3 and 0 of 0.2\. If you run a `get rc` command, you will notice that we still
    have a RC for 0.2 (`node-js-scale-a`). As a final cleanup, we can remove that
    controller completely as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已完全过渡到版本0.3（`node-js-scale-b`）。所有用户现在将看到站点的版本0.3。我们有4个版本0.3的副本，0个版本0.2的副本。如果运行`get
    rc`命令，您会注意到我们仍然有一个0.2的RC（`node-js-scale-a`）。作为最终清理，我们可以完全删除该控制器，如下所示：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In the newly released version 1.1, K8s has a new "Horizontal Pod Autoscaler"
    construct which allows you to automatically scale pods based on CPU utilization.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在新发布的版本1.1中，K8s有一个新的“水平Pod自动缩放器”构造，允许您根据CPU利用率自动扩展pod。
- en: Growing your cluster
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展您的集群
- en: All these techniques are great for the scaling of the application, but what
    about the cluster itself. At some point, you will pack the nodes full and need
    more resources to schedule new pods for your workloads.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术对于应用程序的扩展都非常有用，但是集群本身呢？在某个时候，您将会将节点填满，并且需要更多资源来为工作负载调度新的pod。
- en: Tip
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'When you create your cluster, you can customize the starting number of (minions)
    nodes with the `NUM_MINIONS` environment variable. By default, it is set to `4`.
    The following example shows how to set it to `5` before running `kube-up.sh`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群时，您可以使用`NUM_MINIONS`环境变量自定义（minions）节点的起始数量。默认情况下，它设置为`4`。以下示例显示了如何在运行`kube-up.sh`之前将其设置为`5`：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Bear in mind that changing this after the cluster is started will have no effect.
    You would need to tear down the cluster and create it once again. Thus, this section
    will show you how to add nodes to an existing cluster without rebuilding it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在集群启动后更改这个设置将不会生效。您需要拆除集群并重新创建。因此，本节将向您展示如何在不重建集群的情况下向现有集群添加节点。
- en: Scaling up the cluster on GCE
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在GCE上扩展集群
- en: Scaling up your cluster on GCE is actually quite easy. The existing plumbing
    uses managed instance groups in GCE, which allow you to easily add more machines
    of a standard configuration to the group via an instance template.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCE上扩展您的集群实际上非常容易。现有的管道使用GCE中的托管实例组，允许您通过实例模板轻松地向组中添加更多标准配置的机器。
- en: You can see this template easily in the GCE console. First, open the console;
    by default, this should open your default project console. If you are using another
    project for your Kuberenetes cluster, simply select it from the project dropdown
    at the top of the page.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GCE控制台上轻松查看此模板。首先，打开控制台；默认情况下，这应该会打开您的默认项目控制台。如果您正在为Kubernetes集群使用另一个项目，只需在页面顶部的项目下拉菜单中选择它。
- en: 'On the side panel under **Compute** and then **Compute Engine**, select **Instance
    templates**. You should see a template titled **kuberenetes-minion-template**.
    Note that the name could vary slightly if you''ve customized your cluster naming
    settings. Click on that template to see the details. Refer to the following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在侧边栏下的**计算**然后**计算引擎**下，选择**实例模板**。您应该会看到一个名为**kuberenetes-minion-template**的模板。请注意，如果您已自定义了集群命名设置，名称可能会略有不同。单击该模板以查看详细信息。参考以下截图：
- en: '![Scaling up the cluster on GCE](../images/00047.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![在GCE上扩展集群](../images/00047.jpeg)'
- en: Figure 4.3\. The GCE Instance template for minions
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3。minions的GCE实例模板
- en: You'll see a number of settings, but the meat of the template is under **Custom**
    metadata. Here, you will see a number of environment variables and also a startup
    script that is run after a new machine instance is created. These are the core
    components that allow us to create new machines and have them automatically added
    to the available cluster nodes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到许多设置，但模板的核心部分位于**自定义**元数据下。在这里，您将看到许多环境变量，还有一个在创建新机器实例后运行的启动脚本。这些是允许我们创建新机器并自动将它们添加到可用集群节点的核心组件。
- en: 'Because the template for new machines is already created, it is very simple
    to scale out our cluster in GCE. Simply go to the **Instance groups** located
    right above the **Instance templates** link on the side panel. Again, you should
    see a group titled **kubernetes-minion-group** or something similar. Click on
    that group to see the details, as shown in the following screenshot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因为新机器的模板已经创建，所以在GCE中扩展我们的集群非常简单。只需转到侧边栏上方的**实例模板**链接上方的**实例组**。同样，您应该看到一个名为**kubernetes-minion-group**或类似的组。单击该组以查看详细信息，如下面的屏幕截图所示：
- en: '![Scaling up the cluster on GCE](../images/00048.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![在GCE上扩展集群](../images/00048.jpeg)'
- en: Figure 4.4\. The GCE Instance group for minions
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4\. 用于从属节点的GCE实例组
- en: You'll see a page with a CPU metrics graph and four instances listed here. By
    default, the cluster creates four nodes. We can modify this group by clicking
    the **Edit group** button at the top of the page.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个CPU指标图和四个实例列表。默认情况下，集群创建四个节点。我们可以通过点击页面顶部的**编辑组**按钮来修改这个组。
- en: '![Scaling up the cluster on GCE](../images/00049.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![在GCE上扩展集群](../images/00049.jpeg)'
- en: Figure 4.5\. The GCE Instance group edit page
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5\. GCE实例组编辑页面
- en: You should see **kubernetes-minion-template** selected in **Instance template**
    that we reviewed a moment ago. You'll also see an **Autoscaling** setting, which
    is **Off** by default and an instance count of `4`. Simply, increment this to
    `5` and click on **Save**. You'll be taken back to the group details page and
    see a pop-up dialog showing the pending changes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在**实例模板**中看到选择了**kubernetes-minion-template**，这是我们刚才审查过的。您还会看到一个**自动缩放**设置，默认为**关闭**，实例计数为`4`。只需将其增加到`5`并单击**保存**。您将被带回到组详细信息页面，并看到一个弹出对话框显示待定更改。
- en: 'In a few minutes, you''ll have a new instance listed on the details page. We
    can test that this is ready by using the `get nodes` command from the command
    line:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，您将在详细信息页面上看到一个新实例。我们可以使用命令行中的`get nodes`命令来测试是否准备就绪。
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Autoscaling and scaling down
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动缩放和缩小
- en: In the preceding example, we left autoscaling turned off. However, there may
    be some cases where you want to automatically scale your cluster up and down.
    Turning on autoscaling will allow you to choose a metric to monitor and scale
    on. A minimum and maximum number of instances can be defined as well as a cool
    down period between actions. For more information on autoscaling in GCE, refer
    to the link [https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization](https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们关闭了自动缩放。但是，可能会有一些情况需要自动扩展和缩小集群。打开自动缩放将允许您选择要监视和缩放的指标。还可以定义实例的最小和最大数量，以及操作之间的冷却时间。有关GCE中自动缩放的更多信息，请参考链接[https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization](https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization)。
- en: Note
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**A word of caution on autoscaling and scale down in general**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于自动缩放和一般缩小的警告**'
- en: First, if we repeat the earlier process and decrease the countdown to four,
    GCE will remove one node. However, it will not necessarily be the node you just
    added. The good news is that pods will be rescheduled on the remaining nodes.
    However, it can only reschedule where resources are available. If you are close
    to full capacity and shut down a node, there is a good chance that some pods will
    not have a place to be rescheduled. In addition, this is not a live migration,
    so any application state will be lost in the transition. The bottom line is that
    you should carefully consider the implications before scaling down or implementing
    an autoscaling scheme.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果我们重复之前的过程并将倒计时减少到四，GCE将删除一个节点。但是，它不一定是您刚刚添加的节点。好消息是pod将重新调度到剩余的节点上。但是，它只能在资源可用的地方重新调度。如果您接近满负荷并关闭一个节点，很有可能一些pod将无法重新调度。此外，这不是实时迁移，因此在过渡中将丢失任何应用程序状态。最重要的是，在缩减规模或实施自动缩放方案之前，您应该仔细考虑其影响。
- en: Scaling up the cluster on AWS
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS上扩展集群
- en: The AWS provider code also makes it very easy to scale up your cluster. Similar
    to GCE, the AWS setup uses autoscaling groups to create the default four minion
    nodes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供程序代码还可以很容易地扩展集群。与GCE类似，AWS设置使用自动缩放组来创建默认的四个minion节点。
- en: 'This can also be easily modified using the CLI or the web console. In the console,
    from the EC2 page, simply go to the **Auto Scaling Groups** section at the bottom
    of the menu on the left. You should see a name similar to **kubernetes-minion-group**.
    Select that group and you will see details as shown in Figure 4.6:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以很容易地使用CLI或Web控制台进行修改。在控制台上，从EC2页面，只需转到左侧菜单底部的**自动缩放组**部分。您应该会看到一个类似于**kubernetes-minion-group**的名称。选择该组，您将看到如图4.6所示的详细信息：
- en: '![Scaling up the cluster on AWS](../images/00050.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![在AWS上扩展集群](../images/00050.jpeg)'
- en: Figure 4.6\. Kubernetes minion autoscaling details
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6。Kubernetes minion自动缩放详细信息
- en: We can scale this group up easily by clicking **Edit**. Then, change the **Desired**,
    **Min**, and **Max** values to `5` and click on **Save**. In a few minutes, you'll
    have the fifth node available. You can once again check this using the `get nodes`
    command.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过单击**编辑**轻松地将该组扩展。然后，将**期望**、**最小**和**最大**值更改为`5`，然后单击**保存**。几分钟后，您将有第五个节点可用。您可以再次使用`get
    nodes`命令来检查这一点。
- en: Scaling down is the same process, but remember that we discussed the same considerations
    in the previous *Scaling the cluster on GCE* section. Workloads could get abandoned
    or at the very least unexpectedly restarted.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减规模是相同的过程，但请记住我们在前面的*在GCE上扩展集群*部分中讨论了相同的注意事项。工作负载可能会被放弃，或者至少会意外重新启动。
- en: Scaling manually
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动缩放
- en: For other providers, creating new minions may not be an automated process. Depending
    on your provider, you'll need to perform various manual steps. It can be helpful
    to look at the provider-specific scripts under the `cluster` directory.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他提供商，创建新的minion可能不是一个自动化的过程。根据您的提供商，您需要执行各种手动步骤。查看`cluster`目录下的特定于提供商的脚本可能会有所帮助。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We should now be a bit more comfortable with the basics of application scaling
    in Kubernetes. We also looked at the built-in functions in order to roll updates
    as well a manual process for testing and slowly integrating updates. Finally,
    we took a look at scaling the nodes of our underlying cluster and increasing overall
    capacity for our Kubernetes resources.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对Kubernetes中应用程序扩展的基础知识应该更加了解。我们还研究了内置功能，以便滚动更新以及用于测试和慢慢集成更新的手动过程。最后，我们看了一下如何扩展底层集群的节点，并增加Kubernetes资源的整体容量。
