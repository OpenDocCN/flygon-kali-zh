- en: Introduction to Docker Swarm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Docker Swarm
- en: In the last chapter, we introduced orchestrators. Like a conductor in an orchestra,
    an orchestrator makes sure that all of our containerized application services
    play together nicely and contribute harmoniously to a common goal. Such orchestrators
    have quite a few responsibilities, which we discussed in detail. Finally, we provided
    a short overview of the most important container orchestrators on the market.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了编排器。就像管弦乐队中的指挥一样，编排器确保我们所有的容器化应用服务和谐地共同演奏，为共同的目标做出贡献。这样的编排器有很多责任，我们详细讨论了这些责任。最后，我们简要概述了市场上最重要的容器编排器。
- en: This chapter introduces Docker's native orchestrator, SwarmKit. It elaborates
    on all of the concepts and objects SwarmKit uses to deploy and run distributed,
    resilient, robust, and highly available applications in a cluster on premises
    or in the cloud. This chapter also introduces how SwarmKit ensures secure applications by
    using a **Software-Defined Network** (**SDN**) to isolate containers. Additionally,
    this chapter demonstrates how to install a highly available Docker Swarm in the
    cloud. It introduces the routing mesh, which provides layer-4 routing and load
    balancing. Finally, it demonstrates how to deploy a first application consisting
    of multiple services onto the swarm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Docker的本地编排器SwarmKit。它详细阐述了SwarmKit用于在本地或云上部署和运行分布式、有弹性、健壮和高可用应用的所有概念和对象。本章还介绍了SwarmKit如何通过使用软件定义网络（SDN）来隔离容器来确保安全应用。此外，本章演示了如何在云中安装一个高可用的Docker
    Swarm。它介绍了路由网格，提供了第四层路由和负载平衡。最后，它演示了如何在群集上部署由多个服务组成的第一个应用程序。
- en: 'These are the topics we are going to discuss in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: The Docker Swarm architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm架构
- en: Swarm nodes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm节点
- en: Stacks, services, and tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆栈、服务和任务
- en: Multi-host networking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多主机网络
- en: Creating a Docker Swarm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个Docker Swarm
- en: Deploying a first application
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署第一个应用程序
- en: The Swarm routing mesh
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm路由网格
- en: 'After completing this chapter, you will be able to do the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够做到以下事项：
- en: Sketch the essential parts of a highly available Docker Swarm on a whiteboard
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在白板上勾画一个高可用的Docker Swarm的基本部分
- en: Explain in two or three simple sentences to an interested layman what a (swarm)
    service is
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用两三个简单的句子向感兴趣的门外汉解释（群）服务是什么
- en: Create a highly available Docker Swarm in AWS, Azure, or GCP consisting of three
    manager and two worker nodes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS、Azure或GCP中创建一个高可用的Docker Swarm，包括三个管理节点和两个工作节点
- en: Successfully deploy a replicated service such as Nginx on a Docker Swarm
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功在Docker Swarm上部署一个复制的服务，如Nginx
- en: Scale a running Docker Swarm service up and down
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展正在运行的Docker Swarm服务
- en: Retrieve the aggregated log of a replicated Docker Swarm service
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索复制的Docker Swarm服务的聚合日志
- en: Write a simple stack file for a sample application consisting of at least two
    interacting services
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为一个由至少两个相互作用的服务组成的示例应用程序编写一个简单的堆栈文件
- en: Deploy a stack into a Docker Swarm
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一个堆栈部署到Docker Swarm中
- en: The Docker Swarm architecture
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm架构
- en: 'The architecture of a Docker Swarm from a 30,000-foot view consists of two
    main parts—a raft consensus group of an odd number of manager nodes, and a group
    of worker nodes that communicate with each other over a gossip network, also called
    the control plane. The following diagram illustrates this architecture:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从30,000英尺的视角来看，Docker Swarm的架构由两个主要部分组成——一个由奇数个管理节点组成的raft一致性组，以及一个与控制平面上的八卦网络相互通信的工作节点组。以下图表说明了这种架构：
- en: '![](assets/185aff48-e453-4420-a5c7-35859f604904.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/185aff48-e453-4420-a5c7-35859f604904.png)'
- en: High-level architecture of a Docker Swarm
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm的高级架构
- en: The **manager** nodes manage the swarm while the **worker** nodes execute the
    applications deployed into the swarm. Each **manager** has a complete copy of
    the full state of the Swarm in its local raft store. Managers synchronously communicate
    with each other and their raft stores are always in sync.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**管理节点**管理Swarm，而**工作节点**执行部署到Swarm中的应用程序。每个**管理节点**在其本地Raft存储中都有完整的Swarm状态副本。管理节点之间同步通信，它们的Raft存储始终保持同步。'
- en: The **workers**, on the other hand, communicate with each other asynchronously
    for scalability reasons. There can be hundreds if not thousands of **worker**
    nodes in a Swarm. Now that we have a high-level overview of what a Docker Swarm
    is, let's describe all of the individual elements of a Docker Swarm in more detail.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，为了可伸缩性的原因，**工作节点**是异步通信的。在一个Swarm中可能有数百甚至数千个**工作节点**。现在我们已经对Docker Swarm有了一个高层次的概述，让我们更详细地描述Docker
    Swarm的所有单个元素。
- en: Swarm nodes
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm节点
- en: A Swarm is a collection of nodes. We can classify a node as a physical computer or **Virtual
    Machine** (**VM**). Physical computers these days are often referred to as *bare
    metal*. People say *we're running on bare metal* to distinguish from running on
    a VM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm是节点的集合。我们可以将节点分类为物理计算机或虚拟机（VM）。如今，物理计算机通常被称为“裸金属”。人们说“我们在裸金属上运行”以区别于在虚拟机上运行。
- en: 'When we install Docker on such a node, we call this node a Docker host. The
    following diagram illustrates a bit better what a node and what a Docker host
    is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在这样的节点上安装Docker时，我们称这个节点为Docker主机。以下图表更好地说明了节点和Docker主机是什么：
- en: '![](assets/45b82a12-5b32-4290-8970-944fe94532eb.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/45b82a12-5b32-4290-8970-944fe94532eb.png)'
- en: Bare metal and VM types of Docker Swarm nodes
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 裸金属和虚拟机类型的Docker Swarm节点
- en: To become a member of a Docker Swarm, a node must be a Docker host. A node in
    a Docker Swarm can have one of two roles. It can be a manager or it can be a worker.
    Manager nodes do what their name implies; they manage the Swarm. The worker nodes,
    in turn, execute the application workload.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为Docker Swarm的成员，节点必须是Docker主机。Docker Swarm中的节点可以担任两种角色之一。它可以是管理节点，也可以是工作节点。管理节点做其名字所示的事情；它们管理Swarm。而工作节点则执行应用程序工作负载。
- en: Technically, a manager node can also be a worker node and hence run application
    workload—although that is not recommended, especially if the Swarm is a production
    system running mission-critical applications.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，管理节点也可以是工作节点，因此运行应用程序工作负载，尽管这并不被推荐，特别是如果Swarm是运行关键任务应用程序的生产系统。
- en: Swarm managers
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm管理节点
- en: Each Docker Swarm needs to include at least one manager node. For high availability
    reasons, we should have more than one manager node in a Swarm. This is especially
    true for production or production-like environments. If we have more than one
    manager node, then these nodes work together using the Raft consensus protocol.
    The Raft consensus protocol is a standard protocol that is often used when multiple
    entities need to work together and always need to agree with each other as to
    which activity to execute next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Docker Swarm至少需要包括一个**管理节点**。出于高可用性的原因，我们应该在Swarm中有多个管理节点。这对于生产环境或类似生产环境尤为重要。如果我们有多个管理节点，那么这些节点将使用Raft一致性协议一起工作。Raft一致性协议是一个标准协议，当多个实体需要共同工作并且始终需要就下一步执行的活动达成一致意见时，通常会使用该协议。
- en: To work well, the Raft consensus protocol asks for an odd number of members in
    what is called the consensus group. Hence, we should always have 1, 3, 5, 7, and
    so on manager nodes. In such a consensus group, there is always a leader. In the
    case of Docker Swarm, the first node that starts the Swarm initially becomes the
    leader. If the leader goes away then the remaining manager nodes elect a new leader.
    The other nodes in the consensus group are called followers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了良好运行，Raft共识协议要求在所谓的共识组中有奇数个成员。因此，我们应该始终有1、3、5、7等管理者节点。在这样的共识组中，总是有一个领导者。在Docker
    Swarm的情况下，最初启动Swarm的第一个节点成为领导者。如果领导者离开，剩下的管理者节点将选举新的领导者。共识组中的其他节点称为跟随者。
- en: Now, let's assume that we shut down the current leader node for maintenance
    reasons. The remaining manager nodes will elect a new leader. When the previous
    leader node comes back online, it will now become a follower. The new leader remains
    the leader.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设出于维护原因关闭当前的领导节点。剩下的管理者节点将选举新的领导者。当之前的领导节点恢复在线时，它将成为跟随者。新的领导者仍然是领导者。
- en: All of the members of the consensus group communicate synchronously with each
    other. Whenever the consensus group needs to make a decision, the leader asks
    all followers for agreement. If a majority of the manager nodes give a positive
    answer, then the leader executes the task. That means if we have three manager
    nodes, then at least one of the followers has to agree with the leader. If we
    have five manager nodes, then at least two followers have to agree.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 共识组的所有成员之间进行同步通信。每当共识组需要做出决策时，领导者会要求所有跟随者同意。如果大多数管理者节点给出积极答复，那么领导者执行任务。这意味着如果我们有三个管理者节点，那么至少有一个跟随者必须同意领导者。如果我们有五个管理者节点，那么至少有两个跟随者必须同意。
- en: Since all manager follower nodes have to communicate synchronously with the
    leader node to make a decision in the cluster, the decision-making process gets
    slower and slower the more manager nodes we have forming the consensus group.
    The recommendation of Docker is to use one manager for development, demo, or test
    environments. Use three managers nodes in small to medium size Swarms, and use
    five managers in large to extra large Swarms. To use more than five managers in
    a Swarm is hardly ever justified.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有管理者跟随者节点都必须与领导节点同步通信，以在集群中做出决策，所以随着形成共识组的管理者节点数量增加，决策过程变得越来越慢。Docker的建议是在开发、演示或测试环境中使用一个管理者。在小到中等规模的Swarm中使用三个管理者节点，在大型到超大型的Swarm中使用五个管理者。在Swarm中使用超过五个管理者几乎没有理由。
- en: 'The manager nodes are not only responsible for managing the Swarm but also
    for maintaining the state of the Swarm. *What do we mean by that?* When we talk
    about the state of the Swarm we mean all of the information about it—for example, *how
    many nodes are in the Swarm and* *what are the properties of each node, such as
    name or IP address*. We also mean what containers are running on which node in
    the Swarm and more. What, on the other hand, is not included in the state of the
    Swarm is data produced by the application services running in containers on the
    Swarm. This is called application data and is definitely not part of the state
    that is managed by the manager nodes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 管理者节点不仅负责管理Swarm，还负责维护Swarm的状态。*我们指的是什么？*当我们谈论Swarm的状态时，我们指的是关于它的所有信息，例如*Swarm中有多少节点*，*每个节点的属性是什么，比如名称或IP地址*。我们还指的是Swarm中哪个节点上运行了哪些容器等更多信息。另一方面，Swarm状态中不包括由Swarm上容器中运行的应用服务产生的数据。这被称为应用数据，绝对不是由管理者节点管理的状态的一部分。
- en: '![](assets/cf1b7c95-25a4-43a4-aa20-168b1b938059.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/cf1b7c95-25a4-43a4-aa20-168b1b938059.png)'
- en: A Swarm manager consensus group
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Swarm管理器共识组
- en: All of the Swarm states are stored in a high-performance key-value store (**kv-store**)
    on each **manager** node. That's right, each **manager** node stores a complete replica
    of the whole Swarm state. This redundancy makes the Swarm highly available. If
    a **manager** node goes down, the remaining **managers** all have the complete
    state at hand.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Swarm状态都存储在每个**manager**节点上的高性能键值存储（**kv-store**）中。没错，每个**manager**节点都存储了整个Swarm状态的完整副本。这种冗余使Swarm具有高可用性。如果一个**manager**节点宕机，剩下的**manager**都有完整的状态可用。
- en: If a new **manager** joins the consensus group, then it synchronizes the Swarm
    state with the existing members of the group until it has a complete replica.
    This replication is usually pretty fast in typical Swarms but can take a while
    if the Swarm is big and many applications are running on it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个新的**manager**加入共识组，那么它会与现有组成员同步Swarm状态，直到拥有完整的副本。在典型的Swarm中，这种复制通常非常快，但如果Swarm很大并且有许多应用程序在其中运行，可能需要一段时间。
- en: Swarm workers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm工人
- en: As we mentioned earlier, a Swarm worker node is meant to host and run containers that
    contain the actual application services we're interested in running on our cluster.
    They are the workhorses of the Swarm. In theory, a manager node can also be a
    worker. But, as we already said, this is not recommended on a production system.
    On a production system, we should let managers be managers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Swarm工作节点旨在托管和运行包含我们感兴趣在集群上运行的实际应用服务的容器。它们是Swarm的工作马。理论上，管理节点也可以是工作节点。但是，正如我们已经说过的，这在生产系统上是不推荐的。在生产系统上，我们应该让管理节点成为管理节点。
- en: Worker nodes communicate with each other over the so-called control plane. They
    use the gossip protocol for their communication. This communication is asynchronous,
    which means that, at any given time, it is likely that not all worker nodes are
    in perfect sync.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点通过所谓的控制平面彼此交流。它们使用流言协议进行通信。这种通信是异步的，这意味着在任何给定时间，可能并非所有工作节点都完全同步。
- en: 'Now, you might ask—*what information do worker nodes exchange?* It is mostly
    information that is needed for service discovery and routing, that is, information
    about which containers are running on with nodes and more:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能会问——*工作节点交换什么信息？*主要是用于服务发现和路由的信息，即关于哪些容器正在哪些节点上运行等信息：
- en: '![](assets/8049bdd7-03d8-405a-9ed4-e48e4d2216b4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8049bdd7-03d8-405a-9ed4-e48e4d2216b4.png)'
- en: Worker nodes communicating with each other
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点之间的通信
- en: In the preceding diagram, you can see how workers communicate with each other.
    To make sure the gossiping scales well in a large Swarm, each **worker** node
    only synchronizes its own state with three random neighbors. For those who are
    familiar with Big O notation, that means that the synchronization of the **worker**
    nodes using the gossip protocol scales with O(0).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，您可以看到工人如何彼此交流。为了确保流言蜚语在大型Swarm中能够良好扩展，每个**worker**节点只与三个随机邻居同步自己的状态。对于熟悉大O符号的人来说，这意味着使用流言协议同步**worker**节点的规模为O(0)。
- en: '**Worker** nodes are kind of passive. They never actively do something other
    than run the workloads that they get assigned by the manager nodes. The **worker** makes sure,
    though, that it runs these workloads to the best of its capabilities. Later on
    in this chapter, we will get to know more about exactly what workloads the worker
    nodes are assigned by the manager nodes.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**Worker**节点有点被动。除了运行由管理节点分配的工作负载之外，它们从不主动做任何事情。然而，**worker**确保以最佳能力运行这些工作负载。在本章后面，我们将更多地了解由管理节点分配给工作节点的工作负载。'
- en: Stacks, services, and tasks
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆栈、服务和任务
- en: 'When using a Docker Swarm versus a single Docker host, there is a paradigm
    change. Instead of talking of individual containers that run processes, we are
    abstracting away to services that represent a set of replicas of each process,
    and, in this way, become highly available. We also do not speak anymore of individual
    Docker hosts with well-known names and IP addresses to which we deploy containers;
    we''ll now be referring to clusters of hosts to which we deploy services. We don''t
    care about an individual host or node anymore. We don''t give it a meaningful
    name; each node rather becomes a number to us. We also don''t care about individual
    containers and where they are deployed any longer—we just care about having a
    desired state defined through a service. We can try to depict that as shown in
    the following diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Docker Swarm而不是单个Docker主机时，会有一种范式变化。我们不再谈论运行进程的单个容器，而是将其抽象为代表每个进程的一组副本的服务，并以这种方式变得高度可用。我们也不再谈论具有众所周知的名称和IP地址的单个Docker主机，我们现在将会提到部署服务的主机集群。我们不再关心单个主机或节点。我们不给它一个有意义的名称；对我们来说，每个节点都变成了一个数字。我们也不再关心个别容器以及它们被部署到哪里——我们只关心通过服务定义所需状态。我们可以尝试将其描述如下图所示：
- en: '![](assets/f1ff1173-269e-4448-a409-8279610fc9be.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f1ff1173-269e-4448-a409-8279610fc9be.png)'
- en: Containers are deployed to well-known servers
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 容器部署到众所周知的服务器
- en: 'Instead of deploying individual containers to well-known servers like in the
    preceding diagram, where we deploy the **web** container to the **alpha** server with
    the IP address `52.120.12.1`, and the **payments** container to the **beta** server with
    the IP `52.121.24.33`, we switch to this new paradigm of services and Swarms (or,
    more generally, clusters):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的图中将个别容器部署到众所周知的服务器不同，其中我们将**web**容器部署到具有IP地址`52.120.12.1`的**alpha**服务器，将**payments**容器部署到具有IP`52.121.24.33`的**beta**服务器，我们转向了这种新的服务和Swarm（或更一般地说，集群）的范式：
- en: '![](assets/6dd5f3db-4cb9-4052-9071-a8cb8256586f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6dd5f3db-4cb9-4052-9071-a8cb8256586f.png)'
- en: Services are deployed to Swarms
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 服务部署到Swarm
- en: 'In the preceding diagram, we see that a **web** service and an **inventory** service are
    both deployed to a **Swarm** that consists of many nodes. Each of the services
    has a certain number of replicas: six for **web** and five for **inventory**.
    We don''t really care on which node the replicas will run; we only care that the
    requested number of replicas is always running on whatever nodes the **Swarm**
    scheduler decides to put them on.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们看到一个**web**服务和一个**inventory**服务都部署到了由许多节点组成的**Swarm**中。每个服务都有一定数量的副本：**web**有六个，**inventory**有五个。我们并不关心副本将在哪个节点上运行；我们只关心所请求的副本数量始终在**Swarm**调度器决定放置它们的任何节点上运行。
- en: Services
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务
- en: 'A Swarm service is an abstract thing. It is a description of the desired state
    of an application or application service that we want to run in a Swarm. The Swarm
    service is like a manifest describing such things as the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm服务是一个抽象的东西。它是对我们想要在Swarm中运行的应用程序或应用程序服务的期望状态的描述。Swarm服务就像一个描述，描述了以下内容：
- en: Name of the service
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的名称
- en: Image from which to create the containers
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于创建容器的镜像
- en: Number of replicas to run
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要运行的副本数量
- en: Network(s) that the containers of the service are attached to
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的容器附加到的网络
- en: Ports that should be mapped
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该映射的端口
- en: Having this service manifest, the Swarm manager then makes sure that the described
    desired state is always reconciled if ever the actual state should deviate from
    it. So, if for example, one instance of the service crashes, then the scheduler
    on the Swarm manager schedules a new instance of this particular service on a
    node with free resources so that the desired state is reestablished.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个服务清单，Swarm管理器确保所描述的期望状态始终得到调和，如果实际状态偏离了期望状态。因此，例如，如果服务的一个实例崩溃，那么Swarm管理器上的调度程序会在具有空闲资源的节点上调度这个特定服务的新实例，以便重新建立期望状态。
- en: Task
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: We have learned that a service corresponds to a description of the desired state
    in which an application service should be at all times. Part of that description
    was the number of replicas the service should be running. Each replica is represented
    by a task. In this regard, a Swarm service contains a collection of tasks. On
    Docker Swarm, a task is the atomic unit of deployment. Each task of a service
    is deployed by the Swarm scheduler to a worker node. The task contains all of
    the necessary information that the worker node needs to run a container based
    on the image, which is part of the service description. Between a task and a container,
    there is a one-to-one relation. The container is the instance that runs on the
    worker node, while the task is the description of this container as a part of
    a Swarm service.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，服务对应于应用程序服务应始终处于的期望状态的描述。该描述的一部分是服务应该运行的副本数量。每个副本由一个任务表示。在这方面，Swarm服务包含一组任务。在Docker
    Swarm上，任务是部署的原子单位。服务的每个任务由Swarm调度程序部署到工作节点。任务包含工作节点运行基于服务描述的镜像的所有必要信息。在任务和容器之间存在一对一的关系。容器是在工作节点上运行的实例，而任务是这个容器作为Swarm服务的一部分的描述。
- en: Stack
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆栈
- en: Now that we have a good idea about what a Swarm service is and what tasks are,
    we can introduce the stack. A stack is used to describe a collection of Swarm
    services that are related, most probably because they are part of the same application.
    In that sense, we could also say that a stack describes an application that consists
    of one to many services that we want to run on the Swarm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Swarm服务和任务有了一个很好的了解，我们可以介绍堆栈。堆栈用于描述一组相关的Swarm服务，很可能是因为它们是同一应用程序的一部分。在这种意义上，我们也可以说堆栈描述了一个由我们想要在Swarm上运行的一到多个服务组成的应用程序。
- en: 'Typically, we describe a stack declaratively in a text file that is formatted
    using the YAML format and that uses the same syntax as the already-known Docker
    Compose file. This leads to a situation where people sometimes say that a stack
    is described by a `docker-compose` file. A better wording would be: a stack is
    described in a stack file that uses similar syntax to a `docker-compose` file.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在一个文本文件中以YAML格式进行格式化描述堆栈，并使用与已知的Docker Compose文件相同的语法。这导致有时人们会说堆栈是由`docker-compose`文件描述的。更好的措辞应该是：堆栈是在使用类似于`docker-compose`文件的堆栈文件中描述的。
- en: 'Let''s try to illustrate the relationship between the stack, services, and
    tasks in the following diagram and connect it with the typical content of a stack
    file:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试用下图来说明堆栈、服务和任务之间的关系，并将其与堆栈文件的典型内容联系起来：
- en: '![](assets/34c40ca5-56fa-4ce6-a61c-aa6d5ded8230.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/34c40ca5-56fa-4ce6-a61c-aa6d5ded8230.png)'
- en: Diagram showing the relationship between stack, services, and tasks
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 显示堆栈、服务和任务之间关系的图表
- en: In the preceding diagram, we see on the right-hand side a declarative description
    of a sample **Stack**. The **Stack** consists of three services called **web**, **payments**,
    and **inventory**. We also see that the **web** service uses the **example/web:1.0** image and has
    four replicas.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到右侧是一个样本**Stack**的声明性描述。**Stack**包括了三种服务，分别是**web**，**payments**和**inventory**。我们还可以看到**web**服务使用**example/web:1.0**镜像，并且有四个副本。
- en: On the left-hand side of the diagram, we see that the **Stack** embraces the
    three services mentioned. Each service, in turn, contains a collection of **Tasks**,
    as many as there are replicas. In the case of the **web** service, we have a collection
    of four **Tasks**. Each **Task** contains the name of the **Image** from which
    it will instantiate a container once the **Task** is scheduled on a Swarm node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表的左侧，我们可以看到**Stack**包含了提到的三种服务。每种服务又包含了一系列的**Tasks**，数量与副本一样多。在**web**服务的情况下，我们有一个包含四个**Tasks**的集合。每个**Task**包含了它将实例化容器的**Image**的名称，一旦**Task**被安排在Swarm节点上。
- en: Multi-host networking
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多主机网络
- en: 'In [Chapter 10](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml), *Single-Host Networking,* we
    discussed how containers communicate on a single Docker host. Now, we have a Swarm
    that consists of a cluster of nodes or Docker hosts. Containers that are located
    on different nodes need to be able to communicate with each other. Many techniques
    can help us to achieve this goal. Docker has chosen to implement an **overlay
    network** driver for Docker Swarm. This **overlay network** allows containers
    attached to the same **overlay network** to discover each other and freely communicate
    with each other. The following is a schema for how an **overlay network** works:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml)中，*单主机网络*，我们讨论了容器在单个Docker主机上的通信。现在，我们有一个由节点或Docker主机组成的Swarm。位于不同节点上的容器需要能够相互通信。有许多技术可以帮助我们实现这个目标。Docker选择为Docker
    Swarm实现了一个**覆盖网络**驱动程序。这个**覆盖网络**允许连接到同一**覆盖网络**的容器相互发现并自由通信。以下是**覆盖网络**的工作原理的示意图：
- en: '![](assets/1731fa28-e8f4-459e-b4c2-40e63b876032.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1731fa28-e8f4-459e-b4c2-40e63b876032.png)'
- en: Overlay network
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: We have two nodes or Docker hosts with the IP addresses `172.10.0.15` and `172.10.0.16`.
    The values we have chosen for the IP addresses are not important; what is important
    is that both hosts have a distinct IP address and are connected by a physical
    network (a network cable), which is called the **underlay network**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个节点或Docker主机，IP地址分别为`172.10.0.15`和`172.10.0.16`。我们选择的IP地址的值并不重要；重要的是两个主机都有不同的IP地址，并且通过一个物理网络（网络电缆）连接，这个网络称为**底层网络**。
- en: On the node on the left-hand side we have a container running with the IP address `10.3.0.2`, and
    on the node on the right-hand side another container with the IP address `10.3.0.5`.
    Now, the former container wants to communicate with the latter. *How can this
    happen?* In [Chapter 10](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml), *Single-Host
    Networking,* we saw how this works when both containers are located on the same
    node—by using a Linux bridge. But Linux bridges only operate locally and cannot
    span across nodes. So, we need another mechanism. Linux VXLAN comes to the rescue.
    VXLAN has been available on Linux since way before containers were a thing.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧的节点上有一个运行着IP地址为`10.3.0.2`的容器，右侧的节点上有另一个IP地址为`10.3.0.5`的容器。现在，前者的容器想要与后者通信。*这怎么可能？*在[第10章](f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml)中，*单主机网络*，我们看到了当两个容器位于同一节点上时，这是如何工作的——通过使用Linux桥接。但Linux桥接只能在本地操作，无法跨越节点。所以，我们需要另一种机制。Linux
    VXLAN来解救。VXLAN在容器出现之前就已经在Linux上可用。
- en: When the left-hand container sends a data packet, the **bridge** realizes that
    the target of the packet is not on this host. Now, each node participating in
    an overlay network gets a so-called **VXLAN Tunnel Endpoint** (**VTEP**) object,
    which intercepts the packet (the packet at that moment is an OSI layer 2 data
    packet), wraps it with a header containing the target IP address of the host that
    runs the destination container (this makes it now an OSI layer 3 data packet),
    and sends it over the **VXLAN tunnel**. The **VTEP** on the other side of the
    tunnel unpacks the data packet and forwards it to the local bridge, which in turn
    forwards it to the destination container.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当左侧容器发送数据包时，**桥接**意识到数据包的目标不在此主机上。现在，参与覆盖网络的每个节点都会得到一个所谓的**VXLAN隧道端点**（**VTEP**）对象，它拦截数据包（此时的数据包是OSI第2层数据包），用包含运行目标容器的主机的目标IP地址的头部包装它（这样它现在是OSI第3层数据包），并将其发送到**VXLAN隧道**。隧道另一侧的**VTEP**解包数据包并将其转发到本地桥接，本地桥接再将其转发到目标容器。
- en: The overlay driver is included in SwarmKit and is in most cases the recommended
    network driver for Docker Swarm. There are other multi-node-capable network drivers
    available from third parties that can be installed as plugins to each participating
    Docker host. Certified network plugins are available from the Docker store.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖驱动程序包含在SwarmKit中，在大多数情况下是Docker Swarm的推荐网络驱动程序。还有其他来自第三方的多节点网络驱动程序可作为插件安装到每个参与的Docker主机上。Docker商店提供认证的网络插件。
- en: Creating a Docker Swarm
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个Docker Swarm
- en: Creating a Docker Swarm is almost trivial. It is so easy that it seems unreal
    if you know what an orchestrator is all about. But it is true, Docker has done
    a fantastic job in making Swarms simple and elegant to use. At the same time,
    Docker Swarm has been proven in use by large enterprises to be very robust and
    scalable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Docker Swarm几乎是微不足道的。如果你知道编排器是什么，那么它是如此容易，以至于似乎不真实。但事实是，Docker在使Swarm简单而优雅的使用方面做得非常出色。与此同时，Docker
    Swarm已被大型企业证明在使用中非常稳健和可扩展。
- en: Creating a local single node swarm
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个本地单节点Swarm
- en: So, enough imagining — let's demonstrate how we can create a Swarm. In its most
    simple form, a fully functioning Docker Swarm consists only of a single node.
    If you're using Docker for Mac or Windows, or even if you're using Docker Toolbox,
    then your personal computer or laptop is such a node. Hence, we can start right
    there and demonstrate some of the most important features of a Swarm.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，想象足够了，让我们演示一下我们如何创建一个Swarm。在其最简单的形式中，一个完全功能的Docker Swarm只包括一个单节点。如果你正在使用Docker
    for Mac或Windows，甚至是使用Docker Toolbox，那么你的个人计算机或笔记本电脑就是这样一个节点。因此，我们可以从这里开始，演示Swarm的一些最重要的特性。
- en: 'Let''s initialize a Swarm. On the command line, just enter the following command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化一个Swarm。在命令行上，只需输入以下命令：
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And after an incredibly short time, you should see something like the following
    screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常短的时间后，你应该看到类似以下截图的东西：
- en: '![](assets/39cd7eb9-5916-422b-a23f-a158857f0401.png)Output of the Docker Swarm
    init command'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/39cd7eb9-5916-422b-a23f-a158857f0401.png)Docker Swarm init命令的输出'
- en: 'Our computer is now a Swarm node. Its role is that of a manager and it is the
    leader (of the managers, which makes sense since there is only one manager at
    this time). Although it took only a very short time to finish `docker swarm init`,
    the command did a lot of things during that time. Some of them are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计算机现在是一个Swarm节点。它的角色是管理者，它是领导者（管理者中的领导者，这是有道理的，因为此时只有一个管理者）。虽然`docker swarm
    init`只花了很短的时间就完成了，但在那段时间里命令做了很多事情。其中一些如下：
- en: It created a root **Certificate Authority** (**CA**).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了一个根**证书颁发机构**（**CA**）。
- en: It created a key-value store that is used to store the state of the whole Swarm.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了一个用于存储整个Swarm状态的键值存储。
- en: 'Now, in the preceding output, we can see a command that can be used to join
    other nodes to the Swarm that we just created. The command is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在前面的输出中，我们可以看到一个命令，可以用来加入我们刚刚创建的Swarm的其他节点。命令如下：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we have the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '`<join-token>` is a token generated by the Swarm leader at the time the Swarm
    was initialized.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <join-token>是Swarm领导者在初始化Swarm时生成的令牌。
- en: '`<IP address>` is the IP address of the leader.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <IP地址>是领导者的IP地址。
- en: 'Although our cluster remains simple, as it consists of only one member, we
    can still ask the Docker CLI to list all of the nodes of the Swarm. This will
    look similar to the following screenshot:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的集群仍然很简单，因为它只包含一个成员，但我们仍然可以要求Docker CLI列出Swarm的所有节点。这将类似于以下屏幕截图：
- en: '![](assets/4519a489-42e8-42d8-9ef0-8b791ce51d14.png)Listing the nodes of the
    Docker Swarm'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列出Docker Swarm的节点
- en: In this output, we first see `ID` that was given to the node. The star (`*`)
    that follows `ID` indicates that this is the node on which `docker node ls` was
    executed—basically saying that this is the active node. Then, we have the (human-readable)
    name of the node, its status, availability, and manager status. As mentioned earlier,
    this very first node of the Swarm automatically became the leader, which is indicated
    in the preceding screenshot. Lastly, we see which version of the Docker engine
    we're using.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在此输出中，我们首先看到赋予节点的ID。跟随ID的星号（*）表示这是执行docker node ls的节点，基本上表示这是活动节点。然后，我们有节点的（人类可读的）名称，其状态，可用性和管理器状态。正如前面提到的，Swarm的第一个节点自动成为领导者，这在前面的屏幕截图中有所指示。最后，我们看到我们正在使用的Docker引擎的版本。
- en: 'To get even more information about a node, we can use the `docker node inspect` command,
    as shown in the following screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取有关节点的更多信息，我们可以使用docker node inspect命令，如下面的屏幕截图所示：
- en: '![](assets/c9182c97-d23d-4bdc-a365-38f1897f4b63.png)Truncated output of the
    docker node inspect command'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用docker node inspect命令的截断输出
- en: There is a lot of information generated by this command, so we only present
    a truncated version of the output. This output can be useful, for example, when
    you need to troubleshoot a misbehaving cluster node.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令生成了大量信息，因此我们只呈现输出的截断版本。例如，当您需要排除集群节点的故障时，此输出可能很有用。
- en: Creating a local Swarm in VirtualBox or Hyper-V
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在VirtualBox或Hyper-V中创建本地Swarm
- en: Sometimes, a single node Swarm is not enough, but we don't have or don't want to
    use an account to create a Swarm in the cloud. In this case, we can create a local
    Swarm in either VirtualBox or Hyper-V. Creating the Swarm in VirtualBox is slightly
    easier than creating it in Hyper-V, but if you're using Windows 10 and have Docker
    for Windows running, then you cannot use VirtualBox at the same time. The two
    hypervisors are mutually exclusive.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，单个节点的Swarm是不够的，但我们没有或不想使用帐户在云中创建Swarm。在这种情况下，我们可以在VirtualBox或Hyper-V中创建本地Swarm。在VirtualBox中创建Swarm比在Hyper-V中创建Swarm稍微容易一些，但是如果您使用Windows
    10并且正在运行Docker for Windows，则无法同时使用VirtualBox。这两个hypervisor是互斥的。
- en: 'Let''s assume we have VirtualBox and `docker-machine` installed on our laptop.
    We can then use `docker-machine` to list all Docker hosts that are currently defined
    and may be running in VirtualBox:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的笔记本电脑上已安装了VirtualBox和docker-machine。然后，我们可以使用docker-machine列出当前定义并可能在VirtualBox中运行的所有Docker主机：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In my case, I have one VM called `default` defined, which is currently stopped.
    I can easily start the VM by issuing the `docker-machine start default` command.
    This command takes a while and will result in the following (shortened) output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我定义了一个名为default的VM，当前已停止。我可以通过发出docker-machine start default命令轻松启动VM。此命令需要一段时间，并将导致以下（缩短的）输出：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, if I list my VMs again, I should see the following screenshot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我再次列出我的虚拟机，我应该看到以下截图：
- en: '![](assets/e64986fa-4614-4968-9d15-b62e7e873916.png)List of all VMs running
    in Hyper-V'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/e64986fa-4614-4968-9d15-b62e7e873916.png)在Hyper-V中运行的所有虚拟机列表'
- en: 'If we do not have a VM called `default` yet, we can easily create one using
    the `create` command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还没有名为`default`的虚拟机，我们可以使用`create`命令轻松创建一个：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](assets/9a6258ef-8613-41e5-95cb-b86bfe858b8e.png)Output of docker-machine
    create'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/9a6258ef-8613-41e5-95cb-b86bfe858b8e.png)docker-machine create的输出'
- en: We can see in the preceding output how `docker-machine`creates the VM from an
    ISO image, defines SSH keys and certificates, and copies them to the VM and to
    the local `~/.docker/machine` directory, where we will use it later when we want
    to remotely access this VM through the Docker CLI. It also provisions an IP address
    for the new VM.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在前面的输出中看到`docker-machine`如何从ISO映像创建虚拟机，定义SSH密钥和证书，并将它们复制到虚拟机和本地`~/.docker/machine`目录，以便我们以后在通过Docker
    CLI远程访问此虚拟机时使用。它还为新的虚拟机提供了一个IP地址。
- en: We're using the `docker-machine create` command with the `--driver virtualbox` parameter.
    The docker-machine can also work with other drivers such as Hyper-V, AWS, Azure,
    DigitalOcean, and many more. Please see the documentation of `docker-machine`
    for more information. By default, a new VM gets 1 GB of memory associated, which
    is enough to use this VM as a node for a development or test Swarm.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`docker-machine create`命令和`--driver virtualbox`参数。docker-machine也可以使用其他驱动程序，如Hyper-V、AWS、Azure、DigitalOcean等。有关更多信息，请参阅`docker-machine`的文档。默认情况下，新的虚拟机关联了1GB的内存，这足以将此虚拟机用作开发或测试Swarm的节点。
- en: If you're on Windows 10 with Docker for Desktop, use the `hyperv` driver instead.
    To be successful though, you need to run as Administrator. Furthermore, you need
    to have an external virtual switch defined on Hyper-V first. You can use the Hyper-V
    Manager to do so. The output of the command will look very similar to the one
    for the `virtualbox` driver.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是带有Docker for Desktop的Windows 10，请改用`hyperv`驱动程序。但是，要成功，您需要以管理员身份运行。此外，您需要在Hyper-V上首先定义一个外部虚拟交换机。您可以使用Hyper-V管理器来完成。该命令的输出将与`virtualbox`驱动程序的输出非常相似。
- en: 'Now, let''s create five VMs for a five-node Swarm. We can use a bit of scripting
    to reduce the manual work:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为一个五节点的Swarm创建五个虚拟机。我们可以使用一些脚本来减少手动工作：
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `docker-machine` will now create five VMs with the names `node-1` to `node-5`.
    This might take a few moments, so this is a good time to get yourself a hot cup
    of tea. After the VMs are created, we can list them:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine`现在将创建五个名为`node-1`到`node-5`的虚拟机。这可能需要一些时间，所以现在是喝杯热茶的好时机。虚拟机创建完成后，我们可以列出它们：'
- en: '![](assets/2820423c-f815-4247-a15e-251dd7791c3d.png)List of all the VMs we
    need for the Swarm'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2820423c-f815-4247-a15e-251dd7791c3d.png)我们需要Swarm的所有虚拟机列表'
- en: 'Now, we''re ready to build a Swarm. Technically, we could SSH into the first
    VM `node-1` and initialize a Swarm and then SSH into all the other VMs and join
    them to the Swarm leader. But this is not efficient. Let''s again use a script
    that does all of the hard work:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备构建一个Swarm。从技术上讲，我们可以SSH到第一个VM `node-1`并初始化一个Swarm，然后SSH到所有其他VM并加入它们到Swarm领导者。但这并不高效。让我们再次使用一个可以完成所有繁重工作的脚本：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have the join token and the IP address of the Swarm leader, we
    can ask the other nodes to join the Swarm as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了加入令牌和Swarm领导者的IP地址，我们可以要求其他节点加入Swarm，如下所示：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To make the Swarm highly available, we can now promote, for example, `node-2` and `node-3 `to
    become managers:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使Swarm具有高可用性，我们现在可以将例如`node-2`和`node-3`提升为管理者：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can list all of the nodes of the Swarm:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以列出Swarm的所有节点：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We should see the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下内容：
- en: '![](assets/dfb95f3f-4cbd-4275-b620-f5a6b5493e88.png)List of all of the nodes
    of the Docker Swarm on VirtualBox'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/dfb95f3f-4cbd-4275-b620-f5a6b5493e88.png)VirtualBox上Docker Swarm的所有节点列表'
- en: 'This is proof that we have just created a highly available Docker Swarm locally
    on our laptop or workstation. Let''s pull all of our code snippets together and
    make the whole thing a bit more robust. The script will look as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明我们刚刚在本地笔记本电脑或工作站上创建了一个高可用的Docker Swarm。让我们把所有的代码片段放在一起，使整个过程更加健壮。脚本如下所示：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding script first deletes (if present) and then recreates five VMs
    called `node-1` to `node-5`, and then initializes a Swarm on `node-1`. After that,
    the remaining four VMs are added to the Swarm, and finally, `node-2` and `node-3` are
    promoted to manager status to make the Swarm highly available. The whole script
    will take less than 5 minutes to execute and can be repeated as many times as
    desired. The complete script can be found in the repository, in the `docker-swarm` subfolder;
    it is called `create-swarm.sh`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本首先删除（如果存在），然后重新创建名为`node-1`到`node-5`的五个虚拟机，然后在`node-1`上初始化一个Swarm。之后，剩下的四个虚拟机被添加到Swarm中，最后，`node-2`和`node-3`被提升为管理者状态，使Swarm高可用。整个脚本执行时间不到5分钟，可以重复执行多次。完整的脚本可以在存储库的`docker-swarm`子文件夹中找到；它被称为`create-swarm.sh`。
- en: It is a highly recommended best practice to always script and hence automate
    operations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的笔记本电脑或工作站上，始终编写脚本并自动化操作是一种强烈推荐的最佳实践。
- en: Using Play with Docker to generate a Swarm
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Play with Docker生成一个Swarm
- en: To experiment with Docker Swarm without having to install or configure anything locally
    on our computer, we can use **Play with Docker** (**PWD**). PWD is a website that
    can be accessed with a browser and that offers us the ability to create a Docker
    Swarm consisting of up to five nodes. It is definitely a playground, as the name
    implies, and the time for which we can use it is limited to four hours per session.
    We can open as many sessions as we want, but each session automatically ends after
    four hours. Other than that, it is a fully functional Docker environment that
    is ideal for tinkering with Docker or to demonstrate some features.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的计算机上**无需**安装或配置**任何**东西的情况下尝试Docker Swarm，我们可以使用**Play with Docker**（**PWD**）。PWD是一个可以通过浏览器访问的网站，它为我们提供了创建一个由最多五个节点组成的Docker
    Swarm的能力。正如名称所示，它绝对是一个游乐场，我们可以使用的时间限制为每个会话四个小时。我们可以打开尽可能多的会话，但每个会话在四小时后会自动结束。除此之外，它是一个完全功能的Docker环境，非常适合尝试Docker或演示一些功能。
- en: 'Let''s access the site now. In your browser, navigate to the website [https://labs.play-with-docker.com](https://labs.play-with-docker.com).
    You will be presented with a welcome and login screen. Use your Docker ID to log
    in. After successfully going so, you will be presented with a screen that looks
    like the following screenshot:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们访问该网站。在浏览器中，导航到网站[https://labs.play-with-docker.com](https://labs.play-with-docker.com)。您将看到一个欢迎和登录屏幕。使用您的Docker
    ID登录。成功登录后，您将看到一个看起来像以下截图的屏幕：
- en: '![](assets/3172277e-9a56-44e6-8651-cfe1d23cf525.png)Play with Docker window'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/3172277e-9a56-44e6-8651-cfe1d23cf525.png)Play with Docker窗口'
- en: 'As we can see immediately, there is a big timer counting down from four hours.
    That''s how much time we have left to play in this session. Furthermore, we see
    a + ADD NEW INSTANCE link. Click it to create a new Docker host. When you do that,
    your screen should look like the following screenshot:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们立即看到的，有一个大计时器从四小时开始倒计时。这是我们在本次会话中剩下的时间。此外，我们看到一个+ ADD NEW INSTANCE链接。单击它以创建一个新的Docker主机。这样做后，您的屏幕应该看起来像以下的截图：
- en: '![](assets/07316581-3443-4328-ba3b-5c57af1e85c8.png)PWD with one new node'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/07316581-3443-4328-ba3b-5c57af1e85c8.png)PWD带有一个新节点'
- en: On the left-hand side, we see the newly created node with its IP address (`192.168.0.48`)
    and its name (`node1`). On the right-hand side, we have some additional information
    about this new node in the upper half of the screen and a Terminal in the lower
    half. Yes, this Terminal is used to execute commands on this node that we just
    created. This node has the Docker CLI installed, and hence we can execute all
    of the familiar Docker commands on it such as `docker version`. Try it out.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们看到了新创建的节点及其IP地址（`192.168.0.48`）和名称（`node1`）。在右侧，屏幕的上半部分显示了有关这个新节点的一些额外信息，下半部分显示了一个终端。是的，这个终端用于在我们刚刚创建的节点上执行命令。这个节点已经安装了Docker
    CLI，因此我们可以在上面执行所有熟悉的Docker命令，比如`docker version`。试一下吧。
- en: 'But now we want to create a Docker Swarm. Execute the following command in
    the Terminal in your browser:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们想要创建一个Docker Swarm。在浏览器的终端中执行以下命令：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output generated by the preceding command corresponds to what we already
    know from our previous trials with the one-node cluster on our workstation and
    the local cluster using VirtualBox or Hyper-V. The important information, once
    again, is the `join` command that we want to use to join additional nodes to the
    cluster we just created.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令生成的输出与我们之前在工作站上使用单节点集群和在VirtualBox或Hyper-V上使用本地集群时已经知道的内容相对应。重要的信息再次是我们想要用来加入额外节点到我们刚刚创建的集群的`join`命令。
- en: You might have noted that this time we specified the `--advertise-addr` parameter in
    the Swarm `init` command. *Why is that necessary here?* The reason is that the
    nodes generated by PWD have more than one IP address associated with them. We
    can easily verify that by executing the `ip a` command on the node. This command
    will show us that there are indeed two endpoints, `eth0` and `eth1`, present.
    We hence have to specify explicitly to the new to-be swarm manager which one we
    want to use. In our case, it is `eth0`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，这次我们在Swarm的`init`命令中指定了`--advertise-addr`参数。*为什么在这里有必要？*原因是PWD生成的节点有多个与之关联的IP地址。我们可以通过在节点上执行`ip
    a`命令轻松验证这一点。这个命令将向我们显示确实存在两个端点，`eth0`和`eth1`。因此，我们必须明确地指定给新的Swarm管理器我们想要使用哪一个。在我们的情况下，是`eth0`。
- en: Create four additional nodes in PWD by clicking four times on the + ADD NEW
    INSTANCE link. The new nodes will be called `node2`, `node3`, `node4`, and `node5` and
    will all be listed on the left-hand side. If you click on one of the nodes on
    the left-hand side, then the right-hand side shows the details of the respective
    node and a Terminal window for that node.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击四次“+添加新实例”链接在PWD中创建四个额外的节点。新节点将被命名为`node2`、`node3`、`node4`和`node5`，并且都将列在左侧。如果你点击左侧的一个节点，右侧将显示相应节点的详细信息和该节点的终端窗口。
- en: 'Select each node (2 to 5) and execute the `docker swarm join` command that you
    have copied from the leader node (`node1`) in the respective Terminal:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 选择每个节点（2到5）并在相应的终端中执行从领导节点（`node1`）复制的`docker swarm join`命令：
- en: '![](assets/22724adb-5938-4487-a1f2-9d491800bb51.png)Joining a node to the Swarm
    in PWD'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/22724adb-5938-4487-a1f2-9d491800bb51.png)加入节点到PWD中的Swarm'
- en: 'Once you have joined all four nodes to the Swarm, switch back to `node1` and
    list all nodes, which, unsurprisingly, results in this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将所有四个节点加入到Swarm中，切换回`node1`并列出所有节点，结果如下：
- en: '![](assets/e270d3d8-8783-4eec-9ba2-2a66f9a43a08.png)List of all of the nodes
    of the swarm in PWD'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/e270d3d8-8783-4eec-9ba2-2a66f9a43a08.png)PWD中Swarm的所有节点列表'
- en: 'Still on `node1`, we can now promote, say, `node2` and `node3`, to make the
    Swarm highly available:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在`node1`上，我们现在可以提升，比如说，`node2`和`node3`，使Swarm高度可用：
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With this, our Swarm on PWD is ready to accept a workload. We have created a
    highly available Docker Swarm with three manager nodes that form a Raft consensus
    group and two worker nodes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们在PWD上的Swarm已经准备好接受工作负载。我们已经创建了一个高可用的Docker Swarm，其中包括三个管理节点，形成一个Raft共识组，以及两个工作节点。
- en: Creating a Docker Swarm in the cloud
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云端创建一个Docker Swarm
- en: All of the Docker Swarms we have created so far are wonderful to use in development or
    to experiment or to use for demonstration purposes. If we want to create a Swarm
    that can be used as a production environment where we run our mission-critical
    applications, though, then we need to create a—I'm tempted to say—real Swarm in
    the cloud or on premises. In this book, we are going to demonstrate how to create
    a Docker Swarm in AWS.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们创建的所有Docker Swarms都非常适合在开发中使用，或者用于实验或演示目的。但是，如果我们想创建一个可以用作生产环境的Swarm，在那里运行我们的关键应用程序，那么我们需要在云端或本地创建一个——我很想说——真正的Swarm。在本书中，我们将演示如何在AWS中创建Docker
    Swarm。
- en: 'One way to create a Swarm is by using **d****ocker-machine** (**DM**). DM has
    a driver for AWS. If we have an account on AWS, we need the AWS access key ID
    and the AWS secret access key. We can add those two values to a file called `~/.aws/configuration`.
    It should look like the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Swarm的一种方法是使用**docker-machine**（**DM**）。DM在AWS上有一个驱动程序。如果我们在AWS上有一个账户，我们需要AWS访问密钥ID和AWS秘密访问密钥。我们可以将这两个值添加到一个名为`~/.aws/configuration`的文件中。它应该看起来像下面这样：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Every time we run `docker-machine create`, DM will look up those values in that
    file. For more in-depth information on how to get an AWS account and how to obtain
    the two secret keys, please consult this link: [http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们运行`docker-machine create`，DM都会在该文件中查找这些值。有关如何获取AWS账户和获取两个秘钥的更深入信息，请参考此链接：[http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT)。
- en: 'Once we have an AWS account in place and have stored the access keys in the
    configuration file, we can start building our Swarm. The necessary code looks
    exactly the same as the one we used to create a Swarm on our local machine in
    VirtualBox. Let''s start with the first node:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了AWS账户并将访问密钥存储在配置文件中，我们就可以开始构建我们的Swarm。所需的代码看起来与我们在VirtualBox上的本地机器上创建Swarm时使用的代码完全相同。让我们从第一个节点开始：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will create an EC2 instance called `aws-node-1` in the requested region
    (`us-east-1` in my case). The output of the preceding command looks like the following
    screenshot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在请求的区域（在我的情况下是`us-east-1`）中创建一个名为`aws-node-1`的EC2实例。前面命令的输出如下截图所示：
- en: '![](assets/ca6013e9-a419-478f-8967-e86ac6defaab.png)Creating a swarm node on
    AWS with DM'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ca6013e9-a419-478f-8967-e86ac6defaab.png)使用DM在AWS上创建一个Swarm节点'
- en: 'It looks very similar to the output we already know from working with VirtualBox.
    We can now configure our Terminal for remote access to that EC2 instance:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来与我们已经知道的与VirtualBox一起工作的输出非常相似。我们现在可以配置我们的终端以远程访问该EC2实例：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will configure the environment variables used by the Docker CLI accordingly:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这将相应地配置Docker CLI使用的环境变量：
- en: '![](assets/2776b997-08d5-4993-8f68-e047a054969e.png)Environment variables used
    by Docker to enable remote access to the AWS EC2 node'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2776b997-08d5-4993-8f68-e047a054969e.png)Docker用于启用对AWS EC2节点的远程访问的环境变量'
- en: For security reasons, **Transport Layer Security** (**TLS**) is used for the communication between
    our CLI and the remote node. The certificates necessary for that were copied by
    DM to the path we assigned to the environment variable `DOCKER_CERT_PATH`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 出于安全原因，**传输层安全**（**TLS**）用于我们的CLI和远程节点之间的通信。DM将必要的证书复制到我们分配给环境变量`DOCKER_CERT_PATH`的路径。
- en: 'All Docker commands that we now execute in our Terminal will be remotely executed
    in AWS on our EC2 instance. Let''s try to run Nginx on this node:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在终端中执行的所有Docker命令都将在我们的EC2实例上远程执行。让我们尝试在此节点上运行Nginx：
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can use `docker container ls` to verify that the container is running. If
    so, then let''s test it using `curl`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker container ls`来验证容器是否正在运行。如果是的话，让我们使用`curl`进行测试：
- en: '[PRE17]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, `<IP address>` is the public IP address of the AWS node; in my case,
    it would be `35.172.240.127`. Sadly, this doesn''t work; the preceding command
    times out:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`<IP地址>`是AWS节点的公共IP地址；在我的情况下，它将是`35.172.240.127`。遗憾的是，这不起作用；前面的命令超时：
- en: '![](assets/0f6eb2e9-c813-4c67-b0a8-3d4e623caa0d.png)Accessing Nginx on the
    AWS node times out'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/0f6eb2e9-c813-4c67-b0a8-3d4e623caa0d.png)访问AWS节点上的Nginx超时'
- en: 'The reason for this is that our node is part of an AWS **Security Group** (**SG**).
    By default, access to objects inside this SG is denied. Hence, we have to find
    out to which SG our instance belongs and configure access explicitly. For this,
    we typically use the AWS console. Go to the EC2 Dashboard and select Instances
    on the left-hand side. Locate the EC2 instance called `aws-node-1` and select
    it. In the details view, under Security groups, click on the docker-machine link, as
    shown in the following screenshot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是我们的节点是AWS **安全组**（SG）的一部分。默认情况下，拒绝对此SG内部的对象的访问。因此，我们必须找出我们的实例属于哪个SG，并显式配置访问权限。为此，我们通常使用AWS控制台。转到EC2仪表板，并在左侧选择实例。找到名为`aws-node-1`的EC2实例并选择它。在详细视图中，在“安全组”下，单击docker-machine链接，如下图所示：
- en: '![](assets/daacdfa4-bfb7-4c54-8333-4f42a645ed2d.png)Locating the SG to which
    our Swarm node belongs'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/daacdfa4-bfb7-4c54-8333-4f42a645ed2d.png)找到我们的Swarm节点所属的SG'
- en: 'This will lead us to the SG page with the `docker-machine` SG pre-selected.
    In the details section under the Inbound tab, add a new rule for your IP address
    (the IP address of workstation):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引导我们到SG页面，其中`docker-machine` SG被预先选择。在“入站”选项卡下的详细信息部分，为您的IP地址（工作站的IP地址）添加一个新规则：
- en: '![](assets/0d0ee48e-dd73-4b8e-87ee-e5069e77183b.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/0d0ee48e-dd73-4b8e-87ee-e5069e77183b.png)'
- en: Open access to SG for our computer
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为我们的计算机打开SG访问权限
- en: In the preceding screenshot, the IP address `70.113.114.234` happens to be the
    one assigned to my personal workstation. I have enabled all inbound traffic coming
    from this IP address to the `docker-machine` SG. Note that in a production system
    you should be very careful about which ports of the SG to open to the public.
    Usually, it is ports `80` and `443` for HTTP and HTTPS access. Everything else
    is a potential invitation to hackers.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，IP地址`70.113.114.234`恰好是分配给我的个人工作站的IP地址。我已经允许来自此IP地址的所有入站流量进入`docker-machine`
    SG。请注意，在生产系统中，您应该非常小心地选择要向公众开放的SG端口。通常，这是用于HTTP和HTTPS访问的端口`80`和`443`。其他所有内容都是对黑客的潜在邀请。
- en: You can get your own IP address through a service such as [https://www.whatismyip.com/](https://www.whatismyip.com/).
    Now, if we execute the `curl` command again, the greeting page of Nginx is returned.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过诸如[https://www.whatismyip.com/](https://www.whatismyip.com/)之类的服务获取自己的IP地址。现在，如果我们再次执行`curl`命令，将返回Nginx的欢迎页面。
- en: 'Before we leave the SG, we should add another rule to it. The Swarm nodes need
    to be able to freely communicate on ports `7946` and `4789` through TCP and UDP
    and on port `2377` through TCP. We could now add five rules with these requirements
    where the source is the SG itself, or we just define a crude rule that allows
    all inbound traffic inside the SG (`sg-c14f4db3` in my case):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开SG之前，我们应该向其添加另一个规则。Swarm节点需要能够通过TCP和UDP自由通信的端口`7946`和`4789`，以及通过TCP的端口`2377`。我们现在可以添加五个符合这些要求的规则，其中源是SG本身，或者我们只需定义一个允许SG内部所有入站流量的粗糙规则（在我的情况下是`sg-c14f4db3`）：
- en: '![](assets/a51e4f76-db2c-4513-8dc2-d878d5e0ba30.png) SG rule to enable intra-Swarm
    communication'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/a51e4f76-db2c-4513-8dc2-d878d5e0ba30.png) SG规则以启用Swarm内部通信'
- en: 'Now, let''s continue with the creation of the remaining four nodes. Once again,
    we can use a script to ease the process:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续创建剩下的四个节点。我们可以再次使用脚本来简化这个过程：
- en: '[PRE18]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After the provisioning of the nodes is done, we can list all nodes with DM.
    In my case, I see this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的配置完成后，我们可以使用DM列出所有节点。在我的情况下，我看到了这个：
- en: '![](assets/9fe33d9f-01b8-4fc8-9bf7-c24f2cfaa1a3.png)List of all the nodes created
    by DM'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/9fe33d9f-01b8-4fc8-9bf7-c24f2cfaa1a3.png)DM创建的所有节点列表'
- en: In the preceding screenshot, we can see the five nodes that we originally created
    in VirtualBox and the five new nodes that we created in AWS. Apparently, the nodes on
    AWS are using a new version of Docker; here, the version is `18.02.0-ce`. The
    IP addresses we see in the `URL` column are the public IP addresses of my EC2
    instances.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们可以看到我们最初在VirtualBox中创建的五个节点和我们在AWS中创建的五个新节点。显然，AWS上的节点正在使用一个新版本的Docker；这里的版本是`18.02.0-ce`。我们在`URL`列中看到的IP地址是我的EC2实例的公共IP地址。
- en: 'Because our CLI is still configured for remote access to the `aws-node-1` node,
    we can just run the `swarm init` command as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的CLI仍然配置为远程访问`aws-node-1`节点，所以我们可以直接运行以下`swarm init`命令：
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To get the join token do the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取加入令牌，请执行以下操作：
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To get the IP address of the leader use the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取领导者的IP地址，请使用以下命令：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With this information, we can now join the other four nodes to the Swarm leader:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们现在可以将其他四个节点加入到Swarm的领导者中：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'An alternative way to achieve the same without needing to SSH into the individual
    nodes would be to reconfigure our client CLI every time we want to access a different
    node:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 实现相同目标的另一种方法是，无需登录到各个节点，每次想要访问不同的节点时都重新配置我们的客户端CLI：
- en: '[PRE23]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As a last step, we want to promote nodes `2` and `3` to manager:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们希望将节点`2`和`3`提升为管理节点：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can then list all of the Swarm nodes, as shown in the following screenshot:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以列出所有Swarm节点，如下截图所示：
- en: '![](assets/0038c9cb-fc2f-4a9d-ac15-af1133dbf6b8.png)List of all nodes of our
    swarm in the cloud'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/0038c9cb-fc2f-4a9d-ac15-af1133dbf6b8.png)云中我们Swarm的所有节点列表'
- en: 'And hence we have a highly available Docker Swarm running in the cloud. To
    clean up the Swarm in the cloud and avoid incurring unnecessary costs, we can
    use the following command:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在云中拥有一个高可用的Docker Swarm。为了清理云中的Swarm并避免产生不必要的成本，我们可以使用以下命令：
- en: '[PRE25]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Deploying a first application
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署第一个应用程序
- en: We have created a few Docker Swarms on various platforms. Once created, a Swarm
    behaves the same way on any platform. The way we deploy and update applications
    on a Swarm is not platform-dependent. It has been one of Docker's main goals to
    avoid vendor lock-in when using a Swarm. Swarm-ready applications can be effortlessly
    migrated from, say, a Swarm running on premises to a cloud-based Swarm. It is
    even technically possible to run part of a Swarm on premises and another part
    in the cloud. It works, yet we have, of course, to consider possible side effects
    due to the higher latency between nodes in geographically distant areas.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在各种平台上创建了一些Docker Swarms。一旦创建，Swarm在任何平台上的行为都是相同的。我们在Swarm上部署和更新应用程序的方式并不依赖于平台。Docker的主要目标之一就是避免在使用Swarm时出现供应商锁定。支持Swarm的应用程序可以轻松地从例如在本地运行的Swarm迁移到基于云的Swarm。甚至在技术上可以在本地运行Swarm的一部分，另一部分在云中运行。这是可行的，但我们当然必须考虑由于地理上相距较远的节点之间的更高延迟可能导致的可能的副作用。
- en: 'Now that we have a highly available Docker Swarm up and running, it is time
    to run some workloads on it. I''m using a local Swarm created with docker-machine.
    We''ll start by first creating a single service. For this, we need to SSH into
    one of the manager nodes. I select `node-1`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个高可用的Docker Swarm正在运行，是时候在其上运行一些工作负载了。我正在使用通过docker-machine创建的本地Swarm。我们将首先创建一个单一服务。为此，我们需要SSH登录到其中一个管理节点。我选择`node-1`：
- en: '[PRE26]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Creating a service
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个服务
- en: 'A service can be either created as part of a stack or directly using the Docker
    CLI. Let''s first look at a sample stack file that defines a single service:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 服务可以作为堆栈的一部分创建，也可以直接使用Docker CLI创建。让我们首先看一个定义单一服务的示例堆栈文件：
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding example, we see what the desired state of a service called `whoami` is:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们看到了一个名为`whoami`的服务的期望状态：
- en: It is based on the `training/whoami:latest` image.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于`training/whoami:latest`镜像。
- en: Containers of the service are attached to the `test-net` network.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的容器连接到`test-net`网络。
- en: The container port `8000` is published to port `81`.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器端口`8000`发布到端口`81`。
- en: It is running with six replicas (or tasks)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以六个副本（或任务）运行
- en: During a rolling update, the individual tasks are updated in batches of two,
    with a delay of 10 seconds between each successful batch.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在滚动更新期间，单个任务以每批两个的方式更新，每个成功批之间延迟10秒。
- en: The service (and its tasks and containers) is assigned the two labels `app` and `environment `with
    the values `sample-app` and `prod-south`, respectively
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该服务（及其任务和容器）被分配了两个标签`app`和`environment`，其值分别为`sample-app`和`prod-south`。
- en: There are many more settings that we could define for a service, but the preceding
    ones are some of the more important ones. Most settings have meaningful default
    values. If, for example, we do not specify the number of replicas, then Docker
    defaults it to `1`. The name and image of a service are, of course, mandatory.
    Note that the name of the service must be unique in the Swarm.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为服务定义许多其他设置，但前面的设置是一些更重要的设置。大多数设置都有有意义的默认值。例如，如果我们没有指定副本的数量，那么Docker会将其默认为`1`。服务的名称和镜像当然是必需的。请注意，服务的名称在Swarm中必须是唯一的。
- en: 'To create the preceding service, we use the `docker stack deploy` command.
    Assuming that the file in which the preceding content is stored is called `stack.yaml`, we
    have the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建前面的服务，我们使用`docker stack deploy`命令。假设存储前面内容的文件名为`stack.yaml`，我们有以下内容：
- en: '[PRE28]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, we have created a stack called `sample-stack` that consists of one service, `whoami`.
    We can list all stacks on our Swarm, whereupon we should get this:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个名为`sample-stack`的堆栈，其中包含一个名为`whoami`的服务。我们可以列出我们的Swarm上的所有堆栈，然后我们应该得到这个：
- en: '[PRE29]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If we list the services defined in our Swarm, we get the following output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们列出我们的Swarm中定义的服务，我们会得到以下输出：
- en: '![](assets/2598bb40-3f60-44b3-8bb8-2d23d4e5a2df.png)List of all services running
    in the Swarm'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2598bb40-3f60-44b3-8bb8-2d23d4e5a2df.png)列出在Swarm中运行的所有服务'
- en: In the output, we can see that currently, we have only one service running,
    which was to be expected. The service has an `ID`. The format of `ID`, contrary
    to what you have used so far for containers, networks, or volumes, is alphanumeric
    (in the latter cases it was always `sha256`). We can also see that `NAME` of the
    service is a combination of the service name we defined in the stack file and
    the name of the stack, which is used as a prefix. This makes sense since we want
    to be able to deploy multiple stacks (with different names) using the same stack
    file into our Swarm. To make sure that service names are unique, Docker decided
    to combine service name and stack name.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，我们可以看到目前只有一个正在运行的服务，这是可以预料的。该服务有一个`ID`。与迄今为止用于容器、网络或卷的格式相反，`ID`的格式是字母数字（在后一种情况下，它总是`sha256`）。我们还可以看到服务的`NAME`是我们在堆栈文件中定义的服务名称和堆栈的名称的组合，堆栈的名称被用作前缀。这是有道理的，因为我们希望能够使用相同的堆栈文件将多个堆栈（具有不同名称）部署到我们的Swarm中。为了确保服务名称是唯一的，Docker决定将服务名称和堆栈名称组合起来。
- en: In the third column, we see the mode, which is `replicated`. The number of `REPLICAS`
    is shown as `6/6`. This tells us that six out of the six requested `REPLICAS`
    are running. This corresponds to the desired state. In the output, we also see
    the image that the service uses and the port mappings of the service.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三列中，我们看到模式是`replicated`。`REPLICAS`的数量显示为`6/6`。这告诉我们，六个请求的`REPLICAS`中有六个正在运行。这对应于期望的状态。在输出中，我们还可以看到服务使用的镜像和服务的端口映射。
- en: Inspecting the service and its tasks
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查服务及其任务
- en: 'In the preceding output, we cannot see the details of the `6` replicas that have been
    created. To get some deeper insight into that, we can use the `docker service
    ps` command. If we execute this command for our service, we will get the following
    output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，我们看不到已创建的`6`个副本的详细信息。为了更深入地了解这一点，我们可以使用`docker service ps`命令。如果我们为我们的服务执行此命令，我们将得到以下输出：
- en: '![](assets/407ffc09-b33c-459b-a73c-fb17de789245.png)Details of the whoami service'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/407ffc09-b33c-459b-a73c-fb17de789245.png)whoami服务的详细信息'
- en: In the preceding output, we can see the list of six tasks that correspond to
    the requested six replicas of our `whoami` service. In the `NODE` column, we can
    also see the node to which each task has been deployed. The name of each task
    is a combination of the service name plus an increasing index. Also note that,
    similar to the service itself, each task gets an alphanumeric ID assigned.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，我们可以看到与我们请求的`whoami`服务的六个副本相对应的六个任务的列表。在`NODE`列中，我们还可以看到每个任务部署到的节点。每个任务的名称是服务名称加上递增索引的组合。还要注意，与服务本身类似，每个任务都被分配了一个字母数字ID。
- en: 'In my case, apparently task 2, with the name `sample-stack_whoami.2`, has been
    deployed to `node-1`, which is the leader of our Swarm. Hence, I should find a
    container running on this node. Let''s see what we get if we list all containers
    running on `node-1`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，显然任务2，名称为`sample-stack_whoami.2`，已部署到了`node-1`，这是我们Swarm的领导者。因此，我应该在这个节点上找到一个正在运行的容器。让我们看看如果我们列出在`node-1`上运行的所有容器会得到什么：
- en: '![](assets/18d77691-b017-4ff3-9ea0-d07b10cd107a.png)List of containers on node-1'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/18d77691-b017-4ff3-9ea0-d07b10cd107a.png)节点1上的容器列表'
- en: 'As expected, we find a container running from the `training/whoami:latest` image
    with a name that is a combination of its parent task name and ID. We can try to
    visualize the whole hierarchy of objects that we generated when deploying our
    sample stack:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 预期地，我们发现一个容器正在运行`training/whoami:latest`镜像，其名称是其父任务名称和ID的组合。我们可以尝试可视化我们部署示例堆栈时生成的所有对象的整个层次结构：
- en: '![](assets/376c4e6d-f65b-4c82-b74b-02f320141812.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/376c4e6d-f65b-4c82-b74b-02f320141812.png)'
- en: Object hierarchy of a Docker Swarm stack
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm堆栈的对象层次结构
- en: 'A **stack** can consist of one to many services. Each service has a collection
    of tasks. Each task has a one-to-one association with a container. Stacks and
    services are created and stored on the Swarm manager nodes. Tasks are then scheduled
    to Swarm worker nodes, where the worker node creates the corresponding container.
    We can also get some more information about our service by inspecting it. Execute
    the following command:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆栈**可以由一个到多个服务组成。每个服务都有一组任务。每个任务与一个容器有一对一的关联。堆栈和服务是在Swarm管理节点上创建和存储的。然后将任务调度到Swarm工作节点，工作节点在那里创建相应的容器。我们还可以通过检查来获取有关我们的服务的更多信息。执行以下命令：'
- en: '[PRE30]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This provides a wealth of information about all of the relevant settings of
    the service. This includes those we have explicitly defined in our `stack.yaml` file,
    but also those that we didn't specify and that therefore got their default values
    assigned. We're not going to list the whole output here, as it is too long, but
    I encourage the reader to inspect it on their own machine. We will discuss part
    of the information in more detail in the *The swarm routing mesh* section.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了有关服务的所有相关设置的丰富信息。这包括我们在`stack.yaml`文件中明确定义的设置，但也包括我们没有指定的设置，因此被分配了它们的默认值。我们不会在这里列出整个输出，因为它太长了，但我鼓励读者在自己的机器上检查它。我们将在*Swarm路由网格*部分更详细地讨论部分信息。
- en: Logs of a service
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务的日志
- en: 'In an earlier chapter, we worked with the logs produced by a container. Here,
    we''re concentrating on a service. Remember that, ultimately, a service with many
    replicas has many containers running. Hence, we would expect that, if we ask the
    service for its logs, Docker returns an aggregate of all logs of those containers
    belonging to the service. And indeed, that''s what we get if we use the `docker
    service logs` command:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在早些章节中，我们处理了容器产生的日志。在这里，我们专注于一个服务。请记住，最终，具有许多副本的服务有许多容器在运行。因此，我们期望，如果我们要求服务的日志，Docker会返回属于该服务的所有容器的日志的聚合。确实，这就是我们使用`docker
    service logs`命令得到的内容：
- en: '![](assets/cbe0a89f-0434-42a8-ae44-c1abdf29515f.png)Logs of the whoami service'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/cbe0a89f-0434-42a8-ae44-c1abdf29515f.png)whoami服务的日志'
- en: There is not much information in the logs at this point, but it is enough to
    discuss what we get. The first part of each line in the log always contains the
    name of the container combined with the node name from which the log entry originates.
    Then, separated by the vertical bar (`|`), we get the actual log entry. So, if
    we would, say, ask for the logs of the first container in the list directly, we
    would only get a single entry, and the value we would see in this case would be `Listening
    on :8000`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，日志中没有太多信息，但足以讨论我们得到了什么。日志中每行的第一部分始终包含容器的名称，以及日志条目来源的节点名称。然后，通过竖线（`|`）分隔，我们得到实际的日志条目。因此，如果我们直接要求获取列表中第一个容器的日志，我们将只获得一个条目，而在这种情况下我们将看到的值是`Listening
    on :8000`。
- en: The aggregated logs that we get with the `docker service logs` command are not
    sorted in any particular way. So, if the correlation of events is happening in
    different containers, you should add information to your log output that makes
    this correlation possible. Typically, this is a timestamp for each log entry.
    But this has to be done at the source; for example, the application that produces
    a log entry needs to also make sure a timestamp is added.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`docker service logs`命令获取的聚合日志没有按任何特定方式排序。因此，如果事件的相关性发生在不同的容器中，您应该在日志输出中添加信息，使这种相关性成为可能。通常，这是每个日志条目的时间戳。但这必须在源头完成；例如，产生日志条目的应用程序还需要确保添加时间戳。
- en: 'We can as well query the logs of an individual task of the service by providing
    the task ID instead of the service ID or name. So, querying the logs from task
    2 gives us the following output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过提供任务ID而不是服务ID或名称来查询服务的单个任务的日志。因此，查询任务2的日志会给我们以下输出：
- en: '![](assets/edd0bb6b-3bfc-4cb8-97f1-ddab8836e1c6.png)Logs of an individual task
    of the whoami service'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/edd0bb6b-3bfc-4cb8-97f1-ddab8836e1c6.png)whoami服务的单个任务的日志'
- en: Reconciling the desired state
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调和期望的状态
- en: We have learned that a Swarm service is a description or manifest of the desired
    state that we want an application or application service to run in. Now, let's
    see how Docker Swarm reconciles this desired state if we do something that causes
    the actual state of the service to be different from the desired state. The easiest
    way to do this is to forcibly kill one of the tasks or containers of the service.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，Swarm服务是我们希望应用程序或应用程序服务在其中运行的期望状态的描述或清单。现在，让我们看看Docker Swarm如何调和这个期望的状态，如果我们做了一些导致服务的实际状态与期望状态不同的事情。这样做的最简单方法是强制杀死服务的一个任务或容器。
- en: 'Let''s do this with the container that has been scheduled on `node-1`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用安排在`node-1`上的容器来做这个：
- en: '[PRE31]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we do that and then do `docker service ps` right afterward, we will see
    the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做，然后立即运行`docker service ps`，我们将看到以下输出：
- en: '![](assets/2b7a9616-8a86-467d-8e73-eafd14dd9f97.png)Docker Swarm reconciling
    the desired state after one task failed'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2b7a9616-8a86-467d-8e73-eafd14dd9f97.png)Docker Swarm在一个任务失败后调和期望的状态'
- en: We see that task 2 failed with exit code `137` and that the Swarm immediately
    reconciled the desired state by rescheduling the failed task on a node with free
    resources. In this case, the scheduler selected the same node as the failed tasks,
    but this is not always the case. So, without us intervening, the Swarm completely
    fixed the problem, and since the service is running in multiple replicas, at no
    time was the service down.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到任务2以退出码`137`失败，并且Swarm立即通过在具有空闲资源的节点上重新调度失败的任务来调和期望的状态。在这种情况下，调度程序选择了与失败任务相同的节点，但这并不总是这样。因此，在我们不干预的情况下，Swarm完全解决了问题，并且由于服务正在多个副本中运行，服务从未停机。
- en: 'Let''s try another failure scenario. This time we''re going to shut down an
    entire node and are going to see how the Swarm reacts. Let''s take `node-2` for
    this, as it has two tasks (tasks 3 and 4) running on it. For this, we need to
    open a new Terminal window and use `docker-machine` to stop `node-2`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一种失败场景。这一次，我们将关闭整个节点，并看看Swarm的反应。让我们选择`node-2`，因为它上面有两个任务（任务3和任务4）正在运行。为此，我们需要打开一个新的终端窗口，并使用`docker-machine`来停止`node-2`：
- en: '[PRE32]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Back on `node-1`, we can now again run `docker service ps` to see what happened:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 回到`node-1`，我们现在可以再次运行`docker service ps`来看看发生了什么：
- en: '![](assets/e9b37aa0-d5c0-4666-8a11-f57874d4d283.png)Swarm reschedules all tasks
    of a failed node'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/e9b37aa0-d5c0-4666-8a11-f57874d4d283.png)Swarm重新安排了一个失败节点的所有任务'
- en: In the preceding screenshot, we can see that immediately task 3 was rescheduled
    on `node-1` while task 4 was rescheduled on `node-3`. Even this more radical failure
    is handled gracefully by Docker Swarm.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，我们可以看到立即任务3被重新安排在`node-1`上，而任务4被重新安排在`node-3`上。即使这种更激进的失败也能被Docker
    Swarm优雅地处理。
- en: It is important to note though that if `node-2` ever comes back online in the
    Swarm, the tasks that had previously been running on it will not automatically
    be transferred back to it. But the node is now ready for a new workload.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 但需要注意的是，如果`node-2`在Swarm中重新上线，之前在其上运行的任务将不会自动转移到它上面。但是该节点现在已经准备好接受新的工作负载。
- en: Deleting a service or a stack
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除服务或堆栈
- en: 'If we want to remove a particular service from the Swarm, we can use the `docker
    service rm` command. If, on the other hand, we want to remove a stack from the
    Swarm, we analogously use the `docker stack rm` command. This command removes
    all services that are part of the stack definition. In the case of the `whoami` service,
    it was created by using a stack file and hence we''re going to use the latter
    command:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要从Swarm中移除特定的服务，我们可以使用`docker service rm`命令。另一方面，如果我们想要从Swarm中移除一个堆栈，我们类似地使用`docker
    stack rm`命令。这个命令会移除堆栈定义中的所有服务。在`whoami`服务的情况下，它是通过使用堆栈文件创建的，因此我们将使用后者命令：
- en: '![](assets/61fca437-abee-41f3-8cea-6a8a534a58fc.png)Removing a stack'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/61fca437-abee-41f3-8cea-6a8a534a58fc.png)移除一个堆栈'
- en: The preceding command will make sure that all tasks of each service of the stack
    are terminated, and the corresponding containers are stopped by first sending `SIGTERM`,
    and then, if not successful, `SIGKILL` after 10 seconds of timeout.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将确保堆栈的每个服务的所有任务被终止，并且相应的容器首先发送`SIGTERM`，然后，如果不成功，在10秒的超时后发送`SIGKILL`。
- en: It is important to note that the stopped containers are not removed from the
    Docker host. Hence, it is advised to purge containers from time to time on worker
    nodes to reclaim unused resources. Use `docker container purge -f` for this purpose.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，停止的容器不会从Docker主机中删除。因此，建议定期清理工作节点上的容器，以回收未使用的资源。为此，使用`docker container
    purge -f`。
- en: 'Question: Why does it make sense to leave stopped or crashed containers on
    the worker node and not automatically remove them?'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么让停止或崩溃的容器留在工作节点上，而不自动删除它们是有意义的？
- en: Deploying a multi-service stack
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署多服务堆栈
- en: 'In [Chapter 11](412c6f55-a00b-447f-b22a-47b305453507.xhtml), *Docker Compose*, we
    used an application consisting of two services that were declaratively described
    in a Docker compose file. We can use this compose file as a template to create
    a stack file that allows us to deploy the same application into a Swarm. The content
    of our stack file, called `pet-stack.yaml`, looks like this:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](412c6f55-a00b-447f-b22a-47b305453507.xhtml)中，*Docker Compose*，我们使用了一个由两个服务组成的应用程序，在Docker
    compose文件中进行了声明性描述。我们可以使用这个compose文件作为模板，创建一个堆栈文件，允许我们将相同的应用程序部署到Swarm中。我们的堆栈文件的内容，名为`pet-stack.yaml`，如下所示：
- en: '[PRE33]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We request that the `web` service has three replicas, and both services are
    attached to the overlay network, `pets-net`. We can deploy this application using
    the `docker stack deploy` command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求`web`服务有三个副本，并且两个服务都连接到叠加网络`pets-net`。我们可以使用`docker stack deploy`命令部署这个应用程序：
- en: '![](assets/4920e7c3-dc61-4bcf-9960-cc64dcb42e3f.png)Deploy the pets stack'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/4920e7c3-dc61-4bcf-9960-cc64dcb42e3f.png)部署宠物堆栈'
- en: 'Docker creates the `pets_pets-net` overlay network and then the two services `pets_web` and `pets_db`.
    We can then list all of the tasks in the `pets` stack:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Docker创建了`pets_pets-net`叠加网络，然后创建了两个服务`pets_web`和`pets_db`。然后我们可以列出`pets`堆栈中的所有任务：
- en: '![](assets/2af11ff2-9cb7-4056-aa02-409bce27fed2.png)List of all of the tasks
    in the pets stack'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/2af11ff2-9cb7-4056-aa02-409bce27fed2.png)宠物堆栈中所有任务的列表'
- en: 'Finally, let''s test the application using `curl`. And, indeed, the application
    works as expected:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用`curl`测试应用程序。确实，应用程序按预期工作：
- en: '![](assets/7e7fac0b-5781-4038-a0f8-8f53d933da3f.png)Testing the pets application
    using curl'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/7e7fac0b-5781-4038-a0f8-8f53d933da3f.png)使用curl测试宠物应用程序'
- en: The container ID is in the output, where it says `Delivered to you by container 8b906b509a7e`.
    If you run the `curl` command multiple times, the ID should cycle between three
    different values. These are the IDs of the three containers (or replicas) that
    we have requested for the `web` service.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 容器ID在输出中，其中写着`由容器8b906b509a7e提供给您`。如果多次运行`curl`命令，ID应该在三个不同的值之间循环。这些是我们为`web`服务请求的三个容器（或副本）的ID。
- en: Once we're done, we can remove the stack with `docker stack rm pets`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以使用`docker stack rm pets`来删除堆栈。
- en: The swarm routing mesh
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swarm路由网格
- en: 'If you have been paying attention, then you might have noticed something interesting
    in the last section. We had the `pets` application deployed and it resulted in
    the fact that an instance of the `web` service was installed on the three nodes, `node-1`, `node-2`,
    and `node-3`. Yet, we were able to access the `web` service on `node-1` with `localhost` and
    we reached each container from there. *How is that possible?* Well, this is due
    to the so-called Swarm routing mesh. The routing mesh makes sure that when we
    publish a port of a service; that port is then published on all nodes of the Swarm.
    Hence, network traffic that hits any node of the Swarm and requests to use a specific
    port will be forwarded to one of the service containers by the routing mesh. Let''s
    look at the following diagram to see how that works:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在关注，那么你可能已经注意到了上一节中的一些有趣的事情。我们部署了`pets`应用程序，结果是`web`服务的一个实例被安装在三个节点`node-1`、`node-2`和`node-3`上。然而，我们能够通过`localhost`访问`node-1`上的`web`服务，并从那里访问每个容器。*这是怎么可能的？*嗯，这是由于所谓的Swarm路由网格。路由网格确保当我们发布一个服务的端口时，该端口会在Swarm的所有节点上发布。因此，命中Swarm的任何节点并请求使用特定端口的网络流量将通过路由网格转发到服务容器之一。让我们看看下面的图表，看看它是如何工作的：
- en: '![](assets/6ceca300-992c-4267-a309-239315df3cd9.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6ceca300-992c-4267-a309-239315df3cd9.png)'
- en: Docker Swarm routing mesh
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm路由网格
- en: In this situation, we have three nodes, called **Host A** to **Host C**, with
    the IP addresses `172.10.0.15`, `172.10.0.17`, and `172.10.0.33`. In the lower-left
    corner of the diagram, we see the command that created a **web** service with
    two replicas. The corresponding tasks have been scheduled on **Host B** and **Host
    C**. Task 1 landed on **Host B** while task 2 landed on **Host C**.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有三个节点，称为**主机A**到**主机C**，它们的IP地址分别是`172.10.0.15`、`172.10.0.17`和`172.10.0.33`。在图表的左下角，我们看到了创建一个具有两个副本的**web**服务的命令。相应的任务已经被安排在**主机B**和**主机C**上。任务1落在**主机B**上，而任务2落在**主机C**上。
- en: When a service is created on Docker Swarm, it automatically gets a **V****irtual
    IP** (**VIP**) address assigned. This IP address is stable and reserved during
    the whole life cycle of the service. Let's assume that in our case the VIP is `10.2.0.1`.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Docker Swarm上创建服务时，它会自动分配一个**虚拟IP**（VIP）地址。这个IP地址在整个服务的生命周期内是稳定和保留的。假设在我们的情况下，VIP是`10.2.0.1`。
- en: If now a request for port `8080` coming from an external **Load Balancer** (**LB**)
    is targeted at one of the nodes of our Swarm, then this request is handled by
    the Linux **IP Virtual Server** (**IPVS**) service on that node. This service
    makes a lookup with the given port `8080` in the IP table and will find that this
    corresponds to the VIP of the **web** service. Now, since the VIP is not a real
    target, the IPVS service will load balance the IP addresses of the tasks that
    are associated with this service. In our case, it picked task 2, with the IP address, `10.2.0.3`.
    Finally, the **ingress**** Network** (**O****verlay**) is used to forward the
    request to the target container on **Host C**.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在来自外部**负载均衡器**（**LB**）的端口`8080`的请求被定向到我们Swarm的一个节点上，那么这个请求将由该节点上的Linux **IP虚拟服务器**（**IPVS**）服务处理。该服务在IP表中使用给定的端口`8080`进行查找，并将找到这对应于**web**服务的VIP。现在，由于VIP不是一个真正的目标，IPVS服务将负载均衡与该服务关联的任务的IP地址。在我们的情况下，它选择了任务2，其IP地址为`10.2.0.3`。最后，**入口**网络（**Overlay**）用于将请求转发到**Host
    C**上的目标容器。
- en: It is important to note that it doesn't matter which Swarm node the external
    request is forwarded to by the **External LB**. The routing mesh will always handle
    the request correctly and forward it to one of the tasks of the targeted service.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，外部请求被**外部LB**转发到哪个Swarm节点并不重要。路由网格将始终正确处理请求并将其转发到目标服务的任务之一。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced Docker Swarm, which, next to Kubernetes, is the
    second most popular orchestrator for containers. We have looked into the architecture
    of a Swarm, discussed all of the types of resources running in a Swarm, such as
    services, tasks, and more, and we have created services in the Swarm and deployed
    an application that consists of multiple related services.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Docker Swarm，它是继Kubernetes之后第二受欢迎的容器编排器。我们研究了Swarm的架构，讨论了在Swarm中运行的所有类型的资源，如服务、任务等，并在Swarm中创建了服务，并部署了由多个相关服务组成的应用程序。
- en: In the next chapter, we are going to explore how to deploy services or applications
    onto a Docker Swarm with zero downtime and automatic rollback capabilities. We
    are also going to introduce secrets as a means to protect sensitive information.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何在Docker Swarm上部署服务或应用程序，实现零停机时间和自动回滚功能。我们还将介绍秘密作为保护敏感信息的手段。
- en: Questions
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'To assess your learning progress, please answer the following questions:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估您的学习进度，请回答以下问题：
- en: How do you initialize a new Docker Swarm?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何初始化一个新的Docker Swarm？
- en: A. `docker init swarm`
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: A. `docker init swarm`
- en: B. `docker swarm init --advertise-addr <IP address>`
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: B. `docker swarm init --advertise-addr <IP地址>`
- en: C. `docker swarm join --token <join token>`
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: C. `docker swarm join --token <加入令牌>`
- en: You want to remove a worker node from a Docker Swarm. What steps are necessary?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您想要从Docker Swarm中删除一个工作节点。需要哪些步骤？
- en: How do you create an overlay network called `front-tier`? Make the network attachable.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何创建一个名为`front-tier`的覆盖网络？使网络可附加。
- en: How would you create a service called `web` from the `nginx:alpine` image with
    five replicas, which exposes port `3000` on the ingress network and is attached
    to the `front-tier` network?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将如何从`nginx:alpine`镜像创建一个名为`web`的服务，该服务有五个副本，将端口`3000`暴露在入口网络上，并附加到`front-tier`网络？
- en: How would you scale the web service down to three instances?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将如何将web服务缩减到三个实例？
- en: Further reading
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please consult the following link for more in-depth information about selected
    topics:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下链接，了解有关所选主题的更深入信息：
- en: AWS EC2 example at [http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT)
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS EC2示例在[http://dockr.ly/2FFelyT](http://dockr.ly/2FFelyT)
- en: The Raft Consensus Algorithm at [https://raft.github.io/](https://raft.github.io/)
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raft一致性算法在[https://raft.github.io/](https://raft.github.io/)
- en: The Gossip Protocol at [https://en.wikipedia.org/wiki/Gossip_protocol](https://en.wikipedia.org/wiki/Gossip_protocol)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gossip Protocol的[https://en.wikipedia.org/wiki/Gossip_protocol](https://en.wikipedia.org/wiki/Gossip_protocol)
- en: VXLAN and Linux at [https://vincent.bernat.ch/en/blog/2017-vxlan-linux](https://vincent.bernat.ch/en/blog/2017-vxlan-linux)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN和Linux的[https://vincent.bernat.ch/en/blog/2017-vxlan-linux](https://vincent.bernat.ch/en/blog/2017-vxlan-linux)
