- en: Auto-scaling Nodes of a Kubernetes Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes集群的自动缩放节点
- en: May I say that I have not thoroughly enjoyed serving with humans? I find their
    illogic and foolish emotions a constant irritant.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以说我并没有完全享受与人类一起工作吗？我发现他们的不合逻辑和愚蠢的情绪是一个不断的刺激。
- en: '- *Spock*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- *斯波克*'
- en: Usage of **HorizontalPodAutoscaler** (**HPA**) is one of the most critical aspects
    of making a resilient, fault-tolerant, and highly-available system. However, it
    is of no use if there are no nodes with available resources. When Kubernetes cannot
    schedule new Pods because there's not enough available memory or CPU, new Pods
    will be unschedulable and in the pending status. If we do not increase the capacity
    of our cluster, pending Pods might stay in that state indefinitely. To make things
    more complicated, Kubernetes might start removing other Pods to make room for
    those that are in the pending state. That, as you might have guessed, might lead
    to worse problems than the issue of our applications not having enough replicas
    to serve the demand.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**HorizontalPodAutoscaler**（**HPA**）是使系统具有弹性、容错和高可用性的最关键方面之一。然而，如果没有可用资源的节点，它就没有用处。当Kubernetes无法调度新的Pod时，因为没有足够的可用内存或CPU，新的Pod将无法调度并处于挂起状态。如果我们不增加集群的容量，挂起的Pod可能会无限期地保持在那种状态。更复杂的是，Kubernetes可能会开始删除其他Pod，以为那些处于挂起状态的Pod腾出空间。你可能已经猜到，这可能会导致比我们的应用程序没有足够的副本来满足需求的问题更严重的问题。
- en: Kubernetes solves the problem of scaling nodes through Cluster Autoscaler.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通过Cluster Autoscaler解决了节点扩展的问题。
- en: Cluster Autoscaler has a single purpose to adjust the size of the cluster by
    adding or removing worker nodes. It adds new nodes when Pods cannot be scheduled
    due to insufficient resources. Similarly, it eliminates nodes when they are underutilized
    for a period of time and when Pods running on one such node can be rescheduled
    somewhere else.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster Autoscaler只有一个目的，那就是通过添加或删除工作节点来调整集群的大小。当Pod由于资源不足而无法调度时，它会添加新节点。同样，当节点在一段时间内未被充分利用，并且在该节点上运行的Pod可以在其他地方重新调度时，它会删除节点。
- en: The logic behind Cluster Autoscaler is simple to grasp. We are yet to see whether
    it is simple to use as well.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Cluster Autoscaler背后的逻辑很容易理解。我们还没有看到它是否也很容易使用。
- en: Let's create a cluster (unless you already have one) and prepare it for autoscaling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个集群（除非您已经有一个），并为其准备自动缩放。
- en: Creating a cluster
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个集群
- en: We'll continue using definitions from the `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository. To be on the safe side, we'll pull the latest version first.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用`vfarcic/k8s-specs`（[https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs)）存储库中的定义。为了安全起见，我们将首先拉取最新版本。
- en: All the commands from this chapter are available in the `02-ca.sh` ([https://gist.github.com/vfarcic/a6b2a5132aad6ca05b8ff5033c61a88f](https://gist.github.com/vfarcic/a6b2a5132aad6ca05b8ff5033c61a88f))
    Gist.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在`02-ca.sh`（[https://gist.github.com/vfarcic/a6b2a5132aad6ca05b8ff5033c61a88f](https://gist.github.com/vfarcic/a6b2a5132aad6ca05b8ff5033c61a88f)）Gist中找到。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we need a cluster. Please use the Gists below as inspiration to create
    a new cluster or to validate that the one you already fulfills all the requirements.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个集群。请使用下面的Gists作为灵感来创建一个新的集群，或者验证您已经满足所有要求。
- en: A note to AKS users At the time of this writing (October 2018), Cluster Autoscaler
    does not (always) work in **Azure Kubernetes Service** (**AKS**). Please jump
    to *Setting up Cluster Autoscaler in AKS* section for more info and the link to
    instructions how to set it up.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AKS用户注意：在撰写本文时（2018年10月），Cluster Autoscaler在**Azure Kubernetes Service**（**AKS**）中并不总是有效。请参阅*在AKS中设置Cluster
    Autoscaler*部分以获取更多信息和设置说明的链接。
- en: '`gke-scale.sh`: **GKE** with 3 n1-standard-1 worker nodes, with **tiller**,
    and with the `--enable-autoscaling` argument ([https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c](https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c)).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gke-scale.sh`：**GKE**有3个n1-standard-1工作节点，带有**tiller**，并带有`--enable-autoscaling`参数（[https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c](https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c)）。'
- en: '`eks-ca.sh`: **EKS** with 3 t2.small worker nodes, with **tiller**, and with
    **Metrics Server** ([https://gist.github.com/vfarcic/3dfc71dc687de3ed98e8f804d7abba0b](https://gist.github.com/vfarcic/3dfc71dc687de3ed98e8f804d7abba0b)).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eks-ca.sh`：**EKS**有3个t2.small工作节点，带有**tiller**，并带有**Metrics Server**（[https://gist.github.com/vfarcic/3dfc71dc687de3ed98e8f804d7abba0b](https://gist.github.com/vfarcic/3dfc71dc687de3ed98e8f804d7abba0b)）。'
- en: '`aks-scale.sh`: **AKS** with 3 Standard_B2s worker nodes and with **tiller**
    ([https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b](https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b)).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aks-scale.sh`：**AKS**有3个Standard_B2s工作节点和**tiller**（[https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b](https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b)）。'
- en: When examining the Gists, you'll notice a few things. First of all, Docker for
    Desktop and minikube are not there. Both are single-node clusters that cannot
    be scaled. We need to run a cluster in a place where we can add and remove the
    nodes on demand. We'll have to use one of the cloud vendors (for example, AWS,
    Azure, GCP). That does not mean that we cannot scale on-prem clusters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当检查Gists时，你会注意到一些事情。首先，Docker for Desktop和minikube都不在其中。它们都是无法扩展的单节点集群。我们需要在一个可以根据需求添加和删除节点的地方运行集群。我们将不得不使用云供应商之一（例如AWS、Azure、GCP）。这并不意味着我们不能在本地集群上扩展。
- en: We can, but that depends on the vendor we're using. Some do have a solution,
    while others don't. For simplicity, we'll stick with one of the big three. Please
    choose between **Google Kuberentes Engine** (**GKE**), Amazon **Elastic Container
    Service** for Kubernetes (**EKS**), or **Azure Kubernetes Service** (**AKS**).
    If you're not sure which one to pick, I suggest GKE, since it's the most stable
    and feature-rich managed Kubernetes cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以，但这取决于我们使用的供应商。有些供应商有解决方案，而其他供应商没有。为简单起见，我们将坚持使用三大云供应商之一。请在**Google Kubernetes
    Engine**（**GKE**）、亚马逊**弹性容器服务**（**EKS**）或**Azure Kubernetes服务**（**AKS**）之间进行选择。如果你不确定选择哪一个，我建议选择GKE，因为它是最稳定和功能丰富的托管Kubernetes集群。
- en: You'll also notice that GKE and AKS Gists are the same as in the previous chapter,
    while EKS changed. As you already know, the former already have the Metrics Server
    baked in. EKS doesn't, so I copied the Gist we used before and added the instructions
    to install Metrics Server. We might not need it in this chapter but we will use
    it heavily later on, and I want you to get used to having it at all times.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到，GKE和AKS的Gists与上一章相同，而EKS发生了变化。正如你已经知道的那样，前者已经内置了Metrics Server。EKS没有，所以我复制了我们之前使用的Gist，并添加了安装Metrics
    Server的说明。也许在这一章中我们不需要它，但以后会经常用到，我希望你习惯随时拥有它。
- en: If you prefer running the examples locally, you might be devastated by the news
    that we won't use a local cluster in this chapter. Don't despair. The costs will
    be kept to a minimum (probably a few dollars in total), and we'll be back to local
    clusters in the next chapter (unless you choose to stay in clouds).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢在本地运行示例，你可能会因为我们在本章中不使用本地集群而感到沮丧。不要绝望。成本将被保持在最低水平（总共可能只有几美元），我们将在下一章回到本地集群（除非你选择留在云端）。
- en: Now that we have a cluster in GKE, EKS, or AKS, our next step is to enable cluster
    auto-scaling.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在GKE、EKS或AKS中有了一个集群，我们的下一步是启用集群自动扩展。
- en: Setting up Cluster Autoscaling
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置集群自动扩展
- en: We might need to install Cluster Autoscaler before we start using it. I said
    that we *might*, instead of saying that we *have to* because some Kubernetes flavors
    do come with Cluster Autoscaler baked in, while others don't. We'll go through
    each of the "big three" managed Kubernetes clusters. You might choose to explore
    all three of them, or to jump to the one you prefer. As a learning experience,
    I believe that it is beneficial to experience running Kubernetes in all three
    providers. Nevertheless, that might not be your view and you might prefer using
    only one. The choice is yours.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用之前，我们可能需要安装集群自动缩放器。我说“可能”，而不是说“必须”，因为某些Kubernetes版本确实预先配置了集群自动缩放器，而其他版本则没有。我们将逐个讨论“三大”托管Kubernetes集群。您可以选择探索它们三个，或者直接跳转到您喜欢的一个。作为学习经验，我认为体验在所有三个提供商中运行Kubernetes是有益的。尽管如此，这可能不是您的观点，您可能更喜欢只使用一个。选择权在您手中。
- en: Setting up Cluster Autoscaler in GKE
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GKE中设置集群自动缩放器
- en: This will be the shortest section ever written. There's nothing to do in GKE
    if you specified the `--enable-autoscaling` argument when creating the cluster.
    It already comes with Cluster Autoscaler pre-configured and ready.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是有史以来最短的部分。如果在创建集群时指定了`--enable-autoscaling`参数，则在GKE中无需进行任何操作。它已经预先配置并准备好了集群自动缩放器。
- en: Setting up Cluster Autoscaler in EKS
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在EKS中设置集群自动缩放器
- en: Unlike GKE, EKS does not come with Cluster Autoscaler. We'll have to configure
    it ourselves. We'll need to add a few tags to the Autoscaling Group dedicated
    to worker nodes, to put additional permissions to the Role we're using, and to
    install Cluster Autoscaler.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与GKE不同，EKS不带有集群自动缩放器。我们将不得不自己配置它。我们需要向专用于工作节点的Autoscaling Group添加一些标签，为我们正在使用的角色添加额外的权限，并安装集群自动缩放器。
- en: Let's get going.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: We'll add a few tags to the Autoscaling Group dedicated to worker nodes. To
    do that, we need to discover the name of the group. Since we created the cluster
    using **eksctl**, names follow a pattern which we can use to filter the results.
    If, on the other hand, you created your EKS cluster without eksctl, the logic
    should still be the same as the one that follows, even though the commands might
    differ slightly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向专用于工作节点的Autoscaling Group添加一些标签。为此，我们需要发现组的名称。由于我们使用**eksctl**创建了集群，名称遵循一种模式，我们可以使用该模式来过滤结果。另一方面，如果您在没有使用eksctl的情况下创建了EKS集群，逻辑应该与接下来的逻辑相同，尽管命令可能略有不同。
- en: First, we'll retrieve the list of the AWS Autoscaling Groups, and filter the
    result with `jq` so that only the name of the matching group is returned.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将检索AWS Autoscaling Groups的列表，并使用`jq`过滤结果，以便只返回匹配组的名称。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output of the latter command should be similar to the one that follows.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令的输出应该类似于接下来的输出。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We stored the name of the cluster in the environment variable `NAME`. Further
    on, we retrieved the list of all the groups and filtered the output with `jq`
    so that only those with names that start with `eksctl-$NAME-nodegroup` are returned.
    Finally, that same `jq` command retrieved the `AutoScalingGroupName` field and
    we stored it in the environment variable `ASG_NAME`. The last command output the
    group name so that we can confirm (visually) that it looks correct.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将集群的名称存储在环境变量`NAME`中。然后，我们检索了所有组的列表，并使用`jq`过滤输出，以便只返回名称以`eksctl-$NAME-nodegroup`开头的组。最后，相同的`jq`命令检索了`AutoScalingGroupName`字段，并将其存储在环境变量`ASG_NAME`中。最后一个命令输出了组名，以便我们可以确认（视觉上）它看起来是否正确。
- en: Next, we'll add a few tags to the group. Kubernetes Cluster Autoscaler will
    work with the one that has the `k8s.io/cluster-autoscaler/enabled` and `kubernetes.io/cluster/[NAME_OF_THE_CLUSTER]`
    tags. So, all we have to do to let Kubernetes know which group to use is to add
    those tags.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向组添加一些标记。Kubernetes Cluster Autoscaler将与具有“k8s.io/cluster-autoscaler/enabled”和“kubernetes.io/cluster/[NAME_OF_THE_CLUSTER]”标记的组一起工作。因此，我们只需添加这些标记，让Kubernetes知道要使用哪个组。
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The last change we'll have to do in AWS is to add a few additional permissions
    to the role created through eksctl. Just as with the Autoscaling Group, we do
    not know the name of the role, but we do know the pattern used to create it. Therefore,
    we'll retrieve the name of the role, before we add a new policy to it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在AWS中需要做的最后一项更改是向通过eksctl创建的角色添加一些额外的权限。与自动缩放组一样，我们不知道角色的名称，但我们知道用于创建它的模式。因此，在添加新策略之前，我们将检索角色的名称。
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output of the latter command should be similar to the one that follows.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 后一条命令的输出应该类似于接下来的输出。
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We listed all the roles, and we used `jq` to filter the output so that only
    the one with the name that starts with `eksctl-$NAME-nodegroup-0-NodeInstanceRole`
    is returned. Once we filtered the roles, we retrieved the `RoleName` and stored
    it in the environment variable `IAM_ROLE`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了所有角色，并使用`jq`过滤输出，以便只返回名称以`eksctl-$NAME-nodegroup-0-NodeInstanceRole`开头的角色。过滤角色后，我们检索了`RoleName`并将其存储在环境变量`IAM_ROLE`中。
- en: Next, we need JSON that describes the new policy. I already prepared one, so
    let's take a quick look at it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要描述新策略的JSON。我已经准备好了，让我们快速看一下。
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output is as follows.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you're familiar with AWS (I hope you are), that policy should be straightforward.
    It allows a few additional actions related to `autoscaling`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉AWS（我希望你是），那个策略应该很简单。它允许与“autoscaling”相关的一些额外操作。
- en: Finally, we can `put` the new policy to the role.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将新策略“put”到角色中。
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we added the required tags to the Autoscaling Group and that we created
    the additional permissions that will allow Kubernetes to interact with the group,
    we can install Cluster Autoscaler Helm Chart.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经向自动缩放组添加了所需的标记，并创建了额外的权限，允许Kubernetes与该组进行交互，我们可以安装Cluster Autoscaler
    Helm Chart。
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once the Deployment is rolled out, the autoscaler should be fully operational.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦部署完成，自动缩放器应该完全可用。
- en: Setting up Cluster Autoscaler in AKS
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AKS中设置Cluster Autoscaler
- en: At the time of this writing (October 2018), Cluster Autoscaler does not work
    in AKS. At least, not always. It is still in beta stage, and I cannot recommend
    it just yet. Hopefully, it will be fully operational and stable soon. When that
    happens, I will update this chapter with AKS-specific instructions. If you feel
    adventurous or you are committed to Azure, please follow the instructions from
    the *Cluster Autoscaler on Azure Kubernetes Service (AKS) - Preview* ([https://docs.microsoft.com/en-in/azure/aks/cluster-autoscaler](https://docs.microsoft.com/en-in/azure/aks/cluster-autoscaler))
    article. If it works, you should be able to follow the rest of this chapter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时（2018年10月），Cluster Autoscaler在AKS中无法正常工作。至少，不总是。它仍处于测试阶段，我暂时不能推荐。希望它很快就能完全运行并保持稳定。一旦发生这种情况，我将使用AKS特定的说明更新本章。如果你感到有冒险精神，或者你致力于Azure，请按照*Azure
    Kubernetes Service（AKS）上的Cluster Autoscaler - 预览*（[https://docs.microsoft.com/en-in/azure/aks/cluster-autoscaler](https://docs.microsoft.com/en-in/azure/aks/cluster-autoscaler)）文章中的说明。如果它有效，你应该能够按照本章的其余部分进行操作。
- en: Scaling up the cluster
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩大集群
- en: The objective is to scale the nodes of our cluster to meet the demand of our
    Pods. We want not only to increase the number of worker nodes when we need additional
    capacity, but also to remove them when they are underused. For now, we'll focus
    on the former, and explore the latter afterward.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是扩展集群的节点，以满足Pod的需求。我们不仅希望在需要额外容量时增加工作节点的数量，而且在它们被闲置时也要删除它们。现在，我们将专注于前者，并在之后探索后者。
- en: Let's start by taking a look at how many nodes we have in the cluster.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下集群中有多少个节点。
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output, from GKE, is as follows.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 来自GKE的输出如下。
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In your case, the number of nodes might differ. That's not important. What matters
    is to remember how many you have right now since that number will change soon.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的情况下，节点的数量可能会有所不同。这并不重要。重要的是要记住您现在有多少个节点，因为这个数字很快就会改变。
- en: Let's take a look at the definition of the `go-demo-5` application before we
    roll it out.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们推出`go-demo-5`应用程序之前，让我们先看一下它的定义。
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出内容，仅限于相关部分，如下所示。
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this context, the only important part of the definition we are about to apply
    is the HPA connected to the `api` Deployment. Its minimum number of replicas is
    `15`. Given that each `api` container requests 500 MB RAM, fifteen replicas (7.5
    GB RAM) should be more than our cluster can sustain, assuming that it was created
    using one of the Gists. Otherwise, you might need to increase the minimum number
    of replicas.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们即将应用的定义中唯一重要的部分是与`api`部署连接的HPA。它的最小副本数是`15`。假设每个`api`容器请求500 MB RAM，那么十五个副本（7.5
    GB RAM）应该超出了我们的集群可以承受的范围，假设它是使用其中一个Gists创建的。否则，您可能需要增加最小副本数。
- en: Let's apply the definition and take a look at the HPAs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个定义并看一下HPA。
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output of the latter command is as follows.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 后一条命令的输出如下。
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It doesn't matter if the targets are still `unknown`. They will be calculated
    soon, but we do not care for them right now. What matters is that the `api` HPA
    will scale the Deployment to at least `15` replicas.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 无论目标是否仍然是`未知`，它们很快就会被计算出来，但我们现在不关心它们。重要的是`api` HPA将会将部署扩展至至少`15`个副本。
- en: Next, we need to wait for a few seconds before we take a look at the Pods in
    the `go-demo-5` Namespace.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要等待几秒钟，然后再看一下`go-demo-5`命名空间中的Pod。
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The output is as follows.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see that some of the `api` Pods are being created, while others are pending.
    There can be quite a few reasons why a Pod would enter into the pending state.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些`api` Pod正在被创建，而其他一些则是挂起的。Pod进入挂起状态可能有很多原因。
- en: In our case, there are not enough available resources to host all the Pods.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，没有足够的可用资源来托管所有的Pod。
- en: '![](assets/a1c839db-c439-4994-a113-8b0a27c59e84.png)Figure 2-1: Unschedulable
    (pending) Pods waiting for the cluster capacity to increase'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/a1c839db-c439-4994-a113-8b0a27c59e84.png)图2-1：无法调度（挂起）的Pod正在等待集群容量增加'
- en: Let's see whether Cluster Autoscaler did anything to help with our lack of capacity.
    We'll explore the ConfigMap that contains Cluster Autoscaler status.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看集群自动缩放器是否有助于解决我们的容量不足问题。我们将探索包含集群自动缩放器状态的ConfigMap。
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output is too big to be presented in its entirety, so we'll focus on the
    parts that matter.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 输出内容太多，无法完整呈现，所以我们将专注于重要的部分。
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The status is split into two sections; `Cluster-wide` and `NodeGroups`. The
    `ScaleUp` section of the cluster-wide status shows that scaling is `InProgress`.
    At the moment, there are `3` ready nodes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 状态分为两个部分：`整个集群`和`节点组`。整个集群状态的`ScaleUp`部分显示缩放正在进行中。目前有`3`个就绪节点。
- en: If we move down to the `NodeGroups`, we'll notice that there is one for each
    group that hosts our nodes. In AWS those groups map to Autoscaling Groups, in
    case of Google to Instance Groups, and in Azure to Autoscale. One of the `NodeGroups`
    in the config has the `ScaleUp` section `InProgress`. Inside that group, `1` node
    is `ready`. The `cloudProviderTarget` value should be set to a number higher than
    the number of `ready` nodes, and we can conclude that Cluster Autoscaler already
    increased the desired amount of nodes in that group.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们移动到`NodeGroups`，我们会注意到每个托管我们节点的组都有一个。在AWS中，这些组映射到自动缩放组，在谷歌的情况下映射到实例组，在Azure中映射到自动缩放。配置中的一个`NodeGroups`具有`ScaleUp`部分`InProgress`。在该组内，`1`个节点是`ready`。`cloudProviderTarget`值应设置为高于`ready`节点数量的数字，我们可以得出结论，集群自动缩放器已经增加了该组中所需的节点数量。
- en: Depending on the provider, you might see three groups (GKE) or one (EKS) node
    group. That depends on how each provider organizes its node groups internally.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供商的不同，您可能会看到三个组（GKE）或一个（EKS）节点组。这取决于每个提供商如何在内部组织其节点组。
- en: Now that we know that Cluster Autoscaler is in progress of scaling up the nodes,
    we might explore what triggered that action.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道集群自动缩放器正在进行节点扩展，我们可以探索是什么触发了该操作。
- en: Let's describe the `api` Pods and retrieve their events. Since we want only
    those related to `cluster-autoscaler`, we'll limit the output using `grep`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述`api` Pod并检索它们的事件。由于我们只想要与`cluster-autoscaler`相关的事件，我们将使用`grep`来限制输出。
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output, on GKE, is as follows.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在GKE上的输出如下。
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can see that several Pods triggered `scale-up` event. Those are the Pods
    that were in the pending state. That does not mean that each trigger created a
    new node. Cluster Autoscaler is intelligent enough to know that it should not
    create new nodes for each trigger, but that, in this case, one or two nodes (depending
    on the missing capacity) should be enough. If that proves to be false, it will
    scale up again a while later.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到几个Pod触发了`scale-up`事件。这些是处于挂起状态的Pod。这并不意味着每个触发都创建了一个新节点。集群自动缩放器足够智能，知道不应该为每个触发创建新节点，但在这种情况下，一个或两个节点（取决于缺少的容量）应该足够。如果证明这是错误的，它将在一段时间后再次扩展。
- en: Let's retrieve the nodes that constitute the cluster and see whether there are
    any changes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检索构成集群的节点，看看是否有任何变化。
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output is as follows.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can see that a new worker node was added to the cluster. It is not yet ready,
    so we'll need to wait for a few moments until it becomes fully operational.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一个新的工作节点被添加到集群中。它还没有准备好，所以我们需要等待一段时间，直到它完全可操作。
- en: Please note that the number of new nodes depends on the required capacity to
    host all the Pods. You might see one, two, or more new nodes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，新节点的数量取决于托管所有Pod所需的容量。您可能会看到一个、两个或更多新节点。
- en: '![](assets/27fff93a-5bb4-452a-bc2f-e480ec2c725a.png)Figure 2-2: The Cluster
    Autoscaler process of scaling up nodes'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/27fff93a-5bb4-452a-bc2f-e480ec2c725a.png)图2-2：集群自动缩放器扩展节点的过程'
- en: Now, let's see what happened to out Pods. Remember, the last time we checked
    them, there were quite a few in the pending state.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的Pod发生了什么。记住，上次我们检查它们时，有相当多的Pod处于挂起状态。
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output is as follows.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Cluster Autoscaler increased the desired number of nodes in the node group (for
    example, Autoscaling Group in AWS) which, in turn, created a new node. Once the
    scheduler noticed the increase in cluster's capacity, it scheduled the pending
    Pods into the new node. Within a few minutes, our cluster expanded and all the
    scaled Pods are running.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器增加了节点组（例如，AWS中的自动缩放组）中所需的节点数量，从而创建了一个新节点。一旦调度程序注意到集群容量的增加，它就会将待定的Pod调度到新节点中。在几分钟内，我们的集群扩展了，所有缩放的Pod都在运行。
- en: '![](assets/3126993e-31ae-4184-bee7-068f2752fa4c.png)Figure 2-3: Creation of
    the new node through node groups and rescheduling of the pending Pods'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/3126993e-31ae-4184-bee7-068f2752fa4c.png)图2-3：通过节点组创建新节点和挂起Pod的重新调度'
- en: So, what are the rules Cluster Autoscaler uses to decide when to scale up the
    nodes?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，集群自动缩放器在何时决定扩大节点的规则是什么？
- en: The rules governing nodes scale-up
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点规模扩大的规则
- en: Cluster Autoscaler monitors Pods through a watch on Kube API. It checks every
    10 seconds whether there are any unschedulable Pods (configurable through the
    `--scan-interval` flag). In that context, a Pod is unschedulable when the Kubernetes
    Scheduler is unable to find a node that can accommodate it. For example, a Pod
    can request more memory than what is available on any of the worker nodes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器通过对Kube API进行监视来监视Pod。它每10秒检查一次是否有任何无法调度的Pod（可通过`--scan-interval`标志进行配置）。在这种情况下，当Kubernetes调度程序无法找到可以容纳它的节点时，Pod是无法调度的。例如，一个Pod可以请求比任何工作节点上可用的内存更多的内存。
- en: Cluster Autoscaler assumes that the cluster is running on top of some kind of
    node groups. As an example, in the case of AWS, those groups are **Autoscaling
    Groups** (**ASGs**). When there is a need for additional nodes, Cluster Autoscaler
    creating a new node by increasing the size of a node group.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器假设集群运行在某种节点组之上。例如，在AWS的情况下，这些组是**自动缩放组**（**ASGs**）。当需要额外的节点时，集群自动缩放器通过增加节点组的大小来创建一个新节点。
- en: Cluster Autoscaler assumes that requested nodes will appear within 15 minutes
    (configurable through the `--max-node-provision-time` flag). If that period expires
    and a new node was not registered, it will attempt to scale up a different group
    if the Pods are still in pending state. It will also remove unregistered nodes
    after 15 minutes (configurable through the `--unregistered-node-removal-time`
    flag).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器假设请求的节点将在15分钟内出现（可通过`--max-node-provision-time`标志进行配置）。如果该时间段到期，新节点未注册，它将尝试扩展不同的组，如果Pod仍处于挂起状态。它还将在15分钟后删除未注册的节点（可通过`--unregistered-node-removal-time`标志进行配置）。
- en: Next, we'll explore how to scale down the cluster.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何缩小集群。
- en: Scaling down the cluster
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩小集群
- en: Scaling up the cluster to meet the demand is essential since it allows us to
    host all the replicas we need to fulfill (some of) our SLAs. When the demand drops
    and our nodes become underutilized, we should scale down. That is not essential
    given that our users will not experience problems caused by having too much hardware
    in our cluster. Nevertheless, we shouldn't have underutilized nodes if we are
    to reduce expenses. Unused nodes result in wasted money. That is true in all situations,
    especially when running in Cloud and paying only for the resources we used. Even
    on-prem, where we already purchased hardware, it is essential to scale down and
    release resources so that they can be used by other clusters.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 扩大集群以满足需求是必不可少的，因为它允许我们托管我们需要满足（部分）SLA的所有副本。当需求下降，我们的节点变得未充分利用时，我们应该缩小规模。鉴于我们的用户不会因为集群中有太多硬件而遇到问题，这并非必要。然而，如果我们要减少开支，我们不应该有未充分利用的节点。未使用的节点会导致浪费。这在所有情况下都是正确的，特别是在云中运行并且只支付我们使用的资源的情况下。即使在本地，我们已经购买了硬件，缩小规模并释放资源以便其他集群使用是必不可少的。
- en: We'll simulate a decrease in demand by applying a new definition that will redefine
    the HPAs threshold to `2` (min) and `5` (max).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过应用一个新的定义来模拟需求下降，这将重新定义HPAs的阈值为`2`（最小）和`5`（最大）。
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output of the latter command is as follows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 后一条命令的输出如下。
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that the min and max values of the `api` HPA changed to `2` and `5`.
    The current number of replicas is still `15`, but that will drop to `5` soon.
    The HPA already changed the replicas of the Deployment, so let's wait until it
    rolls out and take another look at the Pods.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`api` HPA的最小和最大值已经改变为`2`和`5`。当前副本的数量仍然是`15`，但很快会降到`5`。HPA已经改变了部署的副本，所以让我们等待它的部署完成，然后再看一下Pods。
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The output of the latter command is as follows.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令的输出如下。
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let's see what happened to the `nodes`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`nodes`发生了什么。
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The output shows that we still have four nodes (or whatever was your number
    before we de-scaled the Deployment).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们仍然有四个节点（或者在我们缩减部署之前的数字）。
- en: Given that we haven't yet reached the desired state of only three nodes, we
    might want to take another look at the `cluster-autoscaler-status` ConfigMap.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们还没有达到只有三个节点的期望状态，我们可能需要再看一下`cluster-autoscaler-status` ConfigMap。
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE32]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If your output does not contain `ScaleDown: CandidatesPresent`, you might need
    to wait a bit and repeat the previous command.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您的输出不包含`ScaleDown: CandidatesPresent`，您可能需要等一会儿并重复上一个命令。'
- en: If we focus on the `Health` section of the cluster-wide status, all four nodes
    are still ready.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关注整个集群状态的`Health`部分，所有四个节点仍然是就绪的。
- en: Judging by the cluster-wide section of the status, we can see that there is
    one candidate to `ScaleDown` (it might be more in your case). If we move to the
    `NodeGroups`, we can observe that one of them has `CandidatesPresent` set to `1`
    in the `ScaleDown` section (or whatever was your initial value before scaling
    up).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从状态的整个集群部分来看，我们可以看到有一个候选节点进行`ScaleDown`（在您的情况下可能有更多）。如果我们转到`NodeGroups`，我们可以观察到其中一个节点组在`ScaleDown`部分中的`CandidatesPresent`设置为`1`（或者在扩展之前的初始值）。
- en: In other words, one of the nodes is the candidate for removal. If it remains
    so for ten minutes, the node will be drained first to allow graceful shutdown
    of the Pods running inside it. After that, it will be physically removed through
    manipulation of the scaling group.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，其中一个节点是待删除的候选节点。如果它保持这样十分钟，节点将首先被排空，以允许其中运行的Pods优雅关闭。之后，通过操纵扩展组来物理移除它。
- en: '![](assets/ede07cff-dc03-427e-9205-4facebd4711f.png)Figure 2-4: Cluster Autoscaler
    processes of scaling-down'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ede07cff-dc03-427e-9205-4facebd4711f.png)图2-4：集群自动缩放的缩减过程'
- en: We should wait for ten minutes before we proceed, so this is an excellent opportunity
    to grab some coffee (or tea).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们应该等待十分钟，所以这是一个很好的机会去喝杯咖啡（或茶）。
- en: Now that enough time passed, we'll take another look at the `cluster-autoscaler-status`
    ConfigMap.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经过了足够的时间，我们将再次查看`cluster-autoscaler-status` ConfigMap。
- en: '[PRE33]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE34]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: From the cluster-wide section, we can see that now there are `3` ready nodes,
    but that there are still `4` (or more) registered. That means that one of the
    nodes was drained, but it was still not destroyed. Similarly, one of the node
    groups shows that there is `1` ready node, even though `2` are registered (your
    numbers might vary).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从整个集群部分，我们可以看到现在有`3`个就绪节点，但仍然有`4`（或更多）已注册。这意味着其中一个节点已经被排空，但仍然没有被销毁。同样，其中一个节点组显示有`1`个就绪节点，尽管已注册`2`个（您的数字可能有所不同）。
- en: From Kubernetes perspective, we are back to three operational worker nodes,
    even though the fourth is still physically present.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从Kubernetes的角度来看，我们回到了三个操作节点，尽管第四个节点仍然存在。
- en: Now we need to wait a bit more before we retrieve the nodes and confirm that
    only three are available.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要再等一会儿，然后检索节点并确认只有三个可用。
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output, from GKE, is as follows.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 来自GKE的输出如下。
- en: '[PRE36]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can see that the node was removed and we already know from past experience
    that Kube Scheduler moved the Pods that were in that node to those that are still
    operational. Now that you experienced scaling down of your nodes, we'll explore
    the rule that governs the process.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到节点已被移除，我们已经从过去的经验中知道，Kube Scheduler将那个节点中的Pod移动到仍在运行的节点中。现在您已经经历了节点的缩减，我们将探讨管理该过程的规则。
- en: The rules governing nodes scale-down
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制节点缩减的规则
- en: Cluster Autoscaler iterates every 10 seconds (configurable through the `--scan-interval`
    flag). If the conditions for scaling up are not met, it checks whether there are
    unneeded nodes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器每10秒迭代一次（可通过`--scan-interval`标志进行配置）。如果不满足扩展的条件，它会检查是否有不需要的节点。
- en: It will consider a node eligible for removal when all of the following conditions
    are met.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下所有条件时，它将考虑将节点标记为可移除。
- en: The sum of CPU and memory requests of all Pods running on a node is less than
    50% of the node's allocatable resources (configurable through the `--scale-down-utilization-threshold`
    flag).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在节点上运行的所有Pod的CPU和内存请求总和小于节点可分配资源的50%（可通过`--scale-down-utilization-threshold`标志进行配置）。
- en: All Pods running on the node can be moved to other nodes. The exceptions are
    those that run on all the nodes like those created through DaemonSets.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有在节点上运行的Pod都可以移动到其他节点。例外情况是那些在所有节点上运行的Pod，比如通过DaemonSets创建的Pod。
- en: Whether a Pod might not be eligible for rescheduling to a different node when
    one of the following conditions are met.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下条件之一时，Pod可能不符合重新调度到不同节点的条件。
- en: A Pod with affinity or anti-affinity rules that tie it to a specific node.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有亲和性或反亲和性规则将其与特定节点绑定的Pod。
- en: A Pod that uses local storage.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本地存储的Pod。
- en: A Pod created directly instead of through controllers like Deployment, StatefulSet,
    Job, or ReplicaSet.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接创建的Pod，而不是通过部署、有状态集、作业或副本集等控制器创建的Pod。
- en: All those rules boil down to a simple one. If a node contains a Pod that cannot
    be safely evicted, it is not eligible for removal.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些规则归结为一个简单的规则。如果一个节点包含一个不能安全驱逐的Pod，那么它就不符合移除的条件。
- en: Next, we should speak about cluster scaling boundaries.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该谈谈集群扩展的边界。
- en: Can we scale up too much or de-scale to zero nodes?
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们是否可以扩展得太多或将节点缩减到零？
- en: If we let Cluster Autoscaler do its "magic" without defining any thresholds,
    our cluster or our wallet might be at risk.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果让集群自动缩放器在不定义任何阈值的情况下进行"魔术"，我们的集群或钱包可能会面临风险。
- en: We might, for example, misconfigure HPA and end up scaling Deployments or StatefulSets
    to a huge number of replicas. As a result, Cluster Autoscaler might add too many
    nodes to the cluster. As a result, we could end up paying for hundreds of nodes,
    even though we need much less. Luckily, AWS, Azure, and GCP limit how many nodes
    we can have so we cannot scale to infinity. Nevertheless, we should not allow
    Cluster Autoscaler to go over some limits.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能会错误配置HPA，导致将部署或有状态集扩展到大量副本。结果，集群自动缩放器可能会向集群添加过多的节点。因此，我们可能会支付数百个节点的费用，尽管我们实际上需要的要少得多。幸运的是，AWS、Azure和GCP限制了我们可以拥有的节点数量，因此我们无法无限扩展。尽管如此，我们也不应允许集群自动缩放器超出一些限制。
- en: Similarly, there is a danger that Cluster Autoscaler will scale down to too
    few nodes. Having zero nodes is almost impossible since that would mean that we
    have no Pods in the cluster. Still, we should maintain a healthy minimum of nodes,
    even if that means sometimes being underutilized.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，集群自动缩放器可能会缩减到太少的节点。拥有零个节点几乎是不可能的，因为这意味着我们在集群中没有Pod。尽管如此，我们应该保持健康的最小节点数量，即使有时会被低效利用。
- en: A reasonable minimum of nodes is three. That way, we have a worker node in each
    zone (datacenter) of the region. As you already know, Kubernetes requires three
    zones with master nodes to maintain quorum. In some cases, especially on-prem,
    we might have only one geographically collocated datacenter with low latency.
    In that case, one zone (datacenter) is better than none. But, in the case of Cloud
    providers, three zones is the recommended distribution, and having a minimum of
    one worker node in each makes sense. That is especially true if we use block storage.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的合理最小数量是三个。这样，我们在该地区的每个区域（数据中心）都有一个工作节点。正如您已经知道的，Kubernetes需要三个带有主节点的区域来维持法定人数。在某些情况下，特别是在本地，我们可能只有一个地理上相邻的延迟较低的数据中心。在这种情况下，一个区域（数据中心）总比没有好。但是，在云服务提供商的情况下，三个区域是推荐的分布，并且在每个区域至少有一个工作节点是有意义的。如果我们使用块存储，这一点尤为重要。
- en: By its nature, block storage (for example, EBS in AWS, Persistent Disk in GCP,
    and Block Blob in Azure) cannot move from one zone to another. That means that
    we have to have a worker node in each zone so that there is (most likely) always
    a place for it in the same zone as the storage. Of course, we might not use block
    storage in which case this argument is unfounded.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其性质，块存储（例如AWS中的EBS、GCP中的持久磁盘和Azure中的块Blob）无法从一个区域移动到另一个区域。这意味着我们必须在每个区域都有一个工作节点，以便（很可能）总是有一个与存储在同一区域的位置。当然，如果我们不使用块存储，那么这个论点就站不住脚了。
- en: How about the maximum number of worker nodes? Well, that differs from one use
    case to another. You do not have to stick with the same maximum for all eternity.
    It can change over time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 那么工作节点的最大数量呢？嗯，这取决于不同的用例。您不必永远坚持相同的最大值。它可以随着时间的推移而改变。
- en: As a rule of thumb, I'd recommend having a maximum double from the actual number
    of nodes. However, don't take that rule seriously. It truly depends on the size
    of your cluster. If you have only three worker nodes, your maximum size might
    be nine (three times bigger). On the other hand, if you have hundreds or even
    thousands of nodes, it wouldn't make sense to double that number as the maximum.
    That would be too much. Just make sure that the maximum number of nodes reflects
    the potential increase in demand.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个经验法则，我建议将最大值设为实际节点数量的两倍。但是，不要太认真对待这个规则。这确实取决于您的集群大小。如果您只有三个工作节点，那么最大尺寸可能是九个（三倍）。另一方面，如果您有数百甚至数千个节点，将该数字加倍作为最大值就没有意义。那将太多了。只需确保节点的最大数量反映了需求的潜在增长。
- en: In any case, I'm sure that you'll figure out what should be your minimum and
    your maximum number of worker nodes. If you make a mistake, you can correct it
    later. What matters more is how to define those thresholds.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我相信您会弄清楚您的工作节点的最小和最大数量应该是多少。如果您犯了错误，可以随后更正。更重要的是如何定义这些阈值。
- en: Luckily, setting up min and max values is easy in EKS, GKE, and AKS. For EKS,
    if you're using `eksctl` to create the cluster, all we have to do is add `--nodes-min`
    and `--nodes-max` arguments to the `eksctl create cluster` command. GKE is following
    a similar logic with `--min-nodes` and `--max-nodes` arguments of the `gcloud
    container clusters create` command. If one of the two is your preference, you
    already used those arguments if you followed the Gists. Even if you forget to
    specify them, you can always modify Autoscaling Groups (AWS) or Instance Groups
    (GCP) since that's where the limits are actually applied.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在EKS、GKE和AKS中设置最小和最大值很容易。对于EKS，如果您使用`eksctl`来创建集群，我们只需在`eksctl create cluster`命令中添加`--nodes-min`和`--nodes-max`参数。GKE遵循类似的逻辑，使用`gcloud
    container clusters create`命令的`--min-nodes`和`--max-nodes`参数。如果其中一个是您的首选项，那么如果您遵循了Gists，您已经使用了这些参数。即使您忘记指定它们，您也可以随时修改自动缩放组（AWS）或实例组（GCP），因为实际应用限制的地方就在那里。
- en: Azure takes a bit different approach. We define its limits directly in the `cluster-autoscaler`
    Deployment, and we can change them just by applying a new definition.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Azure采取了稍微不同的方法。我们直接在`cluster-autoscaler`部署中定义其限制，并且可以通过应用新的定义来更改它们。
- en: Cluster Autoscaler compared in GKE, EKS, and AKS
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GKE、EKS和AKS中比较的集群自动缩放器
- en: Cluster Autoscaler is a prime example of the differences between different managed
    Kubernetes offerings. We'll use it to compare the three major Kubernetes-as-a-Service
    providers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器是不同托管Kubernetes服务提供商之间差异的一个主要例子。我们将使用它来比较三个主要的Kubernetes即服务提供商。
- en: I'll limit the comparison between the vendors only to the topics related to
    Cluster Autoscaling.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把供应商之间的比较限制在与集群自动缩放相关的主题上。
- en: GKE is a no-brainer for those who can use Google to host their cluster. It is
    the most mature and feature-rich platform. They started **Google Kubernetes Engine**
    (**GKE**) long before anyone else. When we combine their head start with the fact
    that they are the major contributor to Kubernetes and hence have the most experience,
    it comes as no surprise that their offering is way above others.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些可以使用谷歌来托管他们的集群的人来说，GKE是一个不言而喻的选择。它是最成熟和功能丰富的平台。他们比其他人早很久就开始了**Google Kubernetes
    Engine**（**GKE**）。当我们将他们的领先优势与他们是Kubernetes的主要贡献者并且因此拥有最丰富经验这一事实结合起来时，他们的产品远远超过其他人并不足为奇。
- en: When using GKE, everything is baked into the cluster. That includes Cluster
    Autoscaler. We do not have to execute any additional commands. It simply works
    out of the box. Our cluster scales up and down without the need for our involvement,
    as long as we specify the `--enable-autoscaling` argument when creating the cluster.
    On top of that, GKE brings up new nodes and joins them to the cluster faster than
    the other providers. If there is a need to expand the cluster, new nodes are added
    within a minute.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用GKE时，一切都包含在集群中。这包括集群自动缩放器。我们不必执行任何额外的命令。只要我们在创建集群时指定`--enable-autoscaling`参数，它就可以直接使用。此外，GKE比其他提供商更快地启动新节点并将它们加入集群。如果需要扩展集群，新节点将在一分钟内添加。
- en: There are many other reasons I would recommend GKE, but that's not the subject
    right now. Still, Cluster Autoscaling alone should be the proof that GKE is the
    solution others are trying to follow.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我会推荐GKE的许多其他原因，但现在不是讨论的主题。不过，单单集群自动缩放就足以证明GKE是其他人努力追随的解决方案。
- en: Amazon's **Elastic Container Service** for Kubernetes (**EKS**) is somewhere
    in the middle. Cluster Autoscaling works, but it's not baked in. It's as if Amazon
    did not think that scaling clusters is important and left it as an optional add-on.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊的**弹性容器服务**（**EKS**）处于中间位置。集群自动缩放器可以工作，但它并不是内置的。就好像亚马逊认为扩展集群并不重要，所以将其作为一个可选的附加组件。
- en: EKS installation is too complicated (when compared to GKE and AKS) but thanks
    to eksctl ([https://eksctl.io/](https://eksctl.io/)) from the folks from Weaveworks,
    we have that, more or less, solved. Still, there is a lot left to be desired from
    eksctl. For example, we cannot use it to upgrade our clusters.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与GKE和AKS相比，EKS的安装过于复杂，但多亏了来自Weaveworks的eksctl（[https://eksctl.io/](https://eksctl.io/)），我们解决了这个问题。不过，eksctl还有很多需要改进的地方。例如，我们无法使用它来升级我们的集群。
- en: The reason I'm mentioning eksctl in the context of auto-scaling lies in the
    Cluster Autoscaler setup.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到eksctl是在自动缩放设置的上下文中。
- en: I cannot say that setting up Cluster Autoscaler in EKS is hard. It's not. And
    yet, it's not as simple as it should be. We have to tag the Autoscaling Group,
    put additional privileges to the role, and install Cluster Autoscaler. That's
    not much. Still, those steps are much more complicated than they should be. We
    can compare it with GKE. Google understands that auto-scaling Kuberentes clusters
    is a must and it provides that with a single argument (or a checkbox if you prefer
    UIs). AWS, on the other hand, did not deem auto-scaling important enough to give
    us that much simplicity. On top of the unnecessary setup in EKS, the fact is that
    AWS added the internal pieces required for scaling only recently. Metrics Server
    can be used only since September 2018.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我不能说在EKS中设置集群自动缩放器很难。并不是。然而，它并不像应该的那么简单。我们需要给自动缩放组打标签，为角色添加额外的权限，并安装集群自动缩放器。这并不多。然而，这些步骤比应该的复杂得多。我们可以拿GKE来比较。谷歌明白自动缩放Kubernetes集群是必须的，并提供了一个参数（或者如果你更喜欢UI，可以选择一个复选框）。而AWS则认为自动缩放并不重要，没有给我们那么简单的设置。除了EKS中不必要的设置之外，事实上AWS最近才添加了扩展所需的内部组件。Metrics
    Server只能在2018年9月之后使用。
- en: My suspicion is that AWS does not have the interest to make EKS great by itself
    and that they are saving the improvements for Fargate. If that's the case (we'll
    find that out soon), I'd characterize it as "sneaky business." Kubernetes has
    all the tools required for scaling Pod and nodes and they are designed to be extensible.
    The choice not to include Cluster Autoscaler as an integral part of their managed
    Kubernetes service is a big minus.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我怀疑AWS并不急于让EKS变得更好，而是把改进留给了Fargate。如果是这样的话（我们很快就会知道），我会把它称为“隐秘的商业行为”。Kubernetes拥有所有扩展Pod和节点所需的工具，并且它们被设计为可扩展的。选择不将集群自动缩放器作为托管Kubernetes服务的一部分是一个很大的缺点。
- en: What can I say about AKS? I admire the improvements Microsoft made in Azure
    as well as their contributions to Kubernetes. They do recognize the need for a
    good managed Kubernetes offering. Yet, Cluster Autoscaler is still in beta. Sometimes
    it works, more often than not it doesn't. Even when it does work as it should,
    it is slow. Waiting for a new node to join the cluster is an exercise in patience.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: AKS有什么好说的呢？我钦佩微软在Azure上所做的改进，以及他们对Kubernetes的贡献。他们确实意识到了提供一个良好的托管Kubernetes的需求。然而，集群自动缩放器仍处于测试阶段。有时它能正常工作，但更多时候却不能。即使它正常工作，速度也很慢。等待新节点加入集群需要耐心等待。
- en: The steps required to install Cluster Autoscaler in AKS are sort of ridiculous.
    We are required to define a myriad of arguments that were supposed to be already
    available inside the cluster. It should know what is the name of the cluster,
    what is the resource group, and so on and so forth. And yet, it doesn't. At least,
    that's the case at the time of this writing (October 2018). I hope that both the
    process and the experience will improve over time. For now, from the perspective
    of auto-scaling, AKS is at the tail of the pack.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在AKS中安装集群自动缩放器所需的步骤有些荒谬。我们需要定义大量参数，而这些参数本应该已经在集群内可用。它应该知道集群的名称，资源组的名称等等。然而，它并不知道。至少在撰写本文时是这样的（2018年10月）。我希望随着时间的推移，这个过程和体验会得到改善。目前来看，就自动缩放的角度来看，AKS处于队伍的最后。
- en: You might argue that the complexity of the setup does not really matter. You'd
    be right. What matters is how reliable Cluster Autoscaling is and how fast it
    adds new nodes to the cluster. Still, the situation is the same. GKE leads in
    reliability and the speed. EKS is the close second, while AKS is trailing behind.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会说设置的复杂性并不重要。你说得对。重要的是集群自动缩放器的可靠性以及它添加新节点到集群的速度。然而，情况却是一样的。GKE在可靠性和速度方面处于领先地位。EKS紧随其后，而AKS则落后。
- en: What now?
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在呢？
- en: There's not much left to say about Cluster Autoscaler.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 关于集群自动缩放器没有太多要说的了。
- en: We finished exploring fundamental ways to auto-scale Pods and nodes. Soon we'll
    dive into more complicated subjects and explore things that are not "baked" into
    a Kubernetes cluster. We'll go beyond the core project and introduce a few new
    tools and processes.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了自动缩放Pod和节点的基本方法。很快我们将深入探讨更复杂的主题，并探索那些没有“内置”到Kubernetes集群中的东西。我们将超越核心项目，并介绍一些新的工具和流程。
- en: This is the moment when you should destroy your cluster if you're not planning
    to move into the next chapter right away and if your cluster is disposable (for
    example, not on bare-metal). Otherwise, please delete the `go-demo-5` Namespace
    to remove the resources we created in this chapter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不打算立即进入下一章，并且您的集群是可丢弃的（例如，不在裸机上），那么这就是您应该销毁集群的时刻。否则，请删除`go-demo-5`命名空间，以删除我们在本章中创建的资源。
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Before you leave, you might want to go over the main points of this chapter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在您离开之前，您可能希望复习本章的要点。
- en: Cluster Autoscaler has a single purpose to adjust the size of the cluster by
    adding or removing worker nodes. It adds new nodes when Pods cannot be scheduled
    due to insufficient resources. Similarly, it eliminates nodes when they are underutilized
    for a period of time and when Pods running on one such node can be rescheduled
    somewhere else.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群自动缩放器有一个单一的目的，即通过添加或删除工作节点来调整集群的大小。当Pod由于资源不足而无法调度时，它会添加新节点。同样，当节点在一段时间内利用不足，并且运行在该节点上的Pod可以在其他地方重新调度时，它会消除节点。
- en: Cluster Autoscaler assumes that the cluster is running on top of some kind of
    node groups. As an example, in the case of AWS, those groups are Autoscaling Groups
    (ASGs). When there is a need for additional nodes, Cluster Autoscaler creating
    a new node by increasing the size of a node group.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群自动缩放器假设集群正在某种节点组之上运行。例如，在AWS的情况下，这些组是自动缩放组（ASG）。当需要额外的节点时，集群自动缩放器通过增加节点组的大小来创建新节点。
- en: The cluster will be scaled down when the sum of CPU and memory requests of all
    Pods running on a node is less than 50% of the node's allocatable resources and
    when all Pods running on the node can be moved to other nodes (DamonSets are the
    exception).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当运行在节点上的所有Pod的CPU和内存请求总和小于节点可分配资源的50％时，集群将被缩减，并且当运行在节点上的所有Pod可以移动到其他节点时（DamonSets是例外情况）。
