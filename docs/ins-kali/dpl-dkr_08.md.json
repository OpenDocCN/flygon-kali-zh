["```\n$ # Make sure we have an accurate view of repositories\n$ sudo apt-get update \n<snip>\nFetched 3,971 kB in 22s (176 kB/s) \nReading package lists... Done\n\n$ # Install the package\n$ sudo apt-get install ansible \nReading package lists... Done\nBuilding dependency tree \nReading state information... Done\nThe following NEW packages will be installed:\n ansible\n0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n<snip>\nSetting up ansible (2.0.0.2-2ubuntu1) ...\n\n$ # Sanity check\n$ ansible --version \nansible 2.0.0.2\n config file = /home/user/checkout/eos-administration/ansible/ansible.cfg\n configured module search path = /usr/share/ansible\n```", "```\n.\n\u251c\u2500\u2500 group_vars\n\u2502   \u2514\u2500\u2500 all\n\u251c\u2500\u2500 hosts\n\u251c\u2500\u2500 named-role-1-server.yml\n\u2514\u2500\u2500 roles\n \u251c\u2500\u2500 named-role-1\n \u2502   \u251c\u2500\u2500 tasks\n \u2502   \u2502   \u2514\u2500\u2500 main.yml\n \u2502   \u251c\u2500\u2500 files\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u251c\u2500\u2500 templates\n \u2502   \u2502   \u2514\u2500\u2500 ...\n \u2502   \u2514\u2500\u2500 vars\n \u2502       \u2514\u2500\u2500 main.yml\n ...\n```", "```\n$ # First we create our deployment source folder and move there\n$ mkdir ~/ansible_deployment\n$ cd ~/ansible_deployment/\n\n$ # Next we create the directories we will need\n$ mkdir -p roles/swarm_node/files roles/swarm_node/tasks\n\n$ # Make a few placeholder files\n$ touch roles/swarm_node/tasks/main.yml \\\n        swarm_node.yml \\\n        hosts\n\n$ # Let's see what we have so far\n$ tree\n.\n\u251c\u2500\u2500 hosts\n\u251c\u2500\u2500 roles\n\u2502   \u2514\u2500\u2500 swarm_node\n\u2502       \u251c\u2500\u2500 files\n\u2502       \u2514\u2500\u2500 tasks\n\u2502           \u2514\u2500\u2500 main.yml\n\u2514\u2500\u2500 swarm_node.yml\n4 directories, 3 files\n```", "```\n---\n- name: Swarm node setup\n hosts: all\n\n become: True\n\n roles:\n - swarm_node\n```", "```\nnet.netfilter.nf_conntrack_tcp_timeout_established = 43200\nnet.netfilter.nf_conntrack_max = 524288\n```", "```\nfs.file-max = 1000000\n```", "```\nnet.core.optmem_max = 40960\nnet.core.rmem_default = 16777216\nnet.core.rmem_max = 16777216\nnet.core.wmem_default = 16777216\nnet.core.wmem_max = 16777216\nnet.ipv4.tcp_rmem = 4096 87380 16777216\nnet.ipv4.tcp_wmem = 4096 87380 16777216\n```", "```\nroot soft nofile 65536\nroot hard nofile 65536\n* soft nofile 65536\n* hard nofile 65536\n```", "```\n.\n\u251c\u2500\u2500 hosts\n\u251c\u2500\u2500 roles\n\u2502   \u2514\u2500\u2500 swarm_node\n\u2502       \u251c\u2500\u2500 files\n\u2502       \u2502   \u251c\u2500\u2500 conntrack.conf\n\u2502       \u2502   \u251c\u2500\u2500 file-descriptor-increase.conf\n\u2502       \u2502   \u251c\u2500\u2500 socket-buffers.conf\n\u2502       \u2502   \u2514\u2500\u2500 ulimit-open-files-increase.conf\n\u2502       \u2514\u2500\u2500 tasks\n\u2502           \u2514\u2500\u2500 main.yml\n\u2514\u2500\u2500 swarm_node.yml\n```", "```\n- name: A descriptive step name that shows in output\n module_name:\n module_arg1: arg_value\n module_arg2: arg2_value\n module_array_arg3:\n - arg3_item1\n ...\n ...\n```", "```\n---\n- name: Dist-upgrading the image\n apt:\n upgrade: dist\n force: yes\n update_cache: yes\n cache_valid_time: 3600\n\n- name: Fixing ulimit through limits.d\n copy:\n src: \"{{ item }}.conf\"\n dest: /etc/security/limits.d/90-{{ item }}.conf\n with_items:\n - ulimit-open-files-increase\n\n- name: Fixing ulimits through pam_limits\n lineinfile:\n dest: /etc/pam.d/common-session\n state: present\n line: \"session required pam_limits.so\"\n\n- name: Ensuring server-like kernel settings are set\n copy:\n src: \"{{ item }}.conf\"\n dest: /etc/sysctl.d/10-{{ item }}.conf\n with_items:\n - socket-buffers\n - file-descriptor-increase\n - conntrack\n\n# Bug: https://github.com/systemd/systemd/issues/1113\n- name: Working around netfilter loading order\n lineinfile:\n dest: /etc/modules\n state: present\n line: \"{{ item }}\"\n with_items:\n - nf_conntrack_ipv4\n - nf_conntrack_ipv6\n\n- name: Increasing max connection buckets\n command: echo '131072' > /sys/module/nf_conntrack/parameters/hashsize\n\n# Install Docker\n- name: Fetching Docker's GPG key\n apt_key:\n keyserver: hkp://pool.sks-keyservers.net\n id: 58118E89F3A912897C070ADBF76221572C52609D\n\n- name: Adding Docker apt repository\n apt_repository:\n repo: 'deb https://apt.dockerproject.org/repo {{ ansible_distribution | lower }}-{{ ansible_distribution_release | lower }} main'\n state: present\n\n- name: Installing Docker\n apt:\n name: docker-engine\n state: installed\n update_cache: yes\n cache_valid_time: 3600\n```", "```\n$ export AWS_ACCESS_KEY_ID=\"AKIABCDEFABCDEF\"\n$ export AWS_SECRET_ACCESS_KEY=\"123456789ABCDEF123456789ABCDEF\"\n$ export AWS_REGION=\"us-west-1\"\n```", "```\n$ aws configure\nAWS Access Key ID [None]: AKIABCDEFABCEF\nAWS Secret Access Key [None]: 123456789ABCDEF123456789ABCDEF\nDefault region name [None]: us-west-1\nDefault output format [None]: json\n```", "```\n$ # Get the archive\n$ wget -q --show-progress https://releases.hashicorp.com/packer/1.1.1/packer_<release>.zip\npacker_<release>.zip 100%[==============================================>] 15.80M 316KB/s in 40s\n\n$ # Extract our binary\n$ unzip packer_<release>.zip\nArchive: packer_<release>.zip\n inflating: packer\n\n$ # Place the binary somewhere in your path\n$ sudo mv packer /usr/local/bin/\n\n$ packer --version\n1.1.1\n```", "```\n{\n  \"builders\": [\n    {\n      \"ami_description\": \"Cluster Node Image\",\n      \"ami_name\": \"cluster-node\",\n      \"associate_public_ip_address\": true,\n      \"force_delete_snapshot\": true,\n      \"force_deregister\": true,\n      \"instance_type\": \"m3.medium\",\n      \"region\": \"us-west-1\",\n      \"source_ami\": \"ami-1c1d217c\",\n      \"ssh_username\": \"ubuntu\",\n      \"type\": \"amazon-ebs\"\n    }\n  ],\n  \"provisioners\": [\n    {\n      \"inline\": \"sudo apt-get update && sudo apt-get install -y ansible\",\n      \"type\": \"shell\"\n    },\n    {\n      \"playbook_dir\": \".\",\n      \"playbook_file\": \"swarm_node.yml\",\n      \"type\": \"ansible-local\"\n    }\n  ]\n}\n```", "```\n- type: What type of image are we building (EBS-optimized one in our case).\n- region: What region will this AMI build in.\n- source_ami: What is our base AMI? See section below for more info on this.\n- instance_type: Type of instance to use when building the AMI - bigger machine == faster builds.\n- ami_name: Name of the AMI that will appear in the UI.\n- ami_description: Description for the AMI.\n- ssh_username: What username to use to connect to base AMI. For Ubuntu, this is usually \"ubuntu\".\n- associate_public_ip_address: Do we want this builder to have an external IP. Usually this needs to be true.\n- force_delete_snapshot: Do we want to delete the old block device snapshot if same AMI is rebuilt?\n- force_deregister: Do we want to replace the old AMI when rebuilding?\n```", "```\n$ # Install python-boto as it is a prerequisite for Amazon builders\n$ # Also get awscli to check if credentials have been set correctly\n$ sudo apt-get update && sudo apt-get install -y python-boto awscli\n<snip>\n\n$ # Check that AWS API credentials are properly set. \n$ # If you see errors, consult the previous section on how to do this\n$ aws ec2 describe-volumes \n{\n \"Volumes\": [\n ]\n}\n\n$ # Go to the proper directory if we are not in it\n$ cd ~/ansible_deployment\n\n$ # Build our AMI and use standardized output format\n$ packer build -machine-readable packer.json \n<snip>\n1509439711,,ui,say,==> amazon-ebs: Provisioning with shell script: /tmp/packer-shell105349087\n<snip>\n1509439739,,ui,message, amazon-ebs: Setting up ansible (2.0.0.2-2ubuntu1) ...\n1509439741,,ui,message, amazon-ebs: Setting up python-selinux (2.4-3build2) ...\n1509439744,,ui,say,==> amazon-ebs: Provisioning with Ansible...\n1509439744,,ui,message, amazon-ebs: Uploading Playbook directory to Ansible staging directory...\n<snip>\n1509439836,,ui,message, amazon-ebs: TASK [swarm_node : Installing Docker] ******************************************\n1509439855,,ui,message, amazon-ebs: [0;33mchanged: [127.0.0.1][0m\n1509439855,,ui,message, amazon-ebs:\n1509439855,,ui,message, amazon-ebs: PLAY RECAP *********************************************************************\n1509439855,,ui,message, amazon-ebs: [0;33m127.0.0.1[0m : [0;32mok[0m[0;32m=[0m[0;32m10[0m [0;33mchanged[0m[0;33m=[0m[0;33m9[0m unreachable=0 failed=0\n1509439855,,ui,message, amazon-ebs:\n1509439855,,ui,say,==> amazon-ebs: Stopping the source instance...\n<snip>\n1509439970,,ui,say,Build 'amazon-ebs' finished.\n1509439970,,ui,say,--> amazon-ebs: AMIs were created:\\nus-west-1: ami-a694a8c6\\n\n```", "```\n# Region that will accompany all AWS-related module usages\naws_region: us-west-1\n\n# ID of our Packer-built AMI\ncluster_node_ami: ami-a694a8c6\n\n# Key name that will be used to manage the instances. Do not\n# worry about what this is right now - we will create it in a bit\nssh_key_name: swarm_key\n\n# Define the internal IP network for the VPC\nswarm_vpc_cidr: \"172.31.0.0/16\"\n```", "```\n- hosts: localhost\n connection: local\n gather_facts: False\n\n tasks:\n - name: Setting up VPC\n ec2_vpc_net:\n region: \"{{ aws_region }}\"\n name: \"Swarm VPC\"\n cidr_block: \"{{ swarm_vpc_cidr }}\"\n register: swarm_vpc\n\n - set_fact:\n vpc: \"{{ swarm_vpc.vpc }}\"\n\n - name: Setting up the subnet tied to the VPC\n ec2_vpc_subnet:\n region: \"{{ aws_region }}\"\n vpc_id: \"{{ vpc.id }}\"\n cidr: \"{{ swarm_vpc_cidr }}\"\n resource_tags:\n Name: \"Swarm subnet\"\n register: swarm_subnet\n\n - name: Setting up the gateway for the VPC\n ec2_vpc_igw:\n region: \"{{ aws_region }}\"\n vpc_id: \"{{ vpc.id }}\"\n register: swarm_gateway\n\n - name: Setting up routing table for the VPC network\n ec2_vpc_route_table:\n region: \"{{ aws_region }}\"\n vpc_id: \"{{ vpc.id }}\"\n lookup: tag\n tags:\n Name: \"Swarm Routing Table\"\n subnets:\n - \"{{ swarm_subnet.subnet.id }}\"\n routes:\n - dest: 0.0.0.0/0\n gateway_id: \"{{ swarm_gateway.gateway_id }}\"\n\n - name: Setting up security group / firewall\n ec2_group:\n region: \"{{ aws_region }}\"\n name: \"Swarm SG\"\n description: \"Security group for the swarm\"\n vpc_id: \"{{ vpc.id }}\"\n rules:\n - cidr_ip: 0.0.0.0/0\n proto: tcp\n from_port: 22\n to_port: 22\n - cidr_ip: 0.0.0.0/0\n proto: tcp\n from_port: 80\n to_port: 80\n rules_egress:\n - cidr_ip: 0.0.0.0/0\n proto: all\n register: swarm_sg\n\n - name: Provisioning cluster node\n ec2:\n region: \"{{ aws_region }}\"\n image: \"{{ cluster_node_ami }}\"\n key_name: \"{{ ssh_key_name }}\"\n instance_type: \"t2.medium\"\n group_id: \"{{ swarm_sg.group_id }}\"\n vpc_subnet_id: \"{{ swarm_subnet.subnet.id }}\"\n source_dest_check: no\n assign_public_ip: yes\n monitoring: no\n instance_tags:\n Name: cluster-node\n wait: yes\n wait_timeout: 500\n```", "```\n- hosts: localhost\n connection: local\n gather_facts: False\n\n tasks:\n - name: Finding VMs to delete\n ec2_remote_facts:\n region: \"{{ aws_region }}\"\n filters:\n \"tag:Name\": \"cluster-node\"\n register: deletable_instances\n\n - name: Deleting instances\n ec2:\n region: \"{{ aws_region }}\"\n instance_ids: \"{{ item.id }}\"\n state: absent\n wait: yes\n wait_timeout: 600\n with_items: \"{{ deletable_instances.instances }}\"\n when: deletable_instances is defined\n\n # v2.0.0.2 doesn't have ec2_vpc_net_facts so we have to fake it to get VPC info\n - name: Finding route table info\n ec2_vpc_route_table_facts:\n region: \"{{ aws_region }}\"\n filters:\n \"tag:Name\": \"Swarm Routing Table\"\n register: swarm_route_table\n\n - set_fact:\n vpc: \"{{ swarm_route_table.route_tables[0].vpc_id }}\"\n when: swarm_route_table.route_tables | length > 0\n\n - name: Removing security group\n ec2_group:\n region: \"{{ aws_region }}\"\n name: \"Swarm SG\"\n state: absent\n description: \"\"\n vpc_id: \"{{ vpc }}\"\n when: vpc is defined\n\n - name: Deleting gateway\n ec2_vpc_igw:\n region: \"{{ aws_region }}\"\n vpc_id: \"{{ vpc }}\"\n state: absent\n when: vpc is defined\n\n - name: Deleting subnet\n ec2_vpc_subnet:\n region: \"{{ aws_region }}\"\n vpc_id: \"{{ vpc }}\"\n cidr: \"{{ swarm_vpc_cidr }}\"\n state: absent\n when: vpc is defined\n\n - name: Deleting route table\n ec2_vpc_route_table:\n region: \"{{ aws_region }}\"\n vpc_id: \"{{ vpc }}\"\n state: absent\n lookup: tag\n tags:\n Name: \"Swarm Routing Table\"\n when: vpc is defined\n\n - name: Deleting VPC\n ec2_vpc_net:\n region: \"{{ aws_region }}\"\n name: \"Swarm VPC\"\n cidr_block: \"{{ swarm_vpc_cidr }}\"\n state: absent\n```", "```\n$ # Create the key with AWS API and save the private key to ~/.ssh directory\n$ aws ec2 create-key-pair --region us-west-1 \\\n --key-name swarm_key | jq -r '.KeyMaterial' > ~/.ssh/ec2_swarm_key\n\n$ # Check that its not empty by checking the header\n$ head -1 ~/.ssh/ec2_swarm_key \n-----BEGIN RSA PRIVATE KEY-----\n\n$ # Make sure that the permissions are correct on it\n$ chmod 600 ~/.ssh/ec2_swarm_key\n\n$ # Do a sanity check that it has the right size and permissions\n$ ls -la ~/.ssh/ec2_swarm_key\n-rw------- 1 sg sg 1671 Oct 31 16:52 /home/sg/.ssh/ec2_swarm_key\n```", "```\n$ ansible-playbook deploy.yml \n [WARNING]: provided hosts list is empty, only localhost is available\n\nPLAY ***************************************************************************\n\nTASK [Setting up VPC] **********************************************************\nok: [localhost]\n\nTASK [set_fact] ****************************************************************\nok: [localhost]\n\nTASK [Setting up the subnet] ***************************************************\nok: [localhost]\n\nTASK [Setting up the gateway] **************************************************\nok: [localhost]\n\nTASK [Setting up routing table] ************************************************\nok: [localhost]\n\nTASK [Setting up security group] ***********************************************\nok: [localhost]\n\nTASK [Provisioning cluster node] ***********************************************\nchanged: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost : ok=7 changed=1 unreachable=0 failed=0 \n\n$ # Great! It looks like it deployed the machine! \n$ # Let's see what we have. First we need to figure out what the external IP is\n$ aws ec2 describe-instances --region us-west-1 \\\n --filters Name=instance-state-name,Values=running \\\n --query 'Reservations[*].Instances[*].PublicIpAddress'\n[\n [\n \"52.53.240.17\"\n ]\n]\n\n$ # Now let's try connecting to it\nssh -i ~/.ssh/ec2_swarm_key ubuntu@52.53.240.17 \n<snip>\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '52.53.240.17' (ECDSA) to the list of known hosts.\n<snip>\n\nubuntu@ip-172-31-182-20:~$ # Yay! Do we have Docker?\nubuntu@ip-172-31-182-20:~$ sudo docker ps\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n\nubuntu@ip-172-31-182-20:~$ # Create our single-server swarm\nubuntu@ip-172-31-182-20:~$ sudo docker swarm init\nSwarm initialized: current node (n2yc2tedm607rvnjs72fjgl1l) is now a manager.\n<snip>\n\nubuntu@ip-172-31-182-20:~$ # Here we can now do anything else that's needed\nubuntu@ip-172-31-182-20:~$ # Though you would normally automate everything\n```", "```\nubuntu@ip-172-31-182-20:~$ # Get out of our remote machine\nubuntu@ip-172-31-182-20:~$ exit\nlogout\nConnection to 52.53.240.17 closed.\n\n$ # Let's run the cleanup script\nansible-playbook destroy.yml \n [WARNING]: provided hosts list is empty, only localhost is available\n\nPLAY ***************************************************************************\n\nTASK [Finding VMs to delete] ***************************************************\nok: [localhost]\n\nTASK [Deleting instances] ******************************************************\nchanged: [localhost] => <snip>\n\nTASK [Finding route table info] ************************************************\nok: [localhost]\n\nTASK [set_fact] ****************************************************************\nok: [localhost]\n\nTASK [Removing security group] *************************************************\nchanged: [localhost]\n\nTASK [Deleting gateway] ********************************************************\nchanged: [localhost]\n\nTASK [Deleting subnet] *********************************************************\nchanged: [localhost]\n\nTASK [Deleting route table] ****************************************************\nchanged: [localhost]\n\nTASK [Deleting VPC] ************************************************************\nchanged: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost : ok=9 changed=6 unreachable=0 failed=0 \n\n```", "```\nFROM jenkins\n\nUSER root\nRUN apt-get update && \\\n apt-get install -y ansible \\\n awscli \\\n python-boto\n\nUSER jenkins\n```", "```\n$ # Let's build our image\n$ docker build -t jenkins_with_ansible \nSending build context to Docker daemon 2.048kB\nStep 1/4 : FROM jenkins\n<snip>\nSuccessfully tagged jenkins_with_ansible:latest\n\n$ # Run Jenkins with a local volume for the configuration\n$ mkdir jenkins_files\n$ docker run -p 8080:8080 \\\n -v $(pwd)/jenkins_files:/var/jenkins_home \\\n jenkins_with_ansible\n\nRunning from: /usr/share/jenkins/jenkins.war\n<snip>\nJenkins initial setup is required. An admin user has been created and a password generated.\nPlease use the following password to proceed to installation:\n\n3af5d45c2bf04fffb88e97ec3e92127a\n\nThis may also be found at: /var/jenkins_home/secrets/initialAdminPassword\n<snip>\nINFO: Jenkins is fully up and running\n```", "```\n# Export needed AWS credentials\nexport AWS_DEFAULT_REGION=\"us-west-1\"\nexport AWS_ACCESS_KEY_ID=\"AKIABCDEFABCDEF\"\nexport AWS_SECRET_ACCESS_KEY=\"123456789ABCDEF123456789ABCDEF\"\n\n# Change to relevant directory\ncd chapter_8/aws_deployment\n\n# Redeploy the service by cleaning up the old deployment\n# and deploying a new one\nansible-playbook destroy.yml\nansible-playbook deploy.yml\n```"]