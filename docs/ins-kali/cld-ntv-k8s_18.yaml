- en: '*Chapter 14*: Service Meshes and Serverless'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：服务网格和无服务器'
- en: This chapter discusses advanced Kubernetes patterns. First, it details the in-vogue
    service mesh pattern, where observability and service-to-service discovery are
    handled by a sidecar proxy, as well as a guide to setting up Istio, a popular
    service mesh. Lastly, it describes the serverless pattern and how it can be applied
    in Kubernetes. The major case study in this chapter will include setting up Istio
    for an example application and service discovery, along with Istio ingress gateways.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了高级Kubernetes模式。首先，它详细介绍了时髦的服务网格模式，其中通过sidecar代理处理可观察性和服务到服务的发现，以及设置流行的服务网格Istio的指南。最后，它描述了无服务器模式以及如何在Kubernetes中应用它。本章的主要案例研究将包括为示例应用程序和服务发现设置Istio，以及Istio入口网关。
- en: Let's start with a discussion of the sidecar proxy, which builds the foundation
    of service-to-service connectivity for service meshes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论sidecar代理开始，它为服务网格的服务到服务连接性奠定了基础。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Using sidecar proxies
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用sidecar代理
- en: Adding a service mesh to Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向Kubernetes添加服务网格
- en: Implementing serverless on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上实现无服务器
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool, along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，您需要一台支持`kubectl`命令行工具的计算机，以及一个可用的Kubernetes集群。请参阅[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*，了解快速启动和运行Kubernetes的几种方法，以及如何安装`kubectl`工具的说明。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在书的GitHub存储库中找到，网址为[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14)。
- en: Using sidecar proxies
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用sidecar代理
- en: As we mentioned earlier in this book, a sidecar is a pattern where a Pod contains
    another container in addition to the actual application container to be run. This
    additional "extra" container is the sidecar. Sidecars can be used for a number
    of different reasons. Some of the most popular uses for sidecars are monitoring,
    logging, and proxying.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中早些时候提到的，sidecar是一种模式，其中一个Pod包含另一个容器，除了要运行的实际应用程序容器。这个额外的“额外”容器就是sidecar。Sidecar可以用于许多不同的原因。一些最常用的sidecar用途是监控、日志记录和代理。
- en: For logging, a sidecar container can fetch application logs from the application
    container (since they can share volumes and communicate on localhost), before
    sending the logs to a centralized logging stack, or parsing them for the purpose
    of alerting. It's a similar story for monitoring, where the sidecar Pod can track
    and send metrics about the application Pod.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于日志记录，一个sidecar容器可以从应用容器中获取应用程序日志（因为它们可以共享卷并在本地通信），然后将日志发送到集中式日志堆栈，或者解析它们以进行警报。监控也是类似的情况，sidecar
    Pod可以跟踪并发送有关应用程序Pod的指标。
- en: With a sidecar proxy, when requests come into the Pod, they first go to the
    proxy container, which then routes requests (after logging or performing other
    filtering) to the application container. Similarly, when requests leave the application
    container, they first go to the proxy, which can provide routing out of the Pod.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用侧车代理时，当请求进入Pod时，它们首先进入代理容器，然后路由请求（在记录或执行其他过滤之后）到应用程序容器。同样，当请求离开应用程序容器时，它们首先进入代理，代理可以提供Pod的路由。
- en: Normally, proxy sidecars such as NGINX only provide proxying for requests coming
    into a Pod. However, in the service mesh pattern, both requests coming into and
    leaving the Pod go through the proxy, which provides the foundation for the service
    mesh pattern itself.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，诸如NGINX之类的代理侧车只为进入Pod的请求提供代理。然而，在服务网格模式中，进入和离开Pod的请求都通过代理，这为服务网格模式本身提供了基础。
- en: 'Refer to the following diagram to see how a sidecar proxy can interact with
    an application container:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下图表，了解侧车代理如何与应用程序容器交互：
- en: '![Figure 14.1 – Proxy sidecar](image/B14790_14_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 - 代理侧车](image/B14790_14_001.jpg)'
- en: Figure 14.1 – Proxy sidecar
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 - 代理侧车
- en: As you can see, the sidecar proxy is in charge of routing requests to and from
    the application container in the Pod, allowing for functionality such as service
    routing, logging, and filtering.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，侧车代理负责将请求路由到Pod中的应用程序容器，并允许功能，如服务路由、记录和过滤。
- en: The sidecar proxy pattern is an alternative to a DaemonSet-based proxy, where
    a proxy Pod on each node handles proxying to other Pods on that node. The Kubernetes
    proxy itself is similar to a DaemonSet pattern. Using a sidecar proxy can provide
    more flexibility than using a DaemonSet proxy, at the expense of performance efficiency,
    since many extra containers need to be run.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 侧车代理模式是一种替代基于DaemonSet的代理，其中每个节点上的代理Pod处理对该节点上其他Pod的代理。Kubernetes代理本身类似于DaemonSet模式。使用侧车代理可以提供比使用DaemonSet代理更灵活的灵活性，但性能效率会有所降低，因为需要运行许多额外的容器。
- en: 'Some popular proxy options for Kubernetes include the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些用于Kubernetes的流行代理选项包括以下内容：
- en: '*NGINX*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NGINX*'
- en: '*HAProxy*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HAProxy*'
- en: '*Envoy*'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Envoy*'
- en: While NGINX and HAProxy are more traditional proxies, Envoy was built specifically
    for a distributed, cloud-native environment. For this reason, Envoy forms the
    core of popular service meshes and API gateways built for Kubernetes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然NGINX和HAProxy是更传统的代理，但Envoy是专门为分布式、云原生环境构建的。因此，Envoy构成了流行的服务网格和为Kubernetes构建的API网关的核心。
- en: Before we get to Envoy, let's discuss the installation of other proxies as sidecars.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论Envoy之前，让我们讨论安装其他代理作为侧车的方法。
- en: Using NGINX as a sidecar reverse proxy
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NGINX作为侧车反向代理
- en: Before we specify how NGINX can be used as a sidecar proxy, it is relevant to
    note that in an upcoming Kubernetes release, the sidecar will be a Kubernetes
    resource type that will allow easy injection of sidecar containers to large numbers
    of Pods. Currently however, sidecar containers must be specified at the Pod or
    controller (ReplicaSet, Deployment, and others) level.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们指定NGINX如何作为侧车代理之前，值得注意的是，在即将发布的Kubernetes版本中，侧车将成为一个Kubernetes资源类型，它将允许轻松地向大量Pod注入侧车容器。然而，目前侧车容器必须在Pod或控制器（ReplicaSet、Deployment等）级别指定。
- en: Let's take a look at how we can configure NGINX as a sidecar, with the following
    Deployment YAML, which we will not create just yet. This process is a bit more
    manual than using the NGINX Ingress Controller.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用以下部署YAML配置NGINX作为侧车，我们暂时不会创建。这个过程比使用NGINX Ingress Controller要手动一些。
- en: 'We''ve split the YAML into two parts for space reasons and trimmed some of
    the fat, but you can see it in its entirety in the code repository. Let''s start
    with the containers spec for our deployment:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 出于空间原因，我们将YAML分成两部分，并删除了一些冗余内容，但您可以在代码存储库中完整地看到它。让我们从部署的容器规范开始：
- en: 'Nginx-sidecar.yaml:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx-sidecar.yaml：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, we specify two containers, both our main app container, `myapp`,
    and the `nginx` sidecar, where we inject some configuration via volume mounts,
    as well as some TLS certificates.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们指定了两个容器，即我们的主应用程序容器`myapp`和`nginx` sidecar，我们通过卷挂载注入了一些配置，以及一些TLS证书。
- en: 'Next, let''s look at the `volumes` spec in the same file, where we inject some
    certs (from a secret) and `config` (from a `ConfigMap`):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看同一文件中的`volumes`规范，我们在其中注入了一些证书（来自一个密钥）和`config`（来自`ConfigMap`）：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, we need both a cert and a secret key.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们需要一个证书和一个密钥。
- en: 'Next, we need to create the NGINX configuration using `ConfigMap`. The NGINX
    configuration looks like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用`ConfigMap`创建NGINX配置。NGINX配置如下：
- en: 'nginx.conf:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: nginx.conf：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, we have some basic NGINX configuration. Importantly, we have
    the `proxy_pass` field, which proxies requests to a port on `127.0.0.1`, or localhost.
    Since containers in a Pod can share localhost ports, this acts as our sidecar
    proxy. We won't review all the other lines for the purposes of this book, but
    check the NGINX docs for more information about what each line means ([https://nginx.org/en/docs/](https://nginx.org/en/docs/)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们有一些基本的NGINX配置。重要的是，我们有`proxy_pass`字段，它将请求代理到`127.0.0.1`上的端口，或者本地主机。由于Pod中的容器可以共享本地主机端口，这充当了我们的sidecar代理。出于本书的目的，我们不会审查所有其他行，但是请查看NGINX文档，了解每行的更多信息（[https://nginx.org/en/docs/](https://nginx.org/en/docs/)）。
- en: 'Now, let''s create the `ConfigMap` from this file. Use the following command
    to imperatively create the `ConfigMap`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从这个文件创建`ConfigMap`。使用以下命令来命令式地创建`ConfigMap`：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will result in the following output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, let's make our certificates for TLS in NGINX, and embed them in a Kubernetes
    secret. You will need the CFSSL (CloudFlare's PKI/TLS open source toolkit) library
    installed to follow these instructions, but you can use any other method to create
    your cert.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为NGINX创建TLS证书，并将它们嵌入到Kubernetes密钥中。您需要安装CFSSL（CloudFlare的PKI/TLS开源工具包）库才能按照这些说明进行操作，但您也可以使用任何其他方法来创建您的证书。
- en: 'First, we need to create the **Certificate Authority** (**CA**). Start with
    the JSON configuration for the CA:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建**证书颁发机构**（**CA**）。从CA的JSON配置开始：
- en: 'nginxca.json:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: nginxca.json：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, use CFSSL to create the CA certificate:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用CFSSL创建CA证书：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will require the CA config:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要CA配置：
- en: 'Nginxca-config.json:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Nginxca-config.json：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And we''ll also need a cert request config:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个证书请求配置：
- en: 'Nginxcarequest.json:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Nginxcarequest.json：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can actually make our certs! Use the following command:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们实际上可以创建我们的证书了！使用以下命令：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As the final step for our cert secrets, create the Kubernetes secret from the
    certificate files'' output by means of the last `cfssl` command:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为证书密钥的最后一步，通过最后一个`cfssl`命令从证书文件的输出创建Kubernetes密钥：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can finally create our deployment:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以创建我们的部署了：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This produces the following output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In order to check the NGINX proxy functionality, let''s create a service to
    direct to our deployment:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查NGINX代理功能，让我们创建一个服务来指向我们的部署：
- en: 'Nginx-sidecar-service.yaml:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx-sidecar-service.yaml：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, accessing any node of the cluster using `https` should result in a working
    HTTPS connection! However, since our cert is self-signed, browsers will display
    an *insecure* message.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`https`访问集群中的任何节点应该会导致一个正常工作的HTTPS连接！但是，由于我们的证书是自签名的，浏览器将显示一个*不安全*的消息。
- en: Now that you've seen how NGINX can be used as a sidecar proxy with Kubernetes,
    let's move on to a more modern, cloud-native proxy sidecar – Envoy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了NGINX如何与Kubernetes一起作为边车代理使用，让我们转向更现代的、云原生的代理边车 - Envoy。
- en: Using Envoy as a sidecar proxy
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Envoy作为边车代理
- en: Envoy is a modern proxy built for cloud-native environments. In the Istio service
    mesh, which we'll review later in this chapter, Envoy acts as both a reverse and
    forward proxy. Before we get to Istio, however, let's try our hand at deploying
    Envoy as a proxy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy是为云原生环境构建的现代代理。在我们稍后将审查的Istio服务网格中，Envoy充当反向和正向代理。然而，在我们进入Istio之前，让我们尝试部署Envoy作为代理。
- en: We will tell Envoy where to route various requests using routes, listeners,
    clusters, and endpoints. This functionality is what forms the core of Istio, which
    we will review later in this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将告诉Envoy在哪里路由各种请求，使用路由、监听器、集群和端点。这个功能是Istio的核心，我们将在本章后面进行审查。
- en: Let's go through each of the Envoy configuration pieces to see how it all works.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个查看Envoy配置的每个部分，看看它是如何工作的。
- en: Envoy listeners
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy监听器
- en: Envoy allows the configuration of one or more listeners. With each listener,
    we specify a port for Envoy to listen on, as well as any filters we want to apply
    to the listener.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy允许配置一个或多个监听器。对于每个监听器，我们指定Envoy要监听的端口，以及我们想要应用到监听器的任何过滤器。
- en: Filters can provide complex functionality, including caching, authorization,
    **Cross-Origin Resource Sharing** (**CORS**) configuration, and more. Envoy supports
    the chaining of multiple filters together.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器可以提供复杂的功能，包括缓存、授权、**跨源资源共享**（**CORS**）配置等。Envoy支持将多个过滤器链接在一起。
- en: Envoy routes
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy路由
- en: Certain filters have route configuration, which specifies domains from which
    requests should be accepted, route matching, and forwarding rules.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 某些过滤器具有路由配置，指定应接受请求的域、路由匹配和转发规则。
- en: Envoy clusters
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy集群
- en: A Cluster in Envoy represents a logical service where requests can be routed
    to based-on routes in listeners. A cluster likely contains more than one possible
    IP address in a cloud-native setting, so it supports load balancing configurations
    such as *round robin*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy中的集群表示可以根据监听器中的路由将请求路由到的逻辑服务。在云原生环境中，集群可能包含多个可能的IP地址，因此它支持负载均衡配置，如*轮询*。
- en: Envoy endpoints
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Envoy端点
- en: Finally, endpoints are specified within a cluster as one logical instance of
    a service. Envoy supports fetching a list of endpoints from an API (this is essentially
    what happens in the Istio service mesh) and load balancing between them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在集群中指定端点作为服务的一个逻辑实例。Envoy支持从API获取端点列表（这基本上是 Istio 服务网格中发生的事情），并在它们之间进行负载均衡。
- en: In a production Envoy deployment on Kubernetes, it is likely that some form
    of dynamic, API-driven Envoy configuration is going to be used. This feature of
    Envoy is called xDS, and is used by Istio. Additionally, there are other open
    source products and solutions that use Envoy along with xDS, including the Ambassador
    API gateway.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上的生产Envoy部署中，很可能会使用某种形式的动态、API驱动的Envoy配置。Envoy的这个特性称为xDS，并被Istio使用。此外，还有其他开源产品和解决方案使用Envoy与xDS，包括Ambassador
    API网关。
- en: For the purposes of this book, we will look at some static (non-dynamic) Envoy
    configuration; that way, we can pick apart each piece of the config, and you'll
    have a good idea of how everything works when we review Istio.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将查看一些静态（非动态）的Envoy配置；这样，我们可以分解配置的每个部分，当我们审查Istio时，您将对一切是如何工作有一个很好的理解。
- en: 'Let''s now dive into an Envoy configuration for a setup where a single Pod
    needs to be able to route requests to two services, *Service 1* and *Service 2*.
    The setup looks like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入研究一个Envoy配置，用于设置一个单个Pod需要能够将请求路由到两个服务，*Service 1*和*Service 2*。设置如下：
- en: '![Figure 14.2 – Outbound envoy proxy](image/B14790_14_002.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2-出站envoy代理](image/B14790_14_002.jpg)'
- en: Figure 14.2 – Outbound envoy proxy
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2-出站envoy代理
- en: As you can see, the Envoy sidecar in our application Pod will have configurations
    to route to two upstream services, *Service 1* and *Service 2*. Both services
    have two possible endpoints.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们应用Pod中的Envoy sidecar将配置为路由到两个上游服务，*Service 1*和*Service 2*。两个服务都有两个可能的端点。
- en: In a dynamic setting with Envoy xDS, the Pod IPs for the endpoints would be
    loaded from the API, but for the purposes of our review, we will show the static
    Pod IPs in the endpoints. We will completely ignore Kubernetes Services and instead
    directly access Pod IPs in a round robin configuration. In a service mesh scenario,
    Envoy would also be deployed on all of the destination Pods, but we'll keep it
    simple for now.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Envoy xDS的动态设置中，端点的Pod IPs将从API中加载，但是为了我们的审查目的，我们将在端点中显示静态的Pod IPs。我们将完全忽略Kubernetes服务，而是直接访问Pod
    IPs以进行轮询配置。在服务网格场景中，Envoy也将部署在所有目标Pod上，但现在我们将保持简单。
- en: Now, let's look at how this network map is configured in an envoy configuration
    YAML (which you can find in its entirety in the code repository). This is, of
    course, very different from a Kubernetes resource YAML – we will get to that part
    later. The entire configuration has a lot of YAML involved, so let's take it piece
    by piece.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在Envoy配置YAML中配置这个网络映射（您可以在代码存储库中找到完整的配置）。这当然与Kubernetes资源YAML非常不同-我们将在稍后讨论这一部分。整个配置涉及大量的YAML，所以让我们一点一点地来。
- en: Understanding Envoy configuration files
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解Envoy配置文件
- en: 'First off, let''s look at the first few lines of our config—some basic information
    about our Envoy setup:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们配置的前几行-关于我们的Envoy设置的一些基本信息。
- en: 'Envoy-configuration.yaml:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'Envoy-configuration.yaml:'
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see, we specify a port and address for Envoy''s `admin`. As with
    the following configuration, we are running Envoy as a sidecar so the address
    will always be local – `0.0.0.0`. Next, we start our list of listeners with an
    HTTPS listener:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们为Envoy的`admin`指定了一个端口和地址。与以下配置一样，我们将Envoy作为一个sidecar运行，因此地址将始终是本地的- `0.0.0.0`。接下来，我们用一个HTTPS监听器开始我们的监听器列表：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, for each Envoy listener, we have a local address and port for
    the listener (this listener is an HTTPS listener). Then, we have a list of filters
    – though in this case, we only have one. Each envoy filter type has slightly different
    configuration, and we won''t review it line by line (check the Envoy docs for
    more information at [https://www.envoyproxy.io/docs](https://www.envoyproxy.io/docs)),
    but this particular filter matches two routes, `/service/1` and `/service/2`,
    and routes them to two envoy clusters. Still under our first HTTPS listener section
    of the YAML, we have the TLS configuration, including certs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，对于每个Envoy监听器，我们有一个本地地址和端口（此监听器是一个HTTPS监听器）。然后，我们有一个过滤器列表-尽管在这种情况下，我们只有一个。每个envoy过滤器类型的配置略有不同，我们不会逐行审查它（请查看Envoy文档以获取更多信息[https://www.envoyproxy.io/docs](https://www.envoyproxy.io/docs)），但这个特定的过滤器匹配两个路由，`/service/1`和`/service/2`，并将它们路由到两个envoy集群。在我们的YAML的第一个HTTPS监听器部分下，我们有TLS配置，包括证书：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see, this configuration passes in a `private_key` and a `certificate_chain`.
    Next, we have our second and final listener, an HTTP listener:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，此配置传递了`private_key`和`certificate_chain`。接下来，我们有第二个也是最后一个监听器，一个HTTP监听器：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see, this configuration is quite similar to that of our HTTPS listener,
    except that it listens on a different port, and does not include certificate information.
    Next, we move into our cluster configuration. In our case, we have two clusters,
    one for `service1` and one for `service2`. First off, `service1`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个配置与我们的HTTPS监听器的配置非常相似，只是它监听不同的端口，并且不包括证书信息。接下来，我们进入我们的集群配置。在我们的情况下，我们有两个集群，一个用于`service1`，一个用于`service2`。首先是`service1`：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And next, `Service 2`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`Service 2`：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For each of these clusters, we specify where requests should be routed, and
    to which port. For instance, for our first cluster, requests are routed to `http://service1:5000`.
    We also specify a load balancing policy (in this case, round robin) and a timeout
    for the connections. Now that we have our Envoy configuration, we can go ahead
    and create our Kubernetes Pod and inject our sidecar along with the envoy configuration.
    We''ll also split this file into two since it is a bit too big to understand as
    is:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些集群中的每一个，我们指定请求应该路由到哪里，以及到哪个端口。例如，对于我们的第一个集群，请求被路由到`http://service1:5000`。我们还指定了负载均衡策略（在这种情况下是轮询）和连接的超时时间。现在我们有了我们的Envoy配置，我们可以继续创建我们的Kubernetes
    Pod，并注入我们的sidecar以及envoy配置。我们还将把这个文件分成两部分，因为它有点太大了，以至于难以理解：
- en: 'Envoy-sidecar-deployment.yaml:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Envoy-sidecar-deployment.yaml：
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As you can see, this is a typical deployment YAML. In this case, we actually
    have two containers. First off is the Envoy proxy container (or sidecar). It listens
    on two ports. Next up, moving further down the YAML, we have a volume mount for
    that first container (to hold the Envoy config) as well as a start command and
    arguments:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这是一个典型的部署YAML。在这种情况下，我们实际上有两个容器。首先是Envoy代理容器（或边车）。它监听两个端口。接下来，继续向下移动YAML，我们为第一个容器进行了卷挂载（用于保存Envoy配置），以及一个启动命令和参数：
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we have our second container in the Pod, which is an application container:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有我们Pod中的第二个容器，这是一个应用容器：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As you can see, this application responds on port `5000`. Lastly, we also have
    our Pod-level volume definition to match the Envoy config volume mounted in the
    Envoy container. Before we create our deployment, we need to create a `ConfigMap`
    with our Envoy configuration. We can do this using the following command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个应用在端口`5000`上响应。最后，我们还有我们的Pod级别卷定义，以匹配Envoy容器中挂载的Envoy配置卷。在创建部署之前，我们需要创建一个带有我们的Envoy配置的`ConfigMap`。我们可以使用以下命令来做到这一点：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will result in the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we can create our deployment with the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下命令创建我们的部署：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will result in the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we need our downstream services, `service1` and `service2`. For this
    purpose, we will continue to use the `http-responder` open source container image,
    which will respond on port `5000`. The deployment and service specs can be found
    in the code repository, and we can create them using the following commands:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要我们的下游服务，`service1`和`service2`。为此，我们将继续使用`http-responder`开源容器映像，在端口`5000`上进行响应。部署和服务规范可以在代码存储库中找到，并且我们可以使用以下命令创建它们：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we can test our Envoy configuration! From our `my-service` container,
    we can make a request to localhost on port `8080`, with the `/service1` path.
    This should direct to one of our `service1` Pod IPs. To make this request we use
    the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以测试我们的Envoy配置！从我们的`my-service`容器中，我们可以向端口`8080`的本地主机发出请求，路径为`/service1`。这应该会指向我们的`service1`
    Pod IP之一。为了发出这个请求，我们使用以下命令：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''ve set up out services to echo their names on a `curl` request. Look at
    the following output of our `curl` command:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设置了我们的服务来在`curl`请求上回显它们的名称。看一下我们`curl`命令的以下输出：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now that we've looked at how Envoy works with a static configuration, let's
    move on to a dynamic service mesh based on Envoy – Istio.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了Envoy如何与静态配置一起工作，让我们转向基于Envoy的动态服务网格 - Istio。
- en: Adding a service mesh to Kubernetes
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中添加服务网格
- en: A *service mesh* pattern is a logical extension of the sidecar proxy. By attaching
    sidecar proxies to every Pod, a service mesh can control functionality for service-to-service
    requests, such as advanced routing rules, retries, and timeouts. In addition,
    by having every request pass through a proxy, service meshes can implement mutual
    TLS encryption between services for added security and can give administrators
    incredible observability into requests in their cluster.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务网格*模式是侧车代理的逻辑扩展。通过将侧车代理附加到每个Pod，服务网格可以控制服务之间的功能，如高级路由规则、重试和超时。此外，通过让每个请求通过代理，服务网格可以实现服务之间的相互TLS加密，以增加安全性，并且可以让管理员对集群中的请求有非常好的可观察性。'
- en: 'There are several service mesh projects that support Kubernetes. The most popular
    are as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个支持Kubernetes的服务网格项目。最流行的如下：
- en: '*Istio*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio
- en: '*Linkerd*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd
- en: '*Kuma*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kuma*'
- en: '*Consul*'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul
- en: Each of these service meshes has different takes on the service mesh pattern.
    *Istio* is likely the single most popular and comprehensive solution, but is also
    quite complex. *Linkerd* is also a mature project, but is easier to configure
    (though it uses its own proxy instead of Envoy). *Consul* is an option that supports
    Envoy in addition to other providers, and not just on Kubernetes. Finally, *Kuma*
    is an Envoy-based option that is also growing in popularity.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些服务网格中的每一个对服务网格模式有不同的看法。*Istio*可能是最流行和最全面的解决方案，但也非常复杂。*Linkerd*也是一个成熟的项目，但更容易配置（尽管它使用自己的代理而不是Envoy）。*Consul*是一个支持Envoy以及其他提供者的选项，不仅仅在Kubernetes上。最后，*Kuma*是一个基于Envoy的选项，也在不断增长。
- en: Exploring all the options is beyond the scope of this book, so we will stick
    with Istio, as it is often considered the default solution. That said, all of
    these meshes have strengths and weaknesses, and it is worth looking at each one
    when planning to adopt the service mesh.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 探索所有选项超出了本书的范围，因此我们将坚持使用Istio，因为它通常被认为是默认解决方案。也就是说，所有这些网格都有优势和劣势，在计划采用服务网格时值得看看每一个。
- en: Setting up Istio on Kubernetes
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Kubernetes上设置Istio
- en: Although Istio can be installed with Helm, the Helm installation option is no
    longer the officially supported installation method.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Istio可以使用Helm安装，但Helm安装选项不再是官方支持的安装方法。
- en: 'Instead, we use the `Istioctl` CLI tool to install Istio with configuration
    onto our clusters. This configuration can be completely customized, but for the
    purposes of this book, we will just use the "demo" configuration:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们使用`Istioctl` CLI工具将Istio与配置安装到我们的集群上。这个配置可以完全定制，但是为了本书的目的，我们将只使用"demo"配置：
- en: 'The first step to installing Istio on a cluster is to install the Istio CLI
    tool. We can do this with the following command, which installs the newest version
    of the CLI tool:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群上安装Istio的第一步是安装Istio CLI工具。我们可以使用以下命令来完成这个操作，这将安装最新版本的CLI工具：
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we''ll want to add the CLI tool to our path for ease of use:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将希望将CLI工具添加到我们的路径中，以便使用：
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, let's install Istio! Istio configurations are called *profiles* and, as
    mentioned previously, they can be completely customized using a YAML file.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们安装Istio！Istio的配置被称为*配置文件*，如前所述，它们可以使用YAML文件进行完全定制。
- en: 'For this demonstration, we''ll use the inbuilt `demo` profile with Istio, which
    provides some basic setup. Install profile using the following command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示，我们将使用内置的`demo`配置文件与Istio一起使用，这提供了一些基本设置。使用以下命令安装配置文件：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will result in the following output:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![Figure 14.3 – Istioctl profile installation output](image/B14790_14_003.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图14.3 - Istioctl配置文件安装输出](image/B14790_14_003.jpg)'
- en: Figure 14.3 – Istioctl profile installation output
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 - Istioctl配置文件安装输出
- en: Since the sidecar resource has not been released yet as of Kubernetes 1.19,
    Istio will itself inject Envoy proxies into any namespace that is labeled with
    `istio-injection=enabled`.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于截至Kubernetes 1.19，sidecar资源尚未发布，因此Istio本身将在任何打上`istio-injection=enabled`标签的命名空间中注入Envoy代理。
- en: 'To label any namespace with this, run the following command:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要为任何命名空间打上标签，请运行以下命令：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: To test easily, label the `default` namespace with the preceding `label` command.
    Once the Istio components come up, any Pods in that namespace will automatically
    be injected with the Envoy sidecar, just like we created manually in the previous
    section.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便测试，使用前面的`label`命令为`default`命名空间打上标签。一旦Istio组件启动，该命名空间中的任何Pod将自动注入Envoy sidecar，就像我们在上一节中手动创建的那样。
- en: 'In order to remove Istio from the cluster, run the following command:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要从集群中删除Istio，请运行以下命令：
- en: '[PRE34]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This should result in a confirmation message telling you that Istio has been
    removed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会出现一个确认消息，告诉您Istio已被移除。
- en: 'Now, let''s deploy a little something to test our new mesh with! We will deploy
    three different application services, each with a deployment and a service resource:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们部署一些东西来测试我们的新网格！我们将部署三种不同的应用服务，每个都有一个部署和一个服务资源：
- en: a. Service Frontend
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: a. 服务前端
- en: b. Service Backend A
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: b. 服务后端A
- en: c. Service Backend B
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: c. 服务后端B
- en: 'Here''s the Deployment for *Service Frontend*:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*服务前端*的部署：
- en: 'Istio-service-deployment.yaml:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Istio-service-deployment.yaml：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And here''s the Service for *Service Frontend*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*服务前端*的服务：
- en: 'Istio-service-service.yaml:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Istio-service-service.yaml：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The YAML for Service Backends A and B will be the same as *Service Frontend*,
    apart from swapping the names, image names, and selector labels.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 服务后端A和B的YAML与*服务前端*相同，除了交换名称、镜像名称和选择器标签。
- en: Now that we have a couple of services to route to (and between), let's start
    setting up some Istio resources!
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一些要路由到（和之间）的服务，让我们开始设置一些Istio资源！
- en: 'First thing''s first, we need a `Gateway` resource. In this case, we are not
    using the NGINX Ingress Controller, but that''s fine because Istio provides a
    `Gateway` resource that can be used for ingress and egress. Here''s what an Istio
    `Gateway` definition looks like:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个`Gateway`资源。在这种情况下，我们不使用NGINX Ingress Controller，但这没关系，因为Istio提供了一个可以用于入口和出口的`Gateway`资源。以下是Istio`Gateway`定义的样子：
- en: 'Istio-gateway.yaml:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Istio-gateway.yaml：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: These `Gateway` definitions look pretty similar to ingress records. We have
    `name`, and `selector`, which Istio uses to decide which Istio Ingress Controller
    to use. Next, we have one or more servers, which are essentially ingress points
    on our gateway. In this case, we do not restrict the host, and we accept requests
    on port `80`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`Gateway`定义看起来与入口记录非常相似。我们有`name`和`selector`，Istio用它们来决定使用哪个Istio Ingress
    Controller。接下来，我们有一个或多个服务器，它们实质上是我们网关上的入口点。在这种情况下，我们不限制主机，并且接受端口`80`上的请求。
- en: 'Now that we have a gateway for getting requests into our cluster, we can start
    setting up some routes. We do this in Istio using `VirtualService`. `VirtualService`
    in Istio is a set of routes that should be followed when requests to a particular
    hostname are made. In addition, we can use a wildcard host to make global rules
    for requests from anywhere in the mesh. Let''s take a look at an example `VirtualService`
    configuration:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一个用于将请求发送到我们的集群的网关，我们可以开始设置一些路由。我们在Istio中使用`VirtualService`来做到这一点。Istio中的`VirtualService`是一组应该遵循的路由，当对特定主机名的请求时。此外，我们可以使用通配符主机来为网格中的任何地方的请求制定全局规则。让我们看一个示例`VirtualService`配置：
- en: 'Istio-virtual-service-1.yaml:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Istio-virtual-service-1.yaml：
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this `VirtualService`, we route requests to any host to our entry point at
    *Service Frontend* if it matches one of our `uri` prefixes. In this case, we are
    matching on the prefix, but you can use exact matching as well by swapping out
    `prefix` with `exact` in the URI matcher.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`VirtualService`中，如果匹配我们的`uri`前缀，我们将请求路由到任何主机到我们的入口点*Service Frontend*。在这种情况下，我们匹配前缀，但你也可以在URI匹配器中使用精确匹配，将`prefix`替换为`exact`。
- en: So, now we have a setup fairly similar to what we would expect with an NGINX
    Ingress, with entry into the cluster dictated by a route match.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，现在我们有一个设置，与我们预期的NGINX Ingress非常相似，入口进入集群由路由匹配决定。
- en: 'However, what''s that `v1` in our route? This actually represents a version
    of our *Frontend Service*. Let''s go ahead and specify this version using a new
    resource type – the Istio `DestinationRule`. Here''s what a `DestinationRule`
    config looks like:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的路由中，`v1`是什么？这实际上代表了我们*Frontend Service*的一个版本。让我们继续使用一个新的资源类型 - Istio
    `DestinationRule`来指定这个版本。这是一个`DestinationRule`配置的样子：
- en: 'Istio-destination-rule-1.yaml:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'Istio-destination-rule-1.yaml:'
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As you can see, we specify two different versions of our frontend service in
    Istio, each looking at a label selector. From our previous Deployment and Service,
    you see that our current frontend service version is `v2`, but we could be running
    both in parallel! By specifying our `v2` version in the ingress virtual service,
    we tell Istio to route all requests to `v2` of the service. In addition, we have
    our `v1` version also configured, which is referenced in the previous `VirtualService`.
    This hard rule is only one possible way to route requests to different subsets
    in Istio.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们在Istio中指定了我们前端服务的两个不同版本，每个版本都查看一个标签选择器。从我们之前的部署和服务中，你可以看到我们当前的前端服务版本是`v2`，但我们也可以并行运行两者！通过在入口虚拟服务中指定我们的`v2`版本，我们告诉Istio将所有请求路由到服务的`v2`。此外，我们还配置了我们的`v1`版本，它在之前的`VirtualService`中被引用。这个硬规则只是在Istio中将请求路由到不同子集的一种可能的方式。
- en: Now, we've managed to route traffic into our cluster via a gateway, and to a
    virtual service subset based on a destination rule. At this point, we are effectively
    "inside" our service mesh!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经成功通过网关将流量路由到我们的集群，并基于目标规则路由到虚拟服务子集。在这一点上，我们实际上已经“在”我们的服务网格中！
- en: 'Now, from our *Service Frontend*, we want to be able to route to *Service Backend
    A* and *Service Backend B*. How do we do this? More virtual services is the answer!
    Let''s take a look at a virtual service for *Backend Service A*:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，从我们的*Service Frontend*，我们希望能够路由到*Service Backend A*和*Service Backend B*。我们该怎么做？更多的虚拟服务就是答案！让我们来看看*Backend
    Service A*的虚拟服务：
- en: 'Istio-virtual-service-2.yaml:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'Istio-virtual-service-2.yaml:'
- en: '[PRE40]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, this `VirtualService` routes to a `v1` subset for our service,
    `service-backend-a`. We'll also need another `VirtualService` for `service-backend-b`,
    which we won't include in full (but looks nearly identical). To see the full YAML,
    check the code repository for `istio-virtual-service-3.yaml`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这个`VirtualService`路由到我们服务的`v1`子集，`service-backend-a`。我们还需要另一个`VirtualService`用于`service-backend-b`，我们不会完全包含（但看起来几乎相同）。要查看完整的YAML，请检查`istio-virtual-service-3.yaml`的代码存储库。
- en: 'Once our virtual services are ready, we require some destination rules! The
    `DestinationRule` for *Backend Service A* is as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们的虚拟服务准备好了，我们需要一些目标规则！*Backend Service A*的`DestinationRule`如下：
- en: 'Istio-destination-rule-2.yaml:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 'Istio-destination-rule-2.yaml:'
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: And the `DestinationRule` for *Backend Service B* is similar, just with different
    subsets. We won't include the code, but check `istio-destination-rule-3.yaml`
    in the code repository for the exact specifications.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*Backend Service B*的`DestinationRule`类似，只是有不同的子集。我们不会包含代码，但是在代码存储库中检查`istio-destination-rule-3.yaml`以获取确切的规格。'
- en: 'These destination rules and virtual services add up to make the following routing
    diagram:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这些目标规则和虚拟服务相加，形成了以下路由图：
- en: '![Figure 14.4 – Istio routing diagram](image/B14790_14_004.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图14.4 - Istio路由图](image/B14790_14_004.jpg)'
- en: Figure 14.4 – Istio routing diagram
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 - Istio路由图
- en: As you can see, requests from *Frontend Service* Pods can route to *Backend
    Service A version 1* or *Backend Service B version 3*, and each backend service
    can route to the other as well. These requests to Backend Service A or B additionally
    engage one of the most valuable features of Istio – mutual (two-way) TLS. In this
    setup, TLS security is maintained between any two points in the mesh, and this
    all happens automatically!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，来自“前端服务”Pod的请求可以路由到“后端服务A版本1”或“后端服务B版本3”，每个后端服务也可以相互路由。对后端服务A或B的这些请求还额外利用了Istio的最有价值的功能之一
    - 双向TLS。在这种设置中，网格中的任何两点之间都会自动保持TLS安全。
- en: Next, let's take a look at using serverless patterns with Kubernetes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何在Kubernetes上使用无服务器模式。
- en: Implementing serverless on Kubernetes
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上实现无服务器
- en: 'Serverless patterns on cloud providers have quickly been gaining in popularity.
    Serverless architectures consist of compute that can automatically scale up and
    down, even scaling all the way to zero (where zero compute capacity is being used
    to serve a function or other application). **Function-as-a-Service** (**FaaS**)
    is an extension of the serverless pattern, where function code is the only input,
    and the serverless system takes care of routing requests to compute and scale
    as necessary. AWS Lambda, Azure Functions, and Google Cloud Run are some of the
    more popular FaaS/serverless options officially supported by cloud providers.
    Kubernetes also has many different serverless frameworks and libraries that can
    be used to run serverless, scale-to-zero workloads as well as FaaS on Kubernetes.
    Some of the most popular ones are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 云提供商上的无服务器模式迅速变得越来越受欢迎。无服务器架构由可以自动扩展的计算组成，甚至可以扩展到零（即没有使用计算容量来提供函数或其他应用）。函数即服务（FaaS）是无服务器模式的扩展，其中函数代码是唯一的输入，无服务器系统负责根据需要路由请求到计算资源并进行扩展。AWS
    Lambda、Azure Functions和Google Cloud Run是一些更受欢迎的FaaS/无服务器选项，它们得到了云提供商的官方支持。Kubernetes还有许多不同的无服务器框架和库，可以用于在Kubernetes上运行无服务器、扩展到零的工作负载以及FaaS。其中一些最受欢迎的如下：
- en: '*Knative*'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knative
- en: '*Kubeless*'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeless
- en: '*OpenFaaS*'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFaaS
- en: '*Fission*'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fission
- en: 'A full discussion of all serverless options on Kubernetes is beyond the scope
    of this book, so we''ll focus on two different ones, which aim to serve two vastly
    different use cases: *OpenFaaS* and *Knative*.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Kubernetes上所有无服务器选项的全面讨论超出了本书的范围，因此我们将专注于两种不同的选项，它们旨在满足两种完全不同的用例：OpenFaaS和Knative。
- en: While Knative is highly extensible and customizable, it uses multiple coupled
    components that add complexity. This means that some added configuration is necessary
    to get started with an FaaS solution, since functions are just one of many other
    patterns that Knative supports. OpenFaaS, on the other hand, makes getting up
    and running with serverless and FaaS on Kubernetes extremely easy. Both technologies
    are valuable for different reasons.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Knative非常可扩展和可定制，但它使用了多个耦合的组件，增加了复杂性。这意味着需要一些额外的配置才能开始使用FaaS解决方案，因为函数只是Knative支持的许多其他模式之一。另一方面，OpenFaaS使得在Kubernetes上轻松启动和运行无服务器和FaaS变得非常容易。这两种技术出于不同的原因都是有价值的。
- en: For this chapter's tutorial, we will look at Knative, one of the most popular
    serverless frameworks, and one that also supports FaaS via its eventing feature.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的教程中，我们将看看Knative，这是最流行的无服务器框架之一，也支持通过其事件功能的FaaS。
- en: Using Knative for FaaS on Kubernetes
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Kubernetes上使用Knative进行FaaS
- en: As mentioned previously, Knative is a modular set of building blocks for serverless
    patterns on Kubernetes. For this reason, it requires a bit of configuration before
    we can get to the actual functions. Knative can also be installed with Istio,
    which it uses as a substrate for routing and scaling serverless applications.
    Other non-Istio routing options are also available.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Knative是用于Kubernetes上无服务器模式的模块化构建块。因此，在我们实际使用函数之前，它需要一些配置。Knative也可以与Istio一起安装，它用作路由和扩展无服务器应用程序的基础。还有其他非Istio路由选项可用。
- en: 'To use Knative for FaaS, we will need to install both *Knative Serving* and
    *Knative Eventing*. While Knative Serving will allow us to run our serverless
    workloads, Knative Eventing will provide the pathway to make FaaS requests to
    these scale-to-zero workloads. Let''s accomplish this by following these steps:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Knative进行FaaS，我们需要安装*Knative Serving*和*Knative Eventing*。Knative Serving将允许我们运行无服务器工作负载，而Knative
    Eventing将提供通道来向这些规模为零的工作负载发出FaaS请求。让我们按照以下步骤来完成这个过程：
- en: 'First, let''s install the Knative Serving components. We will begin by installing
    the CRDs:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们安装Knative Serving组件。我们将从安装CRDs开始：
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we can install the serving components themselves:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以安装服务组件本身：
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'At this point, we''ll need to install a networking/routing layer for Knative
    to use. Let''s use Istio:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，我们需要安装一个网络/路由层供Knative使用。让我们使用Istio：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We''ll need the gateway IP address from Istio. Depending on where you''re running
    this (in other words, AWS or locally), this value may differ. Pull it using the
    following command:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要从Istio获取网关IP地址。根据您运行的位置（换句话说，是在AWS还是本地），此值可能会有所不同。使用以下命令获取它：
- en: '[PRE45]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Knative requires a specific DNS setup for enabling the serving component. The
    easiest way to do this in a cloud setting is to use `xip.io` "Magic DNS," though
    this will not work for Minikube-based clusters. If you're running one of these
    (or just want to see all the options available), check out the Knative docs at
    [https://knative.dev/docs/install/any-kubernetes-cluster/](https://knative.dev/docs/install/any-kubernetes-cluster/).
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Knative需要特定的DNS设置来启用服务组件。在云设置中最简单的方法是使用`xip.io`的“Magic DNS”，尽管这对基于Minikube的集群不起作用。如果您正在运行其中之一（或者只是想查看所有可用选项），请查看[Knative文档](https://knative.dev/docs/install/any-kubernetes-cluster/)。
- en: 'To set up Magic DNS, use the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置Magic DNS，请使用以下命令：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now that we''ve installed Knative Serving, let''s install Knative Eventing
    to deliver our FaaS requests. First, we''ll need more CRDs. Install them using
    the following command:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Knative Serving，让我们安装Knative Eventing来处理我们的FaaS请求。首先，我们需要更多的CRDs。使用以下命令安装它们：
- en: '[PRE47]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, install the eventing components just like we did with serving:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，安装事件组件，就像我们安装服务一样：
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: At this point, we need to add a queue/messaging layer for our eventing system
    to use. Did we mention that Knative supports lots of modular components?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要为我们的事件系统添加一个队列/消息层来使用。我们是否提到Knative支持许多模块化组件？
- en: Important note
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To make things easy, let's just use the basic in-memory messaging layer, but
    it's good to know all the options available to you. As regards modular options
    for messaging channels, check the docs at [https://knative.dev/docs/eventing/channels/channels-crds/](https://knative.dev/docs/eventing/channels/channels-crds/).
    For event source options, you can look at [https://knative.dev/docs/eventing/sources/](https://knative.dev/docs/eventing/sources/).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化事情，让我们只使用基本的内存消息层，但了解所有可用选项对您也是有好处的。关于消息通道的模块化选项，请查看[https://knative.dev/docs/eventing/channels/channels-crds/](https://knative.dev/docs/eventing/channels/channels-crds/)上的文档。对于事件源选项，您可以查看[https://knative.dev/docs/eventing/sources/](https://knative.dev/docs/eventing/sources/)。
- en: 'To install the `in-memory` messaging layer, use the following command:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`in-memory`消息层，请使用以下命令：
- en: '[PRE49]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Thought we were done? Nope! One last thing. We need to install a broker, which
    will take events from the messaging layer and get them processed in the right
    place. Let''s use the default broker layer, the MT-Channel broker layer. You can
    install it using the following command:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以为我们已经完成了？不！还有最后一件事。我们需要安装一个broker，它将从消息层获取事件并将它们处理到正确的位置。让我们使用默认的broker层，MT-Channel
    broker层。您可以使用以下命令安装它：
- en: '[PRE50]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: With that, we are finally done. We have installed an end-to-end FaaS implementation
    via Knative. As you can tell, this was not an easy task. What makes Knative amazing
    is the same thing that makes it a pain – it offers so many different modular options
    and configurations that even when selecting the most basic options for each step,
    we've still taken a lot of time to explain the install. There are other options
    available, such as OpenFaaS, which are a bit easier to get up and running with,
    and we'll look into that in the next section! On the Knative side, however, now
    that we have our setup finally ready, we can add in our FaaS.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们终于完成了。我们通过Knative安装了一个端到端的FaaS实现。正如你所看到的，这并不是一项容易的任务。Knative令人惊奇的地方与令人头疼的地方是一样的——它提供了许多不同的模块选项和配置，即使在每个步骤选择了最基本的选项，我们仍然花了很多时间来解释安装过程。还有其他可用的选项，比如OpenFaaS，它们更容易上手，我们将在下一节中进行探讨！然而，在Knative方面，现在我们的设置终于准备好了，我们可以添加我们的FaaS。
- en: Implementing an FaaS pattern in Knative
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Knative中实现FaaS模式
- en: 'Now that we have Knative set up, we can use it to implement an FaaS pattern
    where events will trigger some code running in Knative through a trigger. To set
    up a simple FaaS, we will require three things:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了Knative，我们可以使用它来实现一个FaaS模式，其中事件将通过触发器触发在Knative中运行的一些代码。要设置一个简单的FaaS，我们将需要三样东西：
- en: A broker to route our events from an entry point
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个从入口点路由我们的事件的broker
- en: A consumer service to actually process our events
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个消费者服务来实际处理我们的事件
- en: A trigger definition that specifies when to route events to the consumer for
    processing
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个指定何时将事件路由到消费者进行处理的触发器定义
- en: 'First thing''s first, our broker needs to be created. This is simple and similar
    to creating an ingress record or gateway. Our `broker` YAML looks like this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建我们的broker。这很简单，类似于创建入口记录或网关。我们的`broker` YAML如下所示：
- en: 'Knative-broker.yaml:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Knative-broker.yaml：
- en: '[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Next, we can create a consumer service. This component is really just our application
    that is going to process events – our function itself! Rather than showing you
    even more YAML than you've already seen, let's assume our consumer service is
    just a regular old Kubernetes Service called `service-consumer`, which routes
    to a four-replica deployment of Pods running our application.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一个消费者服务。这个组件实际上就是我们的应用程序，它将处理事件——我们的函数本身！我们不打算向你展示比你已经看到的更多的YAML，让我们假设我们的消费者服务只是一个名为`service-consumer`的普通的Kubernetes服务，它路由到一个运行我们应用程序的四个副本的Pod部署。
- en: 'Finally, we''re going to need a trigger. This determines how and which events
    will be routed from the broker. The YAML for a trigger looks like this:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个触发器。这决定了如何以及哪些事件将从broker路由。触发器的YAML如下所示：
- en: 'Knative-trigger.yaml:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Knative-trigger.yaml：
- en: '[PRE52]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: In this YAML, we create a `Trigger` rule that any event that comes through our
    broker, `my-broker`, and has a type of `myeventtype`, will automatically be routed
    to our consumer, `service-consumer`. For full documentation on trigger filters
    in Knative, check out the docs at [https://knative.dev/development/eventing/triggers/](https://knative.dev/development/eventing/triggers/).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 YAML 中，我们创建了一个 `Trigger` 规则，任何通过我们的经纪人 `my-broker` 并且类型为 `myeventtype` 的事件将自动路由到我们的消费者
    `service-consumer`。有关 Knative 中触发器过滤器的完整文档，请查看 [https://knative.dev/development/eventing/triggers/](https://knative.dev/development/eventing/triggers/)
    上的文档。
- en: 'So, how do we create some events? First, check the broker URL using the following
    command:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何创建一些事件呢？首先，使用以下命令检查经纪人 URL：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This should result in the following output:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can now finally test our FaaS solution. Let''s spin up a quick Pod from
    which we can make requests to our trigger:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以测试我们的 FaaS 解决方案了。让我们快速启动一个 Pod，从中我们可以向我们的触发器发出请求：
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now, from inside this Pod, we can go ahead and test our trigger, using `curl`.
    The request we need to make needs to have a `Ce-Type` header that equals `myeventtype`,
    since this is what our trigger requires. Knative uses headers in the form `Ce-Id`,
    `Ce-Type`, as shown in the following code block, to do the routing.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从这个 Pod 内部，我们可以继续测试我们的触发器，使用 `curl`。我们需要发出的请求需要有一个等于 `myeventtype` 的 `Ce-Type`
    标头，因为这是我们触发器所需的。Knative 使用形式为 `Ce-Id`、`Ce-Type` 的标头，如下面的代码块所示，来进行路由。
- en: 'The `curl` request will look like the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 请求将如下所示：'
- en: '[PRE56]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As you can see, we are sending a `curl` `http` request to the broker URL. Additionally,
    we are passing some special headers along with the HTTP request. Importantly,
    we are passing `type=myeventtype`, which our filter on our trigger requires in
    order to send the request for processing.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们正在向经纪人 URL 发送 `curl` `http` 请求。此外，我们还在 HTTP 请求中传递了一些特殊的标头。重要的是，我们传递了
    `type=myeventtype`，这是我们触发器上的过滤器所需的，以便发送请求进行处理。
- en: 'In this example, our consumer service echoes back the payload key of the body
    JSON, along with a `200` HTTP response, so running this `curl` request gives us
    the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的消费者服务会回显请求的 JSON 主体的 payload 键，以及一个 `200` 的 HTTP 响应，因此运行这个 `curl`
    请求会给我们以下结果：
- en: '[PRE57]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Success! We have tested our FaaS and it returns what we are expecting. From
    here, our solution will scale up and down to zero along with the number of events,
    and, as with everything Knative, there are many more customizations and configuration
    options to tailor our solution precisely to what we need.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！我们已经测试了我们的 FaaS，并且它返回了我们期望的结果。从这里开始，我们的解决方案将根据事件的数量进行零扩展和缩减，与 Knative 的所有内容一样，还有许多自定义和配置选项，可以精确地调整我们的解决方案以满足我们的需求。
- en: Next up, we'll look at the same pattern with OpenFaaS instead of Knative in
    order to highlight the differences between the two approaches.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 OpenFaaS 而不是 Knative 来查看相同的模式，以突出两种方法之间的区别。
- en: Using OpenFaaS for FaaS on Kubernetes
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上使用 OpenFaaS 进行 FaaS
- en: Now that we've discussed getting started with Knative, let's do the same with
    OpenFaaS. First, to install OpenFaaS itself, we are going to use the Helm charts
    from the `faas-netes` repository, found at [https://github.com/openfaas/faas-netes](https://github.com/openfaas/faas-netes).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何开始使用 Knative，让我们用 OpenFaaS 做同样的事情。首先，要安装 OpenFaaS 本身，我们将使用来自 `faas-netes`
    仓库的 Helm 图表，该仓库位于 [https://github.com/openfaas/faas-netes](https://github.com/openfaas/faas-netes)。
- en: Installing OpenFaaS components with Helm
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Helm 安装 OpenFaaS 组件
- en: 'First, we will create two namespaces to hold our OpenFaaS components:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建两个命名空间来保存我们的 OpenFaaS 组件：
- en: '`openfaas` to hold the actual service components of OpenFaas'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openfaas` 用于保存 OpenFaas 的实际服务组件'
- en: '`openfaas-fn` to hold our deployed functions'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openfaas-fn` 用于保存我们部署的函数'
- en: 'We can add these two namespaces using a nifty YAML file from the `faas-netes`
    repository using the following command:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令使用`faas-netes`存储库中的一个巧妙的YAML文件来添加这两个命名空间：
- en: '[PRE58]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we need to add the `faas-netes` `Helm` `repository` with the following
    Helm command:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用以下Helm命令添加`faas-netes` `Helm` `存储库`：
- en: '[PRE59]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Finally, we actually deploy OpenFaaS!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实际部署OpenFaaS！
- en: 'The Helm chart for OpenFaaS at the preceding `faas-netes` repository has several
    possible variables, but we will use the following configuration to ensure that
    an initial set of authentication credentials are created, and that ingress records
    are deployed:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的`faas-netes`存储库中的OpenFaaS的Helm图表有几个可能的变量，但我们将使用以下配置来确保创建一组初始的身份验证凭据，并部署入口记录：
- en: '[PRE60]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, that our OpenFaaS infrastructure has been deployed to our cluster, we''ll
    want to fetch the credentials that were generated as part of the Helm install.
    The Helm chart will create these as part of a hook and store them in a secret,
    so we can get them by running the following command:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的OpenFaaS基础设施已经部署到我们的集群中，我们将希望获取作为Helm安装的一部分生成的凭据。Helm图表将作为钩子的一部分创建这些凭据，并将它们存储在一个秘密中，因此我们可以通过运行以下命令来获取它们：
- en: '[PRE61]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: That is all the Kubernetes setup we require!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的所有Kubernetes设置！
- en: Moving on, let's install the OpenFaas CLI, which will make it extremely easy
    to manage our OpenFaas functions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们安装OpenFaas CLI，这将使管理OpenFaas函数变得非常容易。
- en: Installing the OpenFaaS CLI and deploying functions
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装OpenFaaS CLI和部署函数
- en: 'To install the OpenFaaS CLI, we can use the following command (for Windows,
    check the preceding OpenFaaS documents):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装OpenFaaS CLI，我们可以使用以下命令（对于Windows，请查看前面的OpenFaaS文档）：
- en: '[PRE62]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, we can get started with building and deploying some functions. This is
    easiest to do via the CLI.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始构建和部署一些函数。这最容易通过CLI来完成。
- en: When building and deploying functions for OpenFaaS, the OpenFaaS CLI provides
    an easy way to generate boilerplates, and build and deploy functions for specific
    languages. It does this via "templates," and supports various flavors of Node,
    Python, and more. For a full list of the template types, check the templates repository
    at [https://github.com/openfaas/templates](https://github.com/openfaas/templates).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建和部署OpenFaaS的函数时，OpenFaaS CLI提供了一种简单的方法来生成样板，并为特定语言构建和部署函数。它通过“模板”来实现这一点，并支持各种类型的Node、Python等。有关模板类型的完整列表，请查看[https://github.com/openfaas/templates](https://github.com/openfaas/templates)上的模板存储库。
- en: 'The templates created using the OpenFaaS CLI are similar to what you would
    expect from a hosted serverless platform such as AWS Lambda. Let''s create a brand-new
    Node.js function using the following command:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenFaaS CLI创建的模板类似于您从AWS Lambda等托管无服务器平台期望的内容。让我们使用以下命令创建一个全新的Node.js函数：
- en: '[PRE63]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This results in the following output:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE64]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: As you can see, the `new` command generates a folder, and within it some boilerplate
    for the function code itself, and an OpenFaaS YAML file.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，`new`命令生成一个文件夹，在其中有一些函数代码本身的样板，以及一个OpenFaaS YAML文件。
- en: 'The OpenFaaS YAML file will appear as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS YAML文件将如下所示：
- en: 'My-function.yml:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 'My-function.yml:'
- en: '[PRE65]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The actual function code (inside the `my-function` folder) consists of a function
    file – `handler.js` – and a dependencies manifest, `package.json`. For other languages,
    these files will be different, and we won''t delve into the specifics of dependencies
    in Node. However, we will edit the `handler.js` file to return some text. This
    is what the edited file looks like:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的函数代码（在`my-function`文件夹中）包括一个函数文件`handler.js`和一个依赖清单`package.json`。对于其他语言，这些文件将是不同的，我们不会深入讨论Node中的具体依赖。但是，我们将编辑`handler.js`文件以返回一些文本。编辑后的文件如下所示：
- en: 'Handler.js:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 'Handler.js:'
- en: '[PRE66]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This JavaScript code will return a JSON response with our text.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这段JavaScript代码将返回一个包含我们文本的JSON响应。
- en: 'Now that we have our function and handler, we can move on to building and deploying
    our function. The OpenFaaS CLI makes it simple to build the function, which we
    can do with the following command:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的函数和处理程序，我们可以继续构建和部署我们的函数。OpenFaaS CLI使构建函数变得简单，我们可以使用以下命令来完成：
- en: '[PRE67]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The output of this command is long, but when it is complete, we will have a
    new container image built locally with our function handler and dependencies embedded!
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令的输出很长，但当完成时，我们将在本地构建一个新的容器映像，其中包含我们的函数处理程序和依赖项！
- en: 'Next, we push our container image to a container repository as we would for
    any other container. The OpenFaaS CLI has a neat wrapper command for this, which
    will push the image to Docker Hub or an alternate container image repository:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将像对待任何其他容器一样，将我们的容器映像推送到容器存储库。OpenFaaS CLI具有一个很好的包装命令，可以将映像推送到Docker Hub或其他容器映像存储库：
- en: '[PRE68]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, we can deploy our function to OpenFaaS. Once again, this is made easy
    by the CLI. Deploy it using the following command:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的函数部署到OpenFaaS。再次，这由CLI轻松完成。使用以下命令进行部署：
- en: '[PRE69]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Everything is now set up for us to test our function, deployed on OpenFaaS!
    We used an ingress setting when deploying OpenFaaS so requests can go through
    that ingress. However, our generated YAML file from our new function is set to
    make requests on `localhost:8080` for development purposes. We could edit that
    file to the correct `URL` for our ingress gateway (refer to the docs at [https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/)
    for how to do that), but instead, let's just do a shortcut to get OpenFaaS open
    on our localhost.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一切都已准备好让我们测试在OpenFaaS上部署的函数了！我们在部署OpenFaaS时使用了一个入口设置，以便请求可以通过该入口。但是，我们新函数生成的YAML文件设置为在开发目的地对`localhost:8080`进行请求。我们可以编辑该文件以将请求发送到我们入口网关的正确`URL`（请参阅[https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/)中的文档），但相反，让我们通过快捷方式在本地主机上打开OpenFaaS。
- en: 'Let''s use a `kubectl port-forward` command to open our OpenFaaS service on
    localhost port `8080`. We can do this as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`kubectl port-forward`命令在本地主机端口`8080`上打开我们的OpenFaaS服务。我们可以按照以下方式进行：
- en: '[PRE70]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, let''s add our previously generated auth credentials to the OpenFaaS CLI,
    as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照以下方式将先前生成的auth凭据添加到OpenFaaS CLI中：
- en: '[PRE71]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Finally, all we need to do in order to test our function is to run the following
    command:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了测试我们的函数，我们只需运行以下命令：
- en: '[PRE72]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This results in the following output:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE73]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: As you can see, we've successfully received our intended response!
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们成功收到了我们预期的响应！
- en: 'Finally, if we want to delete this specific function, we can do so with the
    following command, similar to how we would use `kubectl delete -f`:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们想要删除这个特定的函数，我们可以使用以下命令，类似于我们使用`kubectl delete -f`的方式：
- en: '[PRE74]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: And that's it! Our function has been removed.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们的函数已被删除。
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about service mesh and serverless patterns on Kubernetes.
    In order to set the stage for these, we first discussed running sidecar proxies
    on Kubernetes, specifically with the Envoy proxy.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关于Kubernetes上的服务网格和无服务器模式。为了为这些做好准备，我们首先讨论了在Kubernetes上运行边车代理，特别是使用Envoy代理。
- en: Then, we moved on to service mesh, and learned how to install and configure
    the Istio service mesh for service-to-service routing with mutual TLS.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们转向服务网格，并学习了如何安装和配置Istio服务网格，以实现服务到服务的互相TLS路由。
- en: Finally, we moved on to serverless patterns on Kubernetes, where you learned
    how to configure and install Knative, and an alternative, OpenFaaS, for serverless
    eventing, and FaaS on Kubernetes.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们转向了在Kubernetes上的无服务器模式，您将学习如何配置和安装Knative，以及另一种选择OpenFaaS，用于Kubernetes上的无服务器事件和FaaS。
- en: The skills you used in this chapter will help you to build service mesh and
    serverless patterns on Kubernetes, setting you up for fully automated service-to-service
    discovery and FaaS eventing.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的技能将帮助您在Kubernetes上构建服务网格和无服务器模式，为您提供完全自动化的服务发现和FaaS事件。
- en: In the next (and final) chapter, we'll discuss running stateful applications
    on Kubernetes.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章（也是最后一章）中，我们将讨论在Kubernetes上运行有状态应用程序。
- en: Questions
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between static and dynamic Envoy configurations?
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静态和动态Envoy配置有什么区别？
- en: What are the four major pieces of Envoy configuration?
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Envoy配置的四个主要部分是什么？
- en: What are some of the downsides to Knative, and how does OpenFaaS compare?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Knative的一些缺点是什么，OpenFaaS又如何比较？
- en: Further reading
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'CNCF landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNCF景观：[https://landscape.cncf.io/](https://landscape.cncf.io/)
- en: 'Official Kubernetes forums: [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方Kubernetes论坛：[https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)
