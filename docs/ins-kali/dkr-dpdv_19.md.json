["```\n$ `export` `DOCKER_CONTENT_TRUST``=``1` \n```", "```\n     $ docker image tag alpine:latest nigelpoulton/dockerbook:v1 \n    ```", "```\n     $ docker login\n     Login with your Docker ID to push and pull images from Docker Hub.\n     Username: nigelpoulton\n     Password:\n     Login Succeeded \n    ```", "```\n     $ docker image push nigelpoulton/dockerbook:v1\n     The push refers to a repository [docker.io/nigelpoulton/dockerbook]\n     cd7100a72410: Mounted from library/alpine\n     v1: digest: sha256:8c03...acbc size: 528\n     Signing and pushing trust metadata\n     <Snip>\n     Enter passphrase for new root key with ID 865e4ec:\n     Repeat passphrase for new root key with ID 865e4ec:\n     Enter passphrase for new repository key with ID bd0d97d:\n     Repeat passphrase for new repository key with ID bd0d97d:\n     Finished initializing \"docker.io/nigelpoulton/sign\"\n     Successfully signed \"docker.io/nigelpoulton/sign\":v1 \n    ```", "```With DCT enabled, the image was automatically signed as part of the push operation.\n\nTwo sets of keys were created as part of the signing operation:\n\n*   Root key\n*   Repository key\n\nBy default, both are stored below a hidden folder in your home directory called `docker`. On Linux this is `~/.docker/trust`.\n\nThe **root key** is the master key (of sorts). It\u2019s used to create and sign new repository keys, and should be kept safe. This means you should protect it with a strong passphrase, and you should store it offline in a secure place when not in use. If it gets compromised, you\u2019ll be in world of pain! You would normally only have one per person, or may be even one per team or organization, and you\u2019ll normally only use it to create new repository keys.\n\nThe **repository key**, also known as the *tagging key* is a per-repository key that is used to sign tagged images pushed to a particular repository. As such, you\u2019ll have one per repository. It\u2019s quite a bit easier to recover from a loss of this key, but you should still protect it with a strong passphrase and keep it safe.\n\nEach time you push an image to a **new repository**, you\u2019ll create a new repository tagging key. You need your **root key** to do this, so you\u2019ll need to enter the root key\u2019s passphrase. Subsequent pushes to the same repository will only require you to enter the passphrase for the repository tagging key.\n\nThere\u2019s another key called the `timestamp key`. This gets stored in the remote repository and is used in more advanced use-cases to ensure things like *freshness*.\n\nLet\u2019s have a look at pulling images with DCT enabled.\n\nPerform the following commands from the same Docker host that has DCT enabled.\n\nPull an unsigned image.\n\n```", "```\n\n `> **Note:** Sometimes the error message will be `No trust data for unsigned`.\n\nSee how Docker has refused to download the image because it is not signed.\n\nYou\u2019ll get similar errors if you try to build new images or run new containers from unsigned images. Let\u2019s test it.\n\nPull the unsigned image by using the `--disable-content-trust` flag to override DCT.\n\n```", "```\n\n `The `--disable-content-trust` flag overrides DCT on a per-command basis. Use it wisely.\n\nNow try and run a container from the unsigned image.\n\n```", "```\n\n `This proves that Docker Content Trust enforces policy on `push`, `pull` and `run` operations. Try a `build` to see it work there as well.\n\nDocker UCP also supports DCT, allowing you to set a UCP-wide signing policy.\n\nTo enable DCT across an entire UCP, expand the `Admin` drop-down and click `Admin Settings`. Select the `Docker Content Trust` option and tick the `Run Only Signed Images` tickbox. This will enforce a signing policy across the entire cluster that will only allow you to deploy services using signed images.\n\nThe default configuration will allow any image signed by a valid UCP user. You can optionally configure a list of teams that are authorized to sign images.\n\nThat\u2019s the basics of Docker Content Trust. Let\u2019s move on to configuring and using Docker Trusted Registry (DTR).\n\n#### Configuring Docker Trusted Registry (DTR)\n\nIn the previous chapter we installed DTR, plugged it in to a shared storage backend, and configured HA. We also learned that UCP and DTR share a common single-sign-on sub-system. But there\u2019s a few other important things you should configure. Let\u2019s take a look.\n\nMost of the DTR configuration settings are located on the `Settings` page of the DTR web UI.\n\nFrom the `General` tab you can configure:\n\n*   Automatic update settings\n*   Licensing\n*   Load balancer address\n*   Certificates\n*   Single-sign-on\n\nThe `TLS Settings` under `Domains & proxies` allows you to change the certificates used by UCP. By default, DTR uses self-signed certificates, but you can use this page to configure the use of custom certificates.\n\nThe `Storage` tab lets you configure the backend used for **image storage**. We saw this in the previous chapter when we configured a shared Amazon S3 backend so that we could configure DTR HA. Other storage options include object storage services from other cloud providers, as well as volumes and NFS shares.\n\nThe `Security` tab is where you enable and disable *Image Scanning* \u2014 binary-level scans that identify known vulnerabilities in images. When you enable *image scanning*, you have the option of updating the vulnerability database *online* or *offline*. Online will automatically sync the database over the internet, whereas the offline method is for DTR instances that do not have internet access and need to update the database manually.\n\nSee the *Security in Docker* chapter for more information on Image Scanning.\n\nLast but not least, the `Garbage Collection` tab lets you configure when DTR will perform garbage collection on image layers that are no longer referenced in the Registry. By default, unreferenced layers are not garbage collected, resulting in large amounts of wasted disk space. If you enable garbage collection, layers that are no longer referenced by an image will be deleted, but layers that are referenced by at least one image manifest will not.\n\nSee the chapter on Images for more information about how image manifests reference image layers.\n\nNow that we know how to configure DTR, let\u2019s use it!\n\n#### Using Docker Trusted Registry\n\nDocker Trusted Registry is a secure on-premises registry that you configure and manage yourself. It\u2019s integrated into UCP for smooth out-of-the-box experience.\n\nIn this section, we\u2019ll look at how to push and pull images from it, and we\u2019ll learn how to inspect and manage repositories using the DTR web UI.\n\n##### Log in to the DTR UI and create a repo and permissions\n\nLet\u2019s log in to DTR and create a new repository that members of the `technology/devs` team can push and pull images from.\n\nLog on to DTR. The DTR URL can be found in the UCP web UI under `Admin` > `Admin Settings` > `Docker Trusted Registry`. Remember that the DTR web UI is accessible over HTTPS on TCP port 443.\n\nCreate a new organization and team, and add a user to it. The example will create an organization called `technology`, a team called `devs`, and add the `nigelpoulton` user to it. You can substitute these values in your environment.\n\n1.  Click `Organizations` in the left navigation pane.\n2.  Click `New organization` and call it `technology`.\n3.  Select the new `technology` organization and click the `+` button next to `TEAMS` as shown in Figure 17.11.![Figure 17.11](images/figure17-11.png)\n\n    Figure 17.11\n\n4.  With the `devs` team selected, add an existing user.\n\n    The example will add the `nigelpoulton` user. Your user will be different in your environment.\n\nThe organization and team changes you have made in DTR will be reflected in UCP. This is because they share the same accounts database.\n\nLet\u2019s create a new repository and add the `technology/devs` team with read/write permission.\n\nPerform all of the following in the DTR web UI.\n\n1.  If you aren\u2019t already, navigate to `Organizations` > `technology` > `devs`.\n2.  Select the `Repositories` tab and create a new repository.\n3.  Configure the repository as follows.\n\n    Make it a **New** repository called **test** under the **technology** organization. Make it **public**, enable **scan on push** and assign **read/write** permissions. Figure 17.12 shows a screenshot of how it should look.\n\n    ![Figure 17.12 Creating a new DTR image repo](images/figure17-12.png)\n\n    Figure 17.12 Creating a new DTR image repo\n\n4.  Save changes.\n\nCongratulations! You have an image repo on DTR called `<dtr-url>/technology`, and members of the `technology/devs` team have read/write access, meaning they can `push` and `pull` from it.\n\n##### Push an image to the DTR repo\n\nIn this step we\u2019ll push a new image to the repo you just created. To do this, we\u2019ll complete the following steps:\n\n1.  Pull an image and re-tag it.\n2.  Configure a client to use a certificate bundle.\n3.  Push the re-tagged image to the DTR repo.\n4.  Verify the operation in the DTR web UI.\n\nLet\u2019s pull an image and tag it so that it can be pushed to the DTR repo.\n\nIt doesn\u2019t matter what image you pull. The example uses the `alpine:latest` image because it\u2019s small.\n\n```", "```\n\n `In order to push an image to a specific repo, you need to tag the image with the name of the repo. The example DTR repo has a fully qualified name of `dtr.mydns.com/technology/test`. This is made by combining the DNS name of the DTR and the name of the repo. Yours will be different.\n\nTag the image so it can be pushed to the DTR repo.\n\n```", "```\n\n `The next job is to configure a Docker client to authenticate as a user in the group that has read/write permission to the repository. The high-level process is to create a certificate bundle for the user and configure a Docker client to use those certificates.\n\n1.  Login to UCP as admin, or a user that has read/write permission to the DTR repo.\n2.  Navigate to the desired user account and create a `client bundle`.\n3.  Copy the bundle file to the Docker client you want to configure.\n4.  Login to the Docker client and perform the following commands from the client.\n5.  Unzip the bundle and run the appropriate shell script to configure your environment.\n\n    The following will work on Mac and Linux.\n\n    ```", "```\n\n`*   Run a `docker version` command to verify the environment has been configured and the certificates are being used.\n\n    As long as the `Server` section of the output shows the `Version` as `ucp/x.x.x` it is working. This is because the shell script configured the Docker client to talk to a remote daemon on a UCP manager. It also configured the Docker client to sign all commands with the certificates.` \n\n `The next job is to log in to DTR. The DTR URL and username will be different in your environment.\n\n```", "```\n\n `You are now ready to push the re-tagged image to DTR.\n\n```", "```\n\n `The push looks successful, but let\u2019s verify the operation in the DTR web UI.\n\n1.  If you aren\u2019t already, login to the DTR web UI.\n2.  Click `Repositories` in the left navigation pane.\n3.  Click `View Details` for the `technology/test` repository.\n4.  Click the `IMAGES` tab.\n\nFigure 17.13 shows what the image looks like in the DTR repo. We can see that the image is a Linux-based image and that it has 3 major vulnerabilities. We know about the vulnerabilities because we configured the repository to scan all newly-pushed images.\n\n![Figure 17.13](images/figure17-13.png)\n\nFigure 17.13\n\nCongratulations. You\u2019ve successfully pushed an image to a new repository on DTR.\n\nYou can select the checkbox to the left of the image and delete it. Be certain before doing this, as the operation cannot be undone.\n\n#### Image promotions\n\nDTR has a couple other interesting features:\n\n*   Image promotions\n*   Immutable repos\n\nImage promotions let you build policy-based automated pipelines that promote images through a set of repositories in the same DTR.\n\nAs an example, you might have developers pushing images to a repository called `base`. But you don\u2019t want them to be able to push images straight to production in case they contain vulnerabilities. To help with situations like this, DTR allows you to assign a policy to the `base` repo, that will scan all pushed images, and promote them to another repo based on the results of the scan. If the scan highlights issues, the policy can *promote* the image to a quarantined repo, whereas if the scan results are clean, it can promote it to a QA or prod repo. You can even re-tag the image as it passes through the pipeline.\n\nLet\u2019s see it in action.\n\nThe example that we\u2019ll walk through has a single DTR with 3 image repos:\n\n*   `base`\n*   `good`\n*   `bad`\n\nThe `good` and `bad` repos are empty, but the `base` repo has two images in it, shown in Figure 17.14.\n\n![Figure 17.14](images/figure17-14.png)\n\nFigure 17.14\n\nAs we can see, both images have been scanned, `v1` is clean and has no known vulnerabilities, but `v2` has 3 majors.\n\nWe\u2019ll create two policies on the `base` repo so that images with a clean bill-of-health are promoted to the `good` repo, and images with known vulnerabilities are promoted to the `bad` repo.\n\nPerform all of the following actions on the `base` repo.\n\n1.  Click the `Policies` tab and make sure that `Is source` is selected.\n2.  Click `New promotion policy`.\n3.  Under \u201cPROMOTE TO TARGET IF\u2026\u201d select `All Vulnerabilities` and create a policy for `equals 0`.![](images/figure17-15.png)\n\n    This will create a policy that acts on all images with zero vulnerabilities.\n\n    Don\u2019t forget to click `Add` before moving to the next step.\n\n4.  Select the `TARGET REPOSITORY` as `technology/good` and hit `Save & Apply`.\n\n    Clicking `Save` will apply the policy to the repo and enforce it for all new images pushed the repo, but it will not affect images already in the repo. `Save & Apply` will do the same, **but also for images already in the repo**.\n\n    If you click `Save & Apply`, the policy will immediately evaluate all images in the repo and promote those that are clean. This means the `v1` image will be promoted to the `technology/good` repo.\n\n5.  Inspect the `technology/good` repo.\n\n    As you can see in Figure 17.16, the `v1` image has been promoted and is showing in the UI as `PROMOTED`.\n\n    ![Figure 17.16](images/figure17-16.png)\n\n    Figure 17.16\n\nThe promotion policy is working. Let\u2019s create another one to *promote* images that do have vulnerabilities to the `technology/bad` repo.\n\nPerform all of the following from the `technology/base` repo.\n\n1.  Create another new promotion policy.\n2.  Create a policy criteria for All Vulnerabilities > 0 and click `Add`.![Figure 17.17](images/figure17-17.png)\n\n    Figure 17.17\n\n3.  Add the target repo as `technology/bad`, and add \u201c-dirty\u201d to the `TAG NAME IN TARGET` box so that it is now \u201c%n-dirty\u201d. This last bit will re-tag the image as part of the promotion.\n4.  Click `Save & Apply`.\n5.  Check the `technology/bad` repo to confirm that the policy is enforcing and the `v2` image has been promoted and re-tagged.![Figure 17.18](images/figure17-18.png)\n\n    Figure 17.18\n\nNow that images are being promoted to the `technology/good` repo if they have no vulnerabilities, it might be a good idea to make the repo immutable. This will prevent images from being overwritten and deleted.\n\n1.  Navigate to the `technology/good` repo and click the `Settings` tab.\n2.  Set `IMMUTABILITY` to `On` and click `Save`.\n3.  Try and delete the image.\n\n    You\u2019ll get the following error.\n\n    ![](images/figure17-19.png)\n\nTime for one last feature!\n\n#### HTTP Routing Mesh (HRM)\n\nDocker Swarm features a layer-4 routing mesh called the Swarm Routing Mesh. This exposes Swarm services on all nodes in the cluster and balances incoming traffic across service replicas. The end results is a moderately even balance of traffic to all service replicas. However, it has no application intelligence. For example, it cannot route based on data at layer 7 in the HTTP headers. To overcome this, UCP implements a layer-7 routing mesh called the HTTP Routing Mesh, or HRM for short. This builds on top of the Swarm Routing Mesh.\n\nThe HRM allows multiple Swarm services to be published on the same Swarm-wide port, with ingress traffic being routed to the right service based on hostname data stored in the HTTP headers of incoming requests.\n\nFigure 17.20 shows a simple two-service example.\n\n![Figure 17.20](images/figure17-20.png)\n\nFigure 17.20\n\nIn the picture, the laptop client is making an HTTP request to `mustang.internal` on TCP port 80\\. The UCP cluster has two services that are both listening on port 80\\. The `mustang` service is published on port 80 and configured to receive traffic intended for the `mustang.internal` hostname. The `camero` service is also published on port 80, but is configured to receive traffic coming in to `camero.internal`.\n\nThere is a third service called HRM that maintains the mapping between hostnames and UCP services. It is the HRM that receives incoming traffic on port 80, inspects the HTTP headers and makes the decision of which service to route it to.\n\nLet\u2019s walk through an example, then explain a bit more detail when we\u2019re done.\n\nWe\u2019ll build the example shown in Figure 17.20\\. The process will be as follows: Enable the HRM on port 80\\. Deploy a *service* called \u201cmustang\u201d using the `nigelpoulton/dockerbook:mustang` image and create a hostname route for the mustang service so that requests to \u201cmustang.internal\u201d get routed to it. Deploy a second service called \u201ccamero\u201d based on the `nigelpoulton/dockerbook:camero` image and create a hostname route for this one that maps it to requests for \u201ccamero.internal\u201d.\n\nYou can use publicly resolvable DNS names such as mustang.mycompany.com, all that is required is that you have name resolution configured so that requests to those addresses resolve to the load balancer in front of your UCP cluster. IF you don\u2019t have a load balancer, you can point traffic to the IP of any node in the cluster.\n\nLet\u2019s see it.\n\n1.  If you aren\u2019t already, log on to the UCP web UI.\n2.  Navigate to `Admin` > `Admin Settings` > `Routing Mesh`.\n3.  Tick the `Enable Routing Mesh` tickbox and make sure that the `HTTP Port` is configured to `80`.\n4.  Click `Save`.\n\nThat\u2019s the UCP cluster configured to use the HRM. Behind the scenes this has deployed a new *system service* called `ucp-hrm`, and a new overlay network called `ucp-hrm`.\n\nIf you inspect the `ucp-hrm` system service, you\u2019ll see that it\u2019s publishing port `80` in *ingress mode*. This means the `ucp-hrm` is deployed on the cluster and bound to port `80` on all nodes in the cluster. This means **all traffic** coming into the cluster on port 80 will be handled by this service. When the `mustang` and `camero` services are deployed, the `ucp-hrm` service will be updated with hostname mappings so that it knows how to route traffic to those services.\n\nNow that the HRM is deployed, it\u2019s time to deploy our services.\n\n1.  Select `Services` in the left navigation pane and click `Create Service`.\n2.  Deploy the \u201cmustang\u201d service as follows:\n    *   **Details/Name:** mustang\n    *   **Details/Image:** nigelpoulton/dockerbook:mustang\n    *   **Network/Ports/Publish Port:** Click the option to `Publish Port +`\n    *   **Network/Ports/Internal Port:** 8080\n    *   **Network/Ports/Add Hostname Based Routes:** Click on the option to add a hostname based route\n    *   **Network/Ports/External Scheme:** Http://\n    *   **Network/Ports/Routing Mesh Host:** mustang.internal\n    *   **Network/Ports/Networks:** Make sure that the service is attached to the `ucp-hrm` network\n3.  Click `Create` to deploy the service.\n4.  Deploy the \u201ccamero\u201d service.\n\n    Deploy this service with the same settings as the \u201cmustang\u201d service, but with the following differences:\n\n    *   **Details/Name:** camero\n    *   **Details/Image:** nigelpoulton/dockerbook:camero\n    *   **Network/Ports/Routing Mesh Host:** camero.internal\n5.  Click `Create`.\n\nIt\u2019ll take a few seconds for each service to deploy, but when they\u2019re done, you\u2019ll be able to point a web browser at `mustang.internal` and reach the mustang service, and `camero.internal` and reach the camero service.\n\n> **Note:** You will obviously need name resolution configured so that `mustang.internal` and `camero.internal` resolve to your UCP cluster. This can be to a load balancer sitting in front of your cluster forwarding traffic to the cluster on port 80, or you\u2019re in a lab without a load balancer, it can be a simple local `hosts` file resolving the DNS names to the IP address of a cluster node.\n\nFigure 17.21 shows the mustang service being reached via `mustang.internal`.\n\n![Figure 17.21](images/figure17-21.png)\n\nFigure 17.21\n\nLet\u2019s remind ourselves of how this works.\n\nThe HTTP Routing Mesh is a Docker UCP feature that builds on top of the transport layer Swarm Routing Mesh. Specifically, the HRM adds application layer intelligence in the form of hostname rules.\n\nEnabling the HRM deploys a new UCP *system service* called `ucp-hrm`. This service is published *swarm-wide* on port 80 and 8443\\. This means that all traffic arriving at the cluster on either of those ports will be sent to the `ucp-hrm` service. This puts the `ucp-hrm` service in a position to receive, inspect, and route all traffic entering the cluster on those ports.\n\nWe then deployed two *user services*. As part of deploying each service, we created a hostname mapping that was added to the `ucp-hrm` service. The \u201cmustang\u201d service created a mapping so that it would receive all traffic arriving on the cluster on port 80 with \u201cmustang.internal\u201d in the HTTP header. The \u201ccamero\u201d service did the same thing for traffic arriving on port 80 with \u201ccamero.internal\u201d in the HTTP header. This resulted in the `ucp-hrm` service having two entries effectively saying the following:\n\n*   All traffic arriving on port 80 for \u201cmustang.internal\u201d gets sent to the \u201cmustang\u201d service.\n*   All traffic arriving on port 80 for \u201ccamero.internal\u201d gets sent to the \u201ccamero\u201d service.\n\nLet\u2019s show Figure 17.20 again.\n\n![Figure 17.20](images/figure17-20.png)\n\nFigure 17.20\n\nHopefully this should be clear now!\n\n### Chapter Summary\n\nUCP and DTR join forces to provide a great suit of features that are valuable to most enterprise organizations.\n\nStrong role-based access control is a fundamental part of UCP, with the ability be extremely granular with permissions \u2013 down to individual API operations. Integration with Active Directory and other corporate LDAP solutions is also supported.\n\nDocker Content Trust (DCT) brings cryptographic guarantees to image-based operations. These include `push`, `pull`, `build`, and `run`. When DCT is enabled, all images pushed to remote repos are signed, and all images pulled are verified. This gives you cryptographic certainty that the image you get is the one you asked for. UCP can be configured to enforce a cluster-wide policy requiring all images to be signed.\n\nDTR can be configured to use self-signed certificates, or certificates from trusted 3rd-party CAs. You can configure it to perform binary-level image scans that identify known vulnerabilities. And you can configure policies to automate the promotion of images through your build pipelines.\n\nFinally, we looked at the HTTP Routing mesh that performs application layer routing based on hostnames in HTTP headers.```", "`````````"]