["```\n{\n    \"variables\": {},\n    \"builders\": [],\n    \"provisioners\": [],\n    \"post-processors\": []\n}\n```", "```\n{\n  \"variables\": {},\n  \"builders\": [\n {\n \"type\": \"amazon-ebs\",\n \"access_key\": \"{{user `aws_access_key_id`}}\",\n \"secret_key\": \"{{user `aws_secret_access_key`}}\",\n \"token\": \"{{user `aws_session_token`}}\",\n \"region\": \"us-east-1\",\n \"source_ami\": \"ami-5e414e24\",\n \"instance_type\": \"t2.micro\",\n \"ssh_username\": \"ec2-user\",\n \"associate_public_ip_address\": \"true\",\n \"ami_name\": \"docker-in-aws-ecs {{timestamp}}\",\n \"tags\": {\n \"Name\": \"Docker in AWS ECS Base Image 2017.09.h\",\n \"SourceAMI\": \"{{ .SourceAMI }}\",\n \"DockerVersion\": \"17.09.1-ce\",\n \"ECSAgentVersion\": \"1.17.0-2\"\n }\n }\n ],\n  \"provisioners\": [],\n  \"post-processors\": []\n}\n```", "```\n{\n  \"variables\": {\n \"aws_access_key_id\": \"{{env `AWS_ACCESS_KEY_ID`}}\",\n \"aws_secret_access_key\": \"{{env `AWS_SECRET_ACCESS_KEY`}}\",\n \"aws_session_token\": \"{{env `AWS_SESSION_TOKEN`}}\",\n \"timezone\": \"US/Eastern\"\n },\n  \"builders\": [\n    {\n      \"type\": \"amazon-ebs\",\n      \"access_key\": \"{{user `aws_access_key_id`}}\",\n      \"secret_key\": \"{{user `aws_secret_access_key`}}\",\n      \"token\": \"{{user `aws_session_token`}}\",\n      \"region\": \"us-east-1\",\n      \"source_ami\": \"ami-5e414e24\",\n      \"instance_type\": \"t2.micro\",\n      \"ssh_username\": \"ec2-user\",\n      \"associate_public_ip_address\": \"true\",\n      \"ami_name\": \"docker-in-aws-ecs {{timestamp}}\",\n      \"tags\": {\n        \"Name\": \"Docker in AWS ECS Base Image 2017.09.h\",\n        \"SourceAMI\": \"{{ .SourceAMI }}\",\n        \"DockerVersion\": \"17.09.1-ce\",\n        \"ECSAgentVersion\": \"1.17.0-2\"\n      }\n    }\n  ],\n  \"provisioners\": [],\n  \"post-processors\": []\n}\n```", "```\n{\n  \"variables\": {\n    \"aws_access_key_id\": \"{{env `AWS_ACCESS_KEY_ID`}}\",\n    \"aws_secret_access_key\": \"{{env `AWS_SECRET_ACCESS_KEY`}}\",\n    \"aws_session_token\": \"{{env `AWS_SESSION_TOKEN`}}\",\n    \"timezone\": \"US/Eastern\"\n  },\n  \"builders\": [\n    {\n      \"type\": \"amazon-ebs\",\n      \"access_key\": \"{{user `aws_access_key_id`}}\",\n      \"secret_key\": \"{{user `aws_secret_access_key`}}\",\n      \"token\": \"{{user `aws_session_token`}}\",\n      \"region\": \"us-east-1\",\n      \"source_ami\": \"ami-5e414e24\",\n      \"instance_type\": \"t2.micro\",\n      \"ssh_username\": \"ec2-user\",\n      \"associate_public_ip_address\": \"true\",\n      \"ami_name\": \"docker-in-aws-ecs {{timestamp}}\",\n      \"tags\": {\n        \"Name\": \"Docker in AWS ECS Base Image 2017.09.h\",\n        \"SourceAMI\": \"ami-5e414e24\",\n        \"DockerVersion\": \"17.09.1-ce\",\n        \"ECSAgentVersion\": \"1.17.0-2\"\n      }\n    }\n  ],\n  \"provisioners\": [\n {\n \"type\": \"shell\",\n \"inline\": [\n \"sudo yum -y -x docker\\\\* -x ecs\\\\* update\"\n ] \n }\n ],\n  \"post-processors\": []\n}\n```", "```\n{\n  \"variables\": {\n    \"aws_access_key_id\": \"{{env `AWS_ACCESS_KEY_ID`}}\",\n    \"aws_secret_access_key\": \"{{env `AWS_SECRET_ACCESS_KEY`}}\",\n    \"aws_session_token\": \"{{env `AWS_SESSION_TOKEN`}}\",\n    \"timezone\": \"US/Eastern\"\n  },\n  \"builders\": [\n    {\n      \"type\": \"amazon-ebs\",\n      \"access_key\": \"{{user `aws_access_key_id`}}\",\n      \"secret_key\": \"{{user `aws_secret_access_key`}}\",\n      \"token\": \"{{user `aws_session_token`}}\",\n      \"region\": \"us-east-1\",\n      \"source_ami\": \"ami-5e414e24\",\n      \"instance_type\": \"t2.micro\",\n      \"ssh_username\": \"ec2-user\",\n      \"associate_public_ip_address\": \"true\",\n      \"ami_name\": \"docker-in-aws-ecs {{timestamp}}\",\n      \"tags\": {\n        \"Name\": \"Docker in AWS ECS Base Image 2017.09.h\",\n        \"SourceAMI\": \"ami-5e414e24\",\n        \"DockerVersion\": \"17.09.1-ce\",\n        \"ECSAgentVersion\": \"1.17.0-2\"\n      }\n    }\n  ],\n  \"provisioners\": [\n    {\n      \"type\": \"shell\",\n      \"inline\": [\n        \"sudo yum -y -x docker\\\\* -x ecs\\\\* update\"\n      ] \n    }\n  ],\n  \"post-processors\": [\n {\n \"type\": \"manifest\",\n \"output\": \"manifest.json\",\n \"strip_path\": true\n }\n ]\n}\n```", "```\n> export AWS_PROFILE=docker-in-aws\n> aws sts assume-role --role-arn=$(aws configure get role_arn) --role-session-name=$(aws configure get role_session_name)\nEnter MFA code for arn:aws:iam::385605022855:mfa/justin.menga: ******\n{\n    \"Credentials\": {\n        \"AccessKeyId\": \"ASIAIIEUKCAR3NMIYM5Q\",\n        \"SecretAccessKey\": \"JY7HmPMf/tPDXsgQXHt5zFZObgrQJRvNz7kb4KDM\",\n        \"SessionToken\": \"FQoDYXdzEM7//////////wEaDP0PBiSeZvJ9GjTP5yLwAVjkJ9ZCMbSY5w1EClNDK2lS3nkhRg34/9xVgf9RmKiZnYVywrI9/tpMP8LaU/xH6nQvCsZaVTxGXNFyPz1BcsEGM6Z2ebIFX5rArT9FWu3v7WVs3QQvXeDTasgdvq71eFs2+qX7zbjK0YHXaWuu7GA/LGtNj4i+yi6EZ3OIq3hnz3+QY2dXL7O1pieMLjfZRf98KHucUhiokaq61cXSo+RJa3yuixaJMSxJVD1myx/XNritkawUfI8Xwp6g6KWYQAzDYz3MIWbA5LyX9Q0jk3yXTRAQOjLwvL8ZK/InJCDoPBFWFJwrz+Wxgep+I8iYoijOhqTUBQ==\",\n        \"Expiration\": \"2018-02-18T05:38:38Z\"\n    },\n    \"AssumedRoleUser\": {\n        \"AssumedRoleId\": \"AROAJASB32NFHLLQHZ54S:justin.menga\",\n        \"Arn\": \"arn:aws:sts::385605022855:assumed-role/admin/justin.menga\"\n    }\n}\n> export AWS_ACCESS_KEY_ID=\"ASIAIIEUKCAR3NMIYM5Q\"\n> export AWS_SECRET_ACCESS_KEY=\"JY7HmPMf/tPDXsgQXHt5zFZObgrQJRvNz7kb4KDM\"\n> export AWS_SESSION_TOKEN=\"FQoDYXdzEM7//////////wEaDP0PBiSeZvJ9GjTP5yLwAVjkJ9ZCMbSY5w1EClNDK2lS3nkhRg34/9xVgf9RmKiZnYVywrI9/tpMP8LaU/xH6nQvCsZaVTxGXNFyPz1BcsEGM6Z2ebIFX5rArT9FWu3v7WVs3QQvXeDTasgdvq71eFs2+qX7zbjK0YHXaWuu7GA/LGtNj4i+yi6EZ3OIq3hnz3+QY2dXL7O1pieMLjfZRf98KHucUhiokaq61cXSo+RJa3yuixaJMSxJVD1myx/XNritkawUfI8Xwp6g6KWYQAzDYz3MIWbA5LyX9Q0jk3yXTRAQOjLwvL8ZK/InJCDoPBFWFJwrz+Wxgep+I8iYoijOhqTUBQ==\"\n```", "```\n.PHONY: build\n.ONESHELL:\n\nbuild:\n  @ $(if $(AWS_PROFILE),$(call assume_role))\n  packer build packer.json\n\n# Dynamically assumes role and injects credentials into environment\ndefine assume_role\n  export AWS_DEFAULT_REGION=$$(aws configure get region)\n  eval $$(aws sts assume-role --role-arn=$$(aws configure get role_arn) \\\n    --role-session-name=$$(aws configure get role_session_name) \\\n    --query \"Credentials.[ \\\n        [join('=',['export AWS_ACCESS_KEY_ID',AccessKeyId])], \\\n        [join('=',['export AWS_SECRET_ACCESS_KEY',SecretAccessKey])], \\\n        [join('=',['export AWS_SESSION_TOKEN',SessionToken])] \\\n      ]\" \\\n    --output text)\nendef\n```", "```) as an alternative syntax for bash command substitutions. In other words, `$(command)` and ``command`` both represent command substitutions that will execute the command and return the output.\n\n# Building the image\n\nNow that we have a mechanism of automating the generation of temporary session credentials, assuming that your `packer.json` file and Makefile are in the root of your packer-ecs repository, let's test out building your Packer image by running `make build`:\n\n```", "```\n\nRunning a Packer build\n\nReferring back to the previous example and the output of the preceding one, notice in the `build` task that the command to build a Packer image is simply `packer build <template-file>`, which in this case is `packer build packer.json`.\n\nIf you review the output of the preceding example, you can see the following steps are performed by Packer:\n\n*   Packer initially validates the source AMI and then generates a temporary SSH key pair and security group so that it is able to access the temporary EC2 instance.\n*   Packer launches a temporary EC2 instance from the source AMI and then waits until it is able to establish SSH access.\n*   Packer executes the provisioning actions as defined in the provisioners section of the template. In this case, you can see the output of the yum `update` command, which is our current single provisioning action.\n*   Once complete, Packer stops the instance and creates a snapshot of the EBS volume instance, which produces an AMI with an appropriate name and ID.\n*   With the AMI created, Packer terminates the instance, deletes the temporary SSH key pair and security group, and outputs the new AMI ID.\n\nRecall in the earlier example, that you added a manifest post-processor to your template, and you should find a file called `manifest.json` has been output at the root of your repository, which you typically would not want to commit to your **packer-ecs** repository:\n\n```", "```\n\nViewing the Packer build manifest\n\n# Building custom ECS container instance images using Packer\n\nIn the previous section, you established a base template for building a custom AMI using Packer, and proceed to build and publish your first custom AMI. At this point, you have not performed any customization that is specific to the use case of provisioning ECS container instances, so this section will focus on enhancing your Packer template to include such customizations.\n\nThe customizations you will learn about now include the following:\n\n*   Defining a custom storage configuration\n*   Installing additional packages and configuring operating system settings\n*   Configuring a cleanup script\n*   Creating a first-run script\n\nWith these customizations in place, we will complete the chapter by building your final custom ECS Container Instance AMI and launching an instance to verify the various customizations.\n\n# Defining a custom storage configuration\n\nThe AWS ECS-Optimized AMI includes a default storage configuration that uses a 30 GB EBS volume, which is partitioned as follows:\n\n*   `/dev/xvda`: An 8 GB volume that is mounted as the root filesystem and serves as the operating system partition.\n*   `dev/xvdcz`: A 22 GB volume that is configured as a logical volume management (LVM) device and is used for Docker image and metadata storage.\n\nThe ECS-Optimized AMI uses the devicemapper storage driver for Docker image and metadata storage, which you can learn more about at [https://docs.docker.com/v17.09/engine/userguide/storagedriver/device-mapper-driver/](https://docs.docker.com/storage/storagedriver/device-mapper-driver/).\n\nFor most use cases, this storage configuration should be sufficient, however there are a couple of scenarios in which you may want to modify the default configuration:\n\n*   **You need more Docker image and metadata storage**: This is easily addressed by simply configuring your ECS container instances with a larger volume size. The default storage configuration will always reserve 8GB for the operating system and root filesystem, with the remainder of the storage allocated for Docker image and metadata storage.\n*   **You need to support Docker volumes that have large storage requirements**: By default, the ECS-Optimized AMI stores Docker volumes at `/var/lib/docker/volumes`, which is part of the root filesystem on the 8GB `/dev/xvda` partition. If you have larger volume requirements, this can cause your operating system partition to quickly become full, so in this scenario you should separate out the volume storage to a separate EBS volume.\n\nLet's now see how you can modify your Packer template to add a new dedicated volume for Docker volume storage and ensure this volume is mounted correctly on instance creation.\n\n# Adding EBS volumes\n\nTo add an EBS volume to your custom AMIs, you can configure the `launch_block_device_mappings` parameter within the Amazon EBS builder:\n\n```", "```\n\nAdding a launch block device mapping\n\nIn the preceding example, I have truncated other portions of the Packer template for brevity, and you can see that we have added a single 20 GB volume called `/dev/xvdcy`, which is configured to be destroyed on instance termination. Notice that the `volume_type` parameter is set to `gp2`, which is the general-purpose SSD storage type that typically offers the best overall price/performance in AWS.\n\n# Formatting and mounting volumes\n\nWith the configuration of the preceding example in place, we next need to format and mount that new volume. Because we used the `launch_block_device_mappings` parameter (as opposed to the `ami_block_device_mappings` parameter), the block device is actually attached at image build time (the latter parameter is attached upon image creation only) and we can perform all formatting and mount configuration settings at build time.\n\nTo perform this configuration, we will add a shell provisioner that references a file called `scripts/storage.sh` to your Packer template:\n\n```", "```\n\nAdding a shell provisioner for configuring storage\n\nThe referenced script is expressed as a path relative to the the Packer template, so you now need to create this script:\n\n```", "```\n\nCreating a scripts folder\n\nWith the script file in place, you can now define the various shell provisioning actions in this script as demonstrated in the following example:\n\n```", "```\n\nStorage provisioning script\n\nAs you can see in the preceding example, the script is a regular bash script, and it's important to always set the error flag for all of your Packer shell scripts (`set -e`), which ensures the script will exit with an error code should any command fail within the script.\n\nYou first create a folder called `/data`, which you will use to store Docker volumes, and then format the `/dev/xvdcy` device you attached earlier in the earlier example with the `.ext4` filesystem, and attach a label called `docker`, which makes mount operations simpler to perform. The next `echo` command adds an entry to the `/etc/fstab` file, which defines all filesystem mounts that will be applied at boot, and notice that you must pipe the `echo` command through to `sudo tee -a /etc/fstab`, which appends the `echo` output to the `/etc/fstab` file with the correct sudo privileges.\n\nFinally, you auto-mount the new entry in the `/etc/fstab` file by running the `mount -a` command, which although not required at image build time, is a simple way to verify that the mount is actually configured correctly (if not, this command will fail and the resulting build will fail).\n\n# Installing additional packages and configuring system settings\n\nThe next customizations you will perform are to install additional packages and configuring system settings.\n\n# Installing additional packages\n\nThere are a few additional packages that we need to install into our custom ECS container instance, which include the following:\n\n*   **CloudFormation helper scripts**: When you use CloudFormation to deploy your infrastructure, AWS provide a set of CloudFormation helper scripts, collectively referred to as **cfn-bootstrap**, that work with CloudFormation to obtain initialization metadata that allows you to perform custom initialization tasks at instance-creation time, and also signal CloudFormation when the instance has successfully completed initialization. We will explore the benefits of this approach in later chapters, however, for now you need to ensure these helper scripts are present in your custom ECS container-instance image.\n*   **CloudWatch logs agent**: The AWS CloudWatch logs service offers central storage of logs from various sources, including EC2 instances, ECS containers, and other AWS services. To ship your ECS container instance (EC2 instance) logs to CloudWatch logs, you must install the CloudWatch logs agent locally, which you can then use to forward various system logs, including operating system, Docker, and ECS agent logs.\n*   **`jq` utility**: The `jq` utility ([https://stedolan.github.io/jq/manual/](https://stedolan.github.io/jq/manual/)) is handy for parsing JSON output, and you will need this utility later on in this chapter when you define a simple health check that verifies the ECS container instance has joined to the configured ECS cluster.\n\nInstalling these additional packages is very straightforward, and can be achieved by modifying the inline shell provisioner you created earlier:\n\n```", "```\n\nInstalling additional operating system packages\n\nAs you can see in the preceding example, each of the required packages can be easily installed using the `yum` package manager.\n\n# Configuring system settings\n\nThere are a few minor system settings that you need to make to your custom ECS container instance:\n\n*   Configure time-zone settings\n*   Modify default cloud-init behavior\n\n# Configuring timezone settings\n\nEarlier, you defined a variable called `timezone`, which so far you have not referenced in your template. You can use this variable to configure the timezone of your custom ECS container instance image.\n\nTo do this, you first need to add a new shell provisioner into your Packer template:\n\n```", "```\n\nAdding a provisioner to configure time settings\n\nIn the preceding example, we reference a script called `scripts/time.sh`, which you will create shortly, but notice that we also include a parameter called `environment_vars`, which allows you to inject your Packer variables (`timezone` in this example) as environment variables into your shell provisioning scripts.\n\nThe following example shows the required `scripts/time.sh` script that is referenced in the new Packer template provisioning task:\n\n```", "```\n\nTime settings provisioning script\n\nIn the preceding example, you first configure the [AWS recommended settings for configuring time](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html), configuring the `/etc/sysconfig/clock` file with the configured `TIMEZONE` environment variable, creating the symbolic `/etc/localtime` link, and finally ensuring the `ntpd` service is configured to use the [AWS NTP sync](https://aws.amazon.com/blogs/aws/keeping-time-with-amazon-time-sync-service/) service and start at instance boot.\n\nThe AWS NTP sync service is a free AWS service that provides an NTP server endpoint at the `169.254.169.123` local address, ensuring your EC2 instances can obtain accurate time without having to traverse the network or internet.\n\n# Modifying default cloud-init behavior\n\ncloud-init is a standard set of utilities for performing initialization of cloud images and associated instances. The most popular feature of cloud-init is the user-data mechanism, which is a simple means of running your own custom initialization commands at instance creation.\n\ncloud-init is also used in the ECS-Optimized AMI to perform automatic security patching at instance creation, and although this sounds like a useful feature, it can cause problems, particularly in environments where your instances are located in private subnets and require an HTTP proxy to communicate with the internet.\n\nThe issue with the cloud-init security mechanism is that although it can be configured to work with an HTTP proxy by setting proxy environment variables, it is invoked prior to when userdata is executed, leading to a chicken-and-egg scenario where you have no option but to disable automated security patching if you are using a proxy.\n\nTo disable this mechanism, you first need to configure a new shell provisioner in your Packer template:\n\n```", "```\n\nAdding a provisioner to configure cloud-init settingsThe referenced `scripts/cloudinit.sh` script can now be created as follows:\n\n```", "```\n\nDisabling security updates for cloud-init\n\nIn the following example, the rather scary-looking `sed` expressions will either add or replace lines beginning with `repo_update` and `repo_upgrade` in the `/etc/cloud/cloud.cfg` cloud-init configuration file and ensure they are set to `false` and `none`, respectively.\n\n# Configuring a cleanup script\n\nAt this point, we have performed all required installation and configuration shell provisioning tasks. We will create one final shell provisioner, a cleanup script, which will remove any log files created while the instance used to build the custom image was running and to ensure the machine image is in a state ready to be launched.\n\nYou first need to add a shell provisioner to your Packer template that references the `scripts/cleanup.sh` script:\n\n```", "```\n\nAdding a provisioner to clean up the Image\n\nWith the provisioner defined in the Packer template, you next need to create the cleanup script, as defined here:\n\n```", "```\n\nCleanup script\n\nIn the following example, notice you don't execute the command `set -e`, given this is a cleanup script that you are not too worried about if there is an error and you don't want your build to fail should a service already be stopped. The ECS agent is first stopped, with the `docker system prune` command used to clear any ECS container state that may be present, and next the Docker service is stopped and then disabled using the `chkconfig` command. The reason for this is that on instance creation, we will always invoke a first-run script that will perform initial configuration of the instance and requires the Docker service to be stopped. Of course this means that once the first-run script has completed its initial configuration, it will be responsible for ensuring the Docker service is both started and enabled to start on boot.\n\nFinally, the cleanup script removes any Docker and ECS agent log files that may have been created during the short period the instance was up during the custom machine-image build process.\n\n# Creating a first-run script\n\nThe final set of customizations we will apply to your custom ECS container instance image is to create a first-run script, which will be responsible for performing runtime configuration of your ECS container instance at instance creation, by performing the following tasks:\n\n*   Configuring ECS cluster membership\n*   Configuring HTTP proxy support\n*   Configuring the CloudWatch logs agent\n*   Starting required services\n*   Performing health checks\n\nTo provision the first-run script, you need to define a file provisioner task in your Packer template, as demonstrated here:\n\n```", "```\n\nAdding a file provisioner\n\nNotice that the provisioner type is configured as `file`, and specifies a local source file that needs to be located in `files/firstrun.sh`. The `destination` parameter defines the location within the AMI where the first-run script will be located. Note that the file provisioner task copies files as the `ec2-user` user, hence it has limited permissions as to where this script can be copied.\n\n# Configuring ECS cluster membership\n\nYou can now create the first-run script at the files/firstrun.sh location referenced by your Packer template. Before you get started configuring this file, it is important to bear in mind that the first-run script is designed to be run at initial boot of an instance created from your custom machine image, so you must consider this when configuring the various commands that will be executed.\n\nWe will first create configure the ECS agent to join the ECS cluster that the ECS container instance is intended to join, as demonstrated in the following example:\n\n```", "```\n\nConfiguring ECS cluster membership\n\nBack in [Chapter 5](a00edb3f-1989-4e3c-8835-b99bb4a1b582.xhtml), *Publishing Docker Images using ECR*, you saw how the ECS cluster wizard configured ECS container instances using this same approach, although one difference is that the script is expecting an environment variable called `ECS_CLUSTER` to be configured in the environment, as designated by the `${ECS_CLUSTER}` expression. Rather than hardcode the ECS cluster name, which would make the first-run script very inflexible, the idea here is that the configuration being applied to a given instance defines the `ECS_CLUSTER` environment variable with the correct cluster name, meaning the script is reusable and can be configured with any given ECS cluster.\n\n# Configuring HTTP proxy support\n\nA common security best practice is to place your ECS container instances in private subnets, meaning they are located in subnets that possess no default route to the internet. This approach makes it more difficult for attackers to compromise your systems, and even if they do, provides a means to restrict what information they can transmit back to the internet.\n\nDepending on the nature of your application, you typically will require your ECS container instances to be able to connect to the internet, and using an HTTP proxy provides an effective mechanism to provide such access in a controlled manner with Layer 7 application-layer inspection capabilities.\n\nRegardless of the nature of your application, it is important to understand that ECS container instances require internet connectivity for the following purposes:\n\n*   ECS agent control-plane and management-plane communications with ECS\n*   Docker Engine communication with ECR and other repositories for downloading Docker images\n*   CloudWatch logs agent communication with the CloudWatch logs service\n*   CloudFormation helper-script communication with the CloudFormation service\n\nAlthough configuring a complete end-to-end proxy solution is outside the scope of this book, it is useful to understand how you can customize your ECS container instances to use an HTTP proxy, as demonstrated in the following example:\n\n```", "```\n\nConfiguring HTTP proxy support\n\nIn the preceding example, the script checks for the existence of a non-empty environment variable called `PROXY_URL`, and if present proceeds to configure proxy settings for various components of the ECS container instance:\n\n*   Docker Engine: Configured via `/etc/sysconfig/docker`\n*   ECS agent: Configured via `/etc/ecs/ecs.config`\n*   CloudWatch logs agent: Configured via `/etc/awslogs/proxy.conf`\n\nNotice that in some cases you need to configure the `NO_PROXY` setting, which disables proxy communications for the following IP addresses:\n\n*   `169.254.169.254`: This is a special local address that is used to communicate with the EC2 metadata service to obtain instance metadata, such as EC2 instance role credentials.\n*   `169.254.170.2`: This is a special local address that is used to obtained ECS task credentials.\n\n# Configuring the CloudWatch logs agent\n\nThe next configuration task that you will perform in the first-run script is to configure the CloudWatch logs agent. On an ECS container instance, the CloudWatch logs agent is responsible for collecting system logs, such as operating system, Docker, and ECS agent logs.\n\nNote that this agent is NOT required to implement CloudWatch logs support for your Docker containers - this is already implemented within the Docker Engine via the `awslogs` logging driver.\n\nConfiguring the CloudWatch logs agent requires you to perform the following configuration tasks:\n\n*   **Configure the correct AWS region**: For this task, you will inject the value of an environment variable called `AWS_DEFAULT_REGION` and write this to the `/etc/awslogs/awscli.conf` file.\n*   **Define the various log group and log stream settings that the CloudWatch logs agent will log to**: For this task, you will define the recommended set of log groups for ECS container instances, which is described at [https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html#configure_cwl_agent](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html#configure_cwl_agent)\n\nThe following example demonstrates the required configuration:\n\n```", "```\n\nConfiguring the CloudWatch logs agent\n\nYou can see that the first-run script includes references to environment variables in the `log_group_name` parameter for each defined log group, which helps ensure unique log group naming in your AWS account:\n\n*   `STACK_NAME`: The name of the CloudFormation stack\n*   `AUTOSCALING_GROUP`: The name of the Autoscaling Group\n\nAgain, these environment variables must be injected at instance creation to the first-run script, so bear this in mind for future chapters when we will learn how to do this.\n\nOne other point to note in the preceding example is the value of each `log_stream_name` parameter - this is set to a special variable called `{instance_id}`, which the CloudWatch logs agent will automatically configure with the EC2 instance ID of the instance.\n\nThe end result is that you will get several log groups for each type of log, which are scoped to the context of a given CloudFormation stack and EC2 auto scaling group, and within each log group, a log stream for each ECS container instance will be created, as illustrated in the following diagram:\n\n![](assets/a199eec7-ad83-4e86-bd90-ac0febe0df2f.png)CloudWatch logs group configuration for ECS container instances\n\n# Starting required services\n\nRecall in the previous examples, that you added a cleanup script as part of the image-build process, which disables the Docker Engine service from starting on boot. This approach allows you to perform required initialization tasks prior to starting the Docker Engine, and at this point in the first-run script we are ready to start the Docker Engine and other important system services:\n\n```", "```\n\nStarting services\n\nIn the preceding example, note that I have omitted the earlier parts of the first-run script for brevity. Notice that you first start the awslogs service, which ensures the CloudWatch logs agent will pick up all Docker Engine logs, and then proceed to enable Docker to start on boot, start Docker, and finally start the ECS agent.\n\n# Performing required health checks\n\nThe final task we need to perform in the first-run script is a health check, which ensures the ECS container instance has initialized and successfully registered to the configured ECS cluster. This is a reasonable health check for your ECS container instances, given the ECS agent can only run if the Docker Engine is operational, and the ECS agent must be registered with the ECS cluster in order to deploy your applications.\n\nRecall in the previous chapter, when you examined the internals of an ECS container instance that the ECS agent exposes a local HTTP endpoint that can be queried for current ECS agent status. You can use this endpoint to create a very simple health check, as demonstrated here:\n\n```", "```\n\nPerforming a health check\n\nIn the preceding example, a bash `until` loop is configured, which uses curl to query the `http://localhost:51678/v1/metadata` endpoint every five seconds. The output of this command is piped through to `jq`, which will either return the Cluster property or an empty value if this property is not present. Once the ECS agent registers to the correct ECS cluster and returns this property in the JSON response, the loop will complete and the first-run script will complete.\n\n# Testing your custom ECS container instance image\n\nYou have now completed all customizations and it is time to rebuild your image using the `packer build` command. Before you do this, now is a good time to verify you have the correct Packer template in place, and also have created the associated supporting files. The following example shows the folder and file structure you should now have in your packer-ecs repository:\n\n```", "```\n\nVerifying the Packer repository\n\nAssuming everything is in place, you can now run your Packer build once again by running the `make build` command.\n\nOnce everything is complete and your AMI has been successfully created, you can now view your AMI in the AWS console by navigating to **Services** | **EC2** and selecting AMIs from the menu on the left:\n\n![](assets/286a5d2f-7e98-467b-a671-5f3ad55eaf24.png)EC2 dashboard AMIs\n\nIn the preceding screenshot, you can see the two AMIs you built earlier in this chapter and just now. Notice that the most recent AMI now includes three block devices, with `/dev/xvdcy` representing the additional 20 GB gp2 volume you added earlier in this chapter.\n\nAt this point, you can actually test out your AMI by clicking on the **Launch** button, which will start the EC2 Instance Wizard.\u00a0After clicking the **Review and Launch** button, click on the **Edit security groups** link to grant your IP address access via SSH to the instance, as shown in the following screenshot:\n\n![](assets/c0fd0ffb-bc0b-4026-b9c0-9d8232b143cd.png)Launching a new EC2 instance\n\nOnce complete, click on **Review and Launch**, then click the **Launch** button, and finally configure an appropriate SSH key pair that you have access to.\n\nOn the launching instance screen, you can now click the link to your new EC2 instance, and copy the public IP address so that you can SSH to the instance, as shown in the following screenshot:\n\n![](assets/99e70e6b-1183-437c-bf3b-7d2c04fde547.png)Connecting to a new EC2 instance\n\nOnce you have connected to the instance, you can verify that the additional 20 GB volume you configured for Docker volume storage has been successfully mounted:\n\n```", "```\n\nVerifying storage mounts\n\nYou can check the timezone is configured correctly by running the `date` command, which should display the correct timezone (US/Eastern), and also verify the `ntpd` service is running:\n\n```", "```\n\nVerifying time settings\n\nNext, you can verify the cloud-init configuration has been configured to disable security updates, by viewing the `/etc/cloud/cloud.cfg` file:\n\n```", "```\n\nVerifying cloud-init settings\n\nYou should also verify that the Docker service is stopped and was disabled on boot, as per the cleanup script you configured:\n\n```", "```\n\nVerifying disabled services\n\nFinally, you can verify that the first-run script is present in the `ec2-user` home directory:\n\n```"]