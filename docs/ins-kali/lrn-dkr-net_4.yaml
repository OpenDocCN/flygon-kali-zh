- en: Chapter 4. Networking in a Docker Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。Docker集群中的网络
- en: In this chapter, you will learn how Docker containers are networked when using
    frameworks like Kubernetes, Docker Swarm, and Mesosphere.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习在使用Kubernetes、Docker Swarm和Mesosphere等框架时，Docker容器是如何进行网络化的。
- en: 'We will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Docker Swarm
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Networked containers in a Kubernetes cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes集群中的网络化容器
- en: How Kubernetes networking differs from Docker networking
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络与Docker网络的不同之处
- en: Kubernetes on AWS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS上的Kubernetes
- en: Mesosphere
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesosphere
- en: Docker Swarm
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: Docker Swarm is a native clustering system for Docker. Docker Swarm exposes
    the standard Docker API so that any tool that communicates with the Docker daemon
    can communicate with Docker Swarm as well. The basic aim is to allow the creation
    and usage of a pool of Docker hosts together. The cluster manager of Swarm schedules
    the containers based on the availability resources in a cluster. We can also specify
    the constrained resources for a container while deploying it. Swarm is designed
    to pack containers onto a host by saving other host resources for heavier and
    bigger containers rather than scheduling them randomly to a host in the cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm是Docker的本地集群系统。Docker Swarm公开标准的Docker API，以便与Docker守护程序通信的任何工具也可以与Docker
    Swarm通信。基本目标是允许一起创建和使用一组Docker主机。Swarm的集群管理器根据集群中的可用资源调度容器。我们还可以在部署容器时指定受限资源。Swarm旨在通过将容器打包到主机上来保存其他主机资源，以便为更重和更大的容器而不是将它们随机调度到集群中的主机。
- en: Similar to other Docker projects, Docker Swarm uses a Plug and Play architecture.
    Docker Swarm provides backend services to maintain a list of IP addresses in your
    Swarm cluster. There are several services, such as etcd, Consul, and Zookeeper;
    even a static file can be used. Docker Hub also provides a hosted discovery service,
    which is used in the normal configuration of Docker Swarm.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他Docker项目类似，Docker Swarm使用即插即用架构。Docker Swarm提供后端服务来维护您的Swarm集群中的IP地址列表。有几种服务，如etcd、Consul和Zookeeper；甚至可以使用静态文件。Docker
    Hub还提供托管的发现服务，用于Docker Swarm的正常配置。
- en: 'Docker Swarm scheduling uses multiple strategies in order to rank nodes. When
    a new container is created, Swarm places it on the node on the basis of the highest
    computed rank, using the following strategies:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm调度使用多种策略来对节点进行排名。当创建新容器时，Swarm根据最高计算出的排名将其放置在节点上，使用以下策略：
- en: '**Spread**: This optimizes and schedules the containers on the nodes based
    on the number of containers running on the node at that point of time'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Spread**：这根据节点上运行的容器数量来优化和调度容器'
- en: '**Binpack**: The node is selected to schedule the container on the basis of
    CPU and RAM utilization'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Binpack**：选择节点以基于CPU和RAM利用率来调度容器'
- en: '**Random strategy**: This uses no computation; it selects the node randomly
    to schedule containers'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机策略**：这不使用计算；它随机选择节点来调度容器'
- en: 'Docker Swarm also uses filters in order to schedule containers, such as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm还使用过滤器来调度容器，例如：
- en: '**Constraints**: These use key/value pairs associated with nodes, such as `environment=production`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Constraints**：这些使用与节点关联的键/值对，例如`environment=production`'
- en: '**Affinity filter**: This is used to run a container and instruct it to locate
    and run next to another container based on the label, image, or identifier'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亲和力过滤器**：这用于运行一个容器，并指示它基于标签、镜像或标识符定位并运行在另一个容器旁边'
- en: '**Port filter**: In this case, the node is selected on the basis of the ports
    available on it'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端口过滤器**：在这种情况下，选择节点是基于其上可用的端口'
- en: '**Dependency filter**: This co-schedules dependent containers on the same node'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖过滤器**：这会在同一节点上协同调度依赖容器'
- en: '**Health filter**: This prevents the scheduling of containers on unhealthy
    nodes'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健康过滤器**：这可以防止在不健康的节点上调度容器'
- en: 'The following figure explains various components of a Docker Swarm cluster:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图解释了Docker Swarm集群的各个组件：
- en: '![Docker Swarm](../images/00025.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Docker Swarm](../images/00025.jpeg)'
- en: Docker Swarm setup
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker Swarm设置
- en: Let's set up our Docker Swarm setup, which will have two nodes and one master.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置我们的Docker Swarm设置，其中将有两个节点和一个主节点。
- en: We will be using a Docker client in order to access the Docker Swarm cluster.
    A Docker client can be set up on a machine or laptop and should have access to
    all the machines present in the Swarm cluster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Docker客户端来访问Docker Swarm集群。Docker客户端可以在一台机器或笔记本上设置，并且应该可以访问Swarm集群中的所有机器。
- en: 'After installing Docker on all three machines, we will restart the Docker service
    from a command line so that it can be accessed from TCP port 2375 on the localhost
    (`0.0.0.0:2375`) or from a specific host IP address and can allow connections
    using a Unix socket on all the Swarm nodes, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有三台机器上安装Docker后，我们将从命令行重新启动Docker服务，以便可以从本地TCP端口2375（`0.0.0.0:2375`）或特定主机IP地址访问，并且可以使用Unix套接字在所有Swarm节点上允许连接，如下所示：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Docker Swarm images are required to be deployed as Docker containers on the
    master node. In our example, the master node''s IP address is `192.168.59.134`.
    Replace it with your Swarm''s master node. From the Docker client machine, we
    will be installing Docker Swarm on the master node using the following command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm镜像需要部署为Docker容器在主节点上。在我们的示例中，主节点的IP地址是`192.168.59.134`。请将其替换为您的Swarm主节点。从Docker客户端机器上，我们将使用以下命令在主节点上安装Docker
    Swarm：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Swarm token generated after the execution of the command should be noted,
    as it will be used for the Swarm setup. In our case, it is this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行命令后生成的Swarm令牌应予以注意，因为它将用于Swarm设置。在我们的案例中，它是这样的：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following are the steps to set up a two-node Docker Swarm cluster:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置两节点Docker Swarm集群的步骤：
- en: 'From the Docker client node, the following `docker` command is required to
    be executed with Node 1''s IP address (in our case, `192.168.59.135`) and the
    Swarm token generated in the preceding code in order to add it to the Swarm cluster:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Docker客户端节点，需要执行以下`docker`命令，使用Node 1的IP地址（在我们的案例中为`192.168.59.135`）和在前面的代码中生成的Swarm令牌，以便将其添加到Swarm集群中：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Repeat the preceding steps for Node 2 by replacing Node 1's IP address with
    Node 2's.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过用Node 2的IP地址替换Node 1的IP地址，重复上述步骤来为Node 2执行相同的操作。
- en: 'Swarm manager is required to be set up on the master node using the following
    command on the Docker client node:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要在Docker客户端节点上使用以下命令在主节点上设置Swarm管理器：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Swarm cluster is set up and can be managed using the Swarm manager residing
    on the master node. To list all the nodes, the following command can be executed
    using a Docker client:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm集群已设置，并且可以使用驻留在主节点上的Swarm管理器进行管理。要列出所有节点，可以使用Docker客户端执行以下命令：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following command can be used to get information about the cluster:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令可用于获取有关集群的信息：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The test `ubuntu` container can be launched onto the cluster by specifying
    the name as `swarm-ubuntu` and using the following command:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过指定名称为`swarm-ubuntu`并使用以下命令，在集群上启动测试`ubuntu`容器：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The container can be listed using the Swarm master''s IP address:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用Swarm主节点的IP地址列出容器：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This completes the setup of a two-node Docker Swarm cluster.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就完成了两节点Docker Swarm集群的设置。
- en: Docker Swarm networking
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker Swarm网络设置
- en: Docker Swarm networking has integration with libnetwork and even provides support
    for overlay networks. libnetwork provides a Go implementation to connect containers;
    it is a robust container network model that provides network abstraction for applications
    and the programming interface of containers. Docker Swarm is now fully compatible
    with the new networking model in Docker 1.9 (note that we will be using Docker
    1.9 in the following setup). The key-value store is required for overlay networks,
    which includes discovery, networks, IP addresses, and more information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm网络与libnetwork集成，甚至支持覆盖网络。libnetwork提供了一个Go实现来连接容器；它是一个强大的容器网络模型，为应用程序和容器的编程接口提供网络抽象。Docker
    Swarm现在完全兼容Docker 1.9中的新网络模型（请注意，我们将在以下设置中使用Docker 1.9）。覆盖网络需要键值存储，其中包括发现、网络、IP地址和更多信息。
- en: 'In the following example, we will be using Consul to understand Docker Swarm
    networking in a better way:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用Consul更好地了解Docker Swarm网络：
- en: 'We will provision a VirtualBox machine called `sample-keystore` using `docker-machine`:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`docker-machine`提供一个名为`sample-keystore`的VirtualBox机器：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will also deploy the `progrium/consul` container on the `sample-keystore`
    machine on port `8500` with the following command:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将在`sample-keystore`机器上使用以下命令在端口`8500`部署`progrium/consul`容器：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Set the local environment to the `sample-keystore` machine:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本地环境设置为`sample-keystore`机器：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can list the consul container as follows:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以按以下方式列出consul容器：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a Swarm cluster using `docker-machine`. The two machines can be created
    in VirtualBox; one can act as the Swarm master. As we create each Swarm node,
    we will be passing the options required for Docker Engine to have an overlay network
    driver:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`docker-machine`创建Swarm集群。两台机器可以在VirtualBox中创建；一台可以充当Swarm主节点。在创建每个Swarm节点时，我们将传递Docker
    Engine所需的选项以具有覆盖网络驱动程序：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The use of the parameters used in the preceding command is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中使用的参数如下：
- en: '`--swarm`: This is used to configure a machine with Swarm.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--swarm`：用于配置具有Swarm的机器。'
- en: '`--engine-opt`: This option is used to define arbitrary daemon options required
    to be supplied. In our case, we will supply the engine daemon with the `--cluster-store`
    option during creation time, which tells the engine the location of the key-value
    store for the overlay network usability. The `--cluster-advertise` option will
    put the machine on the network at the specific port.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--engine-opt`：此选项用于定义必须提供的任意守护程序选项。在我们的情况下，我们将在创建时使用`--cluster-store`选项，告诉引擎覆盖网络可用性的键值存储的位置。`--cluster-advertise`选项将在特定端口将机器放入网络中。'
- en: '`--swarm-discovery`: It is used to discover services to use with Swarm, in
    our case, `consul` will be that service.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--swarm-discovery`：用于发现与Swarm一起使用的服务，在我们的情况下，`consul`将是该服务。'
- en: '`--swarm-master`: This is used to configure a machine as the Swarm master.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--swarm-master`：用于将机器配置为Swarm主节点。'
- en: 'Another host can also be created and added to Swarm cluster, like this:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还可以创建另一个主机并将其添加到Swarm集群，就像这样：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The machines can be listed as follows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以按以下方式列出机器：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we will set the Docker environment to `swarm-master`:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将将Docker环境设置为`swarm-master`：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following command can be executed on the master in order to create the
    overlay network and have multihost networking:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以在主节点上执行以下命令以创建覆盖网络并实现多主机网络：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The network bridge can be checked on the master using the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下命令在主节点上检查网络桥：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When switching to a Swarm node, we can easily list the newly created overlay
    network, like this:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到Swarm节点时，我们可以轻松地列出新创建的覆盖网络，就像这样：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once the network is created, we can start the container on any of the hosts,
    and it will be part of the network:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建网络后，我们可以在任何主机上启动容器，并且它将成为网络的一部分：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Start the sample `ubuntu` container with the constraint environment set to
    the first node:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用约束环境设置为第一个节点启动示例`ubuntu`容器：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can check using the `ifconfig` command that the container has two network
    interfaces, and it will be accessible from the container deployed using Swarm
    manager on any other host.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`ifconfig`命令检查容器是否有两个网络接口，并且可以通过Swarm管理器部署的容器在任何其他主机上都可以访问。
- en: Kubernetes
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is a container cluster management tool. Currently, it supports Docker
    and Rocket. It is an open source project supported by Google, and the project
    was launched in June 2014 at Google I/O. It supports deployment on various cloud
    providers such as GCE, Azure, AWS, and vSphere as well as on bare metal. The Kubernetes
    manager is lean, portable, extensible, and self-healing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个容器集群管理工具。目前，它支持Docker和Rocket。这是一个由Google支持的开源项目，于2014年6月在Google
    I/O上推出。它支持在各种云提供商上部署，如GCE、Azure、AWS和vSphere，以及在裸机上部署。Kubernetes管理器是精简的、可移植的、可扩展的和自愈的。
- en: 'Kubernetes has various important components, as explained in the following
    list:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes有各种重要组件，如下列表所述：
- en: '**Node**: This is a physical or virtual-machine part of a Kubernetes cluster,
    running the Kubernetes and Docker services, onto which pods can be scheduled.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Node**：这是Kubernetes集群的物理或虚拟机部分，运行Kubernetes和Docker服务，可以在其上调度pod。'
- en: '**Master**: This maintains the runtime state of the Kubernetes server runtime.
    It is the point of entry for all the client calls to configure and manage Kubernetes
    components.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Master**：这维护Kubernetes服务器运行时的运行状态。这是所有客户端调用的入口点，用于配置和管理Kubernetes组件。'
- en: '**Kubectl**: This is the command-line tool used to interact with the Kubernetes
    cluster to provide master access to Kubernetes APIs. Through it, the user can
    deploy, delete, and list pods.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubectl**：这是用于与Kubernetes集群交互的命令行工具，以提供对Kubernetes API的主访问权限。通过它，用户可以部署、删除和列出pod。'
- en: '**Pod**: This is the smallest scheduling unit in Kubernetes. It is a collection
    of Docker containers that share volumes and don''t have port conflicts. It can
    be created by defining a simple JSON file.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod**：这是Kubernetes中最小的调度单元。它是一组共享卷且没有端口冲突的Docker容器集合。可以通过定义一个简单的JSON文件来创建它。'
- en: '**Replication controller**: It manages the lifecycle of a pod and ensures that
    a specified number of pods are running at a given time by creating or killing
    pods as required.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制控制器**：它管理pod的生命周期，并确保在给定时间运行指定数量的pod，通过根据需要创建或销毁pod。'
- en: '**Label**: Labels are used to identify and organize pods and services based
    on key-value pairs.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：标签用于基于键值对识别和组织pod和服务。'
- en: 'The following diagram shows the Kubernetes Master/Minion flow:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Kubernetes Master/Minion流程：
- en: '![Kubernetes](../images/00026.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes](../images/00026.jpeg)'
- en: Deploying Kubernetes on AWS
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS上部署Kubernetes
- en: 'Let''s get started with Kubernetes cluster deployment on AWS, which can be
    done by using the config file that already exists in the Kubernetes codebase:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始在AWS上部署Kubernetes集群，可以使用Kubernetes代码库中已经存在的配置文件来完成：
- en: Log in to AWS Console at [http://aws.amazon.com/console/](http://aws.amazon.com/console/).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[http://aws.amazon.com/console/](http://aws.amazon.com/console/)上登录AWS控制台。
- en: Open the IAM console at [https://console.aws.amazon.com/iam/home?#home](https://console.aws.amazon.com/iam/home?#home).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[https://console.aws.amazon.com/iam/home?#home](https://console.aws.amazon.com/iam/home?#home)上打开IAM控制台。
- en: Choose the IAM username, select the **Security Credentials** tab, and click
    on the **Create Access Key** option.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择IAM用户名，选择**安全凭证**选项卡，然后单击**创建访问密钥**选项。
- en: After the keys have been created, download and keep them in a secure place.
    The downloaded `.csv` file will contain an `Access Key ID` and `Secret Access
    Key`, which will be used to configure the AWS CLI.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建密钥后，下载并保存在安全的地方。下载的`.csv`文件将包含`访问密钥ID`和`秘密访问密钥`，这将用于配置AWS CLI。
- en: 'Install and configure the AWS CLI. In this example, we have installed AWS CLI
    on Linux using the following command:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并配置AWS CLI。在本例中，我们使用以下命令在Linux上安装了AWS CLI：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In order to configure the AWS CLI, use the following command:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置AWS CLI，请使用以下命令：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After configuring the AWS CLI, we will create a profile and attach a role to
    it with full access to S3 and EC2:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置AWS CLI后，我们将创建一个配置文件并附加一个角色，该角色具有对S3和EC2的完全访问权限：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The role can be created separately using the console or AWS CLI with a JSON
    file that defines the permissions the role can have:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用控制台或AWS CLI单独创建角色，并使用定义角色权限的JSON文件创建角色：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A role can be attached to the preceding profile, which will have complete access
    to EC2 and S3, as shown in the following screenshot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将角色附加到上述配置文件，该配置文件将完全访问EC2和S3，如下截图所示：
- en: '![Deploying Kubernetes on AWS](../images/00027.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![在AWS上部署Kubernetes](../images/00027.jpeg)'
- en: 'After the creation of the role, it can be attached to a policy using the following
    command:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建角色后，可以使用以下命令将其附加到策略：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'By default, the script uses the default profile. We can change it as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，脚本使用默认配置文件。我们可以按照以下方式进行更改：
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The Kubernetes cluster can be easily deployed using one command, as follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes集群可以使用一个命令轻松部署，如下所示：
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding command will call `kube-up.sh` and, in turn, `utils.sh` using
    the `config-default.sh` script, which contains the basic configuration of a K8S
    cluster with four nodes, as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令将调用`kube-up.sh`，然后使用`config-default.sh`脚本调用`utils.sh`，该脚本包含一个具有四个节点的K8S集群的基本配置，如下所示：
- en: '[PRE29]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The instances are `t2.micro` running Ubuntu OS. The process takes 5 to 10 minutes,
    after which the IP addresses of the master and minions get listed and can be used
    to access the Kubernetes cluster.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例是运行Ubuntu OS的`t2.micro`。该过程需要5到10分钟，之后主节点和从节点的IP地址将被列出，并可用于访问Kubernetes集群。
- en: Kubernetes networking and its differences to Docker networking
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes网络及其与Docker网络的区别
- en: Kubernetes strays from the default Docker system's networking model. The objective
    is for each pod to have an IP at a level imparted by the system's administration
    namespace, which has full correspondence with other physical machines and containers
    over the system. Allocating IPs per pod unit makes for a clean, retrogressive,
    and good model where units can be dealt with much like VMs or physical hosts from
    the point of view of port allotment, system administration, naming, administration
    disclosure, burden adjustment, application design, and movement of pods from one
    host to another. All containers in all pods can converse with all other containers
    in all other pods using their addresses. This also helps move traditional applications
    to a container-oriented approach.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes偏离了默认的Docker系统网络模型。其目标是使每个pod具有由系统管理命名空间赋予的IP，该IP与系统上的其他物理机器和容器具有完全对应关系。为每个pod单元分配IP可以创建一个清晰、向后兼容且良好的模型，在这个模型中，可以像处理VM或物理主机一样处理单元，从端口分配、系统管理、命名、管理披露、负载平衡、应用程序设计以及从一个主机迁移到另一个主机的pod迁移的角度来看。所有pod中的所有容器都可以使用它们的地址与所有其他pod中的所有其他容器进行通信。这也有助于将传统应用程序转移到面向容器的方法。
- en: As every pod gets a real IP address, they can communicate with each other without
    any need for translation. By making the same configuration of IP addresses and
    ports both inside as well as outside of the pod, we can create a NAT-less flat
    address space. This is different from the standard Docker model since there, all
    containers have a private IP address, which will allow them to be able to access
    the containers on the same host. But in the case of Kubernetes, all the containers
    inside a pod behave as if they are on the same host and can reach each other's
    ports on the localhost. This reduces the isolation between containers and provides
    simplicity, security, and performance. Port conflict can be one of the disadvantages
    of this; thus, two different containers inside one pod cannot use the same port.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个pod都有一个真实的IP地址，它们可以在彼此之间进行通信，无需进行任何翻译。通过在pod内外进行相同的IP地址和端口配置，我们可以创建一个无NAT的扁平地址空间。这与标准的Docker模型不同，因为在那里，所有容器都有一个私有IP地址，这将使它们能够访问同一主机上的容器。但在Kubernetes的情况下，pod内的所有容器都表现得好像它们在同一台主机上，并且可以在本地主机上访问彼此的端口。这减少了容器之间的隔离，并提供了简单性、安全性和性能。端口冲突可能是其中的一个缺点；因此，一个pod内的两个不同容器不能使用相同的端口。
- en: In GCE, using IP forwarding and advanced routing rules, each VM in a Kubernetes
    cluster gets an extra 256 IP addresses in order to route traffic across pods easily.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCE中，使用IP转发和高级路由规则，Kubernetes集群中的每个VM都会额外获得256个IP地址，以便轻松地在pod之间路由流量。
- en: Routes in GCE allow you to implement more advanced networking functions in the
    VMs, such as setting up many-to-one NAT. This is leveraged by Kubernetes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GCE中的路由允许您在VM中实现更高级的网络功能，比如设置多对一NAT。这被Kubernetes所利用。
- en: This is in addition to the main Ethernet bridge which the VM has; this bridge
    is termed as the container bridge `cbr0` in order to differentiate it from the
    Docker bridge, `docker0`. In order to transfer packets out of the GCE environment
    from a pod, it should undergo an SNAT to the VM's IP address, which GCE recognizes
    and allows.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了虚拟机具有的主要以太网桥之外，还有一个容器桥`cbr0`，以区分它与Docker桥`docker0`。为了将pod中的数据包传输到GCE环境之外，它应该经历一个SNAT到虚拟机的IP地址，这样GCE才能识别并允许。
- en: Other implementations with the primary aim of providing an IP-per-pod model
    are Open vSwitch, Flannel, and Weave.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其他旨在提供IP-per-pod模型的实现包括Open vSwitch、Flannel和Weave。
- en: In the case of a GCE-like setup of an Open vSwitch bridge for Kubernetes, the
    model where the Docker bridge gets replaced by `kbr0` to provide an extra 256
    subnet addresses is followed. Also, an OVS bridge (`ovs0`) is added, which adds
    a port to the Kubernetes bridge in order to provide GRE tunnels to transfer packets
    across different minions and connect pods residing on these hosts. The IP-per-pod
    model is also elaborated more in the upcoming diagram, where the service abstraction
    concept of Kubernetes is also explained.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似GCE的Open vSwitch桥的Kubernetes设置中，采用了将Docker桥替换为`kbr0`以提供额外的256个子网地址的模型。此外，还添加了一个OVS桥（`ovs0`），它向Kubernetes桥添加了一个端口，以便提供GRE隧道来传输不同minions上的pod之间的数据包。IP-per-pod模型也在即将出现的图表中有更详细的解释，其中还解释了Kubernetes的服务抽象概念。
- en: A service is another type of abstraction that is widely used and suggested for
    use in Kubernetes clusters as it allows a group of pods (applications) to be accessed
    via virtual IP addresses and gets proxied to all internal pods in a service. An
    application deployed in Kubernetes could be using three replicas of the same pod,
    which have different IP addresses. However, the client can still access the application
    on the one IP address which is exposed outside, irrespective of which backend
    pod takes the request. A service acts as a load balancer between different replica
    pods and a single point of communication for clients utilizing this application.
    Kubeproxy, one of the services of Kubernetes, provides load balancing and uses
    rules to access the service IPs and redirects them to the correct backend pod.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是另一种广泛使用并建议在Kubernetes集群中使用的抽象类型，因为它允许一组pod（应用程序）通过虚拟IP地址访问，并且被代理到服务中的所有内部pod。在Kubernetes中部署的应用程序可能使用三个相同pod的副本，它们具有不同的IP地址。但是，客户端仍然可以访问外部公开的一个IP地址上的应用程序，而不管哪个后端pod接受请求。服务充当不同副本pod之间的负载均衡器，并且对于使用此应用程序的客户端来说是通信的单一点。Kubernetes的服务之一Kubeproxy提供负载均衡，并使用规则访问服务IP并将其重定向到正确的后端pod。
- en: Deploying the Kubernetes pod
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署Kubernetes pod
- en: 'Now, in the following example, we will be deploying two nginx replication pods
    (`rc-pod`) and exposing them via a service in order to understand Kubernetes networking.
    Deciding where the application can be exposed via a virtual IP address and which
    replica of the pod (load balancer) the request is to be proxied to is taken care
    of by **Service Proxy**. Please refer to the following diagram for more details:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在以下示例中，我们将部署两个nginx复制pod（`rc-pod`）并通过服务公开它们，以便了解Kubernetes网络。决定应用程序可以通过虚拟IP地址公开以及请求应该代理到哪个pod副本（负载均衡器）由**服务代理**负责。有关更多详细信息，请参考以下图表：
- en: '![Deploying the Kubernetes pod](../images/00028.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![部署Kubernetes pod](../images/00028.jpeg)'
- en: 'The following are the steps to deploy the Kubernetes pod:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Kubernetes pod的步骤如下：
- en: 'In the Kubernetes master, create a new folder:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kubernetes主节点上，创建一个新文件夹：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In the editor of your choice, create the `.yaml` file that will be used to
    deploy the nginx pods:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您选择的编辑器中，创建将用于部署nginx pod的`.yaml`文件：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Copy the following into the file:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容复制到文件中：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create the nginx pod using `kubectl`:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl`创建nginx pod：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the preceding pod creation process, we created two replicas of the nginx
    pod, and its details can be listed using the following command:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的pod创建过程中，我们创建了两个nginx pod的副本，并且可以使用以下命令列出其详细信息：
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following is the output generated:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出如下：
- en: '[PRE35]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To list replication controllers on a cluster, use the `kubectl get` command:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出集群上的复制控制器，请使用`kubectl get`命令：
- en: '[PRE36]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the output generated:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出如下：
- en: '[PRE37]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The container on the deployed minion can be listed using the following command:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下命令列出部署的minion上的容器：
- en: '[PRE38]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following is the output generated:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出如下：
- en: '[PRE39]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Deploy the nginx service using the following .`yaml` file in order to expose
    the nginx pod on host port `82`:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下`.yaml`文件部署nginx服务以在主机端口`82`上公开nginx pod：
- en: '[PRE40]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Copy the following into the file:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下内容复制到文件中：
- en: '[PRE41]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create the nginx service using the `kubectl create` command:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl create`命令创建nginx服务：
- en: '[PRE42]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The nginx service can be listed using the following command:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下命令列出nginx服务：
- en: '[PRE43]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following is the output generated:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出如下：
- en: '[PRE44]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, the nginx server''s test page can be accessed on the following URL via
    the service:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，可以通过服务在以下URL上访问nginx服务器的测试页面：
- en: '`http://192.168.3.43:82`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://192.168.3.43:82`'
- en: Mesosphere
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mesosphere
- en: Mesosphere is a software solution that provides ways of managing server infrastructures
    and basically expands upon the cluster-management capabilities of Apache Mesos.
    Mesosphere has also launched the **DCOS** (**data center operating system**),
    used to manage data centers by spanning all the machines and treating them as
    a single computer, providing a highly scalable and elastic way of deploying apps
    on top of it. DCOS can be installed on any public cloud or your own private data
    center, ranging from AWS, GCE, and Microsoft Azure to VMware. Marathon is the
    framework for Mesos and is designed to launch and run applications; it serves
    as a replacement for the init system. Marathon provides various features such
    as high availability, application health check, and service discovery, which help
    you run applications in Mesos clustered environments.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Mesosphere是一个软件解决方案，提供了管理服务器基础设施的方法，并基本上扩展了Apache Mesos的集群管理能力。Mesosphere还推出了**DCOS**（**数据中心操作系统**），用于通过将所有机器跨越并将它们视为单台计算机来管理数据中心，提供了一种高度可扩展和弹性的部署应用程序的方式。DCOS可以安装在任何公共云或您自己的私有数据中心，从AWS、GCE和Microsoft
    Azure到VMware。Marathon是Mesos的框架，旨在启动和运行应用程序；它用作init系统的替代品。Marathon提供了诸如高可用性、应用程序健康检查和服务发现等各种功能，帮助您在Mesos集群环境中运行应用程序。
- en: This session describes how to bring up a single-node Mesos cluster.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了如何启动单节点Mesos集群。
- en: Docker containers
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker容器
- en: Mesos can run and manage Docker containers using the Marathon framework.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos可以使用Marathon框架来运行和管理Docker容器。
- en: 'In this exercise, we will use CentOS 7 to deploy a Mesos cluster:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用CentOS 7来部署Mesos集群。
- en: 'Install Mesosphere and Marathon using the following command:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装Mesosphere和Marathon：
- en: '[PRE45]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Apache Mesos uses Zookeeper to operate. Zookeeper acts as the master election
    service in the Mesosphere architecture and stores states for the Mesos nodes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mesos使用Zookeeper进行操作。Zookeeper在Mesosphere架构中充当主选举服务，并为Mesos节点存储状态。
- en: 'Install Zookeeper and the Zookeeper server package by pointing to the RPM repository
    for Zookeeper, as follows:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指向Zookeeper的RPM存储库来安装Zookeeper和Zookeeper服务器包，如下所示：
- en: '[PRE46]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Validate Zookeeper by stopping and restarting it:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过停止和重新启动Zookeeper来验证Zookeeper：
- en: '[PRE47]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Mesos uses a simple architecture to give you intelligent task distribution across
    a cluster of machines without worrying about where they are scheduled.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos使用简单的架构，在集群中智能地分配任务，而不用担心它们被安排在哪里。
- en: 'Configure Apache Mesos by starting the `mesos-master` and `mesos-slave` processes
    as follows:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过启动`mesos-master`和`mesos-slave`进程来配置Apache Mesos，如下所示：
- en: '[PRE48]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Mesos will be running on port `5050`. As shown in the following screenshot,
    you can access the Mesos interface with your machine's IP address, here, `http://192.168.10.10:5050`:![Docker
    containers](../images/00029.jpeg)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mesos将在端口`5050`上运行。如下截图所示，您可以使用您机器的IP地址访问Mesos界面，这里是`http://192.168.10.10:5050`：![Docker
    containers](../images/00029.jpeg)
- en: 'Test Mesos using the `mesos-execute` command:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mesos-execute`命令测试Mesos：
- en: '[PRE49]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'With the `mesos-execute` command running, enter *Ctrl* + *Z* to suspend the
    command. You can see how it appears in the web UI and command line:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`mesos-execute`命令后，输入*Ctrl* + *Z*以暂停命令。您可以看到它在Web UI和命令行中的显示方式：
- en: '[PRE50]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The Mesosphere stack uses Marathon to manage processes and services. It serves
    as a replacement for the traditional init system. It simplifies the running of
    applications in a clustered environment. The following figure shows the Mesosphere
    Master slave topology with Marathon:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Mesosphere堆栈使用Marathon来管理进程和服务。它用作传统init系统的替代品。它简化了在集群环境中运行应用程序。下图显示了带有Marathon的Mesosphere主从拓扑结构：
- en: '![Docker containers](../images/00030.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![Docker containers](../images/00030.jpeg)'
- en: Marathon can be used to start other Mesos frameworks; as it is designed for
    long-running applications, it will ensure that the applications it has launched
    will continue running even if the slave nodes they are running on fail.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Marathon可以用来启动其他Mesos框架；因为它设计用于长时间运行的应用程序，它将确保它启动的应用程序即使在它们运行的从节点失败时也会继续运行。
- en: 'Start the Marathon service using the following command:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启动Marathon服务：
- en: '[PRE51]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: You can view the Marathon GUI at `http://192.168.10.10:8080`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`http://192.168.10.10:8080`上查看Marathon GUI。
- en: Deploying a web app using Docker
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Docker部署web应用
- en: 'In this exercise, we will install a simple Outyet web application:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将安装一个简单的Outyet web应用程序：
- en: 'Install Docker using the following commands:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装Docker：
- en: '[PRE52]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following command tests the Docker file before adding it to Marathon:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将其添加到Marathon之前，使用以下命令测试Docker文件：
- en: '[PRE53]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Go to `http://192.168.10.10:6060/` on your browser in order to confirm it works.
    Once it does, you can hit *CTRL* + *C* to exit the Outyet Docker.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中转到`http://192.168.10.10:6060/`以确认它是否正常工作。一旦确认，您可以按下*CTRL* + *C*退出Outyet
    Docker。
- en: 'Create a Marathon application using Marathon Docker support, as follows:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Marathon Docker支持创建Marathon应用程序，如下所示：
- en: '[PRE54]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Containers are configured and managed better with Marathon Docker, as follows:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Marathon Docker更好地配置和管理容器，如下所示：
- en: '[PRE55]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: You can check all your applications on the Marathon GUI at `http://192.168.10.10:8080`,
    as shown in the following screenshot:![Deploying a web app using Docker](../images/00031.jpeg)
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在Marathon GUI上检查所有应用程序，如下截图所示，网址为`http://192.168.10.10:8080`：![使用Docker部署web应用](../images/00031.jpeg)
- en: Deploying Mesos on AWS using DCOS
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DCOS在AWS上部署Mesos
- en: 'In this final section, we will be deploying the latest launch of DCOS by Mesosphere
    on AWS in order to manage and deploy Docker services in our data center:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们将在AWS上部署Mesosphere的最新版本DCOS，以便在我们的数据中心管理和部署Docker服务：
- en: Create an AWS key pair in the region where the cluster is required to be deployed
    by going to the navigation pane and choosing **Key Pairs** under **NETWORK & SECURITY**:![Deploying
    Mesos on AWS using DCOS](../images/00032.jpeg)
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过转到导航窗格并在**网络和安全**下选择**密钥对**来在需要部署集群的区域创建AWS密钥对：![在AWS上使用DCOS部署Mesos](../images/00032.jpeg)
- en: After being created, the key can be viewed as follows and the generated key
    pair (`.pem`) file should be stored in a secure location for future use:![Deploying
    Mesos on AWS using DCOS](../images/00033.jpeg)
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建后，可以按以下方式查看密钥，并应将生成的密钥对（.pem）文件存储在安全位置以备将来使用：![在AWS上使用DCOS部署Mesos](../images/00033.jpeg)
- en: The DCOS cluster can be created by selecting the **1 Master** template on the
    official Mesosphere site:![Deploying Mesos on AWS using DCOS](../images/00034.jpeg)
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过在官方Mesosphere网站上选择**1 Master**模板来创建DCOS集群：![在AWS上使用DCOS部署Mesos](../images/00034.jpeg)
- en: 'It can also be done by providing the link for the Amazon S3 template URL in
    the stack deployment:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过在堆栈部署中提供亚马逊S3模板URL的链接来完成：
- en: '![Deploying Mesos on AWS using DCOS](../images/00035.jpeg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![在AWS上使用DCOS部署Mesos](../images/00035.jpeg)'
- en: Click on the **Next** button. Fill in the details such as **Stack name** and
    **KeyName**, generated in the previous step:![Deploying Mesos on AWS using DCOS](../images/00036.jpeg)
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮。填写诸如**堆栈名称**和**密钥名称**之类的细节，这些细节是在上一步中生成的：![在AWS上使用DCOS部署Mesos](../images/00036.jpeg)
- en: Review the details before clicking on the **Create** button:![Deploying Mesos
    on AWS using DCOS](../images/00037.jpeg)
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在点击**创建**按钮之前，请查看细节：![在AWS上使用DCOS部署Mesos](../images/00037.jpeg)
- en: After 5 to 10 minutes, the Mesos stack will be deployed and the Mesos UI can
    be accessed at the URL shown in the following screenshot:![Deploying Mesos on
    AWS using DCOS](../images/00038.jpeg)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5到10分钟后，Mesos堆栈将被部署，并且可以在以下截图中显示的URL上访问Mesos UI：![在AWS上使用DCOS部署Mesos](../images/00038.jpeg)
- en: 'Now, we will be installing the DCOS CLI on a Linux machine with Python (2.7
    or 3.4) and pip preinstalled, using the following commands:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将在预先安装了Python（2.7或3.4）和pip的Linux机器上安装DCOS CLI，使用以下命令：
- en: '[PRE56]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The DCOS help file can be listed as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: DCOS帮助文件可以列出如下：
- en: '[PRE57]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we will deploy a Spark application on top of the Mesos cluster using the
    DCOS package after updating it. Get a detailed command description with `dcos
    <command> --help`:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用DCOS包在Mesos集群上部署一个Spark应用程序，然后更新它。使用`dcos <command> --help`获取详细的命令描述：
- en: '[PRE58]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The Spark package can be installed as follows:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark包可以按以下方式安装：
- en: '[PRE59]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: After deployment, it can be seen in the DCOS UI under the **Services** tab,
    as shown in the following screenshot:![Deploying Mesos on AWS using DCOS](../images/00039.jpeg)
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署后，可以在DCOS UI的**Services**选项卡下看到，如下图所示：![在AWS上使用DCOS部署Mesos](../images/00039.jpeg)
- en: 'In order to deploy a dummy Docker application on the preceding Marathon cluster,
    we can use the JSON file to define the container image, command to execute, and
    ports to be exposed after deployment:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在前面的Marathon集群上部署一个虚拟的Docker应用程序，我们可以使用JSON文件来定义容器映像、要执行的命令以及部署后要暴露的端口：
- en: '[PRE60]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The app can be added to Marathon and listed as follows:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用程序可以添加到Marathon并列出如下：
- en: '[PRE61]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Three instances of the preceding Docker app can be started as follows:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以按以下方式启动前面的Docker应用程序的三个实例：
- en: '[PRE62]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The deployed application can be seen in the DCOS UI by clicking on the **Tasks**
    tab under **Services**:![Deploying Mesos on AWS using DCOS](../images/00040.jpeg)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过单击**Services**下的**Tasks**选项卡，可以在DCOS UI中看到部署的应用程序：![在AWS上使用DCOS部署Mesos](../images/00040.jpeg)
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learnt about Docker networking using various frameworks,
    such as the native Docker Swarm. Using libnetwork or out-of-the-box overlay networks,
    Swarm provides multihost networking features.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了使用各种框架的Docker网络，例如本地Docker Swarm。使用libnetwork或开箱即用的覆盖网络，Swarm提供了多主机网络功能。
- en: Kubernetes, on the other hand, has a different perspective from Docker, in which
    each pod gets its unique IP address and communication between pods can occur with
    the help of services. Using Open vSwitch or IP forwarding and advanced routing
    rules, Kubernetes networking can be enhanced to provide connectivity between pods
    on different subnets across hosts and the ability to expose the pods to the external
    world. In the case of Mesosphere, we can see that Marathon is used as the backend
    for the networking of the deployed containers. In the case of DCOS by Mesosphere,
    the entire deployed stack of machines is treated as one machine in order to provide
    a rich networking experience between deployed container services.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Kubernetes与Docker有不同的视角，其中每个pod都有一个独特的IP地址，并且可以通过服务的帮助在pod之间进行通信。使用Open
    vSwitch或IP转发和高级路由规则，Kubernetes网络可以得到增强，以提供在不同子网上的主机之间以及将pod暴露给外部世界的连接能力。在Mesosphere的情况下，我们可以看到Marathon被用作部署容器的网络的后端。在Mesosphere的DCOS的情况下，整个部署的机器堆栈被视为一个机器，以提供在部署的容器服务之间丰富的网络体验。
- en: In the next chapter, we will learn about security and QoS for basic Docker networking
    by understanding kernel namespace, cgroups, and virtual firewalls.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过了解内核命名空间、cgroups和虚拟防火墙，学习有关基本Docker网络的安全性和QoS。
