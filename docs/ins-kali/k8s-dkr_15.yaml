- en: '*Chapter 12*: Auditing using Falco and EFK'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：使用Falco和EFK进行审计'
- en: Bad people do bad things.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 坏人做坏事。
- en: Good people do bad things.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 好人做坏事。
- en: Accidents happen.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 事故发生。
- en: 'Each of the preceding statements has one thing in common: when any one of them
    occurs, you need to find out what happened.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 前述每个声明都有一个共同点：当其中任何一个发生时，您需要找出发生了什么。
- en: Too often, auditing is considered only when we think of some form of attack.
    While we certainly require auditing to find "bad people", we also need to audit
    everyday standard system interactions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 太多时候，审计只有在我们考虑某种形式的攻击时才会被考虑。虽然我们当然需要审计来找到“坏人”，但我们也需要审计日常标准的系统交互。
- en: Kubernetes includes logs for most of the important system events that you will
    need to audit, but it doesn't include everything. As we discussed in previous
    chapters, all API interactions will be logged by the system, which includes the
    majority of events you need to audit. However, there are tasks that users execute
    that will not go through the API server and may go undetected if you are relying
    on API logs for all of your auditing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes包括大多数重要系统事件的日志，您需要对其进行审计，但并不包括所有内容。正如我们在前几章中讨论的，系统将记录所有API交互，其中包括您需要审计的大部分事件。但是，用户执行的任务可能不会通过API服务器，并且如果您依赖API日志进行所有审计，可能会被忽略。
- en: There are tools to address the gaps in the native logging functionality. Open
    source projects such as Falco will provide enhanced auditing for your pods, providing
    details for events that are logged by the API server.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有工具可以解决本机日志功能中的差距。Falco等开源项目将为您的pod提供增强的审计，提供API服务器记录的事件的详细信息。
- en: Logs without a logging system are not very useful. Like many components in Kubernetes,
    there are many open source projects that provide a full logging system. One of
    the most popular systems is the EFK stack, which includes Elasticsearch, Fluentd,
    and Kibana.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 没有日志系统的日志并不是很有用。与Kubernetes中的许多组件一样，有许多开源项目提供完整的日志系统。最受欢迎的系统之一是EFK堆栈，其中包括Elasticsearch，Fluentd和Kibana。
- en: All of these projects will be covered in detail throughout this chapter. You
    will deploy each of these components to gain hands-on experience and to reinforce
    the material covered in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍所有这些项目。您将部署每个组件，以获得实践经验，并加强本章所涵盖的内容。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Exploring auditing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索审计
- en: Introducing Falco
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Falco
- en: Exploring Falco's configuration files
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Falco的配置文件
- en: Deploying Falco
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Falco
- en: Falco kernel module
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falco内核模块
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete the exercises in this chapter, you will need to meet the following
    technical requirements:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的练习，您需要满足以下技术要求：
- en: An Ubuntu 18.04 server with a minimum of 8 GB of RAM and at least 5 GB of free
    disk space for Docker volumes
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Ubuntu 18.04服务器，至少有8GB的RAM和至少5GB的空闲磁盘空间用于Docker卷
- en: A KinD cluster installed using the instructions in [*Chapter 4*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083),
    *Deploying Kubernetes using KinD*
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[*第4章*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083)中的说明安装的KinD集群，*使用KinD部署Kubernetes*
- en: Helm3 binary (should also have been installed in [*Chapter 4*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083),
    *Deploying Kubernetes using KinD*)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helm3二进制文件（也应该已经在[*第4章*](B15514_04_Final_ASB_ePub.xhtml#_idTextAnchor083)中安装，*使用KinD部署Kubernetes*）
- en: You can access the code for this chapter at the GitHub repository for the book,
    available at [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在该书的GitHub存储库中访问本章的代码，网址为[https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide)。
- en: Exploring auditing
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索审计。
- en: In most environments where you run Kubernetes clusters, you will need to have
    an auditing system in place. While Kubernetes has some auditing features, they
    are often too limited for an enterprise to rely on for a complete audit trail,
    and logs are often only stored on each host filesystem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数运行Kubernetes集群的环境中，您需要建立一个审计系统。虽然Kubernetes具有一些审计功能，但它们通常对企业来说太有限，无法完全依赖于完整的审计轨迹，日志通常只存储在每个主机文件系统上。
- en: In order to correlate events, you are required to pull all the logs you want
    to search through on your local system, and manually look through logs or pull
    them into a spreadsheet and attempt to create some macros to search and tie information
    together.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了关联事件，您需要在本地系统上提取所有您想要搜索的日志，并手动查看日志或将它们拉入电子表格，并尝试创建一些宏来搜索和关联信息。
- en: Fortunately, there are many third-party logging systems available for Kubernetes.
    Optional pay systems such as Splunk and Datadog are popular solutions and open
    source systems including the EFK stack are commonly used and included with many
    Kubernetes distributions. All of these systems include some form of a log forwarder
    that allows you to centralize your Kubernetes logs so you can create alerts, custom
    queries, and dashboards.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes有许多第三方日志系统可用。像Splunk和Datadog这样的可选付费系统是流行的解决方案，包括EFK堆栈在内的开源系统通常与许多Kubernetes发行版一起使用。所有这些系统都包括某种形式的日志转发器，允许您集中您的Kubernetes日志，以便您可以创建警报、自定义查询和仪表板。
- en: Another limitation of native auditing is the limited scope of events, which
    are limited to API access. While this is important to audit, most enterprises
    will need to augment or customize the base set of auditing targets beyond simple
    API events. Extending the base auditing features can be a challenge and most companies
    will not have the expertise or time to create their own auditing add-ons.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本地审计的另一个局限性是事件的范围有限，仅限于API访问。虽然审计API访问很重要，但大多数企业都需要扩展或定制基本的审计目标集，超出简单的API事件。扩展基本审计功能可能是一个挑战，大多数公司将没有专业知识或时间来创建自己的审计附加组件。
- en: 'One area of auditing that Kubernetes is missing concerns pod events. As we
    mentioned, the base auditing capabilities of Kubernetes focus on API access. Most
    tasks performed by users will trigger a call to the API server. Let''s take an
    example of a user executing a shell on a pod to look at a file. The user would
    use **kubectl exec -it <pod name> bash** to spawn a bash shell on the pod in interactive
    mode. This actually sends a request to the API server, the main call of which
    to execute is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes缺少的审计领域之一涉及Pod事件。正如我们提到的，Kubernetes的基本审计功能集中在API访问上。用户执行的大多数任务都会触发对API服务器的调用。让我们举一个例子，用户在Pod上执行shell以查看文件。用户将使用**kubectl
    exec -it <pod name> bash**在Pod上以交互模式生成一个bash shell。这实际上发送了一个请求到API服务器，其中主要的调用是执行如下：
- en: I0216 11:42:58.872949   13139 round_trippers.go:420] POST https://0.0.0.0:32771/api/v1/namespaces/ingress-nginx/pods/nginx-ingress-controller-7d6bf88c86-knbrx/exec?command=bash&container=nginx-ingress-controller&stdin=true&stdout=true&tty=true
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: I0216 11:42:58.872949 13139 round_trippers.go:420] POST https://0.0.0.0:32771/api/v1/namespaces/ingress-nginx/pods/nginx-ingress-controller-7d6bf88c86-knbrx/exec?command=bash&container=nginx-ingress-controller&stdin=true&stdout=true&tty=true
- en: Looking at the event, you can see that an **exec** command was sent to the **nginx-ingress-controller**
    pod to run the bash process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 查看事件，您可以看到一个**exec**命令被发送到**nginx-ingress-controller**的Pod上运行bash进程。
- en: There may be good reasons that someone is running a shell, for example, to look
    at an error log or to fix an issue quickly. But the issue here is that, once inside
    the running pod, any command that is executed does not access the Kubernetes API,
    and therefore, you will not receive any logged events for the actions executed
    in the pod. To most enterprises, this is a large hole in the auditing system since
    no end-to-end audit trail would exist if the action conducted in the container
    were malicious.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能有人运行shell的原因是合理的，例如查看错误日志或快速解决问题。但问题在于，一旦进入运行中的pod，执行的任何命令都不会访问Kubernetes
    API，因此，你将不会收到有关在pod中执行的操作的任何记录事件。对大多数企业来说，这是审计系统中的一个重大漏洞，因为如果容器中进行的操作是恶意的，就不会存在端到端的审计跟踪。
- en: To audit all shell access to pods would lead to many false-positive leads, and
    in the event that a pod was restarted, you would lose any local audit files in
    the pod. Instead, you may ignore simple shell access, but you want to log an event
    if someone tries to execute certain tasks from the shell, such as modifying the
    **/etc/passwd** file.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 审计所有对pod的shell访问将导致许多误报，而且如果pod重新启动，你将丢失pod中的任何本地审计文件。相反，你可以忽略简单的shell访问，但是如果有人尝试从shell执行某些任务，比如修改**/etc/passwd**文件，你希望记录下这个事件。
- en: So, you may ask, "*What is the solution?*" The answer is to use Falco.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你可能会问，“*解决方案是什么？*” 答案是使用Falco。
- en: Introducing Falco
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入Falco
- en: 'Falco is an open source system from Sysdig that adds anomaly detection functionality
    for pods in Kubernetes clusters. Out of the box, Falco includes a base set of
    powerful, community-created rules that can monitor a number of potentially malicious
    events, including the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Falco是来自Sysdig的开源系统，为Kubernetes集群中的pod添加了异常检测功能。Falco默认包含一组强大的、由社区创建的基本规则，可以监视许多潜在的恶意事件，包括以下内容：
- en: When a user attempts to modify a file under **/etc**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当用户尝试修改**/etc**目录下的文件时
- en: When a user spawns a shell on a pod
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当用户在pod上生成一个shell时
- en: When a user stores sensitive information in a secret
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当用户将敏感信息存储在secret中时
- en: When a pod attempts to make a call to the Kubernetes API server
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当pod尝试调用Kubernetes API服务器时
- en: Any attempts to modify a system ClusterRole
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何尝试修改系统ClusterRole的行为
- en: Or any other custom rule you create to meet your needs
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者你创建的其他自定义规则以满足你的需求
- en: When Falco is running on a Kubernetes cluster it watches events, and based on
    a set of rules, it logs events on the Falco pod that can be picked up by a system
    such as Fluentd, which would then forward the event to an external logging system.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当Falco在Kubernetes集群上运行时，它会监视事件，并根据一组规则，在Falco pod上记录事件，这些事件可以被诸如Fluentd之类的系统捕获，然后将事件转发到外部日志系统。
- en: In this chapter, we will explain the configuration of Falco using the technical
    requirements for our company scenario for FooWidgets. By the end of the chapter,
    you will know how to set up Falco on a Kubernetes cluster using custom configuration
    options. You will also understand the rules used by Falco and how to create rules
    when you need to audit an event that is not included in the base rules. Finally,
    you will forward events using Fluentd to Elasticsearch using Kibana to visualize
    the events generated by Falco.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用FooWidgets公司场景的技术要求来解释Falco的配置。在本章结束时，你将了解如何在Kubernetes集群上使用自定义配置选项设置Falco。你还将了解Falco使用的规则以及在需要审计不包含在基本规则中的事件时如何创建规则。最后，你将使用Fluentd将事件转发到Elasticsearch，并使用Kibana来可视化Falco生成的事件。
- en: Exploring Falco's configuration files
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Falco的配置文件
- en: Before you install Falco, you need to understand the configuration options that
    are available, and that starts with the initial configuration file that will be
    used to configure how Falco creates events.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装Falco之前，您需要了解可用的配置选项，这始于将用于配置Falco如何创建事件的初始配置文件。
- en: The Falco project includes a set of base configuration files that you can use
    for your initial auditing. It is highly likely that you will want to change the
    base configuration to fit your specific enterprise requirements. In this section,
    we will go over a Falco deployment and provide a basic understanding of the configuration
    files.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Falco项目包括一组基本配置文件，您可以用于初始审计。您很可能希望更改基本配置以适应特定的企业要求。在本节中，我们将介绍Falco部署并提供对配置文件的基本理解。
- en: Falco is a powerful system that can be customized to fit almost any requirement
    you may have for security. Since it is so extensible, it's not possible to cover
    every detail of the configuration in a single chapter, but like many popular projects,
    there is an active GitHub community at [https://github.com/falcosecurity/falco](https://github.com/falcosecurity/falco)
    where you can post issues or join their Slack channel.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Falco是一个强大的系统，可以定制以适应您可能对安全性的任何要求。由于它是如此可扩展，不可能在单个章节中涵盖配置的每个细节，但像许多流行的项目一样，有一个活跃的GitHub社区[https://github.com/falcosecurity/falco](https://github.com/falcosecurity/falco)，您可以在那里发布问题或加入他们的Slack频道。
- en: The Falco configuration files include a base configuration file and the rules
    files that contain the events that will be audited by the system. The base configuration
    file is a simple YAML file that contains **key:value** pairs for each configuration
    option, along with other YAML files that use **key:value** pairs, but they contain
    details and configurations for the audit events.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Falco配置文件包括基本配置文件和包含系统将审计的事件的规则文件。基本配置文件是一个简单的YAML文件，其中包含每个配置选项的**键：值**对，以及其他使用**键：值**对的YAML文件，但它们包含审计事件的详细信息和配置。
- en: 'There are four base configuration files that you can use to configure your
    deployment, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个基本配置文件可用于配置部署，如下：
- en: '**falco.yaml**'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: falco.yaml
- en: '**falco_rules.yaml**'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: falco_rules.yaml
- en: '**falco_rules.local.yaml**'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: falco_rules.local.yaml
- en: '**k8s_audit_rules.yaml**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k8s_audit_rules.yaml
- en: The included configuration files will work out of the box, but you may want
    to change some of the values to fit your logging requirements. In this section,
    we will explain the most important configuration options in detail. The first
    three configuration files are part of a base Falco deployment and will be explained
    in detail in this chapter. The last configuration file is not required for a base
    Falco installation. It is an add-on that can be enabled to add additional auditing
    functionalities to the API server.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 包含的配置文件将立即运行，但您可能希望更改一些值以适应日志记录要求。在本节中，我们将详细解释最重要的配置选项。前三个配置文件是基本Falco部署的一部分，并将在本章中详细解释。最后一个配置文件对于基本Falco安装不是必需的。这是一个附加组件，可以启用以向API服务器添加额外的审计功能。
- en: The falco.yaml configuration file
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: falco.yaml配置文件
- en: The first file you will need to edit is the **base configuration file** to configure
    how Falco creates audit events. It allows you to customize the base settings of
    Falco including the event output format, timestamp configuration, and endpoint
    targets such as a Slack channel. Let's have a detailed walkthrough of this file
    and try to understand it bit by bit.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要编辑的第一个文件是**基本配置文件**，用于配置Falco如何创建审计事件。它允许您自定义Falco的基本设置，包括事件输出格式、时间戳配置和端点目标，例如Slack频道。让我们详细了解这个文件，并逐步理解它。
- en: 'The first section in the configuration file is the **rules_files** section.
    This section takes the format of the key **rules_file**, and the values for the
    rule file(s) with a dash. (This can also be represented as **rules_file: [file1,
    file2, file3, etc…]**.)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '配置文件中的第一部分是**rules_files**部分。该部分采用**rules_file**键的格式，以及带有破折号的规则文件的值。（这也可以表示为**rules_file:
    [file1, file2, file3, etc…]**。）'
- en: 'We will explain the function of each rule file in this chapter. In this example
    configuration, we are telling Falco to use three files as rules, and each file
    is mounted from a ConfigMap during installation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中解释每个规则文件的功能。在此示例配置中，我们告诉Falco使用三个文件作为规则，并且每个文件在安装期间都从ConfigMap中挂载：
- en: 'rules_file:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'rules_file:'
- en: '- /etc/falco/falco_rules.yaml'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '- /etc/falco/falco_rules.yaml'
- en: '- /etc/falco/falco_rules.local.yaml'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '- /etc/falco/falco_rules.local.yaml'
- en: '- /etc/falco/k8s_audit_rules.yaml'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '- /etc/falco/k8s_audit_rules.yaml'
- en: The next set of values will configure how Falco outputs events, including the
    time format, and the option to output events as text or JSON.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下一组值将配置Falco如何输出事件，包括时间格式以及将事件输出为文本或JSON的选项。
- en: By default, the **time_format_iso_8601** value is set to **false**, which tells
    Falco to use the local **/etc/localtime** format. Setting the value to **true**
    tells Falco to stamp each event using the date format of YYYY-MM-DD, a time format
    using a 24-hour clock, and a time zone of UTC.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，**time_format_iso_8601**值设置为**false**，告诉Falco使用本地**/etc/localtime**格式。将值设置为**true**告诉Falco使用YYYY-MM-DD的日期格式，使用24小时制的时间格式，以及UTC时区为每个事件加上时间戳。
- en: 'Selecting the appropriate format is a decision for your organization. If you
    have a global organization it may beneficial to set all of your logging to use
    the ISO 8601 format. However, if you have a regional organization you may be more
    comfortable using your local date-and-time format since you may not need to worry
    about correlating events against logging systems in other time zones:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的格式是您组织的决定。如果您有一个全球组织，将所有日志设置为使用ISO 8601格式可能是有益的。但是，如果您有一个区域性组织，您可能更愿意使用本地日期和时间格式，因为您可能不需要担心将事件与其他时区的日志系统进行关联：
- en: 'time_format_iso_8601: false'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'time_format_iso_8601: false'
- en: 'The next two lines allow you to configure the output of events as either text
    or JSON format. The default value is set to **false**, which tells Falco to output
    events in text format. If the first key is set to **false**, the second value
    will not be evaluated since JSON is not enabled:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两行允许您将事件输出配置为文本或JSON格式。默认值设置为**false**，告诉Falco以文本格式输出事件。如果第一个键设置为**false**，则不会评估第二个值，因为未启用JSON：
- en: 'json_output: false'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'json_output: false'
- en: 'json_include_output_property: true'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'json_include_output_property: true'
- en: You may need to output the events in JSON format, depending on the format that
    your logging system requires. As an example, if you were going to send Falco events
    to an Elasticsearch server, you might want to enable JSON to allow Elasticsearch
    to parse the alerts field. Elasticsearch does not require the events to be sent
    in JSON format and for the lab in this module, we will leave this set to the default
    value, **false**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据日志系统所需的格式，您可能需要以JSON格式输出事件。例如，如果您要将Falco事件发送到Elasticsearch服务器，您可能希望启用JSON以允许Elasticsearch解析警报字段。Elasticsearch不需要以JSON格式发送事件，在本模块的实验中，我们将保持默认值**false**。
- en: 'The following are some examples of the same type of event in both text format
    and JSON format:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相同类型事件的文本格式和JSON格式的一些示例：
- en: 'The Falco text log output looks as follows:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falco文本日志输出如下：
- en: '**19:17:23.139089915: Notice A shell was spawned in a container with an attached
    terminal (user=root k8s.ns=default k8s.pod=falco-daemonset-9mrn4 container=0756e87d121d
    shell=bash parent=runc cmdline=bash terminal=34816 container_id=0756e87d121d image=<NA>)
    k8s.ns=default k8s.pod=falco-daemonset-9mrn4 container=0756e87d121d k8s.ns=default
    k8s.pod=falco-daemonset-9mrn4 container=0756e87d121d**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**19:17:23.139089915: Notice A shell was spawned in a container with an attached
    terminal (user=root k8s.ns=default k8s.pod=falco-daemonset-9mrn4 container=0756e87d121d
    shell=bash parent=runc cmdline=bash terminal=34816 container_id=0756e87d121d image=<NA>)
    k8s.ns=default k8s.pod=falco-daemonset-9mrn4 container=0756e87d121d k8s.ns=default
    k8s.pod=falco-daemonset-9mrn4 container=0756e87d121d**'
- en: 'The Falco JSON log output looks as follows:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falco JSON日志输出如下：
- en: '**{"output":"20:47:39.535071657: Notice A shell was spawned in a container
    with an attached terminal (user=root k8s.ns=default k8s.pod=falco-daemonset-mjv2d
    container=daeaaf1c0551 shell=bash parent=runc cmdline=bash terminal=34816 container_id=daeaaf1c0551
    image=<NA>) k8s.ns=default k8s.pod=falco-daemonset-mjv2d container=daeaaf1c0551
    k8s.ns=default k8s.pod=falco-daemonset-mjv2d container=daeaaf1c0551","priority":"Notice","rule":"Terminal
    shell in container","time":"2020-02-13T20:47:39.535071657Z", "output_fields":
    {"container.id":"daeaaf1c0551","container.image.repository":null,"evt.time":1581626859535071657,"k8s.ns.name":"default","k8s.pod.name":"falco-daemonset-mjv2d","proc.cmdline":"bash","proc.name":"bash","proc.pname":"runc","proc.tty":34816,"user.name":"root"}}**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**{"output":"20:47:39.535071657: Notice A shell was spawned in a container
    with an attached terminal (user=root k8s.ns=default k8s.pod=falco-daemonset-mjv2d
    container=daeaaf1c0551 shell=bash parent=runc cmdline=bash terminal=34816 container_id=daeaaf1c0551
    image=<NA>) k8s.ns=default k8s.pod=falco-daemonset-mjv2d container=daeaaf1c0551
    k8s.ns=default k8s.pod=falco-daemonset-mjv2d container=daeaaf1c0551","priority":"Notice","rule":"Terminal
    shell in container","time":"2020-02-13T20:47:39.535071657Z", "output_fields":
    {"container.id":"daeaaf1c0551","container.image.repository":null,"evt.time":1581626859535071657,"k8s.ns.name":"default","k8s.pod.name":"falco-daemonset-mjv2d","proc.cmdline":"bash","proc.name":"bash","proc.pname":"runc","proc.tty":34816,"user.name":"root"}}**'
- en: 'Continuing on, the next two options tell Falco to log **Falco-level** events
    to **stderr** and **syslog**:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 继续下去，接下来的两个选项告诉Falco将**Falco-level**事件记录到**stderr**和**syslog**中：
- en: 'log_stderr: true'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'log_stderr: true'
- en: 'log_syslog: true'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'log_syslog: true'
- en: 'This setting does not have any impact on the events that your rules file will
    be monitoring, but rather configures how **Falco system events** will be logged:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置不会影响规则文件监视的事件，而是配置了**Falco系统事件**的日志记录方式：
- en: 'log_stderr: true'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'log_stderr: true'
- en: 'log_syslog: true'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 'log_syslog: true'
- en: 'log_level: info'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'log_level: info'
- en: The default for both options is **true**, so all events will be logged to **stderr**
    and **syslog**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 两个选项的默认值都是**true**，因此所有事件都将被记录到**stderr**和**syslog**中。
- en: Next is the logging level you want to capture, with accepted values including
    **emergency**, **alert**, **critical**, **error**, **warning**, **notice**, **info**,
    and **debug**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是您想要捕获的日志级别，接受的值包括**emergency**，**alert**，**critical**，**error**，**warning**，**notice**，**info**和**debug**。
- en: 'Continuing on, the priority level specifies the rulesets that will be used
    by Falco. Any ruleset that has a rule priority equal to or higher than the configured
    value will be evaluated by Falco to generate alerts:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，优先级级别指定了Falco将使用的规则集。任何规则集的规则优先级等于或高于配置值的规则将由Falco评估以生成警报：
- en: 'priority: debug'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'priority: debug'
- en: The default value is **debug**. Other values that can be set are **emergency**,
    **alert**, **critical**, **error**, **warning**, **notice**, and **info**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值为**debug**。可以设置的其他值包括**emergency**，**alert**，**critical**，**error**，**warning**，**notice**和**info**。
- en: 'Next up is the value to enable or disable **buffered_output**. By default,
    **buffered_outputs** is set to **false**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是启用或禁用**buffered_output**的值。默认情况下，**buffered_outputs**设置为**false**：
- en: 'buffered_outputs: false'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'buffered_outputs: false'
- en: To pass system calls, Falco uses a shared buffer that can fill up, and when
    the value is set to **true**, the buffer can be configured to tell Falco how to
    react. The default values are usually a good starting value for an initial configuration.
    The Falco team has a detailed explanation of dropped events on their main documentation
    page at [https://falco.org/docs/event-sources/dropped-events/](https://falco.org/docs/event-sources/dropped-events/).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了传递系统调用，Falco使用一个共享缓冲区，它可以填满，当值设置为**true**时，可以配置缓冲区告诉Falco如何做出反应。默认值通常是初始配置的良好起点。Falco团队在其主要文档页面[https://falco.org/docs/event-sources/dropped-events/](https://falco.org/docs/event-sources/dropped-events/)上对丢弃的事件有详细解释。
- en: 'The **syscall_events_drops** setting can be set to **ignore**, **log**, **alert**,
    and **exit**. The rate configures how often Falco will execute the configured
    actions. The value is actions per second, so this example tells Falco to execute
    one action every 30 seconds:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**syscall_events_drops**设置可以设置为**ignore**、**log**、**alert**和**exit**。速率配置了Falco执行配置的操作的频率。该值是每秒的操作数，因此此示例告诉Falco每30秒执行一次操作。'
- en: 'syscall_event_drops:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 'syscall_event_drops:'
- en: 'actions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'actions:'
- en: '- log'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '- 日志'
- en: '- alert'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '- 警报'
- en: 'rate: .03333'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'rate: .03333'
- en: 'max_burst: 10'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'max_burst: 10'
- en: 'The **outputs** section allows you to throttle the notifications from Falco,
    containing two values, **rate** and **max_burst**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**outputs**部分允许您限制来自Falco的通知，包含两个值，**rate**和**max_burst**。'
- en: 'outputs:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'outputs:'
- en: 'rate: 1'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'rate: 1'
- en: 'max_burst: 1000'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'max_burst: 1000'
- en: 'The **syslog_output** section tells Falco to output events to syslog. By default,
    this value is set to **true**:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**syslog_output**部分告诉Falco将事件输出到syslog。默认情况下，此值设置为**true**。'
- en: 'syslog_output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'syslog_output:'
- en: 'enabled: true'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: true'
- en: 'In certain use cases, you may want to configure Falco to output events to a
    file in addition to, or as a replacement to, stdout. By default, this is set to
    **false**, but you can enable it by setting it to **true** and providing a filename.
    The **keep_alive** value is set to **false** by default, which configures Falco
    to keep the file open and write data continuously without closing the file. If
    it is set to **false**, the file is opened for each event as they occur, and closed
    once the events have been written:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些用例中，您可能希望配置Falco将事件输出到文件中，除了或替代stdout。默认情况下，这被设置为**false**，但您可以通过将其设置为**true**并提供文件名来启用它。**keep_alive**值默认为**false**，这会配置Falco保持文件打开并连续写入数据而不关闭文件。如果设置为**false**，则会在每个事件发生时打开文件，并在事件写入后关闭文件。
- en: 'file_output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'file_output:'
- en: 'enabled: false'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: false'
- en: 'keep_alive: false'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'keep_alive: false'
- en: 'filename: ./events.txt'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '文件名: ./events.txt'
- en: 'By default, Falco will output events to **stdout**, so it is set to **true**.
    If you have a requirement to disable logging events to **stdout**, you can change
    this value to **false**:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Falco将事件输出到**stdout**，因此它被设置为**true**。如果您需要禁用将事件记录到**stdout**，可以将此值更改为**false**。
- en: 'stdout_output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 'stdout_output:'
- en: 'enabled: true'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: true'
- en: The **webserver** configuration is used to integrate Kubernetes audit events
    with Falco. By default, it is enabled to listen on port **8765** using HTTP.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**webserver**配置用于将Kubernetes审计事件与Falco集成。默认情况下，它启用了使用HTTP监听端口**8765**。'
- en: 'You can enable secure communication by changing the **ssl_enabled** value to
    **true**, and supplying a certificate for the **ssl_certificate** value:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将**ssl_enabled**值更改为**true**并为**ssl_certificate**值提供证书来启用安全通信。
- en: 'webserver:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'webserver:'
- en: 'enabled: true'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: true'
- en: 'listen_port: 8765'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'listen_port: 8765'
- en: 'k8s_audit_endpoint: /k8s_audit'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'k8s_audit_endpoint: /k8s_audit'
- en: 'ssl_enabled: false'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 'ssl_enabled: false'
- en: 'ssl_certificate: /etc/falco/falco.pem'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'ssl_certificate: /etc/falco/falco.pem'
- en: 'Falco can be configured to alerts to other systems. In our example configuration,
    they show an example using **jq** and **curl** to send an alert to a Slack channel.
    By default, this section is **disabled**, but if you want to call an external
    program when alerts are triggered, you can enable the option and provide the program
    to be executed. Similar to the file output described previously, the **keep_alive**
    option defaults to **false**, which tells Falco to run the program for each event:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Falco可以配置警报发送到其他系统。在我们的示例配置中，他们展示了使用**jq**和**curl**发送警报到Slack频道的示例。默认情况下，此部分是**禁用**的，但如果您想在触发警报时调用外部程序，可以启用该选项并提供要执行的程序。与先前描述的文件输出类似，**keep_alive**选项默认为**false**，这告诉Falco对每个事件运行程序。
- en: 'program_output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'program_output:'
- en: 'enabled: false'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: false'
- en: 'keep_alive: false'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 'keep_alive: false'
- en: 'program: "jq ''{text: .output}'' | curl -d @- -X POST https://hooks.slack.com/services/XXX"'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 'program: "jq ''{text: .output}'' | curl -d @- -X POST https://hooks.slack.com/services/XXX"'
- en: 'Falco can send alerts to an HTTP endpoint. We will be deploying an add-on for
    Falco called **falcosidekick**, which runs a web server to receive requests from
    the Falco pod. It is disabled by default, but we have enabled it and set it to
    the name of the service that will be created later in the chapter when we deploy
    **Falcosidekick**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Falco可以将警报发送到HTTP端点。我们将部署一个名为**falcosidekick**的Falco附加组件，它运行一个Web服务器，用于接收来自Falco
    pod的请求。它默认情况下是禁用的，但我们已经启用它，并将其设置为稍后在部署**Falcosidekick**时将创建的服务的名称：
- en: 'http_output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'http_output:'
- en: 'enabled: true'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: true'
- en: 'url: http://falcosidekick:2801'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'url: http://falcosidekick:2801'
- en: 'The remaining sections of the file are used to enable and configure a gRPC
    server. This is not a common configuration when using Falco with Kubernetes, and
    is only provided here since it''s in the base **falco.yaml** file:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的其余部分用于启用和配置gRPC服务器。这不是在Kubernetes中使用Falco时的常见配置，只是在基本的**falco.yaml**文件中提供，因为我们将在后面的章节中部署**Falcosidekick**时使用：
- en: 'grpc:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'grpc:'
- en: 'enabled: false'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: false'
- en: 'bind_address: "0.0.0.0:5060"'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 'bind_address: "0.0.0.0:5060"'
- en: 'threadiness: 8'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'threadiness: 8'
- en: 'private_key: "/etc/falco/certs/server.key"'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'private_key: "/etc/falco/certs/server.key"'
- en: 'cert_chain: "/etc/falco/certs/server.crt"'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 'cert_chain: "/etc/falco/certs/server.crt"'
- en: 'root_certs: "/etc/falco/certs/ca.crt"'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'root_certs: "/etc/falco/certs/ca.crt"'
- en: 'grpc_output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 'grpc_output:'
- en: 'enabled: false'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 'enabled: false'
- en: The base configuration is just the initial configuration file to a Falco deployment.
    It only sets the Falco system configuration; it doesn't create any rules, which
    are used to create alerts. In the next section, we will explain how to configure
    the files used to create Falco alerts.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 基本配置只是Falco部署的初始配置文件。它只设置Falco系统配置；它不创建任何规则，这些规则用于创建警报。在下一节中，我们将解释如何配置用于创建Falco警报的文件。
- en: Falco rules config files
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Falco规则配置文件
- en: Recall that in our configuration file, the first section had a key called **rules_files**
    and the key can have multiple values. The values that you provide will contain
    the filenames, which are mounted using a **configmap**, telling Falco what to
    audit and how to alert us about a given event.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在我们的配置文件中，第一部分有一个名为**rules_files**的键，该键可以有多个值。您提供的值将包含文件名，这些文件名使用**configmap**进行挂载，告诉Falco要审计什么以及如何警告我们有关给定事件的信息。
- en: 'Rules files can contain three types of elements:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 规则文件可以包含三种类型的元素：
- en: '**Rules**: Configures Falco alerts'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则**：配置Falco警报'
- en: '**Macros**: Creates a function that can shorten definitions in a rule'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宏**：创建一个可以缩短规则中定义的函数'
- en: '**Lists**: A collection of items that can be used in a rule'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列表**：可以在规则中使用的项目集合'
- en: In the upcoming subsections, we'll go over each of these elements.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将逐个讨论这些元素。
- en: Understanding rules
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解规则
- en: Falco includes a set of example Kubernetes rules that you can use as-is, or
    you can modify the existing rules to fit your specialized requirements.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Falco包括一组示例Kubernetes规则，您可以直接使用，或者您可以修改现有规则以满足您的专门要求。
- en: Falco is a powerful auditing system that enhances cluster security. Like any
    system that provides auditing, creating rules to monitor systems can become complex
    and Falco Kubernetes no exception. To use Falco effectively, you need to understand
    how it uses the rules files and how you can correctly customize the rules to fit
    your requirements.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Falco是一个强大的审计系统，可以增强集群安全性。与任何提供审计的系统一样，创建规则来监视系统可能变得复杂，而Falco Kubernetes也不例外。要有效使用Falco，您需要了解它如何使用规则文件，以及如何正确定制规则以满足您的要求。
- en: 'A default Falco installation will include three rulesets:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Falco安装将包括三个规则集：
- en: '![Table 12.1 – Rules files overview'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![12.1表 - 规则文件概述'
- en: '](image/B15514_Table_12.1.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15514_Table_12.1.jpg)'
- en: Table 12.1 – Rules files overview
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 12.1表 - 规则文件概述
- en: Each of the rules files have the same syntax, so before explaining each file
    in greater detail, let's explain how rules, macros, and lists work together to
    create rules.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规则文件都具有相同的语法，因此在更详细地解释每个文件之前，让我们解释一下规则、宏和列表如何共同工作以创建规则。
- en: Our first example will generate an alert when a pod that is not part of Kubernetes
    itself tries to contact the API server. This type of activity may signal that
    an attacker is looking to exploit the Kubernetes API server. To accomplish the
    most efficient alert, we don't want to generate alerts from pods that are part
    of the Kubernetes cluster that need to communicate with the API server.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个示例将在不属于Kubernetes本身的pod尝试联系API服务器时生成警报。这种活动可能表明攻击者正在寻求利用Kubernetes API服务器。为了实现最有效的警报，我们不希望从需要与API服务器通信的Kubernetes集群的pod生成警报。
- en: 'The included rules list includes this event. In the **falco_rules.yaml** file,
    there is a rule for API server communication:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 包含的规则列表包括此事件。在**falco_rules.yaml**文件中，有一个用于API服务器通信的规则：
- en: '- rule: Contact K8S API Server From Container'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '- 规则：从容器联系K8S API服务器'
- en: 'desc: Detect attempts to contact the K8S API Server from a container'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 描述：检测容器尝试联系K8S API服务器
- en: 'condition: evt.type=connect and evt.dir=< and (fd.typechar=4 or fd.typechar=6)
    and container and not k8s_containers and k8s_api_server'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 条件：evt.type=connect and evt.dir=< and (fd.typechar=4 or fd.typechar=6) and container
    and not k8s_containers and k8s_api_server
- en: 'output: Unexpected connection to K8s API Server from container (command=%proc.cmdline
    %container.info image=%container.image.repository:%container.image.tag connection=%fd.name)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：容器意外连接到K8s API服务器（命令=%proc.cmdline %container.info image=%container.image.repository:%container.image.tag
    connection=%fd.name）
- en: 'priority: NOTICE'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级：注意
- en: 'tags: [network, k8s, container, mitre_discovery]'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：[网络，k8s，容器，mitre_discovery]
- en: You can see that a rule may contain multiple conditions and values. Falco includes
    a large set of conditions that can be checked, so let's start by explaining this
    rule in detail.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到规则可能包含多个条件和值。Falco包括一大堆可以检查的条件，所以让我们从详细解释这个规则开始。
- en: 'To explain how this rule works, we break down each section in the following
    table:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释此规则的工作原理，我们将在以下表中分解每个部分：
- en: '![Table 12.2 – Parts of a Falco rule'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![12.2表 - Falco规则的部分'
- en: '](image/B15514_Table_12.2.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15514_Table_12.2.jpg)'
- en: Table 12.2 – Parts of a Falco rule
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 12.2表 - Falco规则的部分
- en: Most of the table is fairly straightforward, but the condition section has some
    complex logic that may not make much sense to you. Like most logging systems,
    Falco uses its own syntax for creating rule conditions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数表格都很简单，但条件部分有一些复杂的逻辑，可能对您来说没有太多意义。与大多数日志系统一样，Falco使用自己的语法来创建规则条件。
- en: Since rules can be difficult to create, the Falco community has provided an
    extensive list of premade rules. Many people will find that the community rules
    will fully meet their needs, but there are scenarios where you might need to create
    custom rules, or need to change one of the existing rules to reduce alerts for
    events you may not be concerned about. Before you attempt to create or change
    an event, you need to understand the full logic of a condition.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于规则可能很难创建，Falco社区提供了大量预制规则的详尽列表。许多人会发现社区规则完全满足他们的需求，但也有一些情况下，您可能需要创建自定义规则，或者需要更改现有规则以减少您可能不关心的事件的警报。在尝试创建或更改事件之前，您需要了解条件的完整逻辑。
- en: Covering all of the logic and syntax that Falco offers is beyond the scope of
    this book, but understanding the example rule is the first step to creating or
    editing existing rules.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖Falco提供的所有逻辑和语法超出了本书的范围，但理解示例规则是创建或编辑现有规则的第一步。
- en: Understanding conditions (fields and values)
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解条件（字段和值）
- en: The example condition contains a few different conditions that we will break
    down here into three sections to describe each part of the condition in steps.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 示例条件包含一些不同的条件，我们将在这里将其分解为三个部分，以步骤描述条件的每个部分。
- en: 'The first component of a condition is the **class** **fields**. A condition
    can contain multiple class fields and can be evaluated using standard **and**,
    **not**, or **equals** conditions. Breaking down the example condition, we are
    using the **event (evt)** and **file descriptor (fd)** class fields:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 条件的第一个组件是**类** **字段**。条件可以包含多个类字段，并可以使用标准的**和**、**非**或**等于**条件进行评估。分解示例条件，我们正在使用**事件（evt）**和**文件描述符（fd）**类字段：
- en: '![Figure 12.1 – Class field example'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.1 – 类字段示例'
- en: '](image/Fig_12.1_B15514.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.1_B15514.jpg)'
- en: Figure 12.1 – Class field example
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 类字段示例
- en: 'Each class may have a **field** value:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类可能有一个**字段**值：
- en: '![Figure 12.2 – Class Field Value'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.2 – 类字段值'
- en: '](image/Fig_12.2_B15514.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.2_B15514.jpg)'
- en: Figure 12.2 – Class Field Value
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 类字段值
- en: 'Finally, each field type will have a **value**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每个字段类型都将有一个**值**：
- en: '![Figure 12.3 – Values in conditions'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.3 – 条件中的值'
- en: '](image/Fig_12.3_B15514.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.3_B15514.jpg)'
- en: Figure 12.3 – Values in conditions
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 条件中的值
- en: Important note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can get a complete list of the available classes from Falco's website at
    [https://falco.org/docs/rules/supported-fields/](https://falco.org/docs/rules/supported-fields/).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从Falco的网站[https://falco.org/docs/rules/supported-fields/](https://falco.org/docs/rules/supported-fields/)获取可用类的完整列表。
- en: 'Falco has a number of class fields and values for rules. There are too many
    classes to explain in a single chapter, but to help with creating your own custom
    rules, we have provided an explanation using the original example condition:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Falco有许多规则的类字段和值。有太多的类无法在单独的章节中解释，但为了帮助您创建自己的自定义规则，我们提供了使用原始示例条件的解释：
- en: 'condition: evt.type=connect and evt.dir=< and (fd.typechar=4 or fd.typechar=6)
    and container and not k8s_containers and k8s_api_server'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 条件：evt.type=connect and evt.dir=< and (fd.typechar=4 or fd.typechar=6) and container
    and not k8s_containers and k8s_api_server
- en: 'The following table explains the event class and its values:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格解释了事件类及其值：
- en: '![Table 12.3 – Event class example'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![表12.3 – 事件类示例'
- en: '](image/B15514_Table_12.3.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15514_Table_12.3.jpg)'
- en: Table 12.3 – Event class example
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.3 – 事件类示例
- en: 'Along with using the event class, the rule also uses the file descriptor class,
    which is explained as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用事件类，规则还使用了文件描述符类，其解释如下：
- en: '![Table 12.4 – File descriptor example'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![表12.4 – 文件描述符示例'
- en: '](image/B15514_Table_12.4.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15514_Table_12.4.jpg)'
- en: Table 12.4 – File descriptor example
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.4 – 文件描述符示例
- en: The last part of the rule that starts with **and container** value will include
    any container. However, since we do not want to send alerts for valid communications
    from Kubernetes itself, the value **and not k8s_containers and k8s_api_server**
    tells the condition to omit the Kubernetes container and the **api_server**. The
    values in this example use macros that have been defined in the **falco_rules.yaml**
    file. We will discuss macros in the next section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以**and container**值开头的规则的最后部分将包括任何容器。但是，由于我们不希望发送来自Kubernetes本身的有效通信的警报，值**and
    not k8s_containers and k8s_api_server**告诉条件省略Kubernetes容器和**api_server**。此示例中的值使用了在**falco_rules.yaml**文件中定义的宏。我们将在下一节讨论宏。
- en: Using macros
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用宏
- en: Macros allow you to create a collection to make rule creation quicker and easier.
    In the previous example, the condition used two macros, **k8s_containers** and
    **k8s_api_server.**
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 宏允许您创建一个集合，以便更快、更容易地创建规则。在前面的示例中，条件使用了两个宏，**k8s_containers**和**k8s_api_server**。
- en: 'The **k8s_containers** macro has been defined to contain the condition:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**k8s_containers**宏已被定义为包含条件：'
- en: In a local/user rules file, list the namespace or container images that are
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地/用户规则文件中，列出与K8s API服务器进行通信的命名空间或容器映像
- en: allowed to contact the K8s API Server from within a container. This
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 允许从容器内部联系K8s API服务器。这
- en: might cover cases where the K8s infrastructure itself is running
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可能涵盖K8s基础设施本身正在运行的情况
- en: within a container.
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在容器内。
- en: '- macro: k8s_containers'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '- 宏：k8s_containers'
- en: 'condition: >'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 条件：>
- en: (container.image.repository in (gcr.io/google_containers/hyperkube-amd64,
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: （container.image.repository in (gcr.io/google_containers/hyperkube-amd64，
- en: gcr.io/google_containers/kube2sky, sysdig/agent, sysdig/falco,
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: gcr.io/google_containers/kube2sky，sysdig/agent，sysdig/falco，
- en: sysdig/sysdig, falcosecurity/falco) or (k8s.ns.name = "kube-system"))
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: sysdig/sysdig，falcosecurity/falco）或（k8s.ns.name = "kube-system"）
- en: 'Macros, like rules, use classes to create conditions. To evaluate **k8s_containers**
    condition, macros use two classes:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 宏和规则一样，使用类来创建条件。要评估**k8s_containers**条件，宏使用两个类：
- en: The **container.image.repository** class field, which validates the repositories
    for the condition.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**container.image.repository** 类字段，用于验证条件的存储库。'
- en: The **k8s.ns.name** class field, which is used to include any containers running
    in the **kube-system** namespace.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k8s.ns.name**类字段，用于包括在**kube-system**命名空间中运行的任何容器。'
- en: 'The **k8s_api_server** has been defined to contain the condition:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**k8s_api_server**已被定义为包含条件：'
- en: '- macro: k8s_api_server'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '- 宏：k8s_api_server'
- en: 'condition: (fd.sip.name="kubernetes.default.svc.cluster.local")'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 条件：（fd.sip.name="kubernetes.default.svc.cluster.local"）
- en: For the **k8s_api_server** condition, macros use a single class field to evaluate
    the condition – the **fd.sip.name** class field – which checks the domain name
    of the **server IP** (**SIP**). If it is equal to **kubernetes.default.svc.cluster.local**
    it is considered a match.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**k8s_api_server**条件，宏使用一个类字段来评估条件 - **fd.sip.name**类字段 - 用于检查**服务器IP**（**SIP**）的域名。如果它等于**kubernetes.default.svc.cluster.local**，则被视为匹配。
- en: Using both of the preceding macros for the rules condition will stop any Kubernetes
    cluster pods from generating alerts when communicating with the API server.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述两个宏来进行规则条件将阻止任何Kubernetes集群pod在与API服务器通信时生成警报。
- en: Understanding lists
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解列表
- en: Lists allow you to group items into a single object that can be used in rules,
    macros, or nested in other lists.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列表允许您将项目分组到一个单一的对象中，该对象可以在规则、宏中使用，或者嵌套在其他列表中。
- en: 'A list only requires two keys in a rules file, **list** and **items**. For
    example, rather than listing a number of binaries on a condition, you could group
    the binaries into a **list**:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 规则文件中列表只需要两个键，**list** 和 **items**。例如，您可以将二进制文件分组到一个**列表**中，而不是在条件中列出多个二进制文件：
- en: '- list: editors'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '- 列表：编辑器'
- en: 'items: [vi, nano, emacs]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: items：[vi，nano，emacs]
- en: Using lists allows you to use a single entry, rather than including multiple
    items in a condition.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表允许您使用单个条目，而不是在条件中包含多个项目。
- en: Rules can be challenging, but as you read more of the included rules and start
    to create your own, it will become easier. So far, we have introduced the basics
    on how to create rules, macros, and lists. With a basic understanding of these
    objects under our belts, we will move on to the next configuration file where
    you will create and append Falco rules.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 规则可能具有挑战性，但随着您阅读更多包含的规则并开始创建自己的规则，它将变得更容易。到目前为止，我们已经介绍了如何创建规则、宏和列表的基础知识。在掌握了这些对象的基本理解之后，我们将继续下一个配置文件，您将在其中创建并附加
    Falco 规则。
- en: Creating and appending to custom rules
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和附加自定义规则
- en: Falco comes with a number of base rules that are located in the **falco_rules.yaml**
    file. This file should never be edited – if you need to change or create a new
    rule, you should edit the **falco_rules.local.yaml** file.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Falco 自带许多基本规则，这些规则位于 **falco_rules.yaml** 文件中。永远不要编辑此文件 - 如果您需要更改或创建新规则，应编辑
    **falco_rules.local.yaml** 文件。
- en: Appending to an existing rule
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附加到现有规则
- en: Important note
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: You are not limited to only appending to rules. Falco allows you to append rules,
    macros, and lists.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您不仅限于附加规则。Falco 允许您附加规则、宏和列表。
- en: The included **falco_rules.local.yaml** is empty by default. You only need to
    edit this file if an existing rule needs to be modified or removed or a new rule
    needs to be added. Since the file is used to change or add values to the base
    **falco_rules.yaml** file, the order in which the files are used by Falco is very
    important.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，包含的 **falco_rules.local.yaml** 是空的。只有在需要修改或删除现有规则或添加新规则时，您才需要编辑此文件。由于该文件用于更改或添加值到基本的
    **falco_rules.yaml** 文件，因此 Falco 使用文件的顺序非常重要。
- en: 'Falco will build rules based on the name from all rules files. The files are
    read and evaluated in the order that they are referenced in the base Falco configuration
    file. The base file that we used as an example at the beginning of this chapter
    has the following order for its rules files:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Falco 将根据所有规则文件的名称构建规则。这些文件按照它们在基本 Falco 配置文件中被引用的顺序进行读取和评估。我们在本章开头使用的示例基本文件具有以下规则文件的顺序：
- en: 'rules_file:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 规则文件：
- en: '- /etc/falco/falco_rules.yaml'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '- /etc/falco/falco_rules.yaml'
- en: '- /etc/falco/falco_rules.local.yaml'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '- /etc/falco/falco_rules.local.yaml'
- en: '- /etc/falco/k8s_audit_rules.yaml'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '- /etc/falco/k8s_audit_rules.yaml'
- en: Notice that the **falco.rules.local.yaml** file is after the base **falco_rules.yaml**
    file. Keeping control of the order of the files will help you to track any expected/unexpected
    behaviors of your rules.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**falco.rules.local.yaml** 文件位于基本 **falco_rules.yaml** 文件之后。控制文件的顺序将帮助您跟踪规则的任何预期/非预期行为。
- en: Using an example from the Falco documentation, let's show how to append to a
    rule.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Falco 文档中的示例，让我们看看如何附加到一个规则。
- en: 'The original rule from **falco_rules.yaml** is shown in the following code
    block:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**falco_rules.yaml** 中的原始规则显示在以下代码块中：'
- en: '- rule: program_accesses_file'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '- 规则：program_accesses_file'
- en: 'desc: track whenever a set of programs opens a file'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 描述：跟踪一组程序何时打开文件
- en: 'condition: proc.name in (cat, ls) and evt.type=open'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 条件：proc.name in (cat, ls) and evt.type=open
- en: 'output: a tracked program opened a file (user=%user.name command=%proc.cmdline
    file=%fd.name)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：已跟踪程序打开了一个文件（用户=%user.name 命令=%proc.cmdline 文件=%fd.name）
- en: 'priority: INFO'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级：信息
- en: As the description states, this rule will trigger whenever a set of programs
    opens a file. The condition will trigger when **cat** or **ls** is used to open
    a file.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如描述所述，此规则将在一组程序打开文件时触发。当使用 **cat** 或 **ls** 打开文件时，条件将触发。
- en: The current rule does not omit the open operation from any users. You have decided
    that you do not need to know when the root user uses either **cat** or **ls**
    to open a file, and you want to stop Falco from generating alerts for root.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当前规则不会忽略任何用户的打开操作。您已经决定，您不需要知道root用户何时使用**cat**或**ls**打开文件，并且您希望阻止Falco为root生成警报。
- en: 'In the **falco_rules.local.yaml** file, you need to create an **append** for
    the existing rule. To append to a rule, you must use the same rule name, then
    add **append: true** and any changes you want to make to the rule. An example
    is shown in the following snippet:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '在**falco_rules.local.yaml**文件中，您需要为现有规则创建一个**append**。要附加到规则，您必须使用相同的规则名称，然后添加**append:
    true**和您想要对规则进行的任何更改。以下代码段中显示了一个示例：'
- en: '- rule: program_accesses_file'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '- rule: program_accesses_file'
- en: 'append: true'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 'append: true'
- en: 'condition: and not user.name=root'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 条件：并且用户名称不是root
- en: Creating a new rule is easier than appending to an existing rule. Let's see
    how it works.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新规则比附加到现有规则更容易。让我们看看它是如何工作的。
- en: Creating a new rule
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建新规则
- en: Since you are creating a new rule, you only need to add a standard rule to the
    **falco_rules.local.yaml**. As it is a new rule, it will simply be added to the
    list of rules that Falco uses to create alerts.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您正在创建一个新规则，您只需要将标准规则添加到**falco_rules.local.yaml**中。作为一个新规则，它将简单地添加到Falco用于创建警报的规则列表中。
- en: Important note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Falco's configuration files are read from a ConfigMap, so you will need to restart
    the Falco pods if you change any values in the ConfigMap.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Falco的配置文件是从ConfigMap中读取的，因此如果更改了ConfigMap中的任何值，您将需要重新启动Falco pods。
- en: Congratulations! A lot of information has been presented to you here, and you
    probably want to see Falco in action to put your knowledge to work. In the next
    section, we explain how to deploy Falco, and you will finally get to see it in
    action.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！这里向您呈现了大量信息，您可能希望看到Falco的实际操作以将您的知识付诸实践。在下一节中，我们将解释如何部署Falco，您最终将看到它的实际操作。
- en: Deploying Falco
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Falco
- en: We have included a script to deploy Falco, called **falco-install.sh**, in the
    GitHub repository in the **chapter12** folder.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在GitHub存储库的**chapter12**文件夹中包含了一个用于部署Falco的脚本，名为**falco-install.sh**。
- en: The two most popular methods of deploying Falco to a Kubernetes cluster are
    using the official Helm chart or a DaemonSet manifest from the Falco repo. For
    the purposes of this module, we will deploy Falco using a modified DaemonSet installation
    from the book's GitHub repository.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 将Falco部署到Kubernetes集群的两种最流行的方法是使用官方Helm图表或来自Falco存储库的DaemonSet清单。对于本模块的目的，我们将使用书籍GitHub存储库中修改过的DaemonSet安装来部署Falco。
- en: To deploy Falco using the included script, execute the script from within the
    **chapter12** folder by executing **./install-falco.sh**. We have also included
    a script called **delete-falco.sh** in the same directory that will remove Falco
    from the cluster.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含的脚本部署Falco，通过在**chapter12**文件夹中执行**./install-falco.sh**脚本来执行。我们还在同一目录中包含了一个名为**delete-falco.sh**的脚本，它将从集群中删除Falco。
- en: The steps that the script performs are detailed in the following list and will
    be explained in additional detail in this section.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本执行的步骤在以下列表中详细说明，并将在本节中进一步详细解释。
- en: 'The script executes the following tasks in two sections:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本在两个部分中执行以下任务：
- en: 'In **Section 1**, it creates a Falco probe and performs the following steps:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在**第1节**中，它创建了一个Falco探针并执行以下步骤：
- en: Installs Go using **apt**
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**apt**安装Go
- en: Pulls Falco's **driverkit-builder** container
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拉取Falco的**driverkit-builder**容器
- en: Pulls the driverkit source from Git and builds the executable
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Git中拉取driverkit源代码并构建可执行文件
- en: Creates an ubuntu-generic Falco probe using driverkit
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用driverkit创建一个ubuntu-generic Falco探针
- en: Copies **falco.ko** to the **modules** folder
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将**falco.ko**复制到**modules**文件夹中
- en: Adds a Falco probe using **modprobe**
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**modprobe**添加Falco探针
- en: 'In **Section 2**, it adds Falco to the cluster, performing the following steps:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在**第2节**中，它将Falco添加到集群中，执行以下步骤：
- en: Creates a Falco namespace
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Falco命名空间
- en: Creates a ConfigMap called **falco-config** from the files in **falco/falco-config**
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**falco/falco-config**中的文件创建一个名为**falco-config**的ConfigMap
- en: Deploys the Falco DaemonSet
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署Falco DaemonSet
- en: To better understand the installation scripts and why these steps are required,
    we will explain the installation details, starting with Falco probes.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解安装脚本以及为什么需要这些步骤，我们将从Falco探针开始解释安装细节。
- en: Falco kernel module
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Falco内核模块
- en: Falco deploys a kernel module to monitor system calls on the host system. Since
    kernel modules must be compatible with the host kernel, you need to have a module
    that works with the worker node's host operating system.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Falco部署一个内核模块来监视主机系统的系统调用。由于内核模块必须与主机内核兼容，因此您需要一个与工作节点的主机操作系统兼容的模块。
- en: 'Falco attempts to load or create a module in a few different ways:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Falco尝试以几种不同的方式加载或创建模块：
- en: If there is a pre-built module available for the hosts kernel, Falco will download
    and use the module automatically.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果主机内核有预构建的模块可用，Falco将自动下载并使用该模块。
- en: If no pre-built module exists for the worker node's kernel, Falco will attempt
    to build a module using any installed kernel-headers from the host.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果工作节点的内核没有预先构建的模块，Falco将尝试使用主机上安装的任何内核头来构建模块。
- en: At the time of writing, Falco offers an early-access alternative method for
    Falco probes, where they are created using a utility called **driverkit**. This
    new utility automates the creation of a new probe based on the kernel information
    of the host machine. The process of creating a probe using driverkit will be covered
    in detail since we will use it to create a Falco probe for our KinD cluster.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Falco提供了一种早期访问替代方法来创建Falco探针，其中它们使用一个名为**driverkit**的实用程序创建。这个新实用程序自动根据主机机器的内核信息创建一个新的探针。使用driverkit创建探针的过程将被详细介绍，因为我们将使用它来为我们的KinD集群创建一个Falco探针。
- en: Important note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If your nodes do not have the correct kernel-headers installed, Falco pods will
    attempt to download a precompiled probe that matched the host's kernel version.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的节点没有安装正确的内核头，Falco pods将尝试下载与主机内核版本匹配的预编译探针。
- en: 'You can find your kernel information by executing **uname -r** on your host,
    then check for support by searching the available probes at the following link:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在主机上执行**uname -r**来查找您的内核信息，然后通过搜索以下链接中的可用探针来检查支持：
- en: '[https://s3.amazonaws.com/download.draios.com/stable/sysdig-probe-binaries/index.html](https://s3.amazonaws.com/download.draios.com/stable/sysdig-probe-binaries/index.html)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://s3.amazonaws.com/download.draios.com/stable/sysdig-probe-binaries/index.html](https://s3.amazonaws.com/download.draios.com/stable/sysdig-probe-binaries/index.html)'
- en: Since this requires internet connectivity, it may not be an option for you to
    use in an enterprise environment where many servers run in air-gapped environments.
    In this type of environment, it is more common to use the driverkit or kernel-headers
    creation methods.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这需要互联网连接，因此在许多服务器在空隔离环境中运行的企业环境中，您可能无法使用此选项。在这种类型的环境中，更常见的是使用driverkit或内核头创建方法。
- en: Creating a kernel module using installed kernel headers
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用已安装的内核头创建内核模块
- en: Important note
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As I mentioned, we will not be using this method to create a kernel module.
    This section is only for your reference. We will instead be using driverkit, which
    is covered in the next section
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所提到的，我们将不使用此方法来创建内核模块。这节只是供您参考。相反，我们将使用driverkit，这将在下一节中介绍
- en: On a standard Kubernetes node, you may or may not need to install the Linux
    headers. Depending on how you created your base worker nodes, the kernel-headers
    may already be included with your installation. If a module isn't available and
    you do not have the headers installed on the hosts, the Falco pods will fail to
    start and the pods will go into a **crashloopback** state. This means that before
    deploying Falco, you need to have your module creation process selected and configured.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的Kubernetes节点上，您可能需要或不需要安装Linux头文件。取决于您如何创建基本的worker节点，内核头文件可能已经包含在您的安装中。如果模块不可用，并且您在主机上没有安装头文件，Falco
    pods将无法启动，并且pods将进入**crashloopback**状态。这意味着在部署Falco之前，您需要选择和配置好您的模块创建过程。
- en: The required packages, version, and repository are different for various Linux
    installations. If you intend to install the headers on your nodes, you will need
    to know what modules are required, along with any additional repos. Since we have
    been using Ubuntu as our distribution for the hands-on exercises, we will provide
    the steps to add the kernel-headers for Ubuntu systems.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的Linux安装需要不同的必需包、版本和存储库。如果您打算在节点上安装头文件，您需要知道需要哪些模块，以及任何额外的存储库。由于我们一直在使用Ubuntu作为我们的实践分发，我们将提供为Ubuntu系统添加内核头文件的步骤。
- en: Using headers to create the Falco module
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用头文件创建Falco模块
- en: Falco has introduced a utility called DriverKit that we will use to create the
    kernel module for our KinD Falco installation. We include the process to use kernel-headers
    as a backup procedure in cases where the Falco DriverKit may not support your
    Linux distribution.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Falco已经推出了一个叫做DriverKit的实用工具，我们将使用它来为我们的KinD Falco安装创建内核模块。我们包括使用kernel-headers的过程作为备用程序，以防Falco
    DriverKit可能不支持您的Linux发行版。
- en: If you plan to have Falco create a kernel module using headers, the first step
    is to download the kernel-headers for your Linux release.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用头文件让Falco创建内核模块，第一步是为您的Linux发行版下载内核头文件。
- en: To download the correct headers for Ubuntu, you can use the **uname -r** command
    along with **apt get** for **linux-headers**.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载Ubuntu的正确头文件，您可以使用**uname -r**命令以及**apt get**来获取**linux-headers**。
- en: '**sudo apt install linux-headers-$(uname -r)**'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '**sudo apt install linux-headers-$(uname -r)**'
- en: '**uname -r** will append the kernel version that is running on the host, providing
    the **apt install** command with the running kernel. On our example host, the
    running kernel is **4.4.0-142-generic**, making our **apt install** command **sudo
    apt install linux-headers- linux-headers-4.4.0-142-generic**.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**uname -r**将附加在主机上运行的内核版本，为**apt install**命令提供运行内核。在我们的示例主机上，运行的内核是**4.4.0-142-generic**，使我们的**apt
    install**命令为**sudo apt install linux-headers- linux-headers-4.4.0-142-generic**。'
- en: After installation, you can verify that the headers have been added by looking
    at the **/lib/modules/** directory, where you will see a directory named after
    the kernel version; in our example, this is **4.4.0-142-generic**.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，您可以通过查看**/lib/modules/**目录来验证是否已添加了头文件，您将看到一个以内核版本命名的目录；在我们的示例中，这是**4.4.0-142-generic**。
- en: Important note
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The headers must be installed on every worker node that will be running Falco.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 头文件必须安装在每个将运行Falco的worker节点上。
- en: Now that the headers are installed, the Falco pods will build a kernel module
    when they start up using the installed headers on the worker node.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在头文件已经安装，Falco pods将在启动时使用worker节点上安装的头文件构建内核模块。
- en: As discussed earlier, a newer method has come out from the team that uses a
    utility called driverkit. This process creates a kernel module that you can add
    to a host using modprobe. We have selected this as our probe creation process
    to make deploying Falco on a KinD cluster easier than using the header creation
    process.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，团队提出了一种新的方法，使用一个名为driverkit的实用程序。这个过程创建了一个内核模块，您可以使用modprobe添加到主机上。我们选择这个作为我们的探针创建过程，以使在KinD集群上部署Falco比使用头文件创建过程更容易。
- en: Creating a kernel module using driverkit
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用driverkit创建内核模块
- en: There are specialized use cases where installing kernel-headers may be challenging
    or impossible. If you cannot use the headers to build your module, you can create
    a module using a Falco utility called driverkit.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 有专门的用例，安装内核头文件可能具有挑战性或不可能。如果您无法使用头文件构建模块，可以使用一个名为driverkit的Falco实用程序创建模块。
- en: 'Driverkit allows you to create a kernel module for a number of different Linux
    distributions. At the time of writing, this utility currently supports the following
    distributions:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Driverkit允许您为许多不同的Linux发行版创建内核模块。在撰写本文时，此实用程序目前支持以下发行版：
- en: Ubuntu-generic
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ubuntu-generic
- en: Ubuntu-aws
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ubuntu-aws
- en: CentOS 8
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS 8
- en: CentOS 7
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS 7
- en: CentOS 6
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS 6
- en: AmazonLinux
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AmazonLinux
- en: AmazonLinux2
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AmazonLinux2
- en: Debian
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debian
- en: Vanilla Kernel
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanilla Kernel
- en: The team is actively looking for suggestions for other distributions, so we
    can be sure that additional distributions will be added as driverkit is developed.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 团队正在积极寻求其他发行版的建议，因此可以确保随着driverkit的开发，将添加其他发行版。
- en: We will go over the details to create a module for Ubuntu, using the Ubuntu-generic
    option.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细介绍为Ubuntu创建模块的细节，使用Ubuntu-generic选项。
- en: Driverkit requirements
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Driverkit要求
- en: 'Before you can create a module using driverkit, you need to meet a few prerequisites:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用driverkit创建模块之前，您需要满足一些先决条件：
- en: A running Docker daemon.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在运行的Docker守护程序。
- en: Go should be installed (since we are using Ubuntu, we will use **longsleep/golang-backports**).
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应安装Go（因为我们使用的是Ubuntu，我们将使用**longsleep/golang-backports**）。
- en: Your target kernel version and kernel revision.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的目标内核版本和内核修订版本。
- en: If you are going to use the installation script in the GitHub repository, all
    of the build and module installation steps are taken care of, but to better understand
    the process, we will explain it in full in the next section.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打算在GitHub存储库中使用安装脚本，所有构建和模块安装步骤都已处理，但为了更好地理解该过程，我们将在下一节中对其进行全面解释。
- en: Installing Falco's driverkit
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Falco的driverkit
- en: 'The first step to building a kernel module is to install the required dependencies
    for driverkit:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 构建内核模块的第一步是安装driverkit所需的依赖项：
- en: 'The first requirement is to install Go. Since we are using Ubuntu, we can install
    Go using **snap**:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个要求是安装Go。由于我们使用的是Ubuntu，我们可以使用**snap**安装Go：
- en: '**sudo snap install --classic go**'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**sudo snap install --classic go**'
- en: You should already have Go variables in your profile from the KinD installation
    in [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*, Kubernetes
    Bootcamp*. If you are using a machine that is different from your KinD host, add
    any required Go variables.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该已经在您的配置文件中有来自KinD安装的Go变量[*第5章*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*，Kubernetes
    Bootcamp*。如果您使用的是与您的KinD主机不同的机器，请添加任何所需的Go变量。
- en: 'We have selected to build using the Docker build method. There are multiple
    methods documented on the driverkit project page with which you can build the
    module if you want to use a different build method. We will pull the Docker image
    so it''s ready to execute when we run the build:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择使用Docker构建方法进行构建。在driverkit项目页面上记录了多种方法，您可以使用这些方法构建模块。我们将拉取Docker镜像，以便在运行构建时准备执行：
- en: '**docker pull falcosecurity/driverkit-builder**'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**docker pull falcosecurity/driverkit-builder**'
- en: 'Once the container has been downloaded, we can build the driverkit executable.
    The build process will download the source from GitHub and then use Go to create
    the executable file. The complete process will take a few minutes to complete:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器下载完成后，我们可以构建driverkit可执行文件。构建过程将从GitHub下载源代码，然后使用Go创建可执行文件。完整的过程将需要几分钟来完成：
- en: '**GO111MODULE="on" go get github.com/falcosecurity/driverkit**'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**GO111MODULE="on" go get github.com/falcosecurity/driverkit**'
- en: 'The executable will be created in your Go path. To verify that the driverkit
    executable was created successfully, check the version by typing the following
    command:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可执行文件将被创建在您的Go路径中。要验证driverkit可执行文件是否成功创建，请输入以下命令检查版本：
- en: '**driverkit -v**'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**driverkit -v**'
- en: 'This may return a version number, or in the current early release, it may just
    return the following:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可能返回一个版本号，或者在当前的早期版本中，它可能只返回以下内容：
- en: '**driverkit version -+**'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**driverkit version -+**'
- en: 'If the driverkit command returns **-+** or a version number, it was successfully
    created. However, if you received a **driverkit: command not found** error when
    you checked the version, the build may have failed or your Go path may not have
    been set correctly in your environment variable. If you cannot find the executable
    after running the build, verify that your Go environment variables are correct,
    and run the Go build step again.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '如果driverkit命令返回**-+**或版本号，则成功创建。但是，如果在检查版本时收到**driverkit: command not found**错误，则构建可能失败，或者您的Go路径可能没有在环境变量中正确设置。如果在运行构建后找不到可执行文件，请验证您的Go环境变量是否正确，并重新运行Go构建步骤。'
- en: Creating the module and adding it to the host
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建模块并将其添加到主机
- en: With driverkit built and verified, we can build our module and add it to the
    host.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 使用driverkit构建和验证后，我们可以构建我们的模块并将其添加到主机。
- en: 'Before building the module, we need to know the kernel version and release
    of our host. For our example, we will use the KinD cluster we have been using
    for the previous chapters in this book. Linux has some commands built in to get
    the two details we need:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模块之前，我们需要知道主机的内核版本和发布版。在我们的示例中，我们将使用本书前几章中一直在使用的KinD集群。Linux内置了一些命令来获取我们需要的这两个细节：
- en: To get the kernel version, execute **uname -v**, and for the release, **uname
    -r**:![Figure 12.4 – Docker host Kernel version
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取内核版本，执行**uname -v**，要获取发布版，执行**uname -r**：![图12.4 - Docker主机内核版本
- en: '](image/Fig_12.4_B15514.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.4_B15514.jpg)'
- en: Figure 12.4 – Docker host Kernel version
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 - Docker主机内核版本
- en: The version is the number after the **#** symbol and before the dash. On our
    host, we have a version of 100\. The release is the full name that was returned
    from the **uname -r** command. You will need provide both of these to the **driverkit**
    command to build the kernel module.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 版本是**#**符号后和破折号前的数字。在我们的主机上，我们有一个版本为100。发布版是从**uname -r**命令返回的完整名称。您需要将这两者都提供给**driverkit**命令来构建内核模块。
- en: 'If you are using the installation script, we retrieve the options and supply
    them automatically. If you are doing this step manually, you can use the following
    two lines of code to store the information in variables to be passed to the build
    command:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您正在使用安装脚本，我们会检索选项并自动提供它们。如果您正在手动执行此步骤，您可以使用以下两行代码将信息存储在变量中，以便传递给构建命令：
- en: kernelversion=$(uname -v | cut -f1 -d'-' | cut -f2 -d'#')
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: kernelversion=$(uname -v | cut -f1 -d'-' | cut -f2 -d'#')
- en: kernelrelease=$(uname -r)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: kernelrelease=$(uname -r)
- en: We use the **cut** command to remove the unnecessary information from the **uname
    -v** command and store it in a variable called **kernelversion**. We also store
    the output from the **uname -r** command in a variable called **kernelrelease**.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**cut**命令从**uname -v**命令中删除不必要的信息，并将其存储在名为**kernelversion**的变量中。我们还将**uname
    -r**命令的输出存储在名为**kernelrelease**的变量中。
- en: 'Now, you can use the Docker image we pulled and the driverkit executable to
    create the module:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以使用我们拉取的Docker镜像和driverkit可执行文件来创建模块：
- en: '**driverkit docker --output-module /tmp/falco.ko --kernelversion=$kernelversion
    --kernelrelease=$kernelrelease --driverversion=dev --target=ubuntu-generic**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**driverkit docker --output-module /tmp/falco.ko --kernelversion=$kernelversion
    --kernelrelease=$kernelrelease --driverversion=dev --target=ubuntu-generic**'
- en: 'The module build process will take a minute, and once the build completes,
    driverkit will show you the location of the new module:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模块构建过程将需要一分钟，一旦构建完成，driverkit将向您显示新模块的位置：
- en: '**INFO driver building, it will take a few seconds   processor=docker**'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**INFO driver building, it will take a few seconds   processor=docker**'
- en: '**INFO kernel module available                       path=/tmp/falco.ko**'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '**INFO kernel module available                       path=/tmp/falco.ko**'
- en: 'For the last step to add the new module, we need to copy it to the correct
    location and load the module using **modprobe**:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新模块的最后一步是将其复制到正确的位置并使用**modprobe**加载模块：
- en: sudo cp /tmp/falco.ko /lib/modules/$kernelrelease/falco.ko
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: sudo cp /tmp/falco.ko /lib/modules/$kernelrelease/falco.ko
- en: sudo depmod
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: sudo depmod
- en: sudo modprobe falco
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: sudo modprobe falco
- en: 'You can verify that the module has been added by running **lsmod**:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过运行**lsmod**来验证模块是否已添加：
- en: '**lsmod | grep falco**'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**lsmod | grep falco**'
- en: 'If the load was successful, you will see an output similar to the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如果加载成功，您将看到类似以下的输出：
- en: '**falco                 634880  4**'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '**falco                 634880  4**'
- en: That's it! You now have the Falco module on the host and it will be available
    to your KinD cluster.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！您现在在主机上有了Falco模块，并且它将可用于您的KinD集群。
- en: Using the module on a cluster
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在集群上使用模块
- en: On a standard Kubernetes cluster, a Falco deployment will map the **/dev** mount
    in the Falco container to the host's **/dev** mount. By mounting **/dev**, the
    Falco pod can use the kernel module that is running on the worker node's host
    operating system.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的Kubernetes集群上，Falco部署将**/dev**挂载在Falco容器中，到主机的**/dev**挂载。通过挂载**/dev**，Falco
    pod可以使用运行在工作节点主机操作系统上的内核模块。
- en: Using the module in KinD
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在KinD中使用模块
- en: You may be asking yourself how adding the Falco module to the host will make
    it available to a KinD cluster? We only added it to the host itself, and the KinD
    cluster is a container running in another Docker container. So, how can a KinD
    pod use a module from the Docker host?
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问自己，将Falco模块添加到主机上如何使其对KinD集群可用？我们只是将其添加到主机本身，而KinD集群是在另一个Docker容器中运行的容器。那么，KinD
    pod如何使用来自Docker主机的模块呢？
- en: 'Remember that KinD has a feature to mount extra volumes when it starts the
    KinD containers? In our installation, we added a mount point for **/dev:/dev**,
    which will create a mount inside our container that mounts to the host''s **/dev**
    filesystem. If we look at the host''s **/dev** filesystem, we will see Falco entries
    in the list, noted by the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，KinD在启动KinD容器时有一个功能可以挂载额外的卷？在我们的安装中，我们为**/dev:/dev**添加了一个挂载点，这将在我们的容器内创建一个挂载点，挂载到主机的**/dev**文件系统。如果我们查看主机的**/dev**文件系统，我们将看到列表中有Falco条目，如下所示：
- en: '**cr--------  1 root root    244,   0 May  4 00:58 falco0**'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '**cr--------  1 root root    244,   0 May  4 00:58 falco0**'
- en: This is what the Falco pod will use as its module when it starts up.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Falco pod在启动时将使用的模块。
- en: But wait! We just said that **/dev** is mounted in our KinD container, pointing
    to the host's **/dev** filesystem. So how does a container in the Kubernetes cluster
    have access to the **/dev** filesystem?
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等！我们刚刚说过**/dev**在我们的KinD容器中被挂载，指向主机的**/dev**文件系统。那么Kubernetes集群中的容器如何访问**/dev**文件系统呢？
- en: If we take a look at the Falco DaemonSet file we will use in the next section,
    we will see that the manifest creates a few mount points for the pod.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下接下来将在下一节中使用的Falco DaemonSet文件，我们将看到该清单为pod创建了一些挂载点。
- en: 'One of the **volumeMount** entries is as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个**volumeMount**条目如下：
- en: '- mountPath: /host/dev'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '- mountPath: /host/dev'
- en: 'name: dev-fs'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 'name: dev-fs'
- en: 'readOnly: true'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 'readOnly: true'
- en: 'The **volumeMount** entry is using a volume that is declared in the *volumes*
    section of the DaemonSet:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**volumeMount**条目使用了在DaemonSet的*volumes*部分中声明的卷：'
- en: '- name: dev-fs'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '- name: dev-fs'
- en: 'hostPath:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 'hostPath:'
- en: 'path: /dev'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 'path: /dev'
- en: When a Falco pod starts it will mount the pod's **/dev** mount to the KinD container's
    **/dev** mount. Finally, the KinD container's **/dev** mount is mounted to the
    Docker host's **/dev** where the Falco module is located. (Remember the metaphor
    of nesting dolls.)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 当Falco pod启动时，它将把pod的**/dev**挂载到KinD容器的**/dev**。最后，KinD容器的**/dev**挂载到Docker主机的**/dev**，Falco模块位于其中。（记住套娃的比喻。）
- en: With all of the prerequisites in place, we are ready to deploy Falco.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备就绪的情况下，我们准备部署Falco。
- en: Deploying the Falco Daemonset
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署Falco Daemonset
- en: If you are going to run the **install-falco.sh** script from the GitHub repository,
    Falco will be installed using the same steps provided in this section. In the
    book's GitHub repo, all of the Falco files are located in the **chapter12** directory.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要从GitHub存储库运行**install-falco.sh**脚本，Falco将使用本节提供的相同步骤进行安装。在书的GitHub存储库中，所有Falco文件都位于**chapter12**目录中。
- en: 'Since this chapter has a few different pieces, a description of the **chapter12**
    directory''s contents is provided in the following diagram:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章有一些不同的部分，下面提供了**chapter12**目录内容的描述：
- en: '![Figure 12.5 – Diagram of the chapter12 directory in the book''s GitHub repository'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.5 - 书的GitHub存储库中**chapter12**目录的图表'
- en: '](image/Fig_12.5_B15514.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.5_B15514.jpg)'
- en: Figure 12.5 – Diagram of the chapter12 directory in the book's GitHub repository
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 - 书的GitHub存储库中**chapter12**目录的图表
- en: Remember that Falco includes a set of standard rules that include standard auditing
    rules. We have put the rules files in the **falco/falco-config** directory. The
    only value we have changed from the default installation is the logging format,
    which we changed to JSON, and additionally set the values for **http_output**
    to use Falcosidekick.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Falco包括一组标准规则，其中包括标准审计规则。我们已经将规则文件放在**falco/falco-config**目录中。我们从默认安装中改变的唯一值是日志格式，我们将其更改为JSON，并另外设置了**http_output**的值以使用Falcosidekick。
- en: To deploy the Falco DaemonSet manually, you need to deploy the three manifests
    in the **install** directory and create a secret using the **falco-config** directory
    contents.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动部署Falco DaemonSet，您需要部署**install**目录中的三个清单，并使用**falco-config**目录内容创建一个secret。
- en: Creating the Falco service account and service
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Falco服务账户和服务
- en: 'Since we want to run Falco in a dedicated namespace, we need to create a namespace
    called **falco** on our cluster. Run the following command:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望在专用命名空间中运行Falco，我们需要在集群上创建一个名为**falco**的命名空间。运行以下命令：
- en: kubectl create ns falco
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl create ns falco
- en: 'Like all Kubernetes applications, we need to create an account that has the
    correct RBAC permission for the application to perform the necessary tasks. Our
    first step is to create that service account, which will be used to assign RBAC
    permissions in the DaemonSet deployment:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 像所有的Kubernetes应用程序一样，我们需要创建一个具有正确RBAC权限的账户，以便应用程序执行必要的任务。我们的第一步是创建该服务账户，该账户将用于在DaemonSet部署中分配RBAC权限：
- en: 'Using **kubectl**, create the service account:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**kubectl**，创建服务账户：
- en: '**kubectl apply -f falco/install/falco-account.yaml -n falco**'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f falco/install/falco-account.yaml -n falco**'
- en: 'Next, we need to create a service for Falco. The included **falco-service.yaml**
    file will create a new service on TCP port **8765**. Using kubectl, apply the
    manifest:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为Falco创建一个服务。包含的**falco-service.yaml**文件将在TCP端口**8765**上创建一个新的服务。使用kubectl，应用清单：
- en: '**kubectl apply -f falco/install/falco-service.yaml -n falco**'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f falco/install/falco-service.yaml -n falco**'
- en: 'Falco uses files for the base configuration and rules. Since we are running
    Falco in Kubernetes, we need to store the files in a Kubernetes object so they
    can be used by the Falco pods. To store the files in a ConfigMap, create a new
    ConfigMap called **falco-config** using all of the files in the **falco-config**
    directory:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Falco使用文件进行基本配置和规则。由于我们在Kubernetes中运行Falco，我们需要将文件存储在Kubernetes对象中，以便它们可以被Falco
    pod使用。要将文件存储在ConfigMap中，请使用**falco-config**目录中的所有文件创建一个名为**falco-config**的新ConfigMap：
- en: '**kubectl create configmap falco-config --from-file=falco/falco-config -n falco**'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl create configmap falco-config --from-file=falco/falco-config -n falco**'
- en: Important note
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you need to modify any of the configuration files after you deploy Falco,
    you should delete the ConfigMap and recreate it using the newly updated files.
    After updating the ConfigMap, you will also need to restart each Falco pod to
    reload the updated files from the ConfigMap.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在部署Falco后修改任何配置文件，您应该删除ConfigMap，并使用新更新的文件重新创建它。更新ConfigMap后，您还需要重新启动每个Falco
    pod，以从ConfigMap重新加载更新的文件。
- en: 'The last step is to deploy the DaemonSet:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是部署DaemonSet：
- en: '**kubectl apply -f falco/install/falco-daemonset-configmap.yaml -n falco**'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl apply -f falco/install/falco-daemonset-configmap.yaml -n falco**'
- en: 'Once the Falco pod(s) are running, you can verify the health by looking at
    the logs for the pod. The output will look similar to the following output (the
    errors are expected, Falco is trying to find the kernel module in all locations,
    some of which do not exist, causing the "errors"):'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Falco pod正在运行，您可以通过查看pod的日志来验证健康状况。输出将类似于以下输出（预期会有错误，Falco正在尝试在所有位置找到内核模块，其中一些不存在，导致“错误”）：
- en: '![Figure 12.6 – Successful Falco pod startup log'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.6 – 成功启动的Falco pod日志'
- en: '](image/Fig_12.6_B15514.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.6_B15514.jpg)'
- en: Figure 12.6 – Successful Falco pod startup log
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – 成功启动的Falco pod日志
- en: You now have a Falco DaemonSet set up that will audit events in your pods.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经设置了一个Falco DaemonSet，它将审计您的pod中的事件。
- en: Important note
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You may receive an error on the last line of the Falco pod logs, similar to
    the following example:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会在Falco pod日志的最后一行收到错误，类似于以下示例：
- en: '**Tue May 5 20:38:14 2020: Runtime error: error opening device /host/dev/falco0\.
    Make sure you have root credentials and that the falco-probe module is loaded.
    Exiting.**'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '**2020年5月5日20:38:14：运行时错误：打开设备/host/dev/falco0时出错。确保您具有root凭据并且falco-probe模块已加载。退出。**'
- en: In this case, your Falco module may not be loaded, so go back to the modprobe
    steps and execute them again. You should not need to restart the Falco pod as
    the change will be picked up and Falco will start logging once it can see the
    module in the **/dev** directory.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您的Falco模块可能没有加载，所以回到modprobe步骤并再次执行它们。您不需要重新启动Falco pod，因为一旦它可以在**/dev**目录中看到模块，更改就会生效，Falco将开始记录日志。
- en: Of course, to be useful, we need the events to be forwarded to a central logging
    system. In a default deployment, Falco logs are only available on the pod running
    on each host. If you have 30 hosts, you will have 30 unique Falco logs, one on
    each host. Finding an event in a decentralized system, as the saying goes, is
    like looking for a needle in a haystack.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为了有用，我们需要将事件转发到中央日志系统。在默认部署中，Falco日志仅在每个主机上运行的pod上可用。如果您有30个主机，您将在每个主机上有30个唯一的Falco日志。在分散式系统中查找事件就像俗话说的那样，就像在一堆草中找针一样困难。
- en: Falco logs use standard output, so we can easily forward the logs to any third-party
    logging system. While there are many options that we could select as our logging
    server, we have chosen to forward our logs using **Elasticsearch, Fluentd, and
    Kibana** (**EFK**) along with Falcosidekick.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: Falco日志使用标准输出，因此我们可以轻松地将日志转发到任何第三方日志系统。虽然有许多选项可供我们选择作为我们的日志服务器，但我们选择使用**Elasticsearch，Fluentd和Kibana**（**EFK**）以及Falcosidekick来转发我们的日志。
- en: Deploying EFK
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署EFK
- en: Our first step will be to deploy **Elasticsearch** to receive event data. To
    install Elasticsearch, we require persistent storage for the data. Luckily, we
    are using a KinD cluster so we have persistent storage thanks to Rancher's local
    provisioner.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是部署**Elasticsearch**以接收事件数据。要安装Elasticsearch，我们需要为数据提供持久存储。幸运的是，我们正在使用KinD集群，因此由于Rancher的本地提供程序，我们拥有持久存储。
- en: To make the deployment easy, we will deploy our stack using Bitnami's Helm charts
    for Elasticsearch and Kibana. You will need to have the Helm binary installed
    to deploy the charts to the cluster. If you are doing the exercises in the book,
    you should already have Helm3 installed from the KinD deployment in [*Chapter
    5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*, Kubernetes Bootcamp*.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使部署变得简单，我们将使用Bitnami的Helm图表来部署我们的堆栈，包括Elasticsearch和Kibana。您需要安装Helm二进制文件才能将图表部署到集群中。如果您正在本书中进行练习，您应该已经从[*第5章*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*，Kubernetes
    Bootcamp*中的KinD部署中安装了Helm3。
- en: 'Verify you have Helm installed and running by running the **helm version**
    command. If Helm is installed on your path, you should receive a reply with the
    version of Helm you are running:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行**helm version**命令来验证您是否已安装并运行Helm。如果Helm已安装在您的路径上，您应该会收到关于您正在运行的Helm版本的回复：
- en: version.BuildInfo{Version:"v3.2.0", GitCommit:"e11b7ce3b12db2941e90399e874513fbd24bcb71",
    GitTreeState:"clean", GoVersion:"go1.13.10"}
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: version.BuildInfo{Version:"v3.2.0", GitCommit:"e11b7ce3b12db2941e90399e874513fbd24bcb71",
    GitTreeState:"clean", GoVersion:"go1.13.10"}
- en: If you receive an error, you will need to reinstall Helm before continuing.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如果收到错误消息，您需要在继续之前重新安装Helm。
- en: In the GitHub repository, we have included a script to deploy EFK. The script
    is called **install-logging.sh** and is located in the **chapter12/logging** directory.
    Like all of the previous scripts, we will go over the details of the script and
    the commands that are executed.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub仓库中，我们已经包含了一个部署EFK的脚本。该脚本名为**install-logging.sh**，位于**chapter12/logging**目录中。与之前的所有脚本一样，我们将详细介绍脚本和执行的命令。
- en: Creating a new namespace
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个新的命名空间
- en: 'Since we may want to delegate access to a centralized logging team, we will
    create a new namespace called **logging**:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可能希望委派对集中日志团队的访问权限，我们将创建一个名为**logging**的新命名空间：
- en: kubectl create ns logging
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl create ns logging
- en: Adding chart repos to Helm
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向Helm添加图表仓库
- en: 'Since we are going to use Helm to deploy charts from Bitnami, we need to add
    the Bitnami chart repository to Helm. You add chart repos using the **helm repo
    add <repo name> <repo url>** command:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用Helm从Bitnami部署图表，我们需要将Bitnami图表仓库添加到Helm中。您可以使用**helm repo add <repo
    name> <repo url>**命令添加图表仓库：
- en: helm repo add bitnami https://charts.bitnami.com/bitnami
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: helm repo add bitnami https://charts.bitnami.com/bitnami
- en: 'You should receive a confirmation that Bitnami has been added:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会收到Bitnami已添加的确认：
- en: '"bitnami" has been added to your repositories'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '"bitnami"已添加到您的仓库'
- en: Once the Bitnami repository has been added, you can start to deploy charts from
    the Bitnami repo.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Bitnami仓库被添加，您就可以开始从Bitnami仓库部署图表。
- en: Deploying the Elasticsearch chart
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署Elasticsearch图表
- en: The Elasticsearch deployment will store data on persistent disks. We want to
    control the size of the created disks, so we pass values in the **helm install**
    command to limit the size to 1 GB.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch部署将在持久磁盘上存储数据。我们希望控制创建的磁盘的大小，因此我们在**helm install**命令中传递值来限制大小为1GB。
- en: 'To deploy Bitnami''s Elasticsearch with the options, use the following **helm
    install** command. We are only setting a few values for our installation, but
    like any Helm chart, there is a long list of options that allow us to customize
    the installation. For our example deployment, we are only setting the persistent
    volume size to 1 GB and the number of data replicas to **2**. We also want the
    chart to be deployed in the **logging** namespace, so we also add the **--namespace
    logging** option:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Bitnami的Elasticsearch部署选项，使用以下**helm install**命令。我们只为我们的安装设置了一些值，但像任何Helm
    chart一样，有一长串的选项可以让我们自定义安装。对于我们的示例部署，我们只设置了持久卷大小为1GB和数据副本数为**2**。我们还希望图表部署在**logging**命名空间中，因此我们还添加了**--namespace
    logging**选项：
- en: helm install elasticsearch bitnami/elasticsearch --set master.persistence.size=1Gi,data.persistence.size=1Gi,data.replicas=2
    --namespace logging
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: helm install elasticsearch bitnami/elasticsearch --set master.persistence.size=1Gi,data.persistence.size=1Gi,data.replicas=2
    --namespace logging
- en: Once you start to deploy the chart, you will receive a warning about the **vm.max_map_count**
    kernel setting. For our KinD clusters, the included **initContainer** will set
    this value on our worker node. In a production environment, you may not allow
    privileged pods to run, which will cause the initContainer to fail. If you do
    not allow privileged pods to run in your cluster (which is a **very** good idea),
    you will need to set this value manually on each host before deploying Elasticsearch.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦开始部署图表，您将收到有关**vm.max_map_count**内核设置的警告。对于我们的KinD集群，包含的**initContainer**将在我们的工作节点上设置此值。在生产环境中，您可能不允许特权pod运行，这将导致initContainer失败。如果您不允许在集群中运行特权pod（这是一个**非常**好的主意），您需要在部署Elasticsearch之前在每个主机上手动设置此值。
- en: 'You can check the status of the deployment by checking the pods in the **logging**
    namespace. Using **kubectl**, verify that all of the pods are in a running state
    before moving on to the next step:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过检查**logging**命名空间中的pod来检查部署的状态。使用**kubectl**，在继续下一步之前，请验证所有的pod是否处于运行状态：
- en: kubectl get pods -n logging
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl get pods -n logging
- en: 'You should receive the following output:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该收到以下输出：
- en: '![Figure 12.7 – Elasticsearch pod list'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.7 - Elasticsearch pod列表'
- en: '](image/Fig_12.7_B15514.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.7_B15514.jpg)'
- en: Figure 12.7 – Elasticsearch pod list
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 - Elasticsearch pod列表
- en: 'As we can see, the Helm chart created a few Kubernetes objects. The main objects
    include the following:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，Helm chart创建了一些Kubernetes对象。主要对象包括以下内容：
- en: The Elasticsearch server pod (**elasticsearch-elasticsearch-coordinating-only**)
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch服务器pod（**elasticsearch-elasticsearch-coordinating-only**）
- en: The Elasticsearch Data StatefulSet (**elasticsearch-elasticsearch-data-x**)
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch Data StatefulSet（**elasticsearch-elasticsearch-data-x**）
- en: The Elasticsearch Master StatefulSet (**elasticsearch-elasticsearch-master-x**)
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch Master StatefulSet（**elasticsearch-elasticsearch-master-x**）
- en: 'Each StatefulSet created a PersistentVolumeClaim of 1 GB for each pod that
    was created. We can view the PVCs using **kubectl get pvc -n logging**, producing
    the following output:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 每个StatefulSet创建了一个1GB的PersistentVolumeClaim，用于创建的每个pod。我们可以使用**kubectl get pvc
    -n logging**来查看PVCs，产生以下输出：
- en: '![Figure 12.8 – PVC list used by Elasticsearch'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.8 - Elasticsearch使用的PVC列表'
- en: '](image/Fig_12.8_B15514.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.8_B15514.jpg)'
- en: Figure 12.8 – PVC list used by Elasticsearch
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 - Elasticsearch使用的PVC列表
- en: 'Three ClusterIP services were created since Elasticsearch will only be used
    by other Kubernetes objects. We can view the services using **kubectl get services
    -n logging**, producing the following output:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Elasticsearch只会被其他Kubernetes对象使用，所以创建了三个ClusterIP服务。我们可以使用**kubectl get services
    -n logging**查看这些服务，产生以下输出：
- en: '![Figure 12.9 – Elasticsearch services'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.9 – Elasticsearch服务'
- en: '](image/Fig_12.9_B15514.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.9_B15514.jpg)'
- en: Figure 12.9 – Elasticsearch services
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – Elasticsearch服务
- en: By looking at the pods, services, and PVCs, we can confirm that the chart deployment
    was successful and we can move on to the next component, Fluentd.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看pod、服务和PVC，我们可以确认图表部署成功，并且可以继续进行下一个组件，Fluentd。
- en: Deploying Fluentd
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署Fluentd
- en: We have included a Fluentd deployment located in the GitHub repo in the **chapter12/logging**
    directory.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在GitHub存储库的**chapter12/logging**目录中包含了一个位于Fluentd部署的Fluentd部署。
- en: Fluentd is a common log forwarder used with Kubernetes to forward logs to a
    central location. We are installing it to forward Kubernetes logs to Elasticsearch
    to provide a complete example of an EFK deployment. Our Falco events will be forwarded
    using Falcosidekick.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd是Kubernetes中常用的日志转发器，用于将日志转发到中心位置。我们正在安装它以将Kubernetes日志转发到Elasticsearch，以提供EFK部署的完整示例。我们将使用Falcosidekick转发Falco事件。
- en: The first step to deploying Fluentd to a cluster is to apply a Fluentd configuration.
    The **fluentd-config.yaml** file will create a ConfigMap that contains the configuration
    options for the Fluentd deployment.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 将Fluentd部署到集群的第一步是应用Fluentd配置。**fluentd-config.yaml**文件将创建一个包含Fluentd部署配置选项的ConfigMap。
- en: Configuring Fluentd is outside of the scope for this book. To forward logs using
    Fluentd, we do need to explain the **output.conf** section of the ConfigMap, which
    configures the host that Fluentd will send logs to.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 配置Fluentd不在本书的范围之内。要使用Fluentd转发日志，我们确实需要解释ConfigMap的**output.conf**部分，该部分配置了Fluentd将日志发送到的主机。
- en: 'In the **fluentd-config.yaml** file, at the bottom of the file, you will see
    a section titled **output.conf**:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在**fluentd-config.yaml**文件的底部，您将看到一个名为**output.conf**的部分：
- en: '![Figure 12.10 – Fluentd output configuration'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.10 – Fluentd输出配置'
- en: '](image/Fig_12.10_B15514.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.10_B15514.jpg)'
- en: Figure 12.10 – Fluentd output configuration
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10 – Fluentd输出配置
- en: 'You can see that we have options set for **id** and **type** of **elasticsearch**,
    and the host setting has been set to **elasticsearch-elasticsearch-coordinating-only.logging.svc**.
    If you go back a few pages and look at the output from the **kubectl get services
    -n logging** command, you will see a service with that name in the output. This
    is the service that must be targeted when interacting with the Elasticsearch deployment:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们已经为**elasticsearch**的**id**和**type**设置了选项，并且主机设置已经设置为**elasticsearch-elasticsearch-coordinating-only.logging.svc**。如果您回到前面的几页并查看**kubectl
    get services -n logging**命令的输出，您将看到输出中有一个名为该名称的服务。这是与Elasticsearch部署交互时必须定位的服务：
- en: elasticsearch-elasticsearch-coordinating-only   ClusterIP   10.107.207.18
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: elasticsearch-elasticsearch-coordinating-only   ClusterIP   10.107.207.18
- en: Notice that we also added the namespace and svc to the hostname. The Fluentd
    DaemonSet will install to the **kube-system** namespace, so to communicate with
    a service in another namespace, we need to supply the full name to the service.
    In our KinD cluster, we do not need to add the cluster name to the **hostname**
    value.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还将命名空间和svc添加到主机名中。Fluentd DaemonSet将安装到**kube-system**命名空间，因此要与另一个命名空间中的服务通信，我们需要为服务提供完整的名称。在我们的KinD集群中，我们不需要将集群名称添加到**主机名**值中。
- en: 'We can deploy the ConfigMap using **kubectl apply**:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**kubectl apply**来部署ConfigMap：
- en: kubectl apply -f fluentd-config.yaml
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl apply -f fluentd-config.yaml
- en: 'After the ConfigMap, we can deploy the DaemonSet with the following command:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在ConfigMap之后，我们可以使用以下命令部署DaemonSet：
- en: kubectl apply -f fluentd-ds.yaml
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl apply -f fluentd-ds.yaml
- en: 'Verify that the Fluentd pod(s) is running by checking the pods in the **kube-system**
    namespace:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查**kube-system**命名空间中的pod来验证Fluentd pod(s)是否正在运行：
- en: kubectl get pods -n kube-system
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl get pods -n kube-system
- en: 'Since we only have one node, we only see one Fluentd pod:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只有一个节点，我们只看到一个Fluentd pod：
- en: '![Figure 12.11 – Fluentd DaemonSet pod list'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.11 - Fluentd DaemonSet pod列表'
- en: '](image/Fig_12.11_B15514.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.11_B15514.jpg)'
- en: Figure 12.11 – Fluentd DaemonSet pod list
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11 - Fluentd DaemonSet pod列表
- en: Fluentd will be used to forward **all** container logs to Elasticsearch.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd将用于将**所有**容器日志转发到Elasticsearch。
- en: To make it easier to use Kibana, which we will look at later in this chapter,
    we want to forward the Falco logs without any other container logs. The easiest
    way to do this is to use another project from the Falco team, called Falcosidekick.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易使用我们将在本章后面介绍的Kibana，我们希望将Falco日志转发而不包括其他容器日志。这样做的最简单方法是使用Falco团队的另一个项目，名为Falcosidekick。
- en: Deploying Falcosidekick
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署Falcosidekick
- en: Falco has a utility that can format and forward Falco events to different logging
    servers. The project is on GitHub at [https://github.com/falcosecurity/falcosidekick](https://github.com/falcosecurity/falcosidekick).
    At the time of writing, it supports 15 different logging systems, including Slack,
    Teams, Datadog, Elasticsearch, AWS Lamda, SMTP, and Webhooks.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Falco有一个实用程序，可以格式化和转发Falco事件到不同的日志服务器。该项目位于GitHub上[https://github.com/falcosecurity/falcosidekick](https://github.com/falcosecurity/falcosidekick)。在撰写本文时，它支持15种不同的日志系统，包括Slack、Teams、Datadog、Elasticsearch、AWS
    Lamda、SMTP和Webhooks。
- en: Since Falcosidekick opens up an easy forwarding method for various different
    backends, we are going to deploy it to forward the Falco events to Elasticsearch.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Falcosidekick为各种不同的后端提供了一个简单的转发方法，我们将部署它来将Falco事件转发到Elasticsearch。
- en: 'To deploy Falcosidekick, we will use Helm to deploy the chart using a local
    copy from our GitHub repository. The chart files are located in the **chapter12/logging/falcosidekick**
    directory:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署Falcosidekick，我们将使用Helm从我们的GitHub存储库中使用本地副本部署图表。图表文件位于**chapter12/logging/falcosidekick**目录中：
- en: 'Like all charts, we can use a **values.yaml** file to configure the chart options.
    We have provided a preconfigured file that has the required entries to send Falco
    events to our Elasticsearch deployment. The entries in the file that we have configured
    are shown in the following code block. We had to configure the host port to target
    our Elasticsearch service with HTTP and port **9200**:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与所有图表一样，我们可以使用**values.yaml**文件来配置图表选项。我们提供了一个预配置文件，其中包含将Falco事件发送到我们的Elasticsearch部署所需的条目。我们配置的文件中的条目如下所示。我们必须配置主机端口以使用HTTP和端口**9200**来定位我们的Elasticsearch服务：
- en: 'elasticsearch:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: elasticsearch：
- en: 'host port: "http://elasticsearch-elasticsearch-coordinating-only.logging.svc:9200"'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 主机端口："http://elasticsearch-elasticsearch-coordinating-only.logging.svc:9200"
- en: 'index: "falco"'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 索引："falco"
- en: 'type: "event"'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 类型："事件"
- en: 'minimumpriority: ""'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 最低优先级：""
- en: 'The easiest way to deploy the chart is to change your working directory to
    the **falcosidkick** directory. Once you are in the directory, run the following
    **helm install** command to deploy the chart:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署图表的最简单方法是将工作目录更改为**falcosidkick**目录。一旦进入该目录，运行以下**helm install**命令来部署图表：
- en: '**helm install falcosidekick -f values.yaml . --namespace falco**'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '**helm install falcosidekick -f values.yaml . --namespace falco**'
- en: 'To verify the chart was deployed correctly, grab the logs from the Falcosidekick
    instance running in the **logging** namespace:'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证图表是否正确部署，请从运行在**logging**命名空间中的Falcosidekick实例中获取日志：
- en: kubectl logs falcosidekick-7656785f89-q2z6q -n logging
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl logs falcosidekick-7656785f89-q2z6q -n logging
- en: '**2020/05/05 23:40:25 [INFO]  : Enabled Outputs : Elasticsearch**'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '2020/05/05 23:40:25 [INFO] : 已启用输出：Elasticsearch'
- en: '**2020/05/05 23:40:25 [INFO]  : Falco Sidekick is up and listening on port
    2801**'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '2020/05/05 23:40:25 [INFO] : Falco Sidekick已启动并正在侦听端口2801'
- en: 'Once the Falcosidekick pod starts to receive data from the Falco pods, the
    log files will have entries showing a successful Elasticsearch Post:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦Falcosidekick pod开始接收来自Falco pods的数据，日志文件将显示成功的Elasticsearch Post条目：
- en: '**2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)**'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '2020/05/05 23:42:40 [INFO] : Elasticsearch - Post OK (201)'
- en: '**2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)**'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '2020/05/05 23:42:40 [INFO] : Elasticsearch - Post OK (201)'
- en: '**2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)**'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '2020/05/05 23:42:40 [INFO] : Elasticsearch - Post OK (201)'
- en: '**2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)**'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '2020/05/05 23:42:40 [INFO] : Elasticsearch - Post OK (201)'
- en: What does this give us so far? We have deployed Elasticsearch to store the information
    that the Fluentd agent will forward from our worker node. Right now, our worker
    node is sending all of its logs to the Elasticsearch instance using the Fluentd
    agent, and Falcosidekick is forwarding the Falco events.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这给了我们什么？我们已经部署了Elasticsearch来存储Fluentd代理将从我们的工作节点转发的信息。现在，我们的工作节点正在使用Fluentd代理将其所有日志发送到Elasticsearch实例，并且Falcosidekick正在转发Falco事件。
- en: Elasticsearch will have a lot of information to sort through to make the data
    useful. To parse the data and create useful information for the logs, we need
    to install a system that we can use to create custom dashboards and to search
    the collected data. This is where the **K** in the **EFK** stack comes in. The
    next step in our deployment is to install Kibana.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch将有大量信息需要整理以使数据有用。为了解析数据并为日志创建有用的信息，我们需要安装一个系统，我们可以使用它来创建自定义仪表板并搜索收集的数据。这就是
    **EFK** 堆栈中的 **K** 所在的地方。我们部署的下一步是安装Kibana。
- en: Deploying Kibana
  id: totrans-499
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署Kibana
- en: 'The next chart will install the Kibana server. We chose to use a deployment
    that is only serving Kibana over HTTP with no authentication. In a production
    environment, you should enable both to increase your security. Of course, Kibana
    is not accessible outside of the cluster yet, so we need to create an ingress
    rule that will configure our NGINX Ingress to direct traffic to the pod:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表将安装Kibana服务器。我们选择使用仅通过HTTP提供Kibana而没有身份验证的部署。在生产环境中，您应该启用两者以增加安全性。当然，Kibana目前无法在集群外部访问，因此我们需要创建一个Ingress规则，该规则将配置我们的NGINX
    Ingress以将流量定向到pod：
- en: 'To deploy Kibana to the cluster using the Bitnami chart, use the following
    commands:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将Kibana部署到集群中的Bitnami图表：
- en: '**helm install kibana --set elasticsearch.hosts[0]=elasticsearch-elasticsearch-coordinating-only
    -- elasticsearch.port=9200,persistence.size=1Gi --namespace logging bitnami/kibana**'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: helm install kibana --set elasticsearch.hosts[0]=elasticsearch-elasticsearch-coordinating-only
    -- elasticsearch.port=9200,persistence.size=1Gi --namespace logging bitnami/kibana
- en: 'Once the deployment has started, you will see some output from Helm that tells
    you how to port-forward using kubectl to access Kibana:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦部署开始，您将看到来自Helm的一些输出，告诉您如何使用kubectl进行端口转发以访问Kibana：
- en: 'Get the application URL by running these commands:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令获取应用程序URL：
- en: '**  export POD_NAME=$(kubectl get pods --namespace logging -l "app.kubernetes.io/name=kibana,app.kubernetes.io/instance=kibana"
    -o jsonpath="{.items[0].metadata.name}")**'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: export POD_NAME=$(kubectl get pods --namespace logging -l "app.kubernetes.io/name=kibana,app.kubernetes.io/instance=kibana"
    -o jsonpath="{.items[0].metadata.name}")
- en: '**  echo "Visit http://127.0.0.1:8080 to use your application"**'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: echo "访问 http://127.0.0.1:8080 使用您的应用程序"
- en: '**  kubectl port-forward svc/kibana 8080:80**'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl port-forward svc/kibana 8080:80**'
- en: You can ignore these instructions since we going to expose Kibana using an ingress
    rule so it can be accessed on any workstation on your network.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以忽略这些说明，因为我们将使用Ingress规则暴露Kibana，以便可以在您网络上的任何工作站上访问它。
- en: Creating an ingress rule for Kibana
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为 Kibana 创建入口规则
- en: 'For the ingress rule, we will create a rule based on a nip.io domain:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 对于入口规则，我们将基于 nip.io 域创建一个规则：
- en: 'To create the ingress rule with the correct nip.io name, we have provided a
    script in the **chaper12/logging** folder called **create-ingress.sh**:'
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用正确的 nip.io 名称创建入口规则，我们在 **chaper12/logging** 文件夹中提供了一个名为 **create-ingress.sh**
    的脚本：
- en: '**ingressip=$(hostname  -I | cut -f1 -d'' '')**'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '**ingressip=$(hostname  -I | cut -f1 -d'' '')**'
- en: '**ingress=`cat "kibana-ingress.yaml" | sed "s/{hostip}/$ingressip/g"`**'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '**ingress=`cat "kibana-ingress.yaml" | sed "s/{hostip}/$ingressip/g"`**'
- en: '**echo "$ingress" | kubectl apply -f -**'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '**echo "$ingress" | kubectl apply -f -**'
- en: The script will find the IP address of the Docker host and patch the ingress
    manifest with a nip.io host using **kibana.w.x.y.z.nip.ip** (here, **w.x.y.z**
    will contain the host's IP address).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将查找 Docker 主机的 IP 地址，并使用 **kibana.w.x.y.z.nip.ip** 对入口清单进行修补（这里，**w.x.y.z**
    将包含主机的 IP 地址）。
- en: 'Once the ingress rule has been created, the details to access your Kibana dashboard
    will be displayed:'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦创建了入口规则，将显示访问 Kibana 仪表板的详细信息：
- en: '**You can access your Kibana dashboard in any browser on your local network
    using http://kibana.10.2.1.107.nip.io**'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '**您可以使用 http://kibana.10.2.1.107.nip.io 在本地网络上的任何浏览器中访问您的 Kibana 仪表板**'
- en: Now that we have Kibana installed, we can open the Kibana dashboard to start
    our configuration.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 Kibana，我们可以打开 Kibana 仪表板来开始我们的配置。
- en: Using the Kibana dashboard
  id: totrans-519
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Kibana 仪表板
- en: 'To browse to the Kibana dashboard, follow these steps:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览到 Kibana 仪表板，按照以下步骤操作：
- en: Open a browser from any machine on your local network.
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从本地网络上的任何计算机上打开浏览器。
- en: Use the ingress name that was shown from the **install-ingress.sh** script.
    In our example, we would browse to [http://kibana.10.2.1.107.nip.io](http://kibana.10.2.1.107.nip.io).
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从 **install-ingress.sh** 脚本中显示的入口名称。在我们的示例中，我们将浏览到 [http://kibana.10.2.1.107.nip.io](http://kibana.10.2.1.107.nip.io)。
- en: The request will come back to your client with the IP address **10.2.1.107**
    and will be sent to your Docker host on port **80**.
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求将返回到您的客户端，IP 地址为 **10.2.1.107**，并将被发送到端口 **80** 的 Docker 主机。
- en: Tip
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Remember that we exposed the Docker container for the KinD worker node on ports
    **80** and **443**.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们将 KinD 工作节点的 Docker 容器暴露在端口 **80** 和 **443** 上。
- en: When your Docker host receives the request for the hostname on port **80**,
    it will be forwarded to the Docker container and ultimately it will then hit the
    NGINX Ingress controller.
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您的 Docker 主机收到端口 **80** 上主机名的请求时，它将被转发到 Docker 容器，最终将命中 NGINX Ingress 控制器。
- en: 'NGINX will look for a rule that matches the hostname and will send the traffic
    to the Kibana pod. In your browser, you will be presented with the Kibana welcome
    screen:'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NGINX 将寻找与主机名匹配的规则，并将流量发送到 Kibana pod。在您的浏览器中，您将看到 Kibana 欢迎屏幕。
- en: '![Figure 12.12 – Kibana welcome screen'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.12 – Kibana 欢迎屏幕'
- en: '](image/Fig_12.12_B15514.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.12_B15514.jpg)'
- en: Figure 12.12 – Kibana welcome screen
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12 – Kibana 欢迎屏幕
- en: 'While you now have a fully functioning audit logging system running, you still
    have one more step to use Kibana: you need to create a default index.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您现在已经运行了一个完全功能的审计日志记录系统，但是要使用 Kibana 还需要进行最后一步：您需要创建一个默认索引。
- en: Creating a Kibana Index
  id: totrans-532
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建 Kibana 索引
- en: To view logs or create visualizations and dashboards, you need to create an
    index. You can have multiple indexes on a single Kibana server, allowing you to
    view different logs from a single location. On our example server, we will have
    two different sets of incoming logs, one that starts with the name logstash and
    the other with the name falco.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看日志或创建可视化和仪表板，您需要创建一个索引。您可以在单个 Kibana 服务器上拥有多个索引，从而可以在单个位置查看不同的日志。在我们的示例服务器上，我们将有两组不同的传入日志，一组以
    logstash 开头，另一组以 falco 开头。
- en: 'The data in the logstash files container consists of the Kubernetes log files,
    which includes all logs that are being forwarded by the Fluentd forwarder. The
    Falco files are being forwarded by Falcosidekick and only contain the alerts from
    the Falco pods. For the purposes of this chapter, we will focus on the Falco files
    since they contain only Falco data:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: logstash文件容器中的数据包括Kubernetes日志文件，其中包括Fluentd转发器转发的所有日志。Falco文件由Falcosidekick转发，并且仅包含来自Falco
    pod的警报。在本章中，我们将专注于Falco文件，因为它们只包含Falco数据。
- en: In Kibana, click the setup tool ![](image/Icon_1.png) located on the left-hand
    side to open the Kibana management page.
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kibana中，单击左侧的设置工具![](image/Icon_1.png)以打开Kibana管理页面。
- en: To create an index and set it to default, click on index patterns link in the
    upper-left section of the browser.
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建索引并将其设置为默认值，请单击浏览器左上部分的索引模式链接。
- en: Next, click on the upper-right button to create a new index pattern.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，单击右上角的按钮创建新的索引模式。
- en: Since we only want to create an index that contains the Falco data, enter **falco***
    in the box. This will create an index that contains all current and future Falco
    logs:![Figure 12.13 – Kibana index pattern definition
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们只想创建一个包含Falco数据的索引，请在框中输入**falco***。这将创建一个包含所有当前和未来Falco日志的索引：![图12.13 –
    Kibana索引模式定义
- en: '](image/Fig_12.13_B15514.jpg)'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.13_B15514.jpg)'
- en: Figure 12.13 – Kibana index pattern definition
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13 – Kibana索引模式定义
- en: Click the **Next step** button to continue.
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**下一步**按钮继续。
- en: In the configuration settings, click the dropdown and select **time**, then
    click **Create index pattern** to create the pattern:![Figure 12.14 – Creating
    an index
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在配置设置中，单击下拉菜单并选择**时间**，然后单击**创建索引模式**以创建模式：![图12.14 – 创建索引
- en: '](image/Fig_12.14_B15514.jpg)'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.14_B15514.jpg)'
- en: Figure 12.14 – Creating an index
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14 – 创建索引
- en: 'Finally, set the index to the default index by clicking the star in the upper
    right of the final screen:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过单击最终屏幕右上角的星号，将索引设置为默认索引：
- en: '![Figure 12.15 – Setting a default index'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.15 – 设置默认索引'
- en: '](image/Fig_12.15_B15514.jpg)'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.15_B15514.jpg)'
- en: Figure 12.15 – Setting a default index
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15 – 设置默认索引
- en: That's it, you now have a full Falco logging system running on your cluster.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，您现在在集群上运行了一个完整的Falco日志系统。
- en: 'To start viewing data, click the discover button ![](image/Icon_2.png) located
    in the upper left of the Kibana screen, which will take you to the main Kibana
    page where you will see events from your cluster:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始查看数据，请单击Kibana屏幕左上角的发现按钮![](image/Icon_2.png)，这将带您到Kibana主页，在那里您将看到来自您的集群的事件：
- en: '![Figure 12.16 – Kibana homepage'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.16 – Kibana主页'
- en: '](image/Fig_12.16_B15514.jpg)'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.16_B15514.jpg)'
- en: Figure 12.16 – Kibana homepage
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.16 – Kibana主页
- en: You can search for events by typing keywords into the search field. This is
    helpful if you are looking for a single type of event, and know what value(s)
    to search for.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在搜索字段中输入关键字来搜索事件。如果您正在寻找单一类型的事件，并且知道要搜索的值，这将非常有帮助。
- en: The real benefit of logging systems like Kibana is the ability to create custom
    dashboards that provide a view into multiple events that can be grouped by counts,
    averages, and more. In the next section, we will explain how to create a dashboard
    that provides a collection of Falco events.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 像Kibana这样的日志系统的真正好处是能够创建自定义仪表板，提供对多个事件的视图，这些事件可以按计数、平均值等进行分组。在下一节中，我们将解释如何创建一个提供Falco事件集合的仪表板。
- en: 'Creating dashboards is a skill that you need to develop, and it will take time
    to understand how to group data and what values to use in a dashboard. This section
    is meant to provide you with the basic tools you need to start creating dashboards
    like the following:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 创建仪表板是一个你需要培养的技能，需要时间来理解如何对数据进行分组以及在仪表板中使用哪些数值。本节旨在为您提供开始创建以下仪表板所需的基本工具：
- en: '![Figure 12.17 – Example dashboard'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.17 – 例子仪表板'
- en: '](image/Fig_12.17_B15514.jpg)'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.17_B15514.jpg)'
- en: Figure 12.17 – Example dashboard
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.17 – 例子仪表板
- en: People love dashboards, and Kibana provides tools to create dynamic and easily
    interpreted views of a system. Dashboards can be created using any data that Kibana
    has access to, including Falco events. Before we create a dashboard, let's understand
    what a *visualization* means.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 人们喜欢仪表板，Kibana提供了创建系统动态和易于解释的视图的工具。仪表板可以使用Kibana可以访问的任何数据来创建，包括Falco事件。在创建仪表板之前，让我们了解一下*可视化*的含义。
- en: Visualizations
  id: totrans-561
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化
- en: A visualization is a graphical representation of a collection of data – in our
    context, from a Kibana index. Kibana includes a set of visualizations that allow
    you to group data into tables, gauges, horizontal bars, pie charts, vertical bars,
    and more.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是数据集的图形表示 - 在我们的上下文中，来自Kibana索引。Kibana包括一组可视化，允许您将数据分组为表格、仪表、水平条形图、饼图、垂直条形图等等。
- en: 'To create a new visualization, click on the visualize icon ![](image/Icon_3_new.png)
    on the left-hand bar that looks like a small graph. This will bring up the new
    visualization selection screen. Then, follow these steps:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建新的可视化，点击左侧栏上看起来像一个小图表的可视化图标![](image/Icon_3_new.png)。这将带来新的可视化选择屏幕。然后，按照以下步骤进行：
- en: To select the visualization you want to create, select it from the list. Let's
    use a common one for this visualization, the pie chart:![Figure 12.18 – Falco
    visualizations
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要选择要创建的可视化，从列表中选择。让我们为这个可视化使用一个常见的，饼图：![图12.18 – Falco可视化
- en: '](image/Fig_12.18_B15514.jpg)'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.18_B15514.jpg)'
- en: Figure 12.18 – Falco visualizations
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.18 – Falco可视化
- en: Each visualization requires a source. For our example, we only have one index
    created called **falco***, so select that as the source:![Figure 12.19 – Selecting
    a visualization source
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个可视化需要一个数据源。对于我们的示例，我们只创建了一个名为**falco**的索引，所以选择它作为数据源：![图12.19 – 选择可视化数据源
- en: '](image/Fig_12.19_B15514.jpg)'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.19_B15514.jpg)'
- en: Figure 12.19 – Selecting a visualization source
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.19 – 选择可视化数据源
- en: The next step is to select a metric and a bucket. A metric defines how you want
    to aggregate the results from the bucket. The bucket is the value you want to
    visualize. For our example, we want our pie chart to display the total count of
    event priorities, ranging from **error**, **notice**, and **warning** to **debug**.
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是选择一个度量和一个桶。度量定义了你想要如何聚合来自桶的结果。桶是你想要可视化的值。对于我们的示例，我们希望我们的饼图显示事件优先级的总计数，从**error**、**notice**和**warning**到**debug**。
- en: First, set the metric aggregation value to **Count**:![Figure 12.20 – Visualization
    metric options
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将度量聚合值设置为**Count**：![图12.20 – 可视化度量选项
- en: '](image/Fig_12.20_B15514.jpg)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.20_B15514.jpg)'
- en: Figure 12.20 – Visualization metric options
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.20 – 可视化度量选项
- en: Next, we need to select the field we want to aggregate. For **Aggregation**,
    select **Terms**, and for **Field**, select **priority.keyword**:![Figure 12.21
    – Selecting bucket values
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择要聚合的字段。对于**聚合**，选择**Terms**，对于**字段**，选择**priority.keyword**：![图12.21
    – 选择桶值
- en: '](image/Fig_12.21_B15514.jpg)'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.21_B15514.jpg)'
- en: Figure 12.21 – Selecting bucket values
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.21 – 选择桶值
- en: Before saving the visualization, you can preview the results by clicking the
    arrow button at the top of the metric box. A preview of the results will be shown
    in the right-hand pane:![Figure 12.22 – Visualization preview
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保存可视化之前，您可以通过单击度量框顶部的箭头按钮来预览结果。结果的预览将显示在右侧窗格中：![图12.22 – 可视化预览
- en: '](image/Fig_12.22_B15514.jpg)'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.22_B15514.jpg)'
- en: Figure 12.22 – Visualization preview
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.22 – 可视化预览
- en: 'If the results are what you expected, you can save the visualization by clicking
    the **Save** link at the top of the main view. Enter a name for the visualization
    so you can find it later when you create a dashboard:'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果结果符合预期，您可以通过单击主视图顶部的**保存**链接来保存可视化。输入可视化的名称，以便以后在创建仪表板时找到它：
- en: '![Figure 12.23 – Saving a new visualization'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.23 – 保存新的可视化'
- en: '](image/Fig_12.23_B15514.jpg)'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.23_B15514.jpg)'
- en: Figure 12.23 – Saving a new visualization
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.23 – 保存新的可视化
- en: When you save the visualization, it will remain on screen, but you should see
    a confirmation in the lower-right corner that the save was successful.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 当您保存可视化时，它将保留在屏幕上，但您应该在右下角看到保存成功的确认信息。
- en: 'To create additional visualizations, you only need to click on the visualization
    button again and select your desired type to create another one. Using what we
    went over for the first visualization, create two additional visualizations that
    use the following parameters:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建其他可视化，您只需再次单击可视化按钮，并选择所需的类型来创建另一个。使用我们为第一个可视化所介绍的内容，创建使用以下参数的另外两个可视化。
- en: '**Visualization Type**: Horizontal bar'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化类型**：水平条'
- en: '**Source**: **falco***'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源**：**falco***'
- en: '**Metrics**: Aggregation: Count'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '**度量**：聚合：计数'
- en: '**Buckets**: X-Axis, Aggregation: Terms, Field: **rule.keyword**'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '**分桶**：X轴，聚合：术语，字段：**rule.keyword**'
- en: '**Metrics**: Count, Size: 5, Custom label: Top 5 Falco Rules'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '**度量**：计数，大小：5，自定义标签：前5个Falco规则'
- en: '**Visualization Name**: Top 5 Falco Rules'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '**可视化名称**：前5个Falco规则'
- en: '**Visualization Type**: Data Table'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化类型**：数据表'
- en: '**Source**: **falco***'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源**：**falco***'
- en: '**Metrics**: Aggregation: Count'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '**度量**：聚合：计数'
- en: '**Buckets**: Split rows, Aggregation: Terms, Field: **output_fields.fd.name.keyword**,
    Metric: Count, Size: 5, Custom label: Top 5 Modified Files'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '**分桶**：拆分行，聚合：术语，字段：**output_fields.fd.name.keyword**，度量：计数，大小：5，自定义标签：前5个修改文件'
- en: '**Visualization Name**: Top 5 Falco Modified Files'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '**可视化名称**：前5个Falco修改文件'
- en: In the next section, we will create a dashboard that displays the visualizations
    that you created.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将创建一个显示您创建的可视化的仪表板。
- en: Creating a dashboard
  id: totrans-598
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建仪表板
- en: 'Dashboards allow you to display visualizations in a collection that is easy
    to read with information updated every minute:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板允许您以易于阅读的方式显示可视化，信息每分钟更新一次：
- en: To create a dashboard, click on the dashboard button ![](image/Icon_4.png) on
    the sidebar.
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建仪表板，请单击侧边栏上的仪表板按钮![](image/Icon_4.png)。
- en: The button looks like 4 stacked blocks.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 按钮看起来像4个堆叠的块。
- en: This will bring up the **Create your first dashboard screen**. Click the **Create
    new dashboard** button to start creating your dashboard.
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将带出**创建您的第一个仪表板屏幕**。单击**创建新仪表板**按钮开始创建您的仪表板。
- en: 'You will be presented a blank dashboard with a single button on the screen:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个空白的仪表板屏幕上只有一个按钮：
- en: '![Figure 12.24 – Creating a new dashboard'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.24 – 创建新的仪表板'
- en: '](image/Fig_12.24_B15514.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.24_B15514.jpg)'
- en: Figure 12.24 – Creating a new dashboard
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.24 – 创建新的仪表板
- en: This button provides you the option to use an existing visualization or to create
    a new one. Since we created three visualizations earlier, click the **Add an existing**
    link. Once selected, all existing visualizations will be presented on the right-hand
    side of the dashboard in the **Add panels** box:![ Figure 12.25 – Adding panels
    to a dashboard
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此按钮可让您选择使用现有可视化或创建新的可视化。由于我们之前创建了三个可视化，所以单击**添加现有**链接。一旦选择，所有现有可视化将显示在仪表板右侧的**添加面板**框中：![图12.25
    – 向仪表板添加面板
- en: '](image/Fig_12.25_B15514.jpg)'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.25_B15514.jpg)'
- en: Figure 12.25 – Adding panels to a dashboard
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.25 – 向仪表板添加面板
- en: 'We want to add the three visualizations that we created: **Falco - Priority
    Count**, **Top 5 Falco Modified Files**, and **Top 5 Falco Rules**. To add each
    one, click on each of them **once**.'
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要添加我们创建的三个可视化：**Falco - 优先级计数**，**前5个Falco修改文件**和**前5个Falco规则**。要添加每个可视化，请单击每个可视化**一次**。
- en: Once you have added all of the visualizations to the dashboard, you can close
    the **Add panels** pane by clicking the **X** in the upper right-hand of that
    pane. Once it's closed, you should see your dashboard with the visualizations
    you selected:![Figure 12.26 – Dashboard view after adding visualizations
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您将所有可视化添加到仪表板，您可以通过单击**X**关闭**添加面板**窗格。关闭后，您应该看到您选择的可视化的仪表板：![图12.26 – 添加可视化后的仪表板视图
- en: '](image/Fig_12.26_B15514.jpg)'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.26_B15514.jpg)'
- en: Figure 12.26 – Dashboard view after adding visualizations
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.26 – 添加可视化后的仪表板视图
- en: 'Oops! It looks like we may have added the Top 5 Falco Rules visualization twice.
    When we went through the steps to add a visualization, we emphasized a key word
    in the step: "To add each one, click on each of them **once**." Visualizations
    are added each time you click on them. When we added the Falco rules to our dashboard,
    we double-clicked it.'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕！看起来我们可能已经两次添加了前5个Falco规则的可视化。当我们执行添加可视化的步骤时，我们强调了一个关键词：“要添加每个可视化，请单击每个可视化**一次**。”每次单击可视化都会添加一次。当我们将Falco规则添加到我们的仪表板时，我们双击了它。
- en: 'If you did happen to double-click a visualization, it will be added to the
    dashboard twice. If you accidentally added a visualization twice, you can simply
    remove one of them by clicking the gear in the corner of the visualization and
    selecting **Delete from dashboard**:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不小心双击了一个可视化，它将被添加到仪表板两次。如果您不小心两次添加了一个可视化，只需单击可视化的角落中的齿轮，然后选择**从仪表板中删除**即可：
- en: '![Figure 12.27 – Deleting a panel from a dashboard'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.27 – 从仪表板中删除面板'
- en: '](image/Fig_12.27_B15514.jpg)'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.27_B15514.jpg)'
- en: Figure 12.27 – Deleting a panel from a dashboard
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.27 – 从仪表板中删除面板
- en: 'After deleting the duplicate visualization, you can save the dashboard by clicking
    the save link at the top of your browser window. This will prompt you to save
    your dashboard, so give it the name **Falco Dashboard** and click **Save**:'
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除重复的可视化后，您可以通过单击浏览器窗口顶部的保存链接来保存仪表板。这将提示您保存您的仪表板，因此给它命名**Falco 仪表板**并单击**保存**：
- en: '![Figure 12.28 – Saving a dashboard'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.28 – 保存仪表板'
- en: '](image/Fig_12.28_B15514.jpg)'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_12.28_B15514.jpg)'
- en: Figure 12.28 – Saving a dashboard
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.28 – 保存仪表板
- en: Once you save a dashboard, it will be available via the dashboards button on
    the left-hand side of the Kibana homepage. This is the same button you used earlier
    to create the first dashboard.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 保存仪表板后，您可以通过 Kibana 主页左侧的仪表板按钮访问它。这与您之前用来创建第一个仪表板的按钮相同。
- en: Summary
  id: totrans-624
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered how to create an enhanced auditing system to your Kubernetes
    cluster. We started the chapter by introducing Falco, an auditing add-on that
    was donated to the CNCF by Sysdig. Falco adds a level of auditing that Kubernetes
    does not include, and combined with the including auditing functionality, provides
    an audit trail for everything from API access to actions in a pod.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了如何为您的Kubernetes集群创建增强的审计系统。我们首先介绍了Falco，这是一个审计附加组件，由Sysdig捐赠给CNCF。Falco增加了Kubernetes不包括的审计级别，并与包括的审计功能结合使用，为从API访问到pod中的操作提供了审计跟踪。
- en: Logs aren't beneficial if you can't store them in a logging system that allows
    you to store logs on persistent storage and usually offers a management interface
    to search logs and create dashboards. We installed the common EFK stack on our
    KinD cluster and created a custom dashboard to show Falco events in Kibana.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法将日志存储在允许您在持久存储上存储日志并通常提供管理界面以搜索日志和创建仪表板的日志系统中，日志就没有好处。我们在我们的KinD集群上安装了常见的EFK堆栈，并创建了一个自定义仪表板来显示Kibana中的Falco事件。
- en: With the topics you learned in this chapter, you should have a strong foundational
    knowledge of how to add Falco to a cluster and use EFK to store logs and present
    data in visualizations and dashboards.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章学习的主题中，您应该对如何将Falco添加到集群并使用EFK存储日志以及在可视化和仪表板中呈现数据有坚实的基础知识。
- en: While logging and auditing are important, it is equally important to have a
    process to restore workloads in the event of a disaster. In the next chapter,
    we will introduce Velero, an open source backup utility from Heptio.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然日志记录和审计很重要，但同样重要的是在灾难发生时有一个恢复工作负载的过程。在下一章中，我们将介绍Velero，这是Heptio提供的一个开源备份实用程序。
- en: Questions
  id: totrans-629
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: If you need to edit an included Falco rule, which file would you edit?
  id: totrans-630
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您需要编辑一个包含的Falco规则，您将编辑哪个文件？
- en: A. **falco.yaml**
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: A. **falco.yaml**
- en: B. **falco_rules.yaml**
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: B. **falco_rules.yaml**
- en: C. **falco_rules.changes.yaml**
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: C. **falco_rules.changes.yaml**
- en: D. **falco_rules.local.yaml**
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: D. **falco_rules.local.yaml**
- en: Which of the following is a common log forwarder used by Kubernetes?
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个是Kubernetes使用的常见日志转发器？
- en: A. Kube-forwarder.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: A. Kube-forwarder.
- en: B. Fluentd.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: B. Fluentd.
- en: C. Forwarder.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: C. 转发器。
- en: D. Kubernetes doesn't use forwarders.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: D. Kubernetes不使用转发器。
- en: What is the product that provides a way to present logs using visualizations
    and dashboards when you deploy the EFK stack?
  id: totrans-640
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您部署EFK堆栈时，提供使用可视化和仪表板呈现日志的产品是什么？
- en: A. Fluentd
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: A. Fluentd
- en: B. Elasticsearch
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: B. Elasticsearch
- en: C. Kibana
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: C. Kibana
- en: D. Excel
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: D. Excel
- en: Which of the following tools forwards only Falco logs to a central logging system?
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个工具仅将Falco日志转发到中央日志系统？
- en: A. Falco.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: A. Falco.
- en: B. Falcosidekick.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: B. Falcosidekick.
- en: C. The Kubernetes API server.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: C. Kubernetes API服务器。
- en: D. All products forward every log, not just the Falco logs.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: D. 所有产品都转发每个日志，而不仅仅是Falco日志。
- en: What is the name of the object in Falco that allows you to create a collection
    of items?
  id: totrans-650
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Falco中，允许您创建项目集合的对象的名称是什么？
- en: A. Lists
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: A. 列表
- en: B. Rules
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: B. 规则
- en: C. Arrays
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: C. 数组
- en: D. Collections
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: D. 集合
