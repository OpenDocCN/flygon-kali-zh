["```\n> docker container run dockeronwindows/ch02-powershell-env:2e\n\nName                           Value\n----                           -----\nALLUSERSPROFILE                C:\\ProgramData\nAPPDATA                        C:\\Users\\ContainerAdministrator\\AppData\\Roaming\nCommonProgramFiles             C:\\Program Files\\Common Files\nCommonProgramFiles(x86)        C:\\Program Files (x86)\\Common Files\nCommonProgramW6432             C:\\Program Files\\Common Files\nCOMPUTERNAME                   8A7D5B9A4021\n...\n```", "```\n> docker container ls --all\nCONTAINER ID  IMAGE       COMMAND    CREATED          STATUS\n8a7d5b9a4021 dockeronwindows/ch02-powershell-env:2e \"powershell.exe C:...\"  30 seconds ago   Exited\n```", "```\n> docker container run --interactive --tty dockeronwindows/ch02-powershell-env:2e `\n powershell\n\nWindows PowerShell\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nPS C:\\> Write-Output 'This is an interactive container'\nThis is an interactive container\nPS C:\\> exit\n```", "```\n> docker container run --detach dockeronwindows/ch02-powershell-env:2e `\n powershell Test-Connection 'localhost' -Count 100\n\nbb326e5796bf48199a9a6c4569140e9ca989d7d8f77988de7a96ce0a616c88e9\n```", "```\n> docker container logs bb3\n\nSource        Destination     IPV4Address      IPV6Address\n------        -----------     -----------      -----------\nBB326E5796BF  localhost       127.0.0.1        ::1\nBB326E5796BF  localhost       127.0.0.1        ::1\n```", "```\nFROM mcr.microsoft.com/windows/servercore:ltsc2019  COPY scripts/print-env-details.ps1 C:\\\\print-env.ps1 CMD [\"powershell.exe\", \"C:\\\\print-env.ps1\"]\n```", "```\ndocker image build --tag dockeronwindows/ch02-powershell-env:2e .\n```", "```\n> docker image build --tag dockeronwindows/ch02-powershell-env:2e .\n\nSending build context to Docker daemon  4.608kB\nStep 1/3 : FROM mcr.microsoft.com/windows/servercore:ltsc2019\n ---> 8b79386f6e3b\nStep 2/3 : COPY scripts/print-env-details.ps1 C:\\\\print-env.ps1\n ---> 5e9ed4527b3f\nStep 3/3 : CMD [\"powershell.exe\", \"C:\\\\print-env.ps1\"]\n ---> Running in c14c8aef5dc5\nRemoving intermediate container c14c8aef5dc5\n ---> 5f272fb2c190\nSuccessfully built 5f272fb2c190\nSuccessfully tagged dockeronwindows/ch02-powershell-env:2e\n```", "```\nFROM microsoft/dotnet:2.2-sdk-nanoserver-1809 WORKDIR /src COPY src/ . USER ContainerAdministrator RUN dotnet restore && dotnet build CMD [\"dotnet\", \"run\"]\n```", "```\n> docker image build --tag dockeronwindows/ch02-dotnet-helloworld:2e . \nSending build context to Docker daemon  192.5kB\nStep 1/6 : FROM microsoft/dotnet:2.2-sdk-nanoserver-1809\n ---> 90724d8d2438\nStep 2/6 : WORKDIR /src\n ---> Running in f911e313b262\nRemoving intermediate container f911e313b262\n ---> 2e2f7deb64ac\nStep 3/6 : COPY src/ .\n ---> 391c7d8f4bcc\nStep 4/6 : USER ContainerAdministrator\n ---> Running in f08f860dd299\nRemoving intermediate container f08f860dd299\n ---> 6840a2a2f23b\nStep 5/6 : RUN dotnet restore && dotnet build\n ---> Running in d7d61372a57b\n\nWelcome to .NET Core!\n...\n```", "```\nFROM  microsoft/dotnet:2.2-runtime-nanoserver-1809\n\nWORKDIR /dotnetapp\nCOPY ./src/bin/Debug/netcoreapp2.2/publish .\n\nCMD [\"dotnet\", \"HelloWorld.NetCore.dll\"]\n```", "```\ndotnet restore src; dotnet publish src\n\ndocker image build --file Dockerfile.slim --tag dockeronwindows/ch02-dotnet-helloworld:2e-slim .\n```", "```\n> docker image ls --filter reference=dockeronwindows/ch02-dotnet-helloworld\n\nREPOSITORY                               TAG     IMAGE ID       CREATED              SIZE\ndockeronwindows/ch02-dotnet-helloworld   2e-slim b6e7dca114a4   About a minute ago   410MB\ndockeronwindows/ch02-dotnet-helloworld   2e      bf895a7452a2   7 minutes ago        1.75GB\n```", "```\n# build stage\nFROM microsoft/dotnet:2.2-sdk-nanoserver-1809 AS builder\n\nWORKDIR /src\nCOPY src/ .\n\nUSER ContainerAdministrator\nRUN dotnet restore && dotnet publish\n\n# final image stage\nFROM microsoft/dotnet:2.2-runtime-nanoserver-1809\n\nWORKDIR /dotnetapp\nCOPY --from=builder /src/bin/Debug/netcoreapp2.2/publish .\n\nCMD [\"dotnet\", \"HelloWorld.NetCore.dll\"]\n```", "```\n# escape=` FROM mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019\nSHELL [\"powershell\"]\n\nARG ENV_NAME=DEV\n\nEXPOSE 80\n\nCOPY template.html C:\\template.html\nRUN (Get-Content -Raw -Path C:\\template.html) `\n -replace '{hostname}', [Environment]::MachineName `\n -replace '{environment}', [Environment]::GetEnvironmentVariable('ENV_NAME') `\n | Set-Content -Path C:\\inetpub\\wwwroot\\index.html\n```", "```\u00a0option for the escape character, to split commands over multiple lines, rather than the default backslash `\\`\u00a0option. With this escape directive I can use backslashes in file paths, and backticks to split long PowerShell commands, which is more natural to Windows users.\n\nThe base image is `microsoft/iis`, which is a Microsoft Windows Server Core image with IIS already set up. I copy an HTML template file from the Docker build context into the root folder. Then I run a PowerShell command to update the content of the template file and save it in the default website location for IIS.\n\nIn this Dockerfile, I use three new instructions:\n\n*   `SHELL` specifies the command line to use in `RUN` commands. The default is `cmd`, and this switches to `powershell`.\n*   `ARG` specifies a build argument to use in the image with a default value.\n*   `EXPOSE` will make a port available in the image, so that containers from the image can have traffic sent in from the host.\n\nThis static website has a single home page, which tells you the name of the server that sent the response, with the name of the environment in the page title. The HTML template file has placeholders for the hostname and the environment name. The `RUN` command executes a PowerShell script to read the file contents, replace the placeholders with the actual hostname and environment value, and then write the contents out.\n\nContainers run in an isolated space, and the host can only send network traffic into the container if the image has explicitly made the port available for use. That's the `EXPOSE` instruction, which is like a very simple firewall; you use it to expose the ports that your application is listening on. When you run a container from this image, port `80` is available to be published so Docker can serve web traffic from the container.\n\nI can build this image in the usual way, and make use of the `ARG`\u00a0command specified in the Dockerfile to override the default value at build time with the `--build-arg` option:\n\n```", "```\n\nDocker processes the new instructions in the same way as those you've already seen: it creates a new, intermediate container from the previous image in the stack, executes the instruction, and extracts a new image layer from the container. After the build, I have a new image which I can run to start the static web server:\n\n```", "```\n\nThis is a detached container so it runs in the background, and the `--publish` option makes port `80` in the container available to the host. Published ports mean that the traffic coming into the host can be directed into containers by Docker. I've specified that port `8081` on the host should map to port `80` on the container.\n\nYou can also let Docker choose a random port on the host, and use the `port` command to list which ports the container exposes, and where they are published on the host:\n\n```", "```\n\nNow I can browse to port `8081` on my machine and see the response from IIS running inside the container, showing me the hostname, which is actually the container ID, and in the title bar is the name of the environment:\n\n![](Images/76a58414-9209-4783-a22d-ef44c904ef3b.png)\n\nThe environment name is just a text description, but the value came from the argument is passed to the `docker image build` command, which overrides the default value from the `ARG` instruction in the Dockerfile. The hostname should show the container ID, but there's a problem with the current implementation.\n\nOn the web page the hostname starts with\u00a0`bf37`, but my container ID actually starts with `6e3d`. To understand why the ID displayed isn't the actual ID of the running container, I'll look again at the temporary containers used during image builds.\n\n# Understanding temporary containers and image state\n\nMy website container has an ID that starts with\u00a0`6e3d`, which is the hostname that the application inside the container should see, but that's not what the website claims. So, what went wrong? Remember that Docker executes every build instruction inside a temporary, intermediate container.\n\nThe `RUN` instruction to generate the HTML ran in a temporary container, so the PowerShell script wrote *that* container's ID as the hostname in the HTML file; this is where the container ID starting with\u00a0`bf37` came from. The intermediate container gets removed by Docker, but the HTML file it created persists within the image.\n\nThis is an important concept: when you build a Docker image, the instructions execute inside temporary containers. The containers are removed, but the state they write persists within the final image and will be present in any containers you run from that image. If I run multiple containers from my website image, they will all show the same hostname from the HTML file, because that's saved inside the image, which is shared by all containers.\n\nOf course, you can also store the state in individual containers, which is not part of the image, so it's not shared between containers. I'll look at how to work with data in Docker now and then finish the chapter with a real-world Dockerfile example.\n\n# Working with data in Docker images and containers\n\nApplications running in a Docker container see a single filesystem which they can read from and write to in the usual way for the operating system. The container sees a single filesystem drive but it's actually a virtual filesystem, and the underlying data can be in many different physical locations.\n\nFiles which a container can access on its `C` drive could actually be stored in an image layer, in the container's own storage layer, or in a volume that is mapped to a location on the host. Docker merges all of these locations into a single virtual filesystem.\n\n# Data in layers and the virtual C drive\n\nThe virtual filesystem is how Docker can take a set of physical image layers and treat them as one logical container image. Image layers are mounted as read-only parts of the filesystem in a container, so they can't be altered, and that's how they can be safely shared by many containers.\n\nEach container has its own writeable layer on top of all of the read-only layers, so every container can modify its own data without affecting any other containers:\n\n![](Images/e5203dc0-e4fa-4e26-a79f-6665788e9d60.png)\n\nThis diagram shows two containers running from the same image. The image (1) is physically composed of many layers: one built from each instruction in the Dockerfile. The two containers (2 and 3) use the same layers from the image when they run, but they each have their own isolated, writeable layers.\n\nDocker presents a single filesystem to the container. The concept of layers and read-only base layers is hidden, and your container just reads and writes data as if it had a full native filesystem, with a single drive. If you create a file when you build a Docker image and then edit the file inside a container, Docker actually creates a copy of the changed file in the container's writeable layer and hides the original read-only file. So the container has an edited copy of the file, but the original file in the image is unchanged.\n\nYou can see this by creating some simple images with data in different layers. The Dockerfile for the `dockeronwindows/ch02-fs-1:2e`\u00a0image uses Nano Server as the base image, creates a directory, and writes a file into it:\n\n```", "```\n\nThe Dockerfile for the `dockeronwindows/ch02-fs-2:2e`\u00a0image creates an image based on that image, and adds a second file to the data directory:\n\n```", "```\n\nThere's nothing special about *base* images; any image can be used in the `FROM` instruction for a new image. It can be an official image curated on Docker Hub, a commercial image from a private registry, a local image built from scratch, or an image that is many levels deep in a hierarchy.\n\nI'll build both images and run an interactive container from `dockeronwindows/ch02-fs-2:2e`, so I can take a look at the files on the `C` drive. This command starts a container and gives it an explicit name, `c1`, so I can work with it without using the random container ID:\n\n```", "```\n\nMany options in the Docker commands have short and long forms. The long form starts with two dashes, like `--interactive`. The short form is a single letter and starts with a single dash, like `-i`. Short tags can be combined, so `-it` is equivalent to `-i -t`, which is equivalent to `--interactive --tty`. Run `docker --help` to navigate the commands and their options.\n\nNano Server is a minimal operating system, built for running apps in containers. It is not a full version of Windows, you can't run Nano Server as the OS on a VM or a physical machine, and you can't run all Windows apps in a Nano Server container. The base image is deliberately small, and even PowerShell is not included to keep the surface area down, meaning you need fewer updates and there are fewer potential attack vectors.\n\nYou need to brush off your old DOS commands to work with Nano Server containers. `dir` lists the directory contents inside the container:\n\n```", "```\n\nBoth of the files are there for the container to use in the `C:\\data` directory; the first file is in a layer from the `ch02-fs-1:2e` image, and the second file is in a layer from the `ch02-fs-2:2e` image. The `dir` executable is available from another layer in the base Nano Server image, and the container sees them all in the same way.\n\nI'll append some more text to one of the existing files and create a new file in the `c1` container:\n\n```", "```\n\nFrom the file listing you can see that `file2.txt` from the image layer has been modified and there is a new file, `file3.txt`. Now I'll exit this container and create a new one using the same image:\n\n```", "```\n\nWhat are you expecting to see in the `C:\\data` directory in this new container? Let's take a look:\n\n```", "```\n\nYou know that image layers are read-only and every container has its own writeable layer, so the results should make sense. The new container,\u00a0`c2`, has the original files from the image without the changes from the first container,\u00a0`c1`, which are stored in the writeable layer for `c1`. Each container's filesystem is isolated, so one container doesn't see any changes made by another container.\n\nIf you want to share data between containers, or between containers and the host, you can use Docker volumes.\n\n# Sharing data between containers with volumes\n\nVolumes are units of storage. They have a separate life cycle to containers, so they can be created independently and then mounted inside one or more containers. You can ensure containers are always created with volume storage using the `VOLUME` instruction in the Dockerfile.\n\nYou specify volumes with a target directory, which is the location inside the container where the volume is surfaced. When you run a container with a volume defined in the image, the volume is mapped to a physical location on the host, which is specific to that one container. More containers running from the same image will have their volumes mapped to a different host location.\n\nIn Windows, volume directories need to be empty. In your Dockerfile, you can't create files in a directory and then expose it as a volume. Volumes also need to be defined on a disk that exists in the image. In the Windows base images, there is only a `C` drive available, so volumes need to be created on the `C` drive.\n\nThe Dockerfile for `dockeronwindows/ch02-volumes:2e` creates an image with two volumes, and explicitly specifies the `cmd` shell as the `ENTRYPOINT` when containers are run from the image:\n\n```", "```\n\nRemember the Nano Server image uses a least-privilege user by default. Volumes are not accessible by that user, so this Dockerfile switches to the administrative account, and when you run a container from the image you can access volume directories.\n\nWhen I run a container from that image, Docker creates a virtual filesystem from three sources. The image layers are read-only, the container's layer is writeable, and the volumes can be set to read-only or writeable:\n\n![](Images/f630b9fe-5ce9-44cb-b8ac-9f76da4fb8ab.png)\n\nBecause volumes are separate from the container, they can be shared with other containers even if the source container isn't running. I can run a task container from this image, with a command to create a new file in the volume:\n\n```", "```\n\nDocker starts the container, which writes the file, and then exits. The container and its volumes haven't been deleted, so I can connect to the volumes in another container using the `--volumes-from` option and by specifying my first container's name:\n\n```", "```\n\nThis is an interactive container, and when I list the contents of the `C:\\app` directory, I'll see the two directories,\u00a0`logs` and `config`, which are volumes from the first container:\n\n```", "```\n\nThe shared volume has read and write access, so I can see the file created in the first container and append to it:\n\n```", "```\n\nSharing data between containers like this is very useful; you can run a task container that takes a backup of data or log files from a long-running background container. The default access is for volumes to be writeable, but that's something to be wary of, as you could edit data and break the application running in the source container.\n\nDocker lets you mount volumes from another container in read-only mode instead, by adding the `:ro` flag to the name of the container in the `--volumes-from` option. This is a safer way to access data if you want to read it without making changes. I'll run a new container, sharing the same volumes from the original container in read-only mode:\n\n```", "```\n\nIn the new container I can't create a new file or write to the existing log file, but I can see the content in the log file from the original container, and the line appended by the second container.\n\n# Sharing data between the container and host with volumes\n\nContainer volumes are stored on the host, so you can access them directly from the machine running Docker, but they'll be in a nested directory somewhere in Docker's program data directory. The `docker container inspect` command tells you the physical location for a container's volumes, along with a lot more information, including the container's ID, name, and the virtual IP address of the container in the Docker network.\n\nI can use JSON formatting in the `container inspect` command, passing a query to extract just the volume information in the `Mounts` field. This command pipes the Docker output into a PowerShell cmdlet, to show the JSON in a friendly format:\n\n```", "```\n\nI've abbreviated the output, but in the `Source` field you can see the full path where the volume data is stored on the host. I can access the container's files directly from the host, using that source directory. When I run this command on my Windows machine, I'll see the file created inside the container volume:\n\n```", "```\n\nAccessing the files on the host is possible this way, but it's awkward to use the nested directory location with the volume ID. Instead you can mount a volume from a specific location on the host when you create a container.\n\n# Mounting volumes from host directories\n\nYou use the `--volume` option to explicitly map a directory in a container from a known location on the host. The target location in the container can be a directory created with the `VOLUME` command, or any directory in the container's filesystem. If the target location already exists in the Docker image, it is hidden by the volume mount, so you won't see any of the image files.\n\nI'll create a dummy configuration file for my app in a directory on the `C` drive on my Windows machine:\n\n```", "```\n\nNow I'll run a container which maps a volume from the host, and read the configuration file which is actually stored on the host:\n\n```", "```\n\nThe `--volume` option specifies the mount in the format `{source}:{target}`. The source is the host location, which needs to exist. The target is the container location, which doesn't need to exist, but the existing contents will be hidden if it does exist.\n\nVolume mounts are different in Windows and Linux containers. In Linux containers, Docker merges the contents from the source into the target, so if files exist in the image, you see them as well as the contents of the volume source. Docker on Linux also lets you mount a single file location, but on Windows you can only mount whole directories.\n\nVolume mounts are useful for running stateful applications in containers, like databases. You can run SQL Server in a container, and have the database files stored in a location on the host, which could be a RAID array on the server. When you have schema updates, you remove the old container and start a new container from the updated Docker image. You use the same volume mount for the new container, so that the data is preserved from the old container.\n\n# Using volumes for configuration and state\n\nApplication state is an important consideration when you're running applications in containers. Containers can be long-running, but they are not intended to be permanent. One of the biggest advantages with containers over traditional compute models is that you can easily replace them, and it only takes a few seconds to do so. When you have a new feature to deploy, or a security vulnerability to patch, you just build and test an upgraded image, stop the old container, and start a replacement from the new image.\n\nVolumes let you manage that upgrade process by keeping your data separate from your application container. I'll demonstrate this with a simple web application that stores the hit count for a page in a text file; each time you browse to the page, the site increments the count.\n\nThe Dockerfile for the `dockeronwindows/ch02-hitcount-website`\u00a0image uses multi-stage builds, compiling the application using the `microsoft/dotnet` image, and packaging the final app using `microsoft/aspnetcore` as the base:\n\n```", "```\n\nIn the Dockerfile I create an empty directory at `C:\\dotnetapp\\app-state`, which is where the application will store the hit count in a text file. I've built the first version of the app into an image with the `2e-v1` tag:\n\n```", "```\n\nI'll create a directory on the host to use for the container's state, and run a container that mounts the application state directory from a directory on the host:\n\n```", "```\n\nThe `publish-all` option tells Docker to publish all the exposed ports from the container image to random ports on the host. This is a quick option for testing containers in a local environment, as Docker will assign a free port from the host and you don't need to worry about which ports are already in use by other containers. You find out the ports a container has published with the `container port` command:\n\n```", "```\n\nI can browse to the site at `http://localhost:51377`. When I refresh the page a few times, I'll see the hit count increasing:\n\n![](Images/47bcf624-7bdc-478d-a735-0edc36d20e14.png)\n\nNow, when I have an upgraded version of the app to deploy, I can package it into a new image tagged with `2e-v2`. When the image is ready, I can stop the old container and start a new one using the same volume mapping:\n\n```", "```\n\nThe volume containing the application state gets reused, so the new version will continue using the saved state from the old version. I have a new container with a new published port. When I fetch the port and browse to it for the first time, I see the updated UI with an attractive icon, but the hit count is carried forward from version 1:\n\n![](Images/6d464a9f-b169-4970-9b8b-cc01198853c4.png)\n\nApplication state can have structural changes between versions, which is something you will need to manage yourself. The Docker image for the open source Git server, GitLab, is a good example of this. The state is stored in a database on a volume, and when you upgrade to a new version, the app checks the database and runs upgrade scripts if needed.\n\nApplication configuration is another way to make use of volume mounts. You can ship your application with a default configuration set built into the image, but users can override the base configuration with their own files using a mount.\n\nYou'll see these techniques put to good use in the next chapter.\n\n# Packaging a traditional ASP.NET web app as a Docker image\n\nMicrosoft has made the Windows Server Core base image available on MCR, and that's a version of Windows Server 2019 which has much of the functionality of the full server edition, but without the UI. As base images go, it's very large: 2 GB compressed on Docker Hub, compared to 100 MB for Nano Server, and 2 MB for the tiny Alpine Linux image. But it means you can Dockerize pretty much any existing Windows app, and that's a great way to start migrating your systems to Docker.\n\nRemember NerdDinner? It was an open source ASP.NET MVC showcase app, originally written by Scott Hanselman and Scott Guthrie among others at Microsoft. You can still get the code at CodePlex, but there hasn't been a change made since 2013, so it's an ideal candidate for proving that old .NET Framework apps can be migrated to Docker Windows containers, and this can be the first step in modernizing them.\n\n# Writing a Dockerfile for NerdDinner\n\nI'll follow the multi-stage build approach for NerdDinner, so the Dockerfile for the `dockeronwindows/ch-02-nerd-dinner:2e` images starts with a builder stage:\n\n```", "```\n\nThe stage uses\u00a0`microsoft/dotnet-framework` as the base image for compiling the application. This is an image which Microsoft maintains on Docker Hub. It's built on top of the Windows Server Core image, and it has everything you need to compile .NET Framework applications, including NuGet and MSBuild. The build stage happens in two parts:\n\n1.  Copy the NuGet `packages.config` file into the image, and then run `nuget restore`.\n2.  Copy the rest of the source tree and run `msbuild`.\n\nSeparating these parts means Docker will use multiple image layers: the first layer will contain all the restored NuGet packages, and the second layer will contain the compiled web app. This means I can take advantage of Docker's layer caching. Unless I change my NuGet references, the packages will be loaded from the cached layer and Docker won't run the restore part, which is an expensive operation. The MSBuild step will run every time any source files change.\n\nIf I had a deployment guide for NerdDinner, before the move to Docker, it would look something like this:\n\n1.  Install Windows on a clean server.\n2.  Run all Windows updates.\n3.  Install IIS.\n4.  Install .NET.\n5.  Set up ASP.NET.\n6.  Copy the web app into the `C` drive.\n7.  Create an application pool in IIS.\n8.  Create the website in IIS using the application pool.\n9.  Delete the default website.\n\nThis will be the basis for the second stage of the Dockerfile, but I will be able to simplify all the steps. I can use Microsoft's ASP.NET Docker image as the `FROM` image, which will give me a clean install of Windows with IIS and ASP.NET installed. That takes care of the first five steps in one instruction. This is the rest of the Dockerfile for `dockeronwindows/ch-02-nerd-dinner:2e`:\n\n```", "```\n\nMicrosoft uses both Docker Hub and MCR to store their Docker images. The .NET Framework SDK is on Docker Hub, but the ASP.NET runtime image is on MCR. You can always find where an image is hosted by checking on Docker Hub.\n\nUsing the `escape` directive and `SHELL` instruction lets me use normal Windows file paths without double backslashes, and PowerShell-style backticks to separate commands over many lines. Removing the default website and creating a new website in IIS is simple with PowerShell, and the Dockerfile clearly shows me the port the app is using and the path of the content.\n\nI'm using the built-in .NET 4.5 application pool, which is a simplification from the original deployment process. In IIS on a VM you'd normally have a dedicated application pool for each website in order to isolate processes from each other. But in the containerized app, there will be only one website running. Any other websites will be running in other containers, so we already have isolation, and each container can use the default application pool without worrying about interference.\n\nThe final `COPY` instruction copies the published web application from the builder stage into the application image. It's the last line in the Dockerfile to take advantage of Docker's caching again. When I'm working on the app, the source code will be the thing I change most frequently. The Dockerfile is structured so that when I change code and run `docker image build`, the only instructions that run are MSBuild in the first stage and the copy in the second stage, so the build is very fast.\n\nThis could be all you need for a fully functioning Dockerized ASP.NET website, but in the case of NerdDinner there is one more instruction, which proves that you can cope with awkward, unexpected details when you containerize your application. The NerdDinner app has some custom configuration settings in the `system.webServer` section of its `Web.config` file, and by default the section is locked by IIS. I need to unlock the section, which I do with `appcmd` in the second `RUN` instruction.\n\nNow I can build the image and run a legacy ASP.NET app in a Windows container:\n\n```"]