["```\n$ docker run -it \\\n             --rm \\\n             -m 30m \\\n             jboss/wildfly \nUnable to find image 'jboss/wildfly:latest' locally\nlatest: Pulling from jboss/wildfly\n<snip>\nStatus: Downloaded newer image for jboss/wildfly:latest\n=========================================================================\n\n JBoss Bootstrap Environment\n\n JBOSS_HOME: /opt/jboss/wildfly\n\n JAVA: /usr/lib/jvm/java/bin/java\n\n JAVA_OPTS:  -server -Xms64m -Xmx512m -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djava.net.preferIPv4Stack=true -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true\n\n=========================================================================\n\n*** JBossAS process (57) received KILL signal ***\n```", "```\n\n$ docker run -it \\\n             --rm \\\n             -m 400m \\\n             jboss/wildfly\n=========================================================================\n\n JBoss Bootstrap Environment\n\n JBOSS_HOME: /opt/jboss/wildfly\n\n JAVA: /usr/lib/jvm/java/bin/java\n\n JAVA_OPTS:  -server -Xms64m -Xmx512m -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djava.net.preferIPv4Stack=true -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true\n\n=========================================================================\n\n14:05:23,476 INFO  [org.jboss.modules] (main) JBoss Modules version 1.5.2.Final\n<snip>\n14:05:25,568 INFO  [org.jboss.ws.common.management] (MSC service thread 1-6) JBWS022052: Starting JBossWS 5.1.5.Final (Apache CXF 3.1.6) \n14:05:25,667 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management interface listening on http://127.0.0.1:9990/management\n14:05:25,667 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console listening on http://127.0.0.1:9990\n14:05:25,668 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: WildFly Full 10.1.0.Final (WildFly Core 2.2.0.Final) started in 2532ms - Started 331 of 577 services (393 services are lazy, passive or on-demand)\n```", "```\n$ # Let's see what a low allocation shows\n$ docker run -it --rm -m 30m ubuntu /usr/bin/free -h\n total        used        free      shared  buff/cache   available\nMem:           7.6G        1.4G        4.4G         54M        1.8G        5.9G\nSwap:            0B          0B          0B\n\n$ # What about a high one?\n$ docker run -it --rm -m 900m ubuntu /usr/bin/free -h\n total        used        free      shared  buff/cache   available\nMem:           7.6G        1.4G        4.4G         54M        1.8G        5.9G\nSwap:            0B          0B          0B\n```", "```\n#!/bin/bash -e\n\nCPU_COUNT=$(nproc --all)\nSTART_AT=$(date +%s)\nSTOP_AT=$(( $START_AT + 60 ))\n\necho \"Detected $CPU_COUNT CPUs\"\necho \"Time range: $START_AT -> $STOP_AT\"\n\ndeclare -a CONTAINERS\n\necho \"Allocating all cores but one with default shares\"\nfor ((i = 0; i < $CPU_COUNT - 1; i++)); do\n  echo \"Starting container $i\"\n  CONTAINERS[i]=$(docker run \\\n                  -d \\\n                  ubuntu \\\n                  /bin/bash -c \"c=0; while [ $STOP_AT -gt \\$(date +%s) ]; do c=\\$((c + 1)); done; echo \\$c\")\ndone\n\necho \"Starting container with high shares\"\n  fast_task=$(docker run \\\n              -d \\\n              --cpu-shares 8192 \\\n              ubuntu \\\n              /bin/bash -c \"c=0; while [ $STOP_AT -gt \\$(date +%s) ]; do c=\\$((c + 1)); done; echo \\$c\")\n\n  CONTAINERS[$((CPU_COUNT - 1))]=$fast_task\n\necho \"Waiting full minute for containers to finish...\"\nsleep 62\n\nfor ((i = 0; i < $CPU_COUNT; i++)); do\n  container_id=${CONTAINERS[i]}\n  echo \"Container $i counted to $(docker logs $container_id)\"\n  docker rm $container_id >/dev/null\ndone\n```", "```\n$ # Make the file executable\n$ chmod +x ./cpu_shares.sh\n\n$ # Run our little program\n$ ./cpu_shares.sh\nDetected 8 CPUs\nTime range: 1507405189 -> 1507405249\nAllocating all cores but one with default shares\nStarting container 0\nStarting container 1\nStarting container 2\nStarting container 3\nStarting container 4\nStarting container 5\nStarting container 6\nStarting container with high shares\nWaiting full minute for containers to finish...\nContainer 0 counted to 25380\nContainer 1 counted to 25173\nContainer 2 counted to 24961\nContainer 3 counted to 24882\nContainer 4 counted to 24649\nContainer 5 counted to 24306\nContainer 6 counted to 24280\nContainer 7 counted to 31938\n```", "```\n$ # First without any limiting\n$ time docker run -it \\\n --rm \\\n ubuntu \\\n /bin/bash -c 'for ((i=0; i<100; i++)); do sha256sum /bin/bash >/dev/null; done'\nreal    0m1.902s\nuser    0m0.030s\nsys    0m0.006s\n\n$ # Now with only a quarter of the CPU available\n$ time docker run -it \\\n --rm \\\n --cpus=0.25 \\\n ubuntu \\\n /bin/bash -c 'for ((i=0; i<100; i++)); do sha256sum /bin/bash >/dev/null; done'\nreal    0m6.456s\nuser    0m0.018s\nsys    0m0.017s\n```", "```\n$ ulimit -a\ncore file size          (blocks, -c) 0\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 29683\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 1024\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 29683\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n```", "```\n$ ulimit -n\n1024\n\n$ # Set max open files to 2048\n$ ulimit -S -n 2048\n\n$ # Let's see the full list again\n$ ulimit -a\n<snip>\nopen files                      (-n) 2048\n<snip>\n```", "```\n$ ulimit -S -n 10240\nbash: ulimit: open files: cannot modify limit: Invalid argument\n```", "```\n$ ulimit -H -a | grep '^open files'\nopen files                      (-n) 4096\n```", "```\n    root soft nofile 65536\n    root hard nofile 65536\n    * soft nofile 65536\n    * hard nofile 65536\n    ```", "```\n    session required pam_limits.so\n    ```", "```\n    LimitNOFILE=65536\n    ```", "```\n$ sysctl fs.file-max\nfs.file-max = 757778\n```", "```\nfs.file-max = 1000000\n```", "```\nnet.core.optmem_max = 20480\nnet.core.rmem_default = 212992\nnet.core.rmem_max = 212992\nnet.core.wmem_default = 212992\nnet.core.wmem_max = 212992\nnet.ipv4.tcp_rmem = 4096 87380 6291456\nnet.ipv4.tcp_wmem = 4096 16384 4194304\n```", "```\nnet.core.optmem_max = 40960\nnet.core.rmem_default = 16777216\nnet.core.rmem_max = 16777216\nnet.core.wmem_default = 16777216\nnet.core.wmem_max = 16777216\nnet.ipv4.tcp_rmem = 4096 87380 16777216\nnet.ipv4.tcp_wmem = 4096 87380 16777216\n```", "```\n$ netstat -an | grep ESTABLISHED\ntcp        0      0 192.168.56.101:46496     <redacted>:443      ESTABLISHED\ntcp        0      0 192.168.56.101:45512     <redacted>:443      ESTABLISHED\ntcp        0      0 192.168.56.101:42014     <redacted>:443      ESTABLISHED\n<snip>\ntcp        0      0 192.168.56.101:45984     <redacted>:443      ESTABLISHED\ntcp        0      0 192.168.56.101:56528     <redacted>:443      ESTABLISHED\n```", "```\n$ sysctl net.ipv4.ip_local_port_range\nnet.ipv4.ip_local_port_range = 32768    60999\n```", "```\n$ # First the temporary change to get us up to 40000\n$ # ports. For our services, we separately have to\n$ # ensure none listen on any ports above 24999.\n$ sudo sysctl -w net.ipv4.ip_local_port_range=\"25000 65000\"\nnet.ipv4.ip_local_port_range = 25000 65000\n\n$ # Sanity check\n$ sysctl net.ipv4.ip_local_port_range\nnet.ipv4.ip_local_port_range = 25000    65000\n\n$ # Now for the permanent change (requires restart)\n$ echo \"net.ipv4.ip_local_port_range = 25000 65000\" | sudo tee /etc/sysctl.d/10-ephemeral-ports.conf\n```", "```\n$ sysctl -a | grep nf_conntrack\nnet.netfilter.nf_conntrack_buckets = 65536\n<snip>\nnet.netfilter.nf_conntrack_generic_timeout = 600\n<snip>\nnet.netfilter.nf_conntrack_max = 262144\n<snip>\nnet.netfilter.nf_conntrack_tcp_timeout_close = 10\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait = 60\nnet.netfilter.nf_conntrack_tcp_timeout_established = 432000\nnet.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120\nnet.netfilter.nf_conntrack_tcp_timeout_last_ack = 30\nnet.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300\nnet.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60\nnet.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait = 120\nnet.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300\n<snip>\n```", "```\nnet.netfilter.nf_conntrack_tcp_timeout_established = 43200\nnet.netfilter.nf_conntrack_max = 524288\n```", "```\necho '131072' | sudo tee /sys/module/nf_conntrack/parameters/hashsize\n```", "```\nnf_conntrack_ipv4\nnf_conntrack_ipv6\n```", "```\n$ # Let's try to run 'sleep' and exit with <Ctrl>-C\n$ docker run -it \\\n ubuntu \\\n bash -c 'sleep 5000'\n^C^C^C^C^C^C^C^C^C^C\n<Ctrl-C not working>\n\n$ # On second terminal\n$ docker ps\nCONTAINER ID IMAGE  COMMAND                CREATED            STATUS \nc7b69001271d ubuntu \"bash -c 'sleep 5000'\" About a minute ago Up About a minute\n\n$ # Can we stop it?\n$ docker stop c7b69001271d\n<nothing happening>\n^C\n\n$ # Last resort - kill the container!\n$ docker kill c7b69001271d\nc7b69001271d\n```", "```\n$ docker run -it \\\n --init \\\n ubuntu \\\n bash -c 'sleep 5000'\n^C\n\n$ # <Ctrl>-C worked just fine!\n```", "```\n$ # Create a new swarm\n$ docker swarm init\nSwarm initialized: current node (j4p08hdfou1tyrdqj3eclnfb6) is now a manager.\n<snip>\n\n$ # Create a service based on mainline NGINX and update-delay\n$ # of 15 seconds\n$ docker service create \\\n --detach=true \\\n --replicas 4 \\\n --name nginx_update \\\n --update-delay 15s \\\n nginx:mainline\ns9f44kn9a4g6sf3ve449fychv\n\n$ # Let's see what we have\n$ docker service ps nginx_update\nID            NAME            IMAGE           DESIRED STATE  CURRENT STATE\nrbvv37cg85ms  nginx_update.1  nginx:mainline  Running        Running 56 seconds ago\ny4l76ld41olf  nginx_update.2  nginx:mainline  Running        Running 56 seconds ago\ngza13g9ar7jx  nginx_update.3  nginx:mainline  Running        Running 56 seconds ago\nz7dhy6zu4jt5  nginx_update.4  nginx:mainline  Running        Running 56 seconds ago\n\n$ # Update our service to use the stable NGINX branch\n$ docker service update \\\n --detach=true \\\n --image nginx:stable \\\n nginx_update\nnginx_update\n\n$ # After a minute, we can now see the new service status\n$ docker service ps nginx_update\nID            NAME               IMAGE           DESIRED STATE  CURRENT STATE\nqa7evkjvdml5  nginx_update.1     nginx:stable    Running        Running about a minute ago\nrbvv37cg85ms  \\_ nginx_update.1  nginx:mainline  Shutdown       Shutdown about a minute ago\nqbg0hsd4nxyz  nginx_update.2     nginx:stable    Running        Running about a minute ago\ny4l76ld41olf  \\_ nginx_update.2  nginx:mainline  Shutdown       Shutdown about a minute ago\nnj5gcf541fgj  nginx_update.3     nginx:stable    Running        Running 30 seconds ago\ngza13g9ar7jx  \\_ nginx_update.3  nginx:mainline  Shutdown       Shutdown 31 seconds ago\n433461xm4roq  nginx_update.4     nginx:stable    Running        Running 47 seconds ago\nz7dhy6zu4jt5  \\_ nginx_update.4  nginx:mainline  Shutdown       Shutdown 48 seconds ago\n\n$ # All our services now are using the new image\n$ # and were started staggered!\n\n$ # Clean up\n$ docker service rm nginx_update \nnginx_update \n$ docker swarm leave --force \nNode left the swarm.\n```"]