- en: Chapter 10. Swarm and the Cloud
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。Swarm和云
- en: 'Throughout this book, we worked with Docker Swarm on a set of different underlying
    technologies without, so far, diving too deep into this implication: We ran Swarm
    on the top of AWS, DigitalOcean, and on our local workstations. For test and staging
    purposes, the platform onto which we run Swarm might be of secondary importance
    (*let''s fire up some AWS instances with Docker Machine and work that way*), but
    for production it''s mandatory to understand the pros and cons, reason, evaluate,
    and follow the trend.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用了Docker Swarm在一系列不同的底层技术上，到目前为止，我们还没有深入探讨这一含义：我们在AWS、DigitalOcean以及我们的本地工作站上运行了Swarm。对于测试和分期目的，我们运行Swarm的平台可能是次要的（*让我们用Docker
    Machine启动一些AWS实例并以这种方式工作*），但对于生产环境来说，了解利弊、原因、评估和跟随趋势是必不可少的。
- en: In this chapter, we're going to review several public and private cloud options
    and technologies and their possible intersections. We'll finally treat the brand
    new buzzwords of **CaaS** (**Container as a Service**) and **IaaC** (**Infrastructure
    as a Code**) in [Chapter 11](ch11.html "Chapter 11. What is next?"), *What it
    Next? *
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将审查几种公共和私有云选项和技术以及它们可能的交集。最后，我们将在[第11章](ch11.html "第11章。接下来是什么？")中讨论**CaaS**（容器即服务）和**IaaC**（基础设施即代码）这两个全新的热词，*接下来是什么？*
- en: 'Mainly, we will be looking at:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 主要我们将关注：
- en: Docker for AWS and Azure
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker for AWS和Azure
- en: Docker Datacenter
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker数据中心
- en: Swarm on OpenStack
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在OpenStack上的Swarm
- en: Docker for AWS and Azure
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker for AWS和Azure
- en: 'As long as with Docker For Mac and Windows, the Docker team started working
    on the *new generation* toolsets for operators: Docker for AWS and Docker for
    Windows. These are intended to provide a taste of automatic for deploying Docker
    infrastructures, especially Swarm-ready ones.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与Docker For Mac和Windows一样，Docker团队开始着手开发*新一代*的运维工具集：Docker for AWS和Docker for
    Windows。这些工具旨在为部署Docker基础架构提供自动化支持，特别是适用于Swarm的基础架构。
- en: The goals are to provide with a standard way of doing things, integrating the
    underlying infrastructure with the Docker tools and let people to run, with no
    effort, the latest software versions on the platform they love. The ultimate goal
    is really to let developers to move things from their laptops with Docker for
    Mac/Windows to the cloud, with Docker for AWS/Azure.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是提供一种标准的做事方式，将底层基础设施与Docker工具集集成，并让人们毫不费力地在他们喜爱的平台上运行最新的软件版本。最终目标实际上是让开发人员将东西从他们的笔记本电脑上移动到云端，使用Docker
    for Mac/Windows和Docker for AWS/Azure。
- en: Docker for AWS
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker for AWS
- en: 'The user experience is, as always in the Docker ecosystem, great. The requirements
    are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 用户体验在Docker生态系统中一如既往地出色。要求如下：
- en: An AWS ID
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个AWS ID
- en: An SSH key imported into your AWS keyring
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入到AWS密钥环中的SSH密钥
- en: Ready security groups
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备好的安全组
- en: Basically, Docker for AWS is a clickable template for CloudForms. CloudForms
    is the orchestration system for AWS, which allows to create templates of complex
    systems, for example, you can specify a web infrastructure made of three web servers,
    one database, and one load balancer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，Docker for AWS是CloudForms的可点击模板。CloudForms是AWS的编排系统，允许创建复杂系统的模板，例如，您可以指定由三个Web服务器、一个数据库和一个负载均衡器组成的Web基础架构。
- en: 'Instead of a web or other generic infrastructure, Docker for AWS of course
    comes with the capability of creating Docker Swarm (mode) infrastructures: it
    creates as many masters and workers as you specify, puts a load balancer in front,
    and configures all networking accordingly.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Docker for AWS当然具备创建Docker Swarm（模式）基础架构的能力，它会根据您的指定创建尽可能多的主节点和工作节点，放置一个负载均衡器，并相应地配置所有网络。
- en: 'This is the welcome screen:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是欢迎界面：
- en: '![Docker for AWS](images/image_10_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Docker for AWS](images/image_10_001.jpg)'
- en: 'Then, you can specify some basic and advanced options:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以指定一些基本和高级选项：
- en: '![Docker for AWS](images/image_10_002.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Docker for AWS](images/image_10_002.jpg)'
- en: As you can see, you can select the number of managers and workers, as well as
    the flavor of the instances to be launched. So far, up to 1,000 workers are supported.
    After that, you just have to click on Create Stack in the next step, and wait
    for a few minutes for CloudForms to bring the infrastructure up.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您可以选择管理者和工作节点的数量，以及要启动的实例的类型。到目前为止，支持最多1000个工作节点。之后，您只需在下一步中点击创建堆栈，并等待几分钟让CloudForms启动基础架构。
- en: 'What the template does is to:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 模板的作用是：
- en: Create a new Virtual Private Cloud inside your AWS account, networks, and subnets
    included.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的AWS帐户中创建一个新的虚拟私有云，包括网络和子网。
- en: Create two auto-scaling groups, One for managers and one for workers.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个自动扩展组，一个用于管理者，一个用于工作节点。
- en: Start the managers and ensure that they are healthy up with Raft quorum reached.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动管理者并确保它们与Raft quorum达成健康状态。
- en: Start and enroll the workers one by one to the Swarm.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐个启动和注册工作节点到Swarm。
- en: Create **Elastic Load Balancers** (**ELBs**) to route traffic
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建**弹性负载均衡器**（**ELBs**）来路由流量。
- en: Finish
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成
- en: Once CloudFormation has finished, it will prompt with a green confirmation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦CloudFormation完成，它将提示一个绿色的确认。
- en: '![Docker for AWS](images/image_10_003.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Docker for AWS](images/image_10_003.jpg)'
- en: 'Now, we''re ready to jump into our new Docker Swarm infrastructure. Just pick
    up one of the manager''s public IPs and connect to it using the SSH key specified
    in the first step:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好进入我们的新Docker Swarm基础架构。只需选择一个管理者的公共IP并使用在第一步中指定的SSH密钥连接到它：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Docker for AWS](images/image_10_004.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Docker for AWS](images/image_10_004.jpg)'
- en: Docker for Azure
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker for Azure
- en: Thanks to an agreement with Microsoft, also automatic Swarm deployment for Azure
    is available as a one-click experience (or almost).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与微软的协议，Azure也可以作为一键式体验（或几乎是）自动部署Swarm。
- en: 'The prerequisites for deploying Swarm on Azure are:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure上部署Swarm的先决条件是：
- en: Have a valid Azure account
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有有效的Azure帐户
- en: Have this account ID associated to Docker for Azure
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此帐户ID与Docker for Azure关联。
- en: An Active Directory Principal application ID
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活动目录主体应用程序ID
- en: 'To generate the last, you can conveniently use a docker image, and launch it
    with:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成最后一个，您可以方便地使用一个docker镜像，并使用以下命令启动它：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: During the process, at some point, you will be required to login through a browser
    to the specified URL. At the end, a pair ID/secret will be available for you to
    input in the Azure wizard form.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程中，您将需要通过浏览器登录到指定的URL。最后，将为您提供一对ID/密钥，供您在Azure向导表单中输入。
- en: '![Docker for Azure](images/image_10_005.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Docker for Azure](images/image_10_005.jpg)'
- en: Once everything is fine, you can just click on **OK** and **Create**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪后，您只需点击**OK**和**Create**。
- en: '![Docker for Azure](images/image_10_006.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Docker for Azure](images/image_10_006.jpg)'
- en: 'A set of classical virtual machines will be created to run the specified number
    of managers (here 1) and workers (here 4), as long as with the proper internal
    networks, load balancers, and routers. Just as in Docker for AWS, you can start
    using your deployed Swarm by SSHing to the public IP of one manager:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将创建一组经典虚拟机来运行指定数量的管理者（这里是1）和工作节点（这里是4），以及适当的内部网络、负载均衡器和路由器。就像在Docker for AWS中一样，您可以通过SSH连接到一个管理者的公共IP来开始使用部署的Swarm：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Docker for Azure](images/image_10_007.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Docker for Azure](images/image_10_007.jpg)'
- en: There is one limitation in the Azure template now, it only supports one manager.
    The possibility to add new managers should, however, come very soon.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 目前Azure模板有一个限制，它只支持一个管理者。然而，很快就会有添加新管理者的可能性。
- en: Docker Datacenter
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker数据中心
- en: Docker Datacenter, formerly Tutum and acquired by Docker, is the single-click
    deploy solution by Docker to use UCP, the Universal Control Panel, Docker's commercial
    and enterprise product.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Docker数据中心，以前是Tutum，被Docker收购，是Docker提供的单击部署解决方案，用于使用UCP，Universal Control Panel，Docker的商业和企业产品。
- en: 'Docker Datacenter includes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Docker数据中心包括：
- en: '**Universal Control Plane** (**UCP**), the UI, refer to [https://docs.docker.com/ucp/overview](https://docs.docker.com/ucp/overview)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Universal Control Plane**（**UCP**），UI，请参阅[https://docs.docker.com/ucp/overview](https://docs.docker.com/ucp/overview)'
- en: '**Docker Trusted Registry (DTR),** the private registry, refer to [https://docs.docker.com/docker-trusted-registry](https://docs.docker.com/docker-trusted-registry)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker Trusted Registry (DTR)**，私有注册表，请参阅[https://docs.docker.com/docker-trusted-registry](https://docs.docker.com/docker-trusted-registry)'
- en: At the Dockercon 16, the team released support (currently in Beta) for Docker
    Datacenter running both on AWS and Azure. To try out Docker Datacenter, you need
    to associate a license to your company/project AWS or Azure ID.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在Dockercon 16上，团队发布了对在AWS和Azure上运行Docker数据中心的支持（目前处于Beta阶段）。要尝试Docker数据中心，你需要将许可证与你的公司/项目的AWS或Azure
    ID关联起来。
- en: 'For Datacenter for AWS, as for Docker for AWS, there is a CloudFormation template
    that makes immediate to start a Docker Datacenter. Requirements are:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AWS的数据中心，就像对于AWS的Docker一样，有一个CloudFormation模板，可以立即启动一个Docker数据中心。要求是：
- en: Have at least one Route53 configured, the AWS DNS service, see [http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有一个配置了Route53的，AWS的DNS服务，请参阅[http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html](http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html)
- en: A Docker datacenter license
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Docker数据中心许可证
- en: What you have to do is to follow the link from your license, to enter the Create
    Stack page. From here, you just input the **HostedZone** ID and the Docker Datacenter
    license and start the Stack creation. Internally, Docker Datacenter places some
    VMs on a Private network (nodes), and some, load balanced by an Elastic Load Balancer
    (ELBs, for controllers), on which it installs the commercially supported version
    of the Engine. The current version of Docker Datacenter VMs run internally Swarm
    standalone and a discovery mechanism, to connect to each other. We can expect
    the stable version of Datacenter to be released soon.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的就是从你的许可证链接进入创建堆栈页面。在这里，你只需输入**HostedZone** ID和Docker数据中心许可证，然后开始创建堆栈。在内部，Docker数据中心在私有网络（节点）上放置一些虚拟机，并且一些通过弹性负载均衡器（ELBs，用于控制器）进行负载平衡，上面安装了商业支持的引擎版本。当前版本的Docker数据中心虚拟机在内部运行Swarm独立和发现机制，以相互连接。我们可以预期数据中心的稳定版本很快就会发布。
- en: The main difference between Docker Datacenter and Docker for AWS is that the
    first one is intended to be all-inclusive enterprise ready. While the latter is
    the fastest way to deploy specifically Swarm clusters, the first is more of a
    complete solution, with a fancy UI, Notary, and optional services from the ecosystem.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Docker数据中心和Docker for AWS之间的主要区别在于前者旨在成为一体化的企业就绪解决方案。而后者是部署Swarm集群的最快方式，前者是更完整的解决方案，具有时尚的UI，Notary和来自生态系统的可选服务。
- en: Swarm on OpenStack
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenStack上的Swarm
- en: 'Speaking of private cloud, the most popular open source solution for IaaS is
    OpenStack. OpenStack is a great ecosystem of programs (formerly known as projects),
    with the goal of providing a so-called cloud operating system. The core OpenStack
    programs are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 说到私有云，最受欢迎的IaaS开源解决方案是OpenStack。OpenStack是一个由程序（以前称为项目）组成的庞大生态系统，旨在提供所谓的云操作系统。核心OpenStack程序包括：
- en: '**Keystone**: The identity and authorization system'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keystone**：身份和授权系统'
- en: '**Nova**: The virtual machine abstraction layer. Nova can be plugged with virtualization
    modules, such as Libvirt, VMware'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nova**：虚拟机抽象层。Nova可以与虚拟化模块（如Libvirt、VMware）进行插接'
- en: '**Neutron**: The network module, which handles tenant networks, instances ports,
    routing, and traffic'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neutron**：处理租户网络、实例端口、路由和流量的网络模块'
- en: '**Cinder**: The storage module responsible for handling volumes'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cinder**：负责处理卷的存储模块'
- en: '**Glance**: The image storage'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Glance**：镜像存储'
- en: 'Everything is glued up by additional actors:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都由额外的参与者粘合在一起：
- en: A database system, such as MySQL, keeping the configurations
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据库系统，比如MySQL，保存配置。
- en: An AMQP broker, such as Rabbit, to queue and deliver operations
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个AMQP代理，比如Rabbit，用于排队和传递操作
- en: A proxy system, such as HAproxy, to proxy HTTP API requests
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代理系统，比如HAproxy，用来代理HTTP API请求
- en: 'In a typical VM creation in OpenStack, the following happens:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenStack中典型的VM创建中，发生以下情况：
- en: A user either from the UI (Horizon) or from the CLI decides to spawn a VM.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户可以从UI（Horizon）或CLI决定生成一个VM。
- en: She/he clicks a button or types a command such as `nova boot ...`
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 他/她点击一个按钮或输入一个命令，比如`nova boot ...`
- en: 'Keystone checks authorization and authentication for this user in his/her tenant,
    by checking in the user''s database or in LDAP (depends on how OpenStack is configured)
    and generates a token that will be used throughout the whole session: `Here is
    your token: gAAAAABX78ldEiY2`*.*'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keystone检查用户在他/她的租户中的授权和认证，通过在用户的数据库或LDAP中检查（取决于OpenStack的配置），并生成一个将在整个会话中使用的令牌：“这是你的令牌：gAAAAABX78ldEiY2”*.*
- en: 'If authentication succeeds and the user is authorized to spawn a VM, Nova is
    invoked by using the authorization token: "We are launching a VM, can you please
    find a suitable physical host where to?"'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果认证成功并且用户被授权生成VM，Nova将使用授权令牌调用：“我们正在启动一个VM，你能找到一个合适的物理主机吗？”
- en: 'If such an host exists, Nova takes the image of choice of the user from Glance:
    "Glance, please pass me an Ubuntu Xenial bootable qcow2 file"'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这样的主机存在，Nova从Glance获取用户选择的镜像：“Glance，请给我一个Ubuntu Xenial可启动的qcow2文件”
- en: 'On the compute host where to physically launch the VM, a `nova-compute` process,
    which talks to the configured plugin, for example, says to Libvirt: "We are starting
    a VM on this host"'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在物理启动VM的计算主机上，一个`nova-compute`进程，它与配置的插件进行通信，例如，对Libvirt说：“我们正在这个主机上启动一个VM”
- en: 'Neutron allocates private (and public, if required) network ports for the VM:
    "Please create these ports on the designated networks, in these subnet pools"'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Neutron为VM分配私有（如果需要还有公共）网络端口：“请在指定的网络上创建这些端口，在这些子网池中”
- en: If the user wants to, Cinder allocates volume/s on the hosts designed by its
    scheduler. That is. Let's create additional volume/s and let's attach them to
    the VM.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果用户愿意，Cinder会在其调度程序设计的主机上分配卷。也就是说。让我们创建额外的卷，并将它们附加到VM。
- en: If KVM is used, a suitable XML is generated with all the information above,
    and Libvirt starts the VM on the compute host
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用KVM，将生成一个包含上述所有信息的适当XML，并且Libvirt在计算主机上启动VM
- en: When the VM is started, some variables are injected via cloud-init, for example,
    an SSH key to allow passwordless SSH logins
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当VM启动时，通过cloud-init注入一些变量，例如，允许无密码SSH登录的SSH密钥
- en: 'This is (except for step 8 on Cinder) exactly how the OpenStack driver of Docker
    Machine behaves: when you create a Docker Host with Machine using `-d openstack`,
    you have to specify an existing glance image, an existing private (and optionally
    a public) network, and (optionally, otherwise is automatically generated) specify
    an SSH image, stored in the Nova database. And, of course, you have to pass to
    Machine the authorization variables to your OpenStack environment, or alternatively,
    source them as exported shell variables.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这（除了Cinder上的第8步）正是Docker Machine的OpenStack驱动程序的行为方式：当你使用`-d openstack`在Machine上创建一个Docker主机时，你必须指定一个现有的glance镜像，一个现有的私有（和可选的公共）网络，并且（可选的，否则会自动生成）指定一个存储在Nova数据库中的SSH镜像。当然，你必须将授权变量传递给你的OpenStack环境，或者作为导出的shell变量源它们。
- en: 'A Machine command creating a Docker Host on OpenStack will then look like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenStack上创建一个Docker主机的Machine命令将如下所示：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: OpenStack Nova
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack Nova
- en: 'So, the classical way to go for Docker Swarm on OpenStack would be starting
    creating instances, say 10 VMs from Ubuntu 16.04 images, on a dedicated network:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在OpenStack上进行Docker Swarm的经典方式将是开始创建实例，比如从Ubuntu 16.04镜像中创建10个VMs，放在一个专用网络中：
- en: From the web UI, specifying 10 as the number of instances
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Web UI中，指定10作为实例的数量
- en: Or from the CLI, using `nova boot ... --max-count 10 machine-`
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者从CLI中，使用`nova boot ... --max-count 10 machine-`
- en: Or by using Docker Machine
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者使用Docker Machine
- en: 'The last is the more promising way because Machine automatically installs Docker,
    without later having to hack or have to use other tools on the newly created instances
    (such as Machine with the generic driver, Belt, Ansible, Salt or other scripts).
    But at the time of writing (Machine 0.8.2), Machine does not support bulk-host
    creations, so you will have to loop a `docker-machine` command with some basic
    shell logic:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种方法更有前途，因为Machine会自动安装Docker，而不必在新创建的实例上进行后续的黑客攻击或使用其他工具（例如使用通用驱动程序的Machine，Belt，Ansible，Salt或其他脚本）。但在撰写本文时（Machine
    0.8.2），Machine不支持批量主机创建，因此您将不得不使用一些基本的shell逻辑循环`docker-machine`命令：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is not a good UX at all, also because Machine scales still very bad when
    we speak of dozens of hosts.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这根本不是一个好的用户体验，因为当我们谈论数十个主机时，机器的扩展性仍然非常糟糕。
- en: The (deprecated) nova-docker driver
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: （不推荐使用的）nova-docker驱动程序
- en: Once, there was a driver for Nova, to plug Docker containers as final destinations
    for Nova (instead of creating KVM or VmWare VMs, for example, these drivers allowed
    to create and manage Docker containers from Nova). If using such a tool for the
    *old* Swarm makes sense (since everything is orchestrated as containers), this
    is of no interest for Swarm Mode, which needs Docker Hosts rather than bare containers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经，有一个用于Nova的驱动程序，可以将Docker容器作为Nova的最终目的地（而不是创建KVM或VmWare虚拟机，例如，这些驱动程序允许从Nova创建和管理Docker容器）。如果对于*旧*的Swarm来说使用这样的工具是有意义的（因为一切都被编排为容器），那么对于Swarm
    Mode来说就没有兴趣了，它需要的是Docker主机而不是裸露的容器。
- en: The reality - OpenStack the friendly way
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现实 - OpenStack友好的方式
- en: 'Luckily, OpenStack is a very vibrant project, and now that it has reached release
    **O** (**Ocata**), it is enriched by many optional modules. From the Docker Swarm
    perspective, the most interesting ones are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，OpenStack是一个非常充满活力的项目，现在它已经发布了**O**（**Ocata**），它通过许多可选模块得到了丰富。从Docker Swarm的角度来看，最有趣的是：
- en: '**Heat:** This is the orchestrator system, which can create VMs configurations
    from templates.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Heat:** 这是一个编排系统，可以从模板创建VMs配置。'
- en: '**Murano:** This is the application catalog that can run applications from
    a catalog maintained by the open source community, including Docker and Kubernetes
    containers.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Murano:** 这是一个应用程序目录，可以从由开源社区维护的目录中运行应用程序，包括Docker和Kubernetes容器。'
- en: '**Magnum:** This is the Container as a Service solution from Rackspace.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Magnum:** 这是来自Rackspace的容器即服务解决方案。'
- en: '**Kuryr:** This is the networking abstractor. With Kuryr, you can link Neutron
    tenant networks and Docker networks created with Docker Libnetwork (such as the
    Swarm ones), and connect OpenStack instances with Docker containers as if they
    were connected to the same network.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kuryr：** 这是网络抽象层。使用Kuryr，您可以将Neutron租户网络和使用Docker Libnetwork创建的Docker网络（比如Swarm网络）连接起来，并将OpenStack实例与Docker容器连接起来，就好像它们连接到同一个网络一样。'
- en: OpenStack Heat
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack Heat
- en: 'OpenStack Heat resembles Docker Compose a bit, allowing you to start systems
    by a template, but it''s much more powerful: You can not only boot a set of instances
    from an image, say Ubuntu 16.04, but you can orchestrate them, which means create
    networks, attach VMs interfaces to networks, place load balancers and execute
    later tasks on the instances, such as installing Docker. Roughly, Heat is the
    equivalent of Amazon''s CloudFormation for OpenStack.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Heat有点类似于Docker Compose，允许您通过模板启动系统，但它更加强大：您不仅可以从镜像启动一组实例，比如Ubuntu
    16.04，还可以对它们进行编排，这意味着创建网络，将VM接口连接到网络，放置负载均衡器，并在实例上执行后续任务，比如安装Docker。粗略地说，Heat相当于OpenStack的Amazon
    CloudFormation。
- en: 'In Heat everything starts from YAML templates, thanks to which you can model
    your infrastructure, before firing it up, in the same fashion as you do with Compose.
    For example, you create a template file like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在Heat中，一切都始于YAML模板，借助它，您可以在启动之前对基础架构进行建模，就像您使用Compose一样。例如，您可以创建一个这样的模板文件：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, you can launch a stack from it (`heat stack-create -f configuration.hot
    dockerhosts`). Heat will call Nova, Neutron, Cinder and all the necessary OpenStack
    services to orchestrate resources and make them available.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以从中启动一个堆栈（`heat stack-create -f configuration.hot dockerhosts`）。Heat将调用Nova、Neutron、Cinder和所有必要的OpenStack服务来编排资源并使其可用。
- en: Here we're not going to show how to start a Docker Swarm infrastructure through
    Heat, rather we'll see here Magnum, which uses Heat underneath to manipulate OpenStack
    objects.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们不会展示如何通过Heat启动Docker Swarm基础架构，而是会看到Magnum，它在底层使用Heat来操作OpenStack对象。
- en: OpenStack Magnum
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack Magnum
- en: Magnum, announced in late 2015 and developed by the OpenStack Containers Team,
    aims to make **Container Orchestration Engines** (**COEs**) such as Docker Swarm
    and **Kubernetes** available as first class resources in OpenStack. There were
    and there will be many projects inside the OpenStack arena focused to provide
    containers support, but Magnum goes further, because it's designed to support
    *containers orchestration*, not bare containers management.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum于2015年底宣布，并由OpenStack容器团队开发，旨在将**容器编排引擎**（**COEs**）如Docker Swarm和**Kubernetes**作为OpenStack中的一流资源。在OpenStack领域内有许多项目专注于提供容器支持，但Magnum走得更远，因为它旨在支持*容器编排*，而不仅仅是裸露的容器管理。
- en: '![OpenStack Magnum](images/image_10_008.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![OpenStack Magnum](images/image_10_008.jpg)'
- en: 'So far, the focus has been put especially on Kubernetes, but we''re speaking
    of **Magnum** here because it''s the most promising open source technology for
    providing a convenient way of running CaaS orchestration on the private cloud.
    Magnum does not support the newest Swarm Mode yet, at the time of writing: This
    feature must be addressed. There is a Launchpad blueprint opened by the author,
    who eventually might start working on after the book is published: [https://blueprints.launchpad.net/magnum/+spec/swarm-mode-support](https://blueprints.launchpad.net/magnum/+spec/swarm-mode-support).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，重点特别放在Kubernetes上，但我们在这里谈论**Magnum**，因为它是提供在私有云上运行CaaS编排的最有前途的开源技术。在撰写本文时，Magnum尚不支持最新的Swarm
    Mode：这个功能必须得到解决。作者已经在Launchpad蓝图上开放了一个问题，可能会在书出版后开始着手处理：[https://blueprints.launchpad.net/magnum/+spec/swarm-mode-support](https://blueprints.launchpad.net/magnum/+spec/swarm-mode-support)。
- en: Architecture and core concepts
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构和核心概念
- en: 'Magnum has two main components, running on controller nodes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum有两个主要组件，运行在控制节点上：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first process, `magnum-api` is the typical OpenStack API provider, invoked
    by the magnum Python client or by other processes for operations, such as creating
    a cluster. The latter, `magnum-conductor`, is invoked by `magnum-api` (more or
    less, it has the same functions of `nova-conductor`) through an AMQP server, such
    as Rabbit, and its goal is to interface to the Kubernetes or Docker APIs. In practice,
    these two binaries work together to provide a sort of orchestration abstraction.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个进程`magnum-api`是典型的OpenStack API提供者，由magnum Python客户端或其他进程调用进行操作，例如创建集群。后者`magnum-conductor`由`magnum-api`（或多或少具有`nova-conductor`的相同功能）通过AMQP服务器（如Rabbit）调用，其目标是与Kubernetes或Docker
    API进行接口。实际上，这两个二进制文件一起工作，提供一种编排抽象。
- en: '![Architecture and core concepts](images/image_10_009.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![架构和核心概念](images/image_10_009.jpg)'
- en: 'On the OpenStack cluster compute nodes, nothing special is necessary to run,
    apart from `nova-compute` processes: Magnum conductor exploits Heat directly to
    create stacks, which creates networks and instantiates VMs in Nova.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenStack集群计算节点上，除了`nova-compute`进程外，不需要运行任何特殊的东西：Magnum conductor直接利用Heat创建堆栈，这些堆栈创建网络并在Nova中实例化VM。
- en: 'The Magnum terminology is evolving alongside with the project. But these are
    the main concepts:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum术语随着项目的发展而不断演变。但这些是主要概念：
- en: '**Containers** are Docker containers.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**是Docker容器。'
- en: A **Cluster** (formerly a Bay) is a collection of node objects where work is
    scheduled, for example, Swarm nodes.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群**（以前是Bay）是一组调度工作的节点对象的集合，例如Swarm节点。'
- en: A **ClusterTemplate** (formerly BayModel) is the template storing information
    about the cluster types. For example, a ClusterTemplate defines *a Swarm cluster
    with 3 managers and 5 workers*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterTemplate**（以前是BayModel）是存储有关集群类型信息的模板。例如，ClusterTemplate定义了*具有3个管理节点和5个工作节点的Swarm集群*。'
- en: '**Pods** are a collection of containers running on the same physical or virtual
    machine.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pods**是在同一物理或虚拟机上运行的一组容器。'
- en: As for advanced options, such as storage, new COEs support, and scaling, Magnum
    is a very active project and we recommend you to follow its evolution on [http://docs.openstack.org/developer/magnum/](http://docs.openstack.org/developer/magnum/).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 至于高级选项，如存储、新的COE支持和扩展，Magnum是一个非常活跃的项目，我们建议您在[http://docs.openstack.org/developer/magnum/](http://docs.openstack.org/developer/magnum/)上关注其发展。
- en: Install HA Magnum on Mirantis OpenStack
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Mirantis OpenStack上安装HA Magnum
- en: Installing Magnum is not so trivial, especially if you want to warranty some
    failover typical of OpenStack HA deployments. There are many tutorials on the
    Internet on how to configure Magnum in DevStack (the developer's 1-node staging
    setup), but none showing how to work on real production systems with more than
    one controller. Here we show how to install Magnum on a real setup..
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Magnum并不那么简单，特别是如果您想要保证OpenStack HA部署的一些故障转移。互联网上有许多关于如何在DevStack（开发者的1节点暂存设置）中配置Magnum的教程，但没有一个显示如何在具有多个控制器的真实生产系统上工作。在这里，我们展示了如何在真实设置中安装Magnum。
- en: 'Typically, production OpenStack installations count a number of nodes dedicated
    to different goals. In a minimal HA deployment, there usually are:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，生产OpenStack安装会有一些专门用于不同目标的节点。在最小的HA部署中，通常有：
- en: Three or more (odd number for quorum reasons) **controller nodes**, responsible
    of hosting the OpenStack program's APIs and configuration services, such as Rabbit,
    MySQL, and HAproxy
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个或更多（为了仲裁原因是奇数）**控制节点**，负责托管OpenStack程序的API和配置服务，如Rabbit、MySQL和HAproxy
- en: An arbitrary number of **compute nodes**, where workloads run physically (where
    VMs are hosted)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意数量的**计算节点**，在这里工作负载在物理上运行（VM托管的地方）
- en: Optionally, there may be dedicated storage, monitoring, database, network, and
    other nodes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以选择专用存储、监控、数据库、网络和其他节点。
- en: 'In our setup here, based on **Mirantis OpenStack** running Newton with Heat
    installed, we have three controllers and three computes plus storage nodes. HA
    is configured with Pacemaker, which keeps resources as MySQL, Rabbitmq, and HAproxy
    in high availability. APIs are proxied by HAproxy. This is a screenshot showing
    the resources configured into Pacemaker. They all are started and working properly:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设置中，基于运行Newton的**Mirantis OpenStack**，安装了Heat，我们有三个控制器和三个计算加存储节点。使用Pacemaker配置了HA，它将MySQL、Rabbitmq和HAproxy等资源保持高可用性。API由HAproxy代理。这是一个显示配置到Pacemaker中的资源的屏幕截图。它们都已启动并正常工作：
- en: '![Install HA Magnum on Mirantis OpenStack](images/image_10_010.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![在Mirantis OpenStack上安装HA Magnum](images/image_10_010.jpg)'
- en: All nodes in the cluster run Ubuntu 16.04 (Xenial), for which the stable Magnum
    2.0 packages exist, so it's enough to consume them from upstream and install with
    `apt-get install`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的所有节点都运行Ubuntu 16.04（Xenial），稳定的Magnum 2.0软件包存在，因此只需从上游使用它们并使用`apt-get install`进行安装。
- en: 'Before installing Magnum, however, it''s necessary to prepare the environment.
    First, a database is required. Enter the MySQL console from any controller by
    just typing:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在安装Magnum之前，有必要准备环境。首先需要一个数据库。只需在任何控制器上输入MySQL控制台：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In MySQL, create the magnum database and user, and grant the correct privileges:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在MySQL中，创建magnum数据库和用户，并授予正确的权限：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, it's necessary to create the service credentials in Keystone, beginning
    with defining a magnum OpenStack user, who must be added to the services group.
    The services group is a special group, which includes the OpenStack services running
    across the cluster, such as Nova, Neutron, and so on.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有必要在Keystone中创建服务凭据，首先要定义一个magnum OpenStack用户，必须将其添加到服务组中。服务组是一个特殊的组，其中包括在集群中运行的OpenStack服务，如Nova、Neutron等。
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After that, a new service must be created:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，必须创建一个新的服务：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: OpenStack programs are invoked and talk through their APIs. An API is accessed
    with an endpoint, that is a pair URL and port, which in HA setups is proxied by
    HAproxy. In our setup, HAproxy receives HTTP requests on `10.21.22.2`. and balances
    them across the controller IPs, that are `10.21.22.4, 5`, and `6`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack程序通过其API调用并进行通信。API通过端点访问，这是一个URL和端口的配对，在HA设置中由HAproxy代理。在我们的设置中，HAproxy在`10.21.22.2`上接收HTTP请求，并在控制器IP之间进行负载均衡，即`10.21.22.4,
    5`和`6`。
- en: '![Install HA Magnum on Mirantis OpenStack](images/image_10_011.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![在Mirantis OpenStack上安装HA Magnum](images/image_10_011.jpg)'
- en: 'We have to create such endpoints for Magnum, which listens by default on port
    9511, for each zone (public, internal, and admin):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为Magnum创建这样的端点，默认情况下监听端口9511，对于每个区域（公共、内部和管理员）：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Also, Magnum needs an additional configuration to organize its workloads internally
    in domains, so a dedicated domain plus a domain user must be added:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Magnum需要额外的配置来在域内部组织其工作负载，因此必须添加一个专用域和域用户：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now everything is in place to finally run `apt-get`. On all three controllers,
    run the following command and in the ncurses interface, always answer No, to not
    change the environment, or keep the default configurations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切就绪，最终运行`apt-get`。在所有三个控制器上运行以下命令，并在ncurses界面中，始终选择No，以不更改环境，或保持默认配置：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Configure an HA Magnum installation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置HA Magnum安装
- en: 'The configuration of Magnum is pretty straightforward. What''s needed to be
    done to have it in an up and running state is:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum的配置非常简单。使其处于运行状态所需的操作是：
- en: Configure it through the `magnum.conf` file.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`magnum.conf`文件进行配置。
- en: Restart the magnum binaries.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新启动magnum二进制文件。
- en: Open port `tcp/9511`.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开端口`tcp/9511`。
- en: Configure HAproxy to accept and balance magnum APIs.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置HAproxy以接受和平衡magnum APIs。
- en: Reload HAproxy.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新加载HAproxy。
- en: 'The crucial configurations that must be done on each controller follow. First,
    on every controller the host parameter should be the IP of the interface on the
    management network:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在每个控制器上进行的关键配置如下。首先，在每个控制器上，主机参数应该是管理网络上接口的IP：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If **Barbican** (the OpenStack project dedicated to the management of secrets
    such as password) is not installed, certificates must be handled by the `**x509keypair**`
    plugin:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未安装**Barbican**（专门用于管理密码等秘密的OpenStack项目），则必须由`**x509keypair**`插件处理证书：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, a database connection string is required. In this HA setup, MySQL answers
    on the VIP `10.21.22.2`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，需要一个数据库连接字符串。在这个HA设置中，MySQL在VIP`10.21.22.2`上响应：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The Keystone authentication is configured as follow (the options are rather
    self-explanatory):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Keystone身份验证配置如下（选项相当不言自明）：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Oslo (the message broker) must be configured to messaging:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 必须配置Oslo（消息代理）以进行消息传递：
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Rabbitmq configuration is this, specifying the Rabbit cluster hosts (since
    Rabbit runs on controllers, the IPs of all controllers'' management network):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Rabbitmq的配置是这样的，指定Rabbit集群主机（因为Rabbit在控制器上运行，所以所有控制器的管理网络的IP）：
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, an additional configuration of the trustee is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，受托人的额外配置如下：
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After this reconfiguration, the Magnum services must be restarted:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行此重新配置后，必须重新启动Magnum服务：
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Magnum uses by default port `tcp/9511`, so traffic to this port must be accepted
    in iptables: Modify iptables to add this rule:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum默认使用端口`tcp/9511`，因此必须在iptables中接受到该端口的流量：修改iptables以添加此规则：
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Just after the other OpenStack services, right after `116 openvswitch db`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 就在其他OpenStack服务之后，在`116 openvswitch db`之后。
- en: 'Now, it''s time to configure HAproxy to accept magnum. Add an `180-magnum.cfg`
    file into `/etc/haproxy/conf.d` on all controllers with this content:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候配置HAproxy来接受magnum了。在所有控制器的`/etc/haproxy/conf.d`中添加一个`180-magnum.cfg`文件，内容如下：
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This configures the magnum-api to listen on the VIP `10.21.22.2:9511`, backing
    on the three controllers.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这将配置magnum-api侦听VIP`10.21.22.2:9511`，支持三个控制器。
- en: 'Just right after, HAproxy must be restarted from Pacemaker. From any controller,
    run:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着，必须从Pacemaker重新启动HAproxy。从任何控制器上运行：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Wait until no HAproxy processes are running on all controllers (you can check
    with `ps aux`), but this should be very fast, less than 1 second, then:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 等待直到所有控制器上没有运行HAproxy进程（您可以使用`ps aux`进行检查），但这应该非常快，不到1秒，然后：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After that, Magnum will be available with services up:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，Magnum将可用并且服务已启动：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Configure an HA Magnum installation](images/image_10_012.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![配置HA Magnum安装](images/image_10_012.jpg)'
- en: Create a Swarm cluster on Magnum
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Magnum上创建一个Swarm集群
- en: 'Creating a Swarm cluster, when the COE will be added to Magnum, will require
    these steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Swarm集群，当COE被添加到Magnum时，将需要执行以下步骤：
- en: Create a Swarm Mode template.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Swarm模板。
- en: Launch a cluster from the template.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模板启动一个集群。
- en: 'We''re not diving too much into something that doesn''t exist yet, but the
    commands will be something like this:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入研究尚不存在的东西，但命令可能是这样的：
- en: '[PRE27]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, a cluster template of type swarm-mode based on Ubuntu Xenial with `m1.medium`
    flavors is defined: VMs will be injected the fuel keypair, will have an additional
    external public IP. The UX for creating a cluster based on such a template might
    be expected to be:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，定义了一个基于Ubuntu Xenial的`m1.medium` flavors的swarm-mode类型的集群模板：VM将注入fuel密钥对，将具有额外的外部公共IP。基于这样一个模板创建集群的UX可能会是：
- en: '[PRE28]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, a Swarm cluster is instantiated with three managers and five workers.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用三个管理节点和五个工作节点实例化了一个Swarm集群。
- en: Magnum is a great project, at the highest level of abstraction for running container
    orchestration on OpenStack. It's running on the Rackspace cloud, and it's available
    for public usage through Carina, refer to [http://blog.rackspace.com/carina-by-rackspace-simplifies-containers-with-easy-to-use-instant-on-native-container-environment](http://blog.rackspace.com/carina-by-rackspace-simplifies-containers-with-easy-to-use-instant-on-native-container-environment)
    .
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Magnum是一个很棒的项目，在OpenStack上以最高级别的抽象层次运行容器编排。它在Rackspace云上运行，并且通过Carina可以供公众使用，参见[http://blog.rackspace.com/carina-by-rackspace-simplifies-containers-with-easy-to-use-instant-on-native-container-environment](http://blog.rackspace.com/carina-by-rackspace-simplifies-containers-with-easy-to-use-instant-on-native-container-environment)。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the alternative platforms on which we can run Docker
    Swarm clusters. We worked with the newest Docker tools--Docker for AWS and Docker
    for Azure--and we used them to demonstrate how to install Swarm in a new fashion.
    After introducing Docker Datacenter, we moved to the private cloud part. We worked
    on OpenStack, showing how to run Docker hosts on it, how to install OpenStack
    Magnum, and how to create Swarm objects on it. We've almost finished our travel.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了可以运行Docker Swarm集群的替代平台。我们使用了最新的Docker工具--Docker for AWS和Docker for
    Azure--并用它们来演示如何以新的方式安装Swarm。在介绍了Docker Datacenter之后，我们转向了私有云部分。我们在OpenStack上工作，展示了如何在其上运行Docker主机，如何安装OpenStack
    Magnum，以及如何在其上创建Swarm对象。我们的旅程即将结束。
- en: The next and last chapter will sketch the future of Docker orchestration.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章也是最后一章将勾勒出Docker编排的未来。
