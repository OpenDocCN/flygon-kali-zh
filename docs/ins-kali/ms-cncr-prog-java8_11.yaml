- en: Chapter 10. Integration of Fragments and Implementation of Alternatives
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。片段集成和替代方案的实现
- en: From [Chapter 2](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 2. Managing Lots of Threads – Executors"), *Managing Lots of Threads
    – Executors*, to [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*, you implemented different examples using the most important parts of the
    Java concurrency API. Usually, these examples are real, but most of the times,
    these examples can be parts of a bigger system. For example, in [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, you implemented
    an application to construct an inverted index to be used in an information retrieval
    system. In [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*, you implemented
    the k-means clustering algorithm to cluster a set of documents. However, you can
    implement a full information retrieval application that reads a set of documents,
    represents them using the vector space model, and clusters them using the K-NN
    algorithm. In these cases, you have different parts that may use different concurrency
    technologies (executors, streams, and so on), but they have to synchronize and
    communicate between them to get the desired results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第2章](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba "第2章。管理大量线程
    - Executors")到[第8章](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "第8章。使用并行流处理大规模数据集 - Map和Collect模型")，您使用了Java并发API的最重要部分来实现不同的示例。通常，这些示例是真实的，但大多数情况下，这些示例可以是更大系统的一部分。例如，在[第4章](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "第4章。从任务中获取数据 - Callable和Future接口")中，*从任务中获取数据 - Callable和Future接口*，您实现了一个应用程序来构建一个倒排索引，用于信息检索系统。在[第6章](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "第6章。优化分治解决方案 - Fork/Join框架")中，*优化分治解决方案 - Fork/Join框架*，您实现了k均值聚类算法来对一组文档进行聚类。然而，您可以实现一个完整的信息检索应用程序，该应用程序读取一组文档，使用向量空间模型表示它们，并使用K-NN算法对它们进行聚类。在这些情况下，您可能会使用不同的并发技术（执行器、流等）来实现不同的部分，但它们必须在它们之间同步和通信以获得所需的结果。
- en: Moreover, all the examples presented in this book can be implemented using other
    components of the Java concurrency API. We will discuss some of those alternatives
    too.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本书中提出的所有示例都可以使用Java并发API的其他组件来实现。我们也将讨论其中一些替代方案。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将涵盖以下主题：
- en: Big-block synchronization mechanisms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大块同步机制
- en: An example of a document clustering application
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档聚类应用示例
- en: Implementation alternatives
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现替代方案
- en: Big-block synchronization mechanisms
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大块同步机制
- en: Big computer applications are formed by different components that work together
    to get the desired functionality. Those components have to synchronize and communicate
    between them. In [Chapter 9](part0056_split_000.html#1LCVG2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 9. Diving into Concurrent Data Structures and Synchronization Utilities"),
    *Diving into Concurrent Data Structures and Synchronization Utilities*, you learned
    that you can use different Java classes to synchronize tasks and communicate between
    them. But this task organization is more complicated when the components you want
    to synchronize are concurrent systems too that can use different mechanisms to
    implement their concurrency. For example, you have a component in an application
    that uses the Fork/Join framework to generate their results that are used by other
    tasks synchronized using the `Phaser` class.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型计算机应用程序由不同的组件组成，这些组件共同工作以获得所需的功能。这些组件必须在它们之间进行同步和通信。在[第9章](part0056_split_000.html#1LCVG2-2fff3d3b99304faa8fa9b27f1b5053ba
    "第9章。深入并发数据结构和同步实用程序")中，*深入并发数据结构和同步实用程序*，您学到了可以使用不同的Java类来同步任务并在它们之间进行通信。但是当您要同步的组件也是可以使用不同机制来实现并发的并发系统时，这个任务组织就更加复杂了。例如，您的应用程序中有一个组件使用Fork/Join框架生成其结果，这些结果被使用`Phaser`类同步的其他任务使用。
- en: 'In these cases, you can use the following two mechanisms to synchronize and
    communicate those components:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，您可以使用以下两种机制来同步和通信这些组件：
- en: '**Shared memory**: The systems share a data structure to pass information between
    them.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享内存**：系统共享数据结构以在它们之间传递信息。'
- en: '**Message passing**: One of the systems sends a message to one or more systems.
    There are different ways to implement this. In object-oriented programming languages
    such as Java, the most basic message passing mechanism is when an object calls
    a method of an other object. You can also use **Java Message Service** (**JMS**),
    buffers, or other data structures. You can have the following two kinds of message
    passing techniques:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息传递**：系统之一向一个或多个系统发送消息。有不同的实现方式。在诸如Java之类的面向对象编程语言中，最基本的消息传递机制是一个对象调用另一个对象的方法。您还可以使用**Java消息服务**（**JMS**）、缓冲区或其他数据结构。您可以有以下两种消息传递技术：'
- en: '**Synchronous**: In this case, the class that sends the message waits until
    the receiver has processed its message'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步**：在这种情况下，发送消息的类会等待接收者处理其消息'
- en: '**Asynchronous**: In this case, the class that sends the message doesn''t wait
    for a receiver that processes its message'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步**：在这种情况下，发送消息的类不等待处理其消息的接收者。'
- en: In this section, you're going to implement an application to cluster documents
    formed by four subsystems that communicate and synchronize between them to cluster
    the documents.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，您将实现一个应用程序，用于对由四个子系统组成的文档进行聚类，这些子系统之间进行通信和同步以对文档进行聚类。
- en: An example of a document clustering application
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个文档聚类应用的示例
- en: 'This application will read a set of documents and will organize them using
    the k-means clustering algorithms. To achieve this, we will use four components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序将读取一组文档，并使用k-means聚类算法对其进行组织。为了实现这一点，我们将使用四个组件：
- en: '**The Reader system**: This system will read all the documents and convert
    every document into a list of `String` objects.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reader系统**：该系统将读取所有文档，并将每个文档转换为`String`对象列表。'
- en: '**The Indexer system**: This system will process the documents and convert
    them into a list of words. At the same time, it will generate the global vocabulary
    of the set of documents with all the words that appear on them.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Indexer系统**：该系统将处理文档并将其转换为单词列表。同时，它将生成包含所有出现在文档中的单词的全局词汇表。'
- en: '**The Mapper system**: This system will convert each list of words into a mathematical
    representation using the vector space model. The value of each item will be the
    **Tf-Idf** (short for **term frequency–inverse document frequency**) metric.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mapper系统**：该系统将把每个单词列表转换为数学表示，使用向量空间模型。每个项目的值将是**Tf-Idf**（术语频率-逆文档频率）度量。'
- en: '**The Clustering system**: This system will use the k-means clustering algorithm
    to cluster the documents.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类系统**：该系统将使用k-means聚类算法对文档进行聚类。'
- en: All these systems are concurrent and use their own tasks to implement their
    functionality. Let's see how you can implement this example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些系统都是并发的，并使用自己的任务来实现它们的功能。让我们看看如何实现这个例子。
- en: The four systems of k-means clustering
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means聚类的四个系统
- en: Let's see how to implement the Reader, Indexer, Mapper, and Clustering systems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现Reader、Indexer、Mapper和Clustering系统。
- en: The Reader system
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Reader系统
- en: 'We have implemented this system in the `DocumentReader` class. This class implements
    the `Runnable` interface and internally uses three attributes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在`DocumentReader`类中实现了这个系统。这个类实现了`Runnable`接口，并且内部使用了三个属性：
- en: A `ConcurrentLinkedDeque` class of `String` objects with all the names of the
    files you have to process
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`ConcurrentLinkedDeque`类的`String`对象，其中包含您需要处理的文件的所有名称
- en: A `ConcurrentLinkedQueue` class of `TextFile` objects to store the documents
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`ConcurrentLinkedQueue`类的`TextFile`对象，用于存储文档
- en: A `CountDownLatch` object to control the end of the execution of the tasks
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`CountDownLatch`对象，用于控制任务执行的结束
- en: 'The constructor of the class initializes these attributes (the three are received
    as parameters by the constructor) and the `run()` method given here implements
    all the functionality:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数初始化这些属性（三个属性由构造函数作为参数接收），这里给出的`run()`方法实现了所有功能：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, we read the content of all the files. For every file, we create an object
    of the `TextFile` class. This class contains the name and the content of the text
    file. It has a constructor that receives a `Path` object with the route of the
    file. Finally, we write a message in the console and use the `countDown()` method
    of the `CountDownLatch` object to indicate the finalization of this task.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们读取所有文件的内容。对于每个文件，我们创建一个`TextFile`类的对象。这个类包含文本文件的名称和内容。它有一个构造函数，接收一个包含文件路径的`Path`对象。最后，我们在控制台中写入一条消息，并使用`CountDownLatch`对象的`countDown()`方法来指示该任务的完成。
- en: 'This is the code of the `TextFile` class. Internally, it has two attributes
    to store the file name and its contents. It uses the `readAllLines()` method of
    the `Files` class to convert the content of the file into a `List<String>` data
    structure:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`TextFile`类的代码。在内部，它有两个属性来存储文件名和其内容。它使用`Files`类的`readAllLines()`方法将文件内容转换为`List<String>`数据结构：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Indexer system
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Indexer系统
- en: 'This system is implemented in the `Indexer` class that also implements the
    `Runnable` interface. In this case, we use five internal attributes as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统是在`Indexer`类中实现的，该类还实现了`Runnable`接口。在这种情况下，我们使用五个内部属性，如下所示：
- en: A `ConcurrentLinkedQueue` of `TextFile` with the content of all the documents
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`ConcurrentLinkedQueue`，其中包含所有文档内容的`TextFile`
- en: A `ConcurrentLinkedDeque` of `Document` objects to store the list of words that
    forms each document
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`ConcurrentLinkedDeque`，其中包含形成每个文档的单词列表的`Document`对象
- en: A `CountDownLatch` object to control the finalization of the `Reader` system
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`CountDownLatch`对象，用于控制`Reader`系统的完成
- en: A `CountDownLatch` object to indicate the finalization of the tasks of this
    system
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`CountDownLatch`对象，用于指示该系统任务的完成
- en: A `Vocabulary` object to store all the words that form the document collection
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Vocabulary`对象，用于存储构成文档集合的所有单词
- en: 'The constructor of the class initializes this attributes (receives all of them
    as parameters):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数初始化了这些属性（接收所有这些属性作为参数）：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `run()` method implements all the functionality, as shown here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()`方法实现了所有功能，如下所示：'
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'First, it gets `TextFile` from the queue and if it''s not `null`, uses the
    `parseDoc()` method to convert it into a `Document` object. Then, it processes
    all the words of the document to store them in the global vocabulary object and
    stores the document in the list of documents, as you can see in the following
    code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它从队列中获取`TextFile`，如果不是`null`，则使用`parseDoc()`方法将其转换为`Document`对象。然后，它处理文档的所有单词，将它们存储在全局词汇表对象中，并将文档存储在文档列表中，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `parseDoc()` method receives `List<String>` with the contents of the document
    and return a `Document` object. It creates a `Document` object to process all
    the lines using the `forEach()` method as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`parseDoc()`方法接收包含文档内容的`List<String>`，并返回一个`Document`对象。它创建一个`Document`对象，使用`forEach()`方法处理所有行，如下所示：'
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `parseLine()` method will split the line into its words and store them
    into the `doc` object as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`parseLine()`方法将行分割成单词，并将它们存储在`doc`对象中，如下所示：'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can include an optimization in the code presented before precompiling the
    regular expression used in the `replaceAll()` method:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在之前呈现的代码中包含一个优化，即预编译`replaceAll()`方法中使用的正则表达式：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The Mapper system
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射器系统
- en: 'This system is implemented in the `Mapper` class that also implements the `Runnable`
    interfaces. Internally, it use the following two attributes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统是在`Mapper`类中实现的，该类还实现了`Runnable`接口。在内部，它使用以下两个属性：
- en: A `ConcurrentLinkedDeque` of `Document` objects with the information of all
    the documents
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含所有文档信息的`ConcurrentLinkedDeque`对象
- en: A `Vocabulary` object with all the words in the entire collection
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含整个集合中所有单词的`Vocabulary`对象
- en: 'The code for this is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其代码如下：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The constructor of the class initializes those attributes, and the `run()`
    method implements the functionality of this system:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数初始化了这些属性，`run()`方法实现了该系统的功能：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: First, it gets a document from the `Deque` object of documents using the `pollFirst()`
    method. Then, it processes all the words in the document calculating the `tfxidf`
    measure and creating a new `Attribute` object to store those values. Those attributes
    are stored in a list.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它从`Deque`对象中使用`pollFirst()`方法获取一个文档。然后，它处理文档中的所有单词，计算`tfxidf`度量，并创建一个新的`Attribute`对象来存储这些值。这些属性被存储在一个列表中。
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we convert the list into an array of `Attribute` objects and store
    that array in the `Document` object:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将列表转换为一个`Attribute`对象数组，并将该数组存储在`Document`对象中：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The Clustering system
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类系统
- en: 'This system implements the k-means clustering algorithm. You can use the elements
    presented in [Chapter 5](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 5. Running Tasks Divided into Phases – The Phaser Class"), *Running Tasks
    Divided into Phases – The Phaser Class*, to implement this system. That implementation
    has the following elements:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统实现了k均值聚类算法。您可以使用[第5章](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba
    "第5章。将任务分为阶段运行-Phaser类")中介绍的元素，*将任务分为阶段运行-Phaser类*，来实现该系统。该实现具有以下元素：
- en: '**The DistanceMeasurer class**: This class calculates the Euclidean distance
    between an array of `Attribute` objects with the information of a document and
    the centroid of a cluster'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DistanceMeasurer类**：这个类计算包含文档信息的`Attribute`对象数组与簇的质心之间的欧氏距离'
- en: '**The DocumentCluster class**: This stores the information about a cluster:
    the centroid and the documents of that cluster'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DocumentCluster类**：这个类存储了关于一个簇的信息：质心和该簇的文档'
- en: '**The AssigmentTask class**: This extends the `RecursiveAction` class (of the
    Fork/Join framework) and executes the assignment task of the algorithm where we
    calculate the distance between each document and all the clusters to decide the
    cluster of every document'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AssigmentTask类**：这个类扩展了Fork/Join框架的`RecursiveAction`类，并执行算法的分配任务，其中我们计算每个文档与所有簇之间的距离，以决定每个文档的簇'
- en: '**The UpdateTask class**: This extends the `RecursiveAction` class (of the
    Fork/Join framework) and executes the updated task of the algorithm that recalculates
    the centroid of every cluster as the average of the documents stored on it'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UpdateTask类**：这个类扩展了Fork/Join框架的`RecursiveAction`类，并执行算法的更新任务，重新计算每个簇的质心，作为存储在其中的文档的平均值'
- en: '**The ConcurrentKMeans class**: This class has the static method `calculate()`
    that executes the clustering algorithm and returna an array of `DocumentCluster`
    objects with all the clusters generated'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ConcurrentKMeans类**：这个类有一个静态方法`calculate()`，执行聚类算法并返回一个包含所有生成的簇的`DocumentCluster`对象数组'
- en: 'We have only added a new class, the `ClusterTask` class that implements the
    `Runnable` interface and will call the `calculate()` method of the `ConcurrentKMeans`
    class. Internally, it uses two attributes as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只添加了一个新类，`ClusterTask`类，它实现了`Runnable`接口，并将调用`ConcurrentKMeans`类的`calculate()`方法。在内部，它使用两个属性如下：
- en: An array of `Document` objects with the information of all the documents
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含所有文档信息的`Document`对象数组
- en: The `Vocabulary` object with all the words of the collection
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含集合中所有单词的`Vocabulary`对象
- en: 'The constructor initializes those attributes, and the `run()` method implements
    the logic of the task. We call the `calculate()` method of the `ConcurrentKMeans`
    class passing to the five parameters as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数初始化了这些属性，`run()`方法实现了任务的逻辑。我们调用`ConcurrentKMeans`类的`calculate()`方法，传递五个参数如下：
- en: The array of `Document` objects with the information of all the documents.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含所有文档信息的`Document`对象数组。
- en: The `Vocabulary` object with all the words of the collection.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含集合中所有单词的`Vocabulary`对象。
- en: The number of clusters we want to generate. In this case, we use `10` as the
    number of clusters.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要生成的簇的数量。在这种情况下，我们使用`10`作为簇的数量。
- en: The seed used to initialize the centroids of the clusters. In this case, we
    use `991` as a seed.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于初始化簇质心的种子。在这种情况下，我们使用`991`作为种子。
- en: The reference size to split the tasks in subtasks used in the Fork/Join framework.
    In this case, we use `10` as that minimum size.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Fork/Join框架中用于将任务分割成子任务的参考大小。在这种情况下，我们使用`10`作为最小大小。
- en: 'This is the code of that class:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该类的代码：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The main class of the document clustering application
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档聚类应用程序的主类
- en: Once we have implemented all the elements we use in our application, we have
    to implement the `main()` method of our system. In this case, this method is critical
    because it's responsible for the launching of the systems and the creation of
    the elements needed to synchronize them. The `Reader` and `Indexer` systems will
    be executing at the same time. They will be using a buffer to share information
    between them. When the reader reads a document, it will write the list of `String`
    objects in the buffer and then proceed to process the next document. It doesn't
    wait for a task that processes that `List`. This is an example of **asynchronous
    message passing**. The `Indexer` system will take the documents from the buffer,
    process them, and generate the `Vocabulary` object with all the words of the documents.
    All the tasks executed by the `Indexer` system share the same instance of the
    `Vocabulary` class. This is an example of **shared memory**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们实现了应用程序中使用的所有元素，我们必须实现系统的`main()`方法。在这种情况下，这个方法非常关键，因为它负责启动系统并创建需要同步它们的元素。`Reader`和`Indexer`系统将同时执行。它们将使用一个缓冲区来共享信息。当读取器读取一个文档时，它将在缓冲区中写入`String`对象的列表，然后继续处理下一个文档。它不会等待处理该`List`的任务。这是**异步消息传递**的一个例子。`Indexer`系统将从缓冲区中取出文档，处理它们，并生成包含文档所有单词的`Vocabulary`对象。`Indexer`系统执行的所有任务共享`Vocabulary`类的同一个实例。这是**共享内存**的一个例子。
- en: The main class will wait for the finalization of the `Reader` and `Indexer`
    systems in a synchronous way using the `await()` method of a `CountDownLatch`
    object. This method blocks the execution of the calling thread until its internal
    counter arrives at 0.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 主类将使用`CountDownLatch`对象的`await()`方法以同步的方式等待`Reader`和`Indexer`系统的完成。该方法会阻塞调用线程的执行，直到其内部计数器达到0。
- en: Once both systems have finished their execution, the `Mapper` system will use
    the `Vocabulary` object and the `Document` information to obtain the vector space
    model representation of each document. When the `Mapper` has finished its execution,
    the `Clustering` system clusters all the documents. We have used the `CompletableFuture`
    class to synchronize the end of the `Mapper` system with the start of the `Clustering`
    system. This is another example of asynchronous communication between two systems.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦两个系统都完成了它们的执行，`Mapper`系统将使用`Vocabulary`对象和`Document`信息来获取每个文档的向量空间模型表示。当`Mapper`完成执行后，`Clustering`系统将对所有文档进行聚类。我们使用`CompletableFuture`类来同步`Mapper`系统的结束和`Clustering`系统的开始。这是两个系统之间异步通信的另一个例子。
- en: We have implemented the main class in the `ClusteringDocs` class.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在`ClusteringDocs`类中实现了主类。
- en: 'First, we create a `ThreadPoolExecutor` object and obtain `ConcurrentLinkedDeque`
    with the files that contain the documents using the `readFileNames()` method:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`ThreadPoolExecutor`对象，并使用`readFileNames()`方法获取包含文档的文件的`ConcurrentLinkedDeque`：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we create the buffer of documents, `ConcurrentLinkedDeque`, to store
    the `Document` objects, the `Vocabulary` object, and two `CountDownLatch` objects—one
    to control the end of the tasks of the `Reader` system and other to control the
    end of the tasks of the `Indexer` system. We have the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建文档的缓冲区`ConcurrentLinkedDeque`，用于存储`Document`对象、`Vocabulary`对象和两个`CountDownLatch`对象——一个用于控制`Reader`系统任务的结束，另一个用于控制`Indexer`系统任务的结束。我们有以下代码：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we launch the two tasks to execute the `Reader` system of the `DocumentReader`
    class and another four tasks to execute the `Indexer` system of the `Indexer`
    class. All these tasks are executed in the `Executor` object we have created earlier:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们启动两个任务来执行`DocumentReader`类的`Reader`系统，另外四个任务来执行`Indexer`类的`Indexer`系统。所有这些任务都在我们之前创建的`Executor`对象中执行：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, the `main()` method waits for the finalization of this tasks; first,
    for the `DocumentReader` tasks and then for the `Indexer` tasks as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`main()`方法等待这些任务的完成；首先是`DocumentReader`任务，然后是`Indexer`任务，如下所示：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we convert the `ConcurrentLinkedDeque` class of `Document` objects in
    an array:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将`ConcurrentLinkedDeque`类的`Document`对象转换为数组：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We launch the `Indexer` system executing four tasks of the `Mapper` class using
    the `runAsync()` method of the `CompletableFuture` class as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动`Indexer`系统，使用`CompletableFuture`类的`runAsync()`方法执行`Mapper`类的四个任务，如下所示：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we launch the `Clustering` system to launch one tasks of the `ClusterTask`
    class (remember that these tasks will launch other tasks to execute the algorithm).
    The `main()` method uses the `allOf()` method of the `CompletableFuture` class
    to wait for the finalization of the `Mapper` tasks and then uses the `thenRunAsync()`
    method to launch the clustering algorithm when the `Mapper` system has finished:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们启动`Clustering`系统，启动`ClusterTask`类的一个任务（请记住，这些任务将启动其他任务来执行算法）。`main()`方法使用`CompletableFuture`类的`allOf()`方法等待`Mapper`任务的完成，然后使用`thenRunAsync()`方法在`Mapper`系统完成后启动聚类算法：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we wait for the finalization of the `Clustering` system using the
    `get()` method and finish the execution of the program as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`get()`方法等待`Clustering`系统的完成，并按以下方式结束程序的执行：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `readFileNames()` method receives a string as a parameter that must be the
    path to the directory where the collection of documents is stored and generates
    a `ConcurrentLinkedDeque` class of `String` objects with the name of the files
    contained in that directory.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`readFileNames()`方法接收一个字符串作为参数，该字符串必须是存储文档集合的目录的路径，并生成一个包含该目录中文件名称的`ConcurrentLinkedDeque`类的`String`对象。'
- en: Testing our document clustering application
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试我们的文档聚类应用程序
- en: 'To test this application, we have used a subset of 10,052 documents of the
    100,673 documents with information about movies taken from Wikipedia as the collection
    of documents. In the following image, you can see the results of the first part
    of the execution—from the start of the execution until the indexer execution ends:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个应用程序，我们使用了来自维基百科的有关电影的100,673个文档中的10,052个文档的子集作为文档集。在下图中，您可以看到执行的第一部分的结果-从执行开始到索引器执行结束为止：
- en: '![Testing our document clustering application](img/00033.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![测试我们的文档聚类应用程序](img/00033.jpeg)'
- en: 'The following image shows the rest of the execution of the examples:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了示例执行的其余部分：
- en: '![Testing our document clustering application](img/00034.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![测试我们的文档聚类应用程序](img/00034.jpeg)'
- en: You can see how the tasks are synchronized as seen earlier in this chapter.
    First, the `Reader` and `Indexer` tasks are executed in a concurrent way. When
    they finish, the mapper makes the transformation of the data, and finally, the
    clustering algorithm organizes the examples.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到任务如何在本章前面同步。首先，`Reader`和`Indexer`任务以并发方式执行。当它们完成时，映射器对数据进行转换，最后，聚类算法组织示例。
- en: Implementation of alternatives with concurrent programming
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用并发编程实现替代方案
- en: Most of the examples we have implemented through the chapters of this book can
    be implemented using other components of the Java concurrency API. In this section,
    we will describe how to implement some of these alternatives.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中大多数示例都可以使用Java并发API的其他组件来实现。在本节中，我们将描述如何实现其中一些替代方案。
- en: The k-nearest neighbors' algorithm
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k最近邻算法
- en: 'You have implemented the k-nearest neighbors'' algorithm in [Chapter 2](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 2. Managing Lots of Threads – Executors"), *Managing Lots of Threads
    – Executors*, using an executor. This is a simple machine-learning algorithm used
    for supervised classification. You have a training set of previous classified
    examples. To obtain the class of a new example, you calculate the distance from
    this example to the training set of examples. The majority of classes in the nearest
    examples are the classes selected for the example. You can also implement this
    algorithm with one of the following components of the concurrency API:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第2章](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba "第2章。管理大量线程-执行器")中使用执行器实现了k最近邻算法，*管理大量线程-执行器*，这是一种用于监督分类的简单机器学习算法。您有一组先前分类的示例的训练集。要获得新示例的类别，您需要计算此示例与示例的训练集之间的距离。最近示例中的大多数类别是为示例选择的类别。您还可以使用并发API的以下组件之一实现此算法：
- en: '**Threads**: You can implement this example using `Thread` objects. You have
    to execute the tasks executed in the executor using normal threads. Each thread
    will calculate the distance between the example and a subset of the training set,
    and it will save that distance in a data structure shared between all the threads.
    When all the threads have finished, you can sort the data structure using the
    distance and calculate the class of the example.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：您可以使用`Thread`对象实现此示例。您必须使用普通线程执行执行器中执行的任务。每个线程将计算示例与训练集子集之间的距离，并将该距离保存在所有线程之间共享的数据结构中。当所有线程都完成时，您可以使用距离对数据结构进行排序并计算示例的类别。'
- en: '**Fork/Join framework**: As in the previous solution, each task will calculate
    the distance between the example and the subset of the training set. In this case,
    you define the maximum number of examples in those subsets. If a task has to process
    more examples, you divide that task into two child tasks. After you have joined
    the two tasks, you have to generate a unique data structure with the results of
    the two subtasks. At the end, you will have a data structure with all the distances
    that you can sort to obtain the class of the example.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join框架**：与先前的解决方案一样，每个任务将计算示例与训练集子集之间的距离。在这种情况下，您定义了这些子集中示例的最大数量。如果一个任务需要处理更多的示例，您将该任务分成两个子任务。在加入了两个任务之后，您必须生成一个包含两个子任务结果的唯一数据结构。最后，您将获得一个包含所有距离的数据结构，可以对其进行排序以获得示例的类别。'
- en: '**Streams**: You create a stream from the training data and map each training
    example in a structure that contains the distance between the example you want
    to classify and that example. Then, you sort that structure, get the closest ones
    using `limit()` and calculate the final resultant class.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流**：您可以从训练数据创建一个流，并将每个训练示例映射到一个包含要分类的示例与该示例之间距离的结构中。然后，您对该结构进行排序，使用`limit()`获取最接近的示例，并计算最终的结果类别。'
- en: Building an inverted index of a collection of documents
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建文档集的倒排索引
- en: 'We have implemented this example in [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, using an executor.
    An inverted index is a data structure used in the information retrieval field
    to speed up the searches of information. It stores the words presented in a document
    collection and for each word, the documents where they appear. When you make a
    search of information, you don''t need to process the documents. You look at the
    inverted index to extract the documents where the words you have inserted appear
    and construct the result list. You can also implement this algorithm with one
    of the following components of the concurrency API:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第4章](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba "第4章。从任务中获取数据-Callable和Future接口")中使用执行器实现了此示例，*从任务中获取数据-Callable和Future接口*。倒排索引是信息检索领域中用于加速信息搜索的数据结构。它存储了文档集中出现的单词，对于每个单词，存储了它们出现的文档。当您搜索信息时，您无需处理文档。您查看倒排索引以提取包含您插入的单词的文档，并构建结果列表。您还可以使用并发API的以下组件之一实现此算法：
- en: '**Threads**: Each thread will process a subset of documents. This process includes
    obtaining the vocabulary of the document and updating a common data structure
    with the global index. When all the threads have finished their execution, you
    can create the file in a sequential way.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：每个线程将处理一部分文档。这个过程包括获取文档的词汇并更新一个共同的数据结构与全局索引。当所有线程都完成执行后，可以按顺序创建文件。'
- en: '**Fork/Join framework**: You define the maximum number of documents a task
    can process. If a task has to process more documents, you split that task into
    two subtasks. The result of each task will be a data structure with the inverted
    index of the documents processed by those tasks or its subtasks. After joining
    the two subtasks, you construct a unique inverted index from the inverted indexes
    of its subtasks.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join框架**：您定义任务可以处理的文档的最大数量。如果一个任务必须处理更多的文档，您将该任务分成两个子任务。每个任务的结果将是一个包含由这些任务或其子任务处理的文档的倒排索引的数据结构。在合并两个子任务后，您将从其子任务的倒排索引构造一个唯一的倒排索引。'
- en: '**Streams**: You create a stream to process all the files. You map each file
    in the object with its vocabulary and then you reduce that vocabulary stream to
    get the inverted index.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流**：您创建一个流来处理所有文件。您将每个文件映射到其词汇对象，然后将减少该词汇流以获得倒排索引。'
- en: A best-matching algorithm for words
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词的最佳匹配算法
- en: 'You have implemented this example in [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*. The main objective
    of this algorithm is to find the words most similar to a string passed as a parameter.
    You can also implement this algorithm using one of the following components of
    the concurrency API:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第4章](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba "第4章。从任务中获取数据
    - Callable和Future接口")中实现了这个例子，*从任务中获取数据 - Callable和Future接口*。这个算法的主要目标是找到与作为参数传递的字符串最相似的单词。您还可以使用并发API的以下组件之一来实现此算法：
- en: '**Threads**: Each thread will calculate the distance between the searched word
    and a sublist of the whole list of words. Each thread will generate a partial
    result that will be merged into the final result for a shared class between all
    the threads.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：每个线程将计算搜索词与整个词列表的子列表之间的距离。每个线程将生成一个部分结果，这些结果将合并到所有线程之间共享的最终结果中。'
- en: '**Fork/Join framework**: Each task will calculate the distance between the
    searched word and a sublist of the whole list of words. If the list is too big,
    you have to split the task into two subtasks. Each task will return a partial
    result. After joining the two subtasks, the task will integrate the two sublists
    into one. The original task will return the final result.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join框架**：每个任务将计算搜索词与整个词列表的子列表之间的距离。如果列表太大，必须将任务分成两个子任务。每个任务将返回部分结果。在合并两个子任务后，任务将把两个子列表整合成一个。原始任务将返回最终结果。'
- en: '**Streams**: You create a stream for the whole list of word, map each word
    with a data structure that includes the distance between the searched word and
    that word, sort that list, and get the results.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流**：您为整个单词列表创建一个流，将每个单词与包括搜索词与该单词之间距离的数据结构进行映射，对该列表进行排序，并获得结果。'
- en: A genetic algorithm
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遗传算法
- en: You have implemented this example in [Chapter 5](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 5. Running Tasks Divided into Phases – The Phaser Class"), *Running Tasks
    Divided into Phases – The Phaser Class*. A **genetic algorithm** is an adaptive
    heuristic search algorithm based on the natural selection principles used to generate
    good solutions to **optimization** and **search problems**. There are different
    approaches to use the multiple threads for a genetic algorithm. The most classical
    one is to create *islands*. Each thread represents an island where a part of the
    population evolves. Sometimes, migrations between islands occur while transferring
    some individuals from one island to another. After the algorithm finishes, the
    best specie across all the islands is selected. Such an approach reduces the contention
    considerably as threads rarely talk to each other.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第5章](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba "第5章。分阶段运行任务
    - Phaser类")中实现了这个例子，*分阶段运行任务 - Phaser类*。**遗传算法**是一种基于自然选择原则的自适应启发式搜索算法，用于生成**优化**和**搜索问题**的良好解决方案。有不同的方法可以使用多个线程来进行遗传算法。最经典的方法是创建*岛屿*。每个线程代表一个岛屿，其中一部分种群会进化。有时，岛屿之间会发生迁移，将一些个体从一个岛屿转移到另一个岛屿。算法完成后，选择跨所有岛屿的最佳物种。这种方法大大减少了争用，因为线程很少彼此交流。
- en: There are also other approaches well-described in many publications and websites.
    For example, this handouts set summarizes the approaches pretty well at [https://cw.fel.cvut.cz/wiki/_media/courses/a0m33eoa/prednasky/08pgas-handouts.pdf](https://cw.fel.cvut.cz/wiki/_media/courses/a0m33eoa/prednasky/08pgas-handouts.pdf).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法在许多出版物和网站上有很好的描述。例如，这份讲义集在[https://cw.fel.cvut.cz/wiki/_media/courses/a0m33eoa/prednasky/08pgas-handouts.pdf](https://cw.fel.cvut.cz/wiki/_media/courses/a0m33eoa/prednasky/08pgas-handouts.pdf)上很好地总结了这些方法。
- en: 'You can also implement this algorithm using one of the following components
    of the concurrency API:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用并发API的以下组件之一来实现此算法：
- en: '**Threads**: The population with all the individuals must be a shared data
    structure. You can implement the three phases in the following way: the selection
    phase in a sequential way; the crossover phase with threads, where each thread
    will generate a predefined number of individuals; and the evaluation phase, with
    threads too. Each thread will evaluate a predefined number of individuals.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：所有个体的种群必须是一个共享的数据结构。您可以按以下方式实现三个阶段：选择阶段以顺序方式进行；交叉阶段使用线程，其中每个线程将生成预定义数量的个体；评估阶段也使用线程。每个线程将评估预定义数量的个体。'
- en: '**Executor**: You can implement something similar to the previous one executing
    the tasks in an executor instead of independent threads.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行者：您可以实现类似于之前的内容，将任务在执行者中执行，而不是独立的线程。
- en: '**Fork/Join framework**: The main idea is the same, but in this case, your
    tasks will be divided until they process a predefined number of individuals. The
    join part in this case doesn''t do anything because the results of the tasks will
    be stored in the common data structure.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join框架**：主要思想是相同的，但在这种情况下，您的任务将被分割，直到它们处理了预定义数量的个体。在这种情况下，加入部分不起作用，因为任务的结果将存储在共同的数据结构中。'
- en: A keyword extraction algorithm
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键词提取算法
- en: 'You have implemented this example in [Chapter 5](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 5. Running Tasks Divided into Phases – The Phaser Class"), *Running Tasks
    Divided into Phases – The Phaser Class*. We use this kind of algorithm to extract
    a small set of words that describes a document. We try to find the most informative
    words using measures such as the Tf-Idf. You can also implement this example using
    the following components of the concurrency API:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第5章](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba "第5章。分阶段运行任务-Phaser类")中实现了这个例子，*分阶段运行任务-Phaser类*。我们使用这种算法来提取描述文档的一小组词语。我们尝试使用Tf-Idf等度量标准找到最具信息量的词语。您还可以使用并发API的以下组件来实现此示例：
- en: '**Threads**: You need two kinds of threads. The threads of the first group
    will process the document set to obtain the document frequency of every word.
    You need a shared data structure that stores the vocabulary of the collection.
    The threads of the second group will process the documents again to obtain the
    keywords of each document and update a structure that maintains the whole list
    of keywords.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：您需要两种类型的线程。第一组线程将处理文档集以获得每个词的文档频率。您需要一个共享的数据结构来存储集合的词汇表。第二组线程将再次处理文档，以获得每个文档的关键词，并更新一个维护整个关键词列表的结构。'
- en: '**Fork/Join framework**: The main idea is similar to the previous version.
    You need two kinds of tasks. The first one to obtain the global vocabulary of
    the collection of documents. Each task will calculate the vocabulary of a subset
    of documents. If that subset is too big, the task will execute two subtasks. After
    joining the subtasks, it will group the two vocabularies obtained into one. The
    second group of tasks will calculate the list of keywords. Each task will calculate
    the list of keywords of a subset of documents. If that subset is too big, it will
    execute two subtasks. When those tasks finish, the parent task will generate a
    list of keywords with the lists returned by the child tasks.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join框架**：主要思想与以前的版本类似。您需要两种类型的任务。第一个任务是获得文档集的全局词汇表。每个任务将计算子集文档的词汇表。如果子集太大，任务将执行两个子任务。在加入子任务后，它将将获得的两个词汇表合并为一个。第二组任务将计算关键词列表。每个任务将计算子集文档的关键词列表。如果子集太大，它将执行两个子任务。当这些任务完成时，父任务将使用子任务返回的列表生成关键词列表。'
- en: '**Streams**: You create a stream to process all the documents. You map each
    document with an object that contains the vocabulary of the document and reduce
    it to get the global vocabulary. You generate another stream to process all the
    documents again, map each document with an object that contains its keywords,
    and reduce it to generate the final list of keywords.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流**：您创建一个流来处理所有文档。您将每个文档与包含文档词汇表的对象进行映射，并将其减少以获得全局词汇表。您生成另一个流来再次处理所有文档，将每个文档与包含其关键词的对象进行映射，并将其减少以生成最终的关键词列表。'
- en: A k-means clustering algorithm
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个k均值聚类算法
- en: 'You have implemented this algorithm in [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*. This algorithm
    classifies a set of elements into a previous defined number of clusters. You don''t
    have any information about the class of the elements, so this is an unsupervised
    learning algorithm that tries to find similar items. You can also implement this
    example using the following components of the concurrency API:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第6章](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba "第6章。优化分治解决方案-Fork/Join框架")中实现了这个算法，*优化分治解决方案-Fork/Join框架*。这个算法将一组元素分类到先前定义的一定数量的集群中。您对元素的类别没有任何信息，因此这是一种无监督学习算法，它试图找到相似的项目。您还可以使用并发API的以下组件来实现此示例：
- en: '**Threads**: You will have two kinds of threads. The first one will assign
    a cluster to the examples. Each thread will process a subset of the examples set.
    The second kind of thread will update the centroid of the clusters. The clusters
    and the examples must be data structures shared by all the threads.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：您将有两种类型的线程。第一种将为示例分配一个集群。每个线程将处理示例集的子集。第二种线程将更新集群的质心。集群和示例必须是所有线程共享的数据结构。'
- en: '**Executor**: You can implement the idea presented before but executing the
    tasks in an executor instead of using independent threads.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行者**：您可以实现之前提出的想法，但是在执行任务时使用执行者，而不是使用独立的线程。'
- en: A filtering data algorithm
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个过滤数据算法
- en: 'You have implemented this algorithm in [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*. The main
    objective of this algorithm is to select the objects that satisfy certain conditions
    from a very big set of objects. You can also implement this example using the
    following components of the concurrency API:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第6章](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba "第6章。优化分治解决方案-Fork/Join框架")中实现了这个算法，*优化分治解决方案-Fork/Join框架*。这个算法的主要目标是从一个非常大的对象集中选择满足某些条件的对象。您还可以使用并发API的以下组件来实现此示例：
- en: '**Threads**: Each thread will process a subset of objects. If you''re looking
    for one result, when one thread is found, it must suspend the execution of the
    rest. If you are looking for a list of elements, that list must be a shared data
    structure.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：每个线程将处理对象的一个子集。如果您正在寻找一个结果，当找到一个线程时，它必须暂停其余的执行。如果您正在寻找一个元素列表，那个列表必须是一个共享的数据结构。'
- en: '**Executor**: The same as earlier, but executing the tasks in an executor instead
    of using independent threads.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器**：与之前相同，但在执行器中执行任务，而不是使用独立线程。'
- en: '**Streams**: You can use the `filter()` method of the `Stream` class to make
    the search over the objects. Then, you can reduce those results to get them in
    the format you need.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流**：您可以使用`Stream`类的`filter()`方法来对对象进行搜索。然后，您可以将这些结果减少到您需要的格式。'
- en: Searching an inverted index
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索倒排索引
- en: 'You have implemented this algorithm in [Chapter 7](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model*. In a previous example, we discuss about how to implement the algorithm
    that creates the inverted index to speed up the searches of information. This
    is the algorithm that makes that search of information. You can also implement
    this example using the following components of the concurrency API:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第7章](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba "第7章。使用并行流处理大型数据集-映射和减少模型")中实现了这个算法，*使用并行流处理大型数据集-映射和减少模型*。在之前的例子中，我们讨论了如何实现创建倒排索引以加速信息搜索的算法。这是执行信息搜索的算法。您还可以使用并发API的以下组件来实现此示例：
- en: '**Threads**: This is the list of results in a common data structure. Each thread
    processes a part of the inverted index. Each result is inserted in order to generate
    a sorted data structure. If you get a good enough list of results, you can return
    that list and cancel the execution of the tasks.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：这是一个共同数据结构中的结果列表。每个线程处理倒排索引的一部分。每个结果都按顺序插入以生成一个排序的数据结构。如果您获得了足够好的结果列表，您可以返回该列表并取消任务的执行。'
- en: '**Executor**: This is similar to the previous one, but executes the concurrent
    tasks in an executor.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器**：这与前一个类似，但在执行器中执行并发任务。'
- en: '**Fork/Join framework**: This is similar to the previous one, but each task
    divides the part of the inverted index to process it into smaller chunks until
    they are small enough.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join framework**：这与前一个类似，但每个任务将倒排索引的部分划分为更小的块，直到它们足够小。'
- en: A numeric summarization algorithm
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数字摘要算法
- en: 'You have implemented this example in [Chapter 7](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model*. This kind of algorithm wants to obtain statistical information about a
    very big set of data. You can also implement this example using the following
    components of the concurrency API:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第7章](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba "第7章。使用并行流处理大型数据集-映射和减少模型")中实现了这个例子，*使用并行流处理大型数据集-映射和减少模型*。这种类型的算法希望获得关于非常大的数据集的统计信息。您还可以使用并发API的以下组件来实现此示例：
- en: '**Threads**: We will have an object to store the data generated by the threads.
    Each thread will process a subset of the data and store the results of that data
    in the common object. Maybe, we will have to postprocess that object to generate
    the final results.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：我们将有一个对象来存储线程生成的数据。每个线程将处理数据的一个子集，并将该数据的结果存储在共同的对象中。也许，我们将不得不对该对象进行后处理，以生成最终结果。'
- en: '**Executor**. This is similar to the previous one, but executes the concurrent
    tasks in an executor.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器**：这与前一个类似，但在执行器中执行并发任务。'
- en: '**Fork/Join framework**: This is similar to the previous one, but each task
    divides the part of the inverted index to process into smaller chunks until they
    are small enough.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join framework**：这与前一个类似，但每个任务将倒排索引的部分划分为更小的块，直到它们足够小。'
- en: A search algorithm without indexing
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有索引的搜索算法
- en: 'You have implemented this example in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*. This algorithm obtains the objects that meet certain conditions when you
    don''t have an inverted index to speed up the search. In these cases, you have
    to process all the elements when you make the search. You can also implement this
    example using the following components of the concurrency API:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第8章](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba "第8章。使用并行流处理大型数据集-映射和收集模型")中实现了这个例子，*使用并行流处理大型数据集-映射和收集模型*。当您没有倒排索引来加速搜索时，该算法会获取满足某些条件的对象。在这些情况下，您必须在进行搜索时处理所有元素。您还可以使用并发API的以下组件来实现此示例：
- en: '**Threads**: Each thread will process a subset of objects (files in our case)
    to get a list of results. The list of results will be a shared data structure.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线程**：每个线程将处理一个对象（在我们的案例中是文件）的子集，以获得结果列表。结果列表将是一个共享的数据结构。'
- en: '**Executor**: This is similar to the previous one, but the concurrent tasks
    will be executed in an executor.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行器**：这与前一个类似，但并发任务将在执行器中执行。'
- en: '**Fork/Join framework**: This is similar to the previous one, but the tasks
    divide the part of the inverted index to process into smaller chunks until they
    are small enough.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fork/Join framework**：这与前一个类似，但任务将倒排索引的部分划分为更小的块，直到它们足够小。'
- en: A recommendation system using the Map and Collect model
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用映射和收集模型的推荐系统
- en: 'You have implemented this example in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*. A **recommendation system** recommends a product or service to a customer
    based on the products/services he has bought/used and in the products/services
    bought/used by the users that has bought/used the same services as him. You can
    also implement this example using the Phaser component of the concurrency API.
    This algorithm has three phases:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第8章](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba "第8章。使用并行流处理大型数据集
    - 映射和收集模型")中实现了这个例子，*使用并行流处理大型数据集 - 映射和收集模型*。**推荐系统**根据客户购买/使用的产品/服务以及购买/使用与他购买/使用相同服务的用户购买/使用的产品/服务向客户推荐产品或服务。您还可以使用并发API的Phaser组件来实现这个例子。该算法有三个阶段：
- en: '**First phase**: We need to convert the list of products with their reviews
    into a list of buyers with the products they have bought. Each task will process
    a subset of products, and the list of buyers will be a shared data structure.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一阶段**：我们需要将带有评论的产品列表转换为购买者与他们购买的产品的列表。每个任务将处理产品的一个子集，并且购买者列表将是一个共享的数据结构。'
- en: '**Second phase**: We have to obtain a list of the users that bought the same
    products than the reference user. Each task will process an item of the products
    bought by the user and will add the users who bought that product to a common
    set of users.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二阶段**：我们需要获得购买了与参考用户相同产品的用户列表。每个任务将处理用户购买的产品项目，并将购买了该产品的用户添加到一个共同的用户集合中。'
- en: '**Third phase**: We obtain the recommended products. Each task will process
    a user of the previous list and add the products he has bought to a common data
    structure that will generate the final list of recommended products.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三阶段**：我们获得了推荐的产品。每个任务将处理前一个列表中的用户，并将他购买的产品添加到一个共同的数据结构中，这将生成最终的推荐产品列表。'
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this book, you implemented a lot of real-world examples. Some of these examples
    can be used as a part of a bigger system. These bigger systems normally have different
    concurrent parts that must share information and be synchronized between them.
    To make that synchronization, we can use three mechanisms: the shared memory,
    when two or more tasks share an object or data structure, asynchronous message
    passing, when a task sends a message to another task and doesn''t wait for its
    processing, and synchronous message passing, when a task sends a message to another
    task and waits for its processing.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您实现了许多真实世界的例子。其中一些例子可以作为更大系统的一部分。这些更大的系统通常有不同的并发部分，它们必须共享信息并在它们之间进行同步。为了进行同步，我们可以使用三种机制：共享内存，当两个或更多任务共享一个对象或数据结构时；异步消息传递，当一个任务向另一个任务发送消息并且不等待其处理时；以及同步消息传递，当一个任务向另一个任务发送消息并等待其处理时。
- en: In this chapter, we implemented an application to cluster documents formed by
    four subsystems. We used the mechanisms presented earlier to synchronize and share
    information between those four subsystems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了一个用于聚类文档的应用程序，由四个子系统组成。我们使用了早期介绍的机制来在这四个子系统之间同步和共享信息。
- en: We also revised some of the examples presented in the book to discuss other
    alternatives to their implementation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还修改了书中提出的一些例子，讨论了它们的其他实现方法。
- en: In the next chapter, you will learn how to obtain debug information of the components
    of the concurrency API and how to monitor and test a concurrent application.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何获取并发API组件的调试信息，以及如何监视和测试并发应用程序。
