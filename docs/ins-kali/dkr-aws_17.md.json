["```\n> kubectl get nodes\nNAME                STATUS  ROLES   AGE  VERSION\ndocker-for-desktop  Ready   master  1m   v1.10.3\n```", "```\nsudo apt-get update && sudo apt-get install -y apt-transport-https\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\nsudo touch /etc/apt/sources.list.d/kubernetes.list \necho \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\nsudo apt-get update\nsudo apt-get install -y kubectl\n```", "```\n# Only required if Linux Subsystem home folder is different from Windows home folder\n$ mkdir -p ~/.kube\n$ ln -s /mnt/c/Users/<username>/.kube/config ~/.kube/config\n$ kubectl get nodes\nNAME                STATUS  ROLES   AGE  VERSION\ndocker-for-desktop  Ready   master  1m   v1.10.3\n```", "```\ntodobackend> mkdir -p k8s/app todobackend> touch k8s/app/deployment.yaml\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: todobackend\n  labels:\n    app: todobackend\nspec:\n  containers:\n  - name: todobackend\n    image: 385605022855.dkr.ecr.us-east-1.amazonaws.com/docker-in-aws/todobackend\n    imagePullPolicy: IfNotPresent\n    command:\n    - uwsgi\n    - --http=0.0.0.0:8000\n    - --module=todobackend.wsgi\n    - --master\n    - --die-on-term\n    - --processes=4\n    - --threads=2\n    - --check-static=/public\n    env:\n    - name: DJANGO_SETTINGS_MODULE\n      value: todobackend.settings_release\n```", "```\n> kubectl apply -f k8s/app/deployment.yaml\npod \"todobackend\" created\n> kubectl get pods\nNAME          READY   STATUS    RESTARTS   AGE\ntodobackend   1/1     Running   0          7s\n> docker ps --format \"{{ .Names }}\"\nk8s_todobackend_todobackend_default_1b436412-9001-11e8-b7af-025000000001_0\n> docker ps --format \"{{ .ID }}: {{ .Command }} ({{ .Status }})\"\nfc0c8acdd438: \"uwsgi --http=0.0.0.\u2026\" (Up 16 seconds)\n> docker ps --format \"{{ .ID }} Ports: {{ .Ports }}\"\nfc0c8acdd438 Ports:\n```", "```\n> kubectl proxy\nStarting to serve on 127.0.0.1:8001\n```", "```\n> kubectl proxy\nStarting to serve on 127.0.0.1:8001\n^C\n> kubectl port-forward todobackend 8000:8000\nForwarding from 127.0.0.1:8000 -> 8000\nForwarding from [::1]:8000 -> 8000\nHandling connection for 8000\n```", "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend\n  labels:\n    app: todobackend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: todobackend\n  template:\n    metadata:\n      labels:\n        app: todobackend\n    spec:\n      containers:\n      - name: todobackend\n        image: 385605022855.dkr.ecr.us-east-1.amazonaws.com/docker-in-aws/todobackend\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          httpGet:\n            port: 8000\n        livenessProbe:\n          httpGet:\n            port: 8000\n        command:\n        - uwsgi\n        - --http=0.0.0.0:8000\n        - --module=todobackend.wsgi\n        - --master\n        - --die-on-term\n        - --processes=4\n        - --threads=2\n        - --check-static=/public\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: todobackend.settings_release\n```", "```\n> kubectl delete pods/todobackend\npod \"todobackend\" deleted\n> kubectl apply -f k8s/app/deployment.yaml deployment.apps \"todobackend\" created> kubectl get deployments NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\ntodobackend             2        2        2           2          12s> kubectl get pods NAME                                     READY  STATUS   RESTARTS  AGE\ntodobackend-7869d9965f-lh944             1/1    Running  0         17s\ntodobackend-7869d9965f-v986s             1/1    Running  0         17s\n```", "```\napiVersion: v1\nkind: Service\nmetadata:\n name: todobackend\nspec:\n selector:\n app: todobackend\n ports:\n - protocol: TCP\n port: 80\n    targetPort: 8000\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend\n  labels:\n    app: todobackend\n...\n...\n```", "```\n> kubectl apply -f k8s/app/deployment.yaml\nservice \"todobackend\" created\ndeployment.apps \"todobackend\" unchanged\n> kubectl get svc\nNAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nkubernetes           ClusterIP   10.96.0.1       <none>        443/TCP   8h\ntodobackend          ClusterIP   10.103.210.17   <none>        80/TCP    10s\n> kubectl get endpoints\nNAME          ENDPOINTS                       AGE\nkubernetes    192.168.65.3:6443               1d\ntodobackend   10.1.0.27:8000,10.1.0.30:8000   16h\n```", "```\n> kubectl run dig --image=googlecontainer/dnsutils --restart=Never --rm=true --tty --stdin \\\n --command -- dig todobackend a +search +noall +answer\n; <<>> DiG 9.8.4-rpz2+rl005.12-P1 <<>> todobackend a +search +noall +answer\n;; global options: +cmd\ntodobackend.default.svc.cluster.local. 30 IN A   10.103.210.17\n```", "```\napiVersion: v1\nkind: Service\nmetadata:\n  name: todobackend\nspec:\n  selector:\n    app: todobackend\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000 type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend\n  labels:\n    app: todobackend\n...\n...\n```", "```\n> kubectl apply -f k8s/app/deployment.yaml\nservice \"todobackend\" configured\ndeployment.apps \"todobackend\" unchanged\n> kubectl get svc\nNAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes           ClusterIP      10.96.0.1       <none>        443/TCP        8h\ntodobackend          LoadBalancer   10.103.210.17   localhost     80:31417/TCP   10s\n> curl localhost\n{\"todos\":\"http://localhost/todos\"}\n```", "```\n...\n...\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend\n  labels:\n    app: todobackend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: todobackend\n  template:\n    metadata:\n      labels:\n        app: todobackend\n    spec:\n      securityContext:\n fsGroup: 1000\n volumes:\n - name: public\n emptyDir: {}\n      containers:\n      - name: todobackend\n        image: 385605022855.dkr.ecr.us-east-1.amazonaws.com/docker-in-aws/todobackend\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          httpGet:\n            port: 8000\n        livenessProbe:\n          httpGet:\n            port: 8000\n        volumeMounts:\n - name: public\n mountPath: /public\n        command:\n        - uwsgi\n        - --http=0.0.0.0:8000\n        - --module=todobackend.wsgi\n        - --master\n        - --die-on-term\n        - --processes=4\n        - --threads=2\n        - --check-static=/public\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: todobackend.settings_release\n```", "```\n> kubectl apply -f k8s/app/deployment.yaml\nservice \"todobackend\" unchanged\ndeployment.apps \"todobackend\" configured\n> kubectl exec $(kubectl get pods -l app=todobackend -o=jsonpath='{.items[0].metadata.name}') \\\n    -it bash\nbash-4.4$ touch /public/foo\nbash-4.4$ ls -l /public/foo\n-rw-r--r-- 1 app app 0 Jul 26 11:28 /public/foo\nbash-4.4$ rm /public/foo\n```", "```\n...\n...\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend\n  labels:\n    app: todobackend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: todobackend\n  template:\n    metadata:\n      labels:\n        app: todobackend\n    spec:\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: public\n        emptyDir: {}\n initContainers:\n      - name: collectstatic\n image: 385605022855.dkr.ecr.us-east-1.amazonaws.com/docker-in-aws/todobackend\n imagePullPolicy: IfNotPresent\n volumeMounts:\n - name: public\n mountPath: /public\n command: [\"python3\",\"manage.py\",\"collectstatic\",\"--no-input\"]\n env:\n - name: DJANGO_SETTINGS_MODULE\n value: todobackend.settings_release\n      containers:\n      ...\n      ...\n```", "```\n> kubectl apply -f k8s/app/deployment.yaml\nservice \"todobackend\" unchanged\ndeployment.apps \"todobackend\" configured\n> kubectl logs $(kubectl get pods -l app=todobackend -o=jsonpath='{.items[0].metadata.name}') \\\n    -c collectstatic\nCopying '/usr/lib/python3.6/site-packages/django/contrib/admin/static/admin/fonts/README.txt'\n...\n...\n159 static files copied to '/public/static'.\n```", "```\n> kubectl get sc\nNAME                 PROVISIONER          AGE\nhostpath (default)   docker.io/hostpath   2d\n```", "```\ntodobackend> mkdir -p k8s/db todobackend> touch k8s/db/storage.yaml\n```", "```\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: todobackend-data\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 8Gi         \n```", "```\n> kubectl apply -f k8s/db/storage.yaml\npersistentvolumeclaim \"todobackend-data\" created\n> kubectl get pvc\nNAME               STATUS  VOLUME                                    CAPACITY  ACCESS MODES STORAGECLASS  AGE\ntodobackend-data   Bound   pvc-afba5984-9223-11e8-bc1c-025000000001  8Gi       RWO              hostpath      5s\n```", "```\n> ls -l ~/.docker/Volumes/todobackend-data\ntotal 0\ndrwxr-xr-x 2 jmenga staff 64 28 Jul 17:04 pvc-afba5984-9223-11e8-bc1c-025000000001\n```", "```\n> ln -s /mnt/c/Users/<user-name>/.docker ~/.docker\n> ls -l ~/.docker/Volumes/todobackend-data\ntotal 0\ndrwxrwxrwx 1 jmenga jmenga 4096 Jul 29 17:04 pvc-c02a8614-932d-11e8-b8aa-00155d010401\n```", "```\napiVersion: v1\nkind: Service\nmetadata:\n  name: todobackend-db\nspec:\n  selector:\n    app: todobackend-db\n  clusterIP: None \n  ports:\n  - protocol: TCP\n    port: 3306\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend-db\n  labels:\n    app: todobackend-db\nspec:\n  selector:\n    matchLabels:\n      app: todobackend-db\n  template:\n    metadata:\n      labels:\n        app: todobackend-db\n    spec:\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: todobackend-data\n      containers:\n      - name: db\n        image: mysql:5.7\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - \"mysqlshow -h 127.0.0.1 -u $(MYSQL_USER) -p$(cat /tmp/secrets/MYSQL_PASSWORD)\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n        args:\n        - --ignore-db-dir=lost+found\n        env:\n        - name: MYSQL_DATABASE\n          value: todobackend\n        - name: MYSQL_USER\n          value: todo\n        - name: MYSQL_ROOT_PASSWORD\n          value: super-secret-password\n        - name: MYSQL_PASSWORD\n          value: super-secret-password\n```", "```\n> kubectl apply -f k8s/db/deployment.yaml\nservice \"todobackend-db\" created\ndeployment.apps \"todobackend-db\" created\n> kubectl get svc NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes           ClusterIP      10.96.0.1       <none>        443/TCP        8h\ntodobackend          LoadBalancer   10.103.210.17   localhost     80:31417/TCP   1d\ntodobackend-db       ClusterIP      None            <none>        3306/TCP       6s\n> kubectl get endpoints\nNAME             ENDPOINTS                       AGE\nkubernetes       192.168.65.3:6443               2d\ntodobackend      10.1.0.44:8000,10.1.0.46:8000   1d\ntodobackend-db   10.1.0.55:3306                  14s\n```", "```\n> ls -l ~/.docker/Volumes/todobackend-data/pvc-afba5984-9223-11e8-bc1c-025000000001\ntotal 387152\n-rw-r----- 1 jmenga wheel 56 27 Jul 21:49 auto.cnf\n-rw------- 1 jmenga wheel 1675 27 Jul 21:49 ca-key.pem\n```", "```\n...\n...\ndrwxr-x--- 3 jmenga wheel 96 27 Jul 21:49 todobackend\n```", "```\n> kubectl delete -f k8s/db/deployment.yaml\nservice \"todobackend-db\" deleted\ndeployment.apps \"todobackend-db\" deleted\n> ls -l ~/.docker/Volumes/todobackend-data/pvc-afba5984-9223-11e8-bc1c-025000000001\ntotal 387152\n-rw-r----- 1 jmenga wheel 56 27 Jul 21:49 auto.cnf\n-rw------- 1 jmenga wheel 1675 27 Jul 21:49 ca-key.pem\n...\n...\ndrwxr-x--- 3 jmenga wheel 96 27 Jul 21:49 todobackend\n> kubectl apply -f k8s/db/deployment.yaml\nservice \"todobackend-db\" created\ndeployment.apps \"todobackend-db\" created\n```", "```\n> kubectl create secret generic todobackend-secret \\\n --from-literal=MYSQL_PASSWORD=\"$(openssl rand -base64 32)\" \\\n --from-literal=MYSQL_ROOT_PASSWORD=\"$(openssl rand -base64 32)\" \\\n --from-literal=SECRET_KEY=\"$(openssl rand -base64 50)\"\nsecret \"todobackend-secret\" created\n> kubectl describe secrets/todobackend-secret\nName: todobackend-secret\nNamespace: default\nLabels: <none>\nAnnotations: <none>\n\nType: Opaque\n\nData\n====\nMYSQL_PASSWORD: 44 bytes\nMYSQL_ROOT_PASSWORD: 44 bytes\nSECRET_KEY: 69 bytes\n```", "```\napiVersion: v1\nkind: Service\nmetadata:\n  name: todobackend-db\nspec:\n  selector:\n    app: todobackend-db\n  clusterIP: None \n  ports:\n  - protocol: TCP\n    port: 3306\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend-db\n  labels:\n    app: todobackend-db\nspec:\n  selector:\n    matchLabels:\n      app: todobackend-db\n  template:\n    metadata:\n      labels:\n        app: todobackend-db\n    spec:\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: todobackend-data\n - name: secrets\n secret:\n secretName: todobackend-secret          items:\n - key: MYSQL_PASSWORD\n path: MYSQL_PASSWORD\n - key: MYSQL_ROOT_PASSWORD\n path: MYSQL_ROOT_PASSWORD\n      containers:\n      - name: db\n        image: mysql:5.7\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - \"mysqlshow -h 127.0.0.1 -u $(MYSQL_USER) -p$(cat /tmp/secrets/MYSQL_PASSWORD)\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n - name: secrets\n mountPath: /tmp/secrets\n readOnly: true\n        env:\n        - name: MYSQL_DATABASE\n          value: todobackend\n        - name: MYSQL_USER\n          value: todo\n - name: MYSQL_ROOT_PASSWORD_FILE\n value: /tmp/secrets/MYSQL_ROOT_PASSWORD\n - name: MYSQL_PASSWORD_FILE\n value: /tmp/secrets/MYSQL_PASSWORD\n```", "```\n> kubectl delete -f k8s/db\nservice \"todobackend-db\" deleted\ndeployment.apps \"todobackend-db\" deleted\npersistentvolumeclaim \"todobackend-data\" deleted\n> kubectl apply -f k8s/db\nservice \"todobackend-db\" created\ndeployment.apps \"todobackend-db\" created\npersistentvolumeclaim \"todobackend-data\" created\n```", "```\n> kubectl exec $(kubectl get pods -l app=todobackend-db -o=jsonpath='{.items[0].metadata.name}')\\\n ls /tmp/secrets\nMYSQL_PASSWORD\nMYSQL_ROOT_PASSWORD\n```", "```\n...\n...\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todobackend\n  labels:\n    app: todobackend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: todobackend\n  template:\n    metadata:\n      labels:\n        app: todobackend\n    spec:\n      securityContext:\n        fsGroup: 1000\n      volumes:\n      - name: public\n        emptyDir: {}\n - name: secrets\n secret:\n secretName: todobackend-secret\n          items:\n - key: MYSQL_PASSWORD\n            path: MYSQL_PASSWORD\n - key: SECRET_KEY\n            path: SECRET_KEY\n      initContainers:\n      - name: collectstatic\n        image: 385605022855.dkr.ecr.us-east-1.amazonaws.com/docker-in-aws/todobackend\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: public\n          mountPath: /public\n        command: [\"python3\",\"manage.py\",\"collectstatic\",\"--no-input\"]\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: todobackend.settings_release\n      containers:\n      - name: todobackend\n        image: 385605022855.dkr.ecr.us-east-1.amazonaws.com/docker-in-aws/todobackend\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          httpGet:\n            port: 8000\n        livenessProbe:\n          httpGet:\n            port: 8000\n        volumeMounts:\n        - name: public\n          mountPath: /public\n - name: secrets\n mountPath: /tmp/secrets\n readOnly: true\n        command:\n        - uwsgi\n        - --http=0.0.0.0:8000\n        - --module=todobackend.wsgi\n        - --master\n        - --die-on-term\n        - --processes=4\n        - --threads=2\n        - --check-static=/public\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: todobackend.settings_release\n - name: SECRETS_ROOT\n value: /tmp/secrets\n - name: MYSQL_HOST\n value: todobackend-db\n - name: MYSQL_USER\n value: todo\n```", "```\n> kubectl apply -f k8s/app/\nservice \"todobackend\" unchanged\ndeployment.apps \"todobackend\" configured\n> kubectl get pods\nNAME                             READY   STATUS    RESTARTS   AGE\ntodobackend-74d47dd994-cpvl7     1/1     Running   0          35s\ntodobackend-74d47dd994-s2pp8     1/1     Running   0          35s\ntodobackend-db-574fb5746c-xcg9t  1/1     Running   0          12m\n> kubectl exec todobackend-74d47dd994-cpvl7 ls /tmp/secrets\nMYSQL_PASSWORD\nSECRET_KEY\n```", "```\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todobackend-migrate\nspec:\n  backoffLimit: 4\n  template:\n    spec:\n      restartPolicy: Never\n      volumes:\n      - name: secrets\n        secret:\n          secretName: todobackend-secret\n          items:\n          - key: MYSQL_PASSWORD\n            path: MYSQL_PASSWORD\n      containers:\n      - name: migrate\n        image: 385605022855.dkr.ecr.us-east-1.amazonaws.com/docker-in-aws/todobackend\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: secrets\n          mountPath: /tmp/secrets\n          readOnly: true\n        command: [\"python3\",\"manage.py\",\"migrate\",\"--no-input\"]\n        env:\n        - name: DJANGO_SETTINGS_MODULE\n          value: todobackend.settings_release\n        - name: SECRETS_ROOT\n          value: /tmp/secrets\n        - name: MYSQL_HOST\n          value: todobackend-db\n        - name: MYSQL_USER\n          value: todo\n```", "```\n> kubectl apply -f k8s/app\nservice \"todobackend\" unchanged\ndeployment.apps \"todobackend\" unchanged\njob.batch \"todobackend-migrate\" created\n> kubectl get jobs\nNAME                  DESIRED   SUCCESSFUL   AGE\ntodobackend-migrate   1         1            6s\n> kubectl logs jobs/todobackend-migrate\nOperations to perform:\n  Apply all migrations: admin, auth, contenttypes, sessions, todo\nRunning migrations:\n  Applying contenttypes.0001_initial... OK\n  Applying auth.0001_initial... OK\n  Applying admin.0001_initial... OK\n  Applying admin.0002_logentry_remove_auto_add... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n  Applying auth.0005_alter_user_last_login_null... OK\n  Applying auth.0006_require_contenttypes_0002... OK\n  Applying auth.0007_alter_validators_add_error_messages... OK\n  Applying auth.0008_alter_user_username_max_length... OK\n  Applying auth.0009_alter_user_last_name_max_length... OK\n  Applying sessions.0001_initial... OK\n  Applying todo.0001_initial... OK\n```", "```\n> curl -fs -o /usr/local/bin/aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/darwin/amd64/aws-iam-authenticator\n> chmod +x /usr/local/bin/aws-iam-authenticator\n```", "```\nAWSTemplateFormatVersion: \"2010-09-09\"\n\nDescription: EKS Cluster\n\nParameters:\n  Subnets:\n    Type: List<AWS::EC2::Subnet::Id>\n    Description: Target subnets for EKS cluster\n  VpcId:\n    Type: AWS::EC2::VPC::Id\n    Description: Target VPC\n\nResources:\n  EksServiceRole:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: eks-service-role\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service:\n                - eks.amazonaws.com\n            Action:\n              - sts:AssumeRole\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\n        - arn:aws:iam::aws:policy/AmazonEKSServicePolicy\n  EksClusterSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupName: eks-cluster-control-plane-sg\n      GroupDescription: EKS Cluster Control Plane Security Group\n      VpcId: !Ref VpcId\n      Tags:\n        - Key: Name\n          Value: eks-cluster-sg\n  EksCluster:\n    Type: AWS::EKS::Cluster\n    Properties:\n      Name: eks-cluster\n      RoleArn: !Sub ${EksServiceRole.Arn}\n      ResourcesVpcConfig:\n        SubnetIds: !Ref Subnets\n        SecurityGroupIds: \n          - !Ref EksClusterSecurityGroup\n```", "```\n> export AWS_PROFILE=docker-in-aws\n> aws cloudformation deploy --template-file stack.yml --stack-name eks-cluster \\\n--parameter-overrides VpcId=vpc-f8233a80 Subnets=subnet-a5d3ecee,subnet-324e246f,subnet-d281a2b6\\\n--capabilities CAPABILITY_NAMED_IAM\nWaiting for changeset to be created..\nWaiting for stack create/update to complete\nSuccessfully created/updated stack - eks-cluster\n```", "```\n> aws eks describe-cluster --name eks-cluster --query cluster.status \"ACTIVE\"\n> aws eks describe-cluster --name eks-cluster --query cluster.endpoint\n\"https://E7B5C85713AD5B11625D7A689F99383F.sk1.us-east-1.eks.amazonaws.com\"\n> aws eks describe-cluster --name eks-cluster --query cluster.certificateAuthority.data\n\"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNE1EY3lNakV3TURRME9Gb1hEVEk0TURjeE9URXdNRFEwT0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBUEh5CkVsajhLMUQ4M1V3RDFmdlhqYi9TdGZBK0tvWEtZNkVtZEhudnNXeWh1Snd2aGhkZDU2M0tVdGJnYW15Z0pxMVIKQkNCTWptWXVocG8rWm0ySEJrckZGakFFZDVIN1lWUXVOSm15TXdrQVV5MnpFTUU5SjJid3hkVEpqZ3pZdmlwVgpJc05zd3pIL1lSa1NVSElDK0VSaCtURmZJODhsTTBiZlM1R1pueUx0VkZCS3RjNGxBREVxRE1BTkFoaEc5OVZ3Cm5hL2w5THU2aW1jT1VOVGVCRFB0L1hxNGF3TFNUOEgwQlVvWGFwbEt0cFkvOFdqR055RUhzUHZHdXNXU3lkTHMKK3lKNXBlUm8yR3Nxc0VqMGhsbHpuV0RXWnlqQVU5Ni82QXVKRGZVSTBING1WNkpCZWxVU0tTRTZBOU1GSjRjYgpHeVpkYmh0akg1d3Zzdit1akNjQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFIRkRIODZnNkNoR2FMejBQb21EK2tyc040SUMKRzhOb0xSc2xkTkJjQmlRczFYK0hKenNxTS9TN0svL1RhUndqVjRZTE1hbnBqWGp4TzRKUWh4Q0ZHR1F2SHptUApST1FhQXRjdWRJUHYySlg5eUlOQW1rT0hDaloyNm1Yazk1b2pjekxQRE1NTlFVR2VmbXUxK282T1ZRUldTKzBMClpta211KzVyQVVFMWtTK00yMDFPeFNGcUNnL0VDd0F4ZXd5YnFMNGw4elpPWCs3VzlyM1duMWh6a3NhSnIrRHkKUVRyQ1p2MWJ0ZENpSnhmbFVxWXN5UEs1UDh4NmhKOGN2RmRFUklFdmtYQm1VbjRkWFBWWU9IdUkwdElnU2h1RAp3K0IxVkVOeUF3ZXpMWWxLaGRQQTV4R1BMN2I0ZmN4UXhCS0VlVHpaUnUxQUhMM1R4THIxcVdWbURUbz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\"\n```", "```\napiVersion: v1\nclusters:\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://localhost:6443\n  name: docker-for-desktop-cluster\n- cluster:\n certificate-authority-data: <Paste your EKS cluster certificate data here>\n server: https://E7B5C85713AD5B11625D7A689F99383F.sk1.us-east-1.eks.amazonaws.com\n name: eks-cluster\ncontexts:\n- context:\n    cluster: docker-for-desktop-cluster\n    user: docker-for-desktop\n  name: docker-for-desktop\n- context:\n cluster: eks-cluster\n user: aws\n name: eks\ncurrent-context: docker-for-desktop-cluster\nkind: Config\npreferences: {}\nusers:\n- name: aws\n user:\n exec:\n apiVersion: client.authentication.k8s.io/v1alpha1\n args:\n - token\n - -i\n - eks-cluster\n command: aws-iam-authenticator\n env:\n - name: AWS_PROFILE\n value: docker-in-aws\n- name: docker-for-desktop\n  user:\n    client-certificate-data: ...\n    client-key-data: ...\n```", "```\n> kubectl config get-contexts\nCURRENT   NAME                 CLUSTER                      AUTHINFO            NAMESPACE\n*         docker-for-desktop   docker-for-desktop-cluster   docker-for-desktop\n          eks                  eks-cluster                  aws\n> kubectl config use-context eks\nSwitched to context \"eks\".\n> kubectl get all Assume Role MFA token code: ******\nNAME                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes  ClusterIP   10.100.0.1   <none>        443/TCP   1h\n```", "```\n# Removes user from Users group, removing MFA requirement\n# To restore MFA run: aws iam add-user-to-group --user-name justin.menga --group-name Users\n> aws iam remove-user-from-group --user-name justin.menga --group-name Users\n```", "```\n[profile docker-in-aws]\nsource_profile = docker-in-aws\nrole_arn = arn:aws:iam::385605022855:role/admin\nrole_session_name=justin.menga\nregion = us-east-1\n# mfa_serial = arn:aws:iam::385605022855:mfa/justin.menga\n```", "```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-auth\n  namespace: kube-system\ndata:\n  mapRoles: |\n    - rolearn: arn:aws:iam::847222289464:role/eks-cluster-workers-NodeInstanceRole-RYP3UYR8QBYA\n      username: system:node:{{EC2PrivateDNSName}}\n      groups:\n        - system:bootstrappers\n        - system:nodes\n```", "```\n> kubectl apply -f aws-auth-cm.yaml\nconfigmap \"aws-auth\" created\n> **kubectl get nodes --watch**\nNAME                                          STATUS     ROLES    AGE   VERSION\nip-172-31-15-111.us-west-2.compute.internal   NotReady   <none>   20s   v1.10.3\nip-172-31-28-179.us-west-2.compute.internal   NotReady   <none>   16s   v1.10.3\nip-172-31-38-41.us-west-2.compute.internal    NotReady   <none>   13s   v1.10.3\nip-172-31-15-111.us-west-2.compute.internal   NotReady   <none>   23s   v1.10.3\nip-172-31-28-179.us-west-2.compute.internal   NotReady   <none>   22s   v1.10.3\nip-172-31-38-41.us-west-2.compute.internal    NotReady   <none>   22s   v1.10.3\nip-172-31-15-111.us-west-2.compute.internal   Ready      <none>   33s   v1.10.3\nip-172-31-28-179.us-west-2.compute.internal   Ready      <none>   32s   v1.10.3\nip-172-31-38-41.us-west-2.compute.internal    Ready      <none>   32s   v1.10.3\n```", "```\n> **curl -fs -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml**\n> **curl -fs -O https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml**\n> **curl -fs -O https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml**\n> **curl -fs -O https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml** > **kubectl apply -f kubernetes-dashboard.yaml**\nsecret \"kubernetes-dashboard-certs\" created\nserviceaccount \"kubernetes-dashboard\" created\nrole.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\nrolebinding.rbac.authorization.k8s.io \"kubernetes-dashboard-minimal\" created\ndeployment.apps \"kubernetes-dashboard\" created\nservice \"kubernetes-dashboard\" created\n> **kubectl apply -f heapster.yaml** serviceaccount \"heapster\" createddeployment.extensions \"heapster\" createdservice \"heapster\" created\n> **kubectl apply -f influxdb.yaml**\ndeployment.extensions \"monitoring-influxdb\" created\nservice \"monitoring-influxdb\" created\n> **kubectl apply -f heapster-rbac.yaml** clusterrolebinding.rbac.authorization.k8s.io \"heapster\" created\n```", "```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: eks-admin\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: eks-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: eks-admin\n  namespace: kube-system\n```", "```\n> **kubectl apply -f eks-admin.yaml**\nserviceaccount \"eks-admin\" created\nclusterrolebinding.rbac.authorization.k8s.io \"eks-admin\" created\n```", "```\n> **kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}')**\nName: eks-admin-token-24kh4\nNamespace: kube-system\nLabels: <none>\nAnnotations: kubernetes.io/service-account.name=eks-admin\n              kubernetes.io/service-account.uid=6d8ba3f6-8dba-11e8-b132-02b2aa7ab028\n\nType: kubernetes.io/service-account-token\n\nData\n====\nnamespace: 11 bytes\ntoken: **eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJla3MtYWRtaW4tdG9rZW4tMjRraDQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZWtzLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNmQ4YmEzZjYtOGRiYS0xMWU4LWIxMzItMDJiMmFhN2FiMDI4Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmVrcy1hZG1pbiJ9.h7hchmhGUZKjdnZRk4U1RZVS7P1tvp3TAyo10TnYI_3AOhA75gC6BlQz4yZSC72fq2rqvKzUvBqosqKmJcEKI_d6Wb8UTfFKZPFiC_USlDpnEp2e8Q9jJYHPKPYEIl9dkyd1Po6er5k6hAzY1O1Dx0RFdfTaxUhfb3zfvEN-X56M34B_Gn3FPWHIVYEwHCGcSXVhplVMMXvjfpQ-0b_1La8fb31JcnD48UolkJ1Z_DH3zsVjIR9BfcuPRoooHYQb4blgAJ4XtQYQans07bKD9lmfnQvNpaCdXV_lGOx_I5vEbc8CQKTBdJkCXaWEiwahsfwQrYtfoBlIdO5IvzZ5mg**\nca.crt: 1025 bytes\n```", "```\n> **kubectl proxy** Starting to serve on 127.0.0.1:8001\n```", "```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n name: monitoring-influxdb\n namespace: kube-system\nspec:\n replicas: 1\n template:\n metadata:\n labels:\n task: monitoring\n k8s-app: influxdb\n spec:\n containers:\n - name: influxdb\n image: k8s.gcr.io/heapster-influxdb-amd64:v1.3.3\n...\n...\n```", "```\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: gp2\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp2\nreclaimPolicy: Delete\nmountOptions:\n  - debug\n```", "```\n> kubectl get sc\nNo resources found.\n> kubectl apply -f eks/gp2-storage-class.yaml\nstorageclass.storage.k8s.io \"gp2\" created\n> kubectl patch storageclass gp2 \\\n -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' storageclass.storage.k8s.io \"gp2\" patched\n> kubectl describe sc/gp2 Name: gp2\nIsDefaultClass: Yes\nAnnotations: ...\nProvisioner: kubernetes.io/aws-ebs\nParameters: type=gp2\nAllowVolumeExpansion: <unset>\nMountOptions:\n  debug\nReclaimPolicy: Delete\nVolumeBindingMode: Immediate\nEvents: <none>\n```", "```\napiVersion: v1\nkind: Service\nmetadata:\n  name: todobackend\n  annotations:\n service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: \"true\"\n service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: \"60\"\nspec:\n  selector:\n    app: todobackend\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: LoadBalancer\n---\n...\n...\n```", "```\ntodobackend> kubectl config use-context eks\nSwitched to context \"eks\".\ntodobackend> kubectl config get-contexts\nCURRENT   NAME                 CLUSTER                      AUTHINFO             NAMESPACE\n          docker-for-desktop   docker-for-desktop-cluster   docker-for-desktop\n*         eks                  eks-cluster                  aws\n```", "```\n> kubectl create secret generic todobackend-secret \\\n --from-literal=MYSQL_PASSWORD=\"$(openssl rand -base64 32)\" \\\n --from-literal=MYSQL_ROOT_PASSWORD=\"$(openssl rand -base64 32)\" \\\n --from-literal=SECRET_KEY=\"$(openssl rand -base64 50)\"\nsecret \"todobackend-secret\" created\n```", "```\n> kubectl apply -f k8s/db\nservice \"todobackend-db\" created\ndeployment.apps \"todobackend-db\" created\npersistentvolumeclaim \"todobackend-data\" created\n> kubectl get pv\nNAME                                      CAPACITY STATUS  CLAIM                     STORAGECLASS\npvc-18ac5d3f-925c-11e8-89e1-06186d140068  8Gi      Bound   default/todobackend-data  gp2 \n```", "```\n> kubectl apply -f k8s/app\nservice \"todobackend\" created\ndeployment.apps \"todobackend\" created\njob.batch \"todobackend-migrate\" created\n```", "```\n> kubectl delete -f k8s/app\nservice \"todobackend\" deleted\ndeployment.apps \"todobackend\" deleted\njob.batch \"todobackend-migrate\" deleted\n> kubectl delete -f k8s/db\nservice \"todobackend-db\" deleted\ndeployment.apps \"todobackend-db\" deleted\npersistentvolumeclaim \"todobackend-data\" deleted\n```"]