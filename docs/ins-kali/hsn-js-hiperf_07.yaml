- en: Streams - Understanding Streams and Non-Blocking I/O
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流-理解流和非阻塞I/O
- en: We have touched on almost all of the subjects that help us write highly performant
    code for the server with JavaScript. The two last topics that should be discussed
    are streams and data formats. While these two topics can go hand in hand (since
    most data formats are implemented through read/write streams), we will focus on
    streaming in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涉及了几乎所有帮助我们使用JavaScript为服务器编写高性能代码的主题。应该讨论的最后两个主题是流和数据格式。虽然这两个主题可以并驾齐驱（因为大多数数据格式是通过读/写流实现的），但我们将在本章中重点关注流。
- en: Streaming gives us the capability to write systems that can process data without
    taking up a lot of working memory and without blocking the event queue. For those
    that have been reading this book sequentially, this may sound familiar to the
    concept of generators, and this is correct. We will focus on the four different
    types of streams that Node.js provides and that we can extend easily. From there,
    we will look at how we can combine streams and generators to process data with
    the built-in generator concepts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 流使我们能够编写可以处理数据而不占用大量工作内存并且不阻塞事件队列的系统。对于那些一直按顺序阅读本书的人来说，这可能听起来很熟悉，这是正确的。我们将重点关注Node.js提供的四种不同类型的流，以及我们可以轻松扩展的流。从那里，我们将看看如何结合流和生成器来处理具有内置生成器概念的数据。
- en: 'The following topics are covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Streaming basics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流基础知识
- en: Readable streams
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可读流
- en: Writable streams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可写流
- en: Duplex streams
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双工流
- en: Transform streams
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换流
- en: Aside – generators and streams
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附注-生成器和流
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are prerequisites for this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的先决条件如下：
- en: A code editor or IDE, preferably VS Code
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代码编辑器或IDE，最好是VS Code
- en: An operating system that can run Node.js
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以运行Node.js的操作系统
- en: The code found at [https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter07).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-High-Performance-Web-Development-with-JavaScript/tree/master/Chapter07)找到的代码。
- en: Getting started with streams
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用流
- en: 'Streaming is the act of working on an infinite dataset. This does not mean
    that it is, but it means that we have the possibility of having an unlimited data
    source. If we think in the traditional context of processing data, we usually
    run through three main steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 流是处理无限数据集的行为。这并不意味着它是无限的，但是意味着我们有可能拥有无限的数据源。如果我们从传统的数据处理上下文来思考，通常会经历三个主要步骤：
- en: Open/get access to a data source.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开/获取对数据源的访问。
- en: Process the data source once it is fully loaded in.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据源完全加载，就处理数据源。
- en: Spit out computed data to another location.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将计算出的数据输出到另一个位置。
- en: We can think of this as the basics of **input and output** (**I/O**). Most of
    our concepts of I/O involve batch processing or working on all or almost all of
    the data. This means that we know the limits of that data ahead of time. We can
    make sure that we have enough memory, storage space, computing power, and so on,
    to deal with the process. Once we are done with the process, we kill the program
    or queue up the next batch of data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其视为**输入和输出**（**I/O**）的基础。我们的大多数I/O概念涉及批处理或处理所有或几乎所有数据。这意味着我们提前知道数据的限制。我们可以确保我们有足够的内存、存储空间、计算能力等来处理这个过程。一旦我们完成了这个过程，我们就会终止程序或排队下一批数据。
- en: 'A simple example of this is seen as follows, where we count the number of lines
    that the file has:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的例子如下所示，我们计算文件的行数：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We bring in the `readFileSync` method from the `fs` module and then read in
    the `input.txt` file. From here, we split on `\n` or `\r\n`, which gives us an
    array of all the lines of the file. From there, we get the length and put it on
    our standard output channel. This seems quite simple and seems to work quite well.
    For small- to medium-length files, this works great, but what happens when the
    file becomes abnormally large? Let's go ahead and see. Head over to [https://loremipsum.io](https://loremipsum.io)
    and give it an input of 100 paragraphs. Copy this and paste it a few times into
    the `input.txt` file. Now, when we run this program, we can see in our task manager
    a spike in memory usage.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`fs`模块中引入`readFileSync`方法，然后读取`input.txt`文件。从这里开始，我们在`\n`或`\r\n`上拆分，这给我们一个文件所有行的数组。从那里，我们得到长度并将其放在我们的标准输出通道上。这似乎非常简单，而且似乎运行得很好。对于小到中等长度的文件，这很好用，但是当文件变得异常大时会发生什么呢？让我们继续看下去。前往[https://loremipsum.io](https://loremipsum.io)并输入100段落。将其复制并粘贴几次到`input.txt`文件中。现在，当我们运行这个程序时，我们可以在任务管理器中看到内存使用量的飙升。
- en: 'We are loading a roughly 3 MB file into memory, counting the number of newlines,
    and then printing this out. This should still be quite fast, but we are now starting
    to utilize a good chunk of memory. Let''s do something a bit more complex with
    this file. We will count the number of times the word `lorem` appears in the text. We
    can do that with the following code:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个大约3MB的文件加载到内存中，计算换行符的数量，然后打印出来。这应该仍然非常快，但我们现在开始利用大量内存。让我们用这个文件做一些更复杂的事情。我们将计算文本中单词`lorem`出现的次数。我们可以使用以下代码来实现：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Again, this should process quite quickly, but there should be some lag in how
    it processes. While the use of a regular expression here could give us some false
    positives, it does showcase that we are batch processing on this file. In many
    cases, when we are working in a high-speed environment, we are working with files
    that can be close to or above 1 GB. When we get into these types of files, we
    do not want to load them all into memory. This is where streaming comes into play.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这应该处理得很快，但在处理方式上可能会有一些滞后。虽然在这里使用正则表达式可能会给我们一些错误的结果，但它确实展示了我们在这个文件上进行批处理。在许多情况下，当我们在高速环境中工作时，我们处理的文件可能接近或超过1GB。当我们处理这些类型的文件时，我们不希望将它们全部加载到内存中。这就是流的作用所在。
- en: Many systems that are considered big data are working with terabytes of data.
    While there are some in-memory applications that will store large amounts of data
    in memory, a good chunk of this type of data processing uses a mix of both streaming
    with files and using in-memory data sources to work with the data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 许多被认为是大数据的系统正在处理几TB的数据。虽然有一些内存应用程序会将大量数据存储在内存中，但这种类型的数据处理大部分使用文件流和使用内存数据源来处理数据的混合。
- en: Let's take our first example. We are reading from a file and trying to count
    the number of lines in the file. Well, instead of thinking about the number of
    lines as a whole, we can look for the character that denotes a newline. The character(s)
    we are looking for in our regular expression are the use of a newline character
    (`\n`) or the carriage return plus the newline (`\r\n`) character. With this in
    mind, we should be able to build a streaming application that can read the file
    and count the number of lines without loading the file completely into memory.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拿第一个例子来说。我们正在从文件中读取，并尝试计算文件中的行数。嗯，与其考虑整个行数，我们可以寻找表示换行的字符。我们在正则表达式中寻找的字符是换行符（`\n`）或回车加换行（`\r\n`）字符。有了这个想法，我们应该能够构建一个流应用程序，它可以读取文件并计算行数，而不需要完全将文件加载到内存中。
- en: This example presents the API for utilizing a stream. We will go over what each
    Stream API gives us and how we can utilize it for our purposes. For now, take
    the code examples and run them to see how these types of applications work.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子介绍了利用流的API。我们将讨论每个流API给我们的东西，以及我们如何利用它来实现我们的目的。现在，拿出代码示例并运行它们，看看这些类型的应用是如何工作的。
- en: 'This can be seen in the following code snippet:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在以下代码片段中看到：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We grab a `Readable` stream from the `fs` module and create one. We also create
    a constant for the newline character represented in HEX format. We then listen
    for the data event so we can process data as it comes in. Then, we process each
    byte to see whether it is the same as the newline character. If it is, then we
    have a newline, otherwise we just keep searching. We do not need to explicitly
    look for the carriage return since we know it should be followed by a newline
    character.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`fs`模块中获取一个`Readable`流并创建一个。我们还为HEX格式中表示的换行符创建一个常量。然后，我们监听数据事件，以便在数据到达时处理数据。然后，我们处理每个字节，看它是否与换行符相同。如果是，那么我们有一个换行符，否则我们继续搜索。我们不需要明确寻找回车符，因为我们知道它应该后跟一个换行符。
- en: While this will be slower than loading the entire file into memory, it does
    save us quite a bit of memory when we are processing the data. Another great thing
    about this method is that these are all events. With our full processing example,
    we are taking up the entire event loop until we are done processing. With the
    stream, we have events for when the data comes in. This means that we can have
    multiple streams running at the same time on the same thread without having to
    worry too much about blocking (as long as we are not spending too much time on
    the processing of the data chunk).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这比将整个文件加载到内存中要慢，但在处理数据时它确实节省了我们相当多的内存。这种方法的另一个好处是这些都是事件。在我们的完整处理示例中，我们占用整个事件循环，直到处理完成。而使用流，我们有事件来处理数据进来。这意味着我们可以在同一个线程上同时运行多个流，而不必太担心阻塞（只要我们在数据块的处理上不花费太多时间）。
- en: 'With the previous example, we can see how we could write the counterexample
    in streaming form. Just to drive the point home, let''s go ahead and do just that.
    It should look something like the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的例子，我们可以看到如何以流的形式编写反例。为了更好地说明问题，让我们继续做到这一点。它应该看起来像下面这样：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: First, we create a read `stream` as we did before. Next, we create a `Buffer`
    form of the keyword that we are looking for (working on the raw bytes can be faster
    than trying to convert the stream into text, even if the API allows us to do that).
    Next, we maintain a `found` count and an `actual` count. The `found` count will
    let us know whether we have found the word; the other count keeps track of how
    many instances of `lorem` we have found. Next, we process each byte when a chunk
    comes in on the data event. If we find that the next byte is not the character
    we are looking for, we automatically return the `found` count to `0` (we did not
    find this particular string of text). After this check, we will see whether we
    have the full byte length found. If we do, we can increase the count and move
    `found` back to `0`. We keep the `found` counter outside the data event because
    we receive the data in chunks. Since it is chunked, one part of `lorem` could
    come at the end of one chunk and the other piece of `lorem` could come at the
    beginning of the next chunk. Once the stream ends, we output the count.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个读取`stream`，就像以前一样。接下来，我们创建一个关键字的`Buffer`形式，我们正在寻找的关键字（在原始字节上工作可能比尝试将流转换为文本更快，即使API允许我们这样做）。接下来，我们维护一个`found`计数和一个`actual`计数。`found`计数将告诉我们是否找到了这个单词；另一个计数跟踪我们找到了多少个`lorem`实例。接下来，当数据事件上的一个块到来时，我们处理每个字节。如果我们发现下一个字节不是我们要找的字符，我们会自动将`found`计数返回为`0`（我们没有找到这个特定的文本字符串）。在这个检查之后，我们将看到我们是否找到了完整的字节长度。如果是，我们可以增加计数并将`found`移回`0`。我们将`found`计数器保留在数据事件之外，因为我们以块接收数据。由于它是分块的，`lorem`的一部分可能出现在一个块的末尾，而`lorem`的另一部分可能出现在下一个块的开头。一旦流结束，我们就输出计数。
- en: Now, if we run both versions, we will find that the first actually catches more
    `lorem`. We added the case insensitive flag for regular expressions. If we turn
    this off by removing the trailing `i` and we remove the need for the character
    sequence to be by itself (the `\s` surrounding our character sequence), we will
    see that we get the same result. This example showcases how writing streams can
    be a bit more complicated than the batch processing version, but it usually leads
    to lower memory use and sometimes faster code.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们运行两个版本，我们会发现第一个实际上捕获了更多的`lorem`。我们为正则表达式添加了不区分大小写的标志。如果我们通过删除末尾的`i`来关闭它，并且我们删除字符序列周围的`\s`，我们将看到我们得到相同的结果。这个例子展示了写流可能比批处理版本更复杂一些，但通常会导致更低的内存使用和更快的代码。
- en: While utilizing built-in streams such as the streams inside of the `zlib` and
    `fs` modules will get us quite far, we will see how we can be the producers of
    our own custom streams. We will take each one and write an extended stream type
    that will handle the data framing that we were doing in the previous chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然利用内置流（如`zlib`和`fs`模块中的流）可以让我们走得更远，但我们将看到如何成为我们自己自定义流的生产者。我们将每个流都写成一个扩展流类型，以处理我们在上一章中所做的数据框架。
- en: For those that have forgotten or skipped to this chapter, we were framing all
    of our messages over a socket with the `!!!BEGIN!!!` and `!!!END!!!` tags to let
    us know when the full data had been streamed to us.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些忘记或跳到本章的人，我们正在通过套接字对所有消息进行框架处理，使用`!!!BEGIN!!!`和`!!!END!!!`标记来告诉我们何时将完整数据流式传输给我们。
- en: Building a custom Readable stream
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自定义可读流
- en: A `Readable` stream does exactly what it states, it reads from a streaming source.
    It outputs data based on some criteria. Our example of this is a take on the simple
    example that is shown in the Node.js documentation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`Readable`流确切地做了它所声明的事情，它从流源中读取。它根据某些标准输出数据。我们的例子是对Node.js文档中显示的简单示例的一种理解。'
- en: 'We are going to take our example of counting the number of `lorem` in the text
    file, but we are going to output the location in the file that we found `lorem`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以计算文本文件中`lorem`的数量为例，但我们将输出在文件中找到`lorem`的位置：
- en: 'Import the `Readable` class and the `createReadStream` method from their respective
    modules:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从各自的模块中导入`Readable`类和`createReadStream`方法：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a class that extends the `Readable` class and set up some private variables
    to track the internal state:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个扩展`Readable`类的类，并设置一些私有变量来跟踪内部状态：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Add a constructor that initializes our `#file` variable to a `Readable` stream:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个构造函数，将我们的`#file`变量初始化为`Readable`流：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Based on the constructor, we are going to utilize a `#data` private variable
    that will be a function. We will utilize it to read from our `#file` stream and
    to check for the locations of `lorem`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据构造函数，我们将利用一个`#data`私有变量，它将是一个函数。我们将利用它来从我们的`#file`流中读取，并检查`lorem`的位置：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We run through each byte and check whether we currently have the byte we are
    looking for in the `lorem` word. If we do and it is the `l` of the word, then
    we set our location `#startByteLoc` variable. If we find the entire word, we output
    `#startByteLoc`, otherwise, we reset our lookup variable and keep looping. Once
    we have finished looping, we add to our `#totalCount` the number of bytes we read
    and wait for our `#data` function to get called again. To end our stream and let
    others know that we have fully consumed the resource, we output a `null` value.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历每个字节，并检查我们当前是否拥有我们在`lorem`单词中寻找的字节。如果我们找到了，并且它是单词的`l`，那么我们设置我们的位置`#startByteLoc`变量。如果我们找到整个单词，我们输出`#startByteLoc`，否则，我们重置我们的查找变量并继续循环。一旦我们完成循环，我们将我们读取的字节数添加到我们的`#totalCount`中，并等待我们的`#data`函数再次被调用。为了结束我们的流并让其他人知道我们已完全消耗了资源，我们输出一个`null`值。
- en: The final piece we add is the `_read` method.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加的最后一部分是`_read`方法。
- en: 'This will get called either through the `Readable.read` method or through hooking
    a data event up. This is how we can make sure that a *primitive* stream such as `FileStream`
    is consumed:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过`Readable.read`方法或通过挂接数据事件来调用。这是我们如何确保*原始*流（如`FileStream`）被消耗：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can add some test code to make sure that this stream is working properly:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以添加一些测试代码来确保这个流正常工作：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With all of these concepts, we can see how we are able to consume primitive
    streams and be able to wrap them with a superset stream. Now that we have this
    stream, we could always use the pipe interface and pipe it into a `Writable` stream.
    Let's write the indices out to a file. To do this, we can do something as simple
    as `loremFinder.pipe(writeable)`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过所有这些概念，我们可以看到我们如何能够消耗原始流并能够用超集流包装它们。现在我们有了这个流，我们可以随时使用管道接口并将其管道到`Writable`流中。让我们将索引写入文件。为此，我们可以做一些简单的事情，比如`loremFinder.pipe(writeable)`。
- en: If we open the file, we will see that it is just a bunch of random data. The
    reason for this is that we encoded all of the indices into 32-bit buffers. If
    we wanted to see them, we could rewrite our stream implementation just a little
    bit. This modification could look like this: `this.push(this.#startByteLoc.toString()
    + "\r\n");`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打开文件，我们会发现它只是一堆随机数据。原因是我们将所有索引编码到32位缓冲区中。如果我们想看到它们，我们可以稍微修改我们的流实现。修改可能如下所示：`this.push(this.#startByteLoc.toString()
    + "\r\n");`。
- en: With this modification, we can now look at the `output.txt` file and see all
    of the indices. It should start to become apparent how powerful it is writing
    streams and being able to just pipe one to the next, on top of how readable the
    code can become if we just keep piping them through various stages.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种修改，我们现在可以查看`output.txt`文件并查看所有索引。如果我们只是不断地将它们通过各种阶段进行管道传输，代码变得多么可读。
- en: Understanding the Readable stream interface
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解可读流接口
- en: The `Readable` stream has a few properties that we have available to us. All
    of them are explained in the Node.js documentation, but the main ones that we
    are interested in are `highWaterMark` and `objectMode`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`Readable`流有一些可用的属性。它们都在Node.js文档中有解释，但我们感兴趣的主要是`highWaterMark`和`objectMode`。'
- en: '`highWaterMark` allows us to state how much data the internal buffer should
    hold before the stream will state that it can no longer take any data. One problem
    with our implementation is that we do not handle pauses. A stream can pause if
    this `highWaterMark` is reached. While we may not be worried about it most of
    the time, it can cause problems and is usually where stream implementors will
    run into issues. By setting a higher `highWaterMark` , we can prevent these problems.
    Another way of handling this would be to check the outcome of running `this.push`.
    If it comes back `true` , then we are able to write more data to the stream, otherwise,
    we should pause the stream and then resume when we get the signal from the other
    stream. The default `highWaterMark` for streams is around 16 KB.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`highWaterMark`允许我们声明内部缓冲区在流声明无法再接收任何数据之前应该容纳多少数据。我们实现的一个问题是我们没有处理暂停。如果达到了这个`highWaterMark`，流就会暂停。虽然大多数情况下我们可能不担心这个问题，但它可能会引起问题，通常是流实现者会遇到问题的地方。通过设置更高的`highWaterMark`，我们可以防止这些问题。另一种处理方法是检查运行`this.push`的结果。如果返回`true`，那么我们可以向流写入更多数据，否则，我们应该暂停流，然后在从另一个流得到信号时恢复。流的默认`highWaterMark`大约为16
    KB。'
- en: '`objectMode` allows us to build streams that are not `Buffer` based. This is
    great when we want to run through a list of objects. Instead of utilizing a `for`
    loop or even a `map` function, we could set up a piping system that moves objects
    through the stream and performs some type of operation on it. We are also not
    limited to plain old objects, but to almost any data type other than the `Buffer`.
    One thing to note about `objectMode` is it changes what the `highWaterMark` counts.
    Instead of it counting how much data to store in the internal buffer, it counts
    the number of objects that it will store until it pauses the stream. This defaults
    to `16`, but we can always change it if need be.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`objectMode` 允许我们构建不基于`Buffer`的流。当我们想要遍历对象列表时，这非常有用。我们可以设置一个管道系统，通过流传递对象并对其执行某种操作，而不是使用`for`循环或`map`函数。我们不仅限于普通的对象，而是几乎可以使用除`Buffer`之外的任何数据类型。关于`objectMode`的一点需要注意的是它改变了`highWaterMark`的计数方式。它不再计算存储在内部缓冲区中的数据量，而是计算直到暂停流之前将存储的对象数量。默认值为`16`，但如果需要，我们可以随时更改它。'
- en: With these two properties explained, we should discuss the various internal
    methods that are available to us. For each stream type, there is a method that
    we *need* to implement and there are methods that we *can* implement.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这两个属性的解释，我们应该讨论一下可用的各种内部方法。对于每种流类型，都有一个我们*需要*实现的方法和一些我们*可以*实现的方法。
- en: For the `Readable` stream, we only need to implement the `_read` method. This
    method gives us a `size` argument, which is the number of bytes to read from our
    underlying data source. We do not always need to heed this number, but it is available
    to us if we want to implement our stream with it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`Readable`流，我们只需要实现`_read`方法。这个方法给我们一个`size`参数，表示从底层数据源中读取的字节数。我们不总是需要遵循这个数字，但如果需要，它是可用的。
- en: Other than the `_read` method, we need to utilize the `push` method. This is
    what pushes data onto our internal buffer and helps emit the data event as we
    have seen previously. As we stated before, the `push` method returns a Boolean.
    If this value is `true`, we can continue using `push`, otherwise, we should stop
    pushing data until our `_read` implementation gets called again.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`_read`方法，我们需要使用`push`方法。这是将数据推送到内部缓冲区并帮助发出数据事件的方法，正如我们之前所见。正如我们之前所述，`push`方法返回一个布尔值。如果这个值为`true`，我们可以继续使用`push`，否则，我们应该停止推送数据，直到我们的`_read`实现再次被调用。
- en: As stated previously, when first implementing a `Readable` stream, the return
    value can be ignored. However, if we notice that data is not flowing or data is
    getting lost, the usual culprit is that the `push` method returned `false` and
    we continued to try pushing data on our stream. Once this happens, we should implement
    pausing by stopping the use of the `push` method until `_read` gets called again.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所述，当首次实现`Readable`流时，返回值可以被忽略。但是，如果我们注意到数据没有流动或数据丢失，通常的罪魁祸首是`push`方法返回了`false`，而我们继续尝试向流中推送数据。一旦发生这种情况，我们应该通过停止使用`push`方法直到再次调用`_read`来实现暂停。
- en: Two other pieces of the readable interface are the `_destroy` method and how
    to make our stream error out if something comes through that we are not able to
    handle. The `_destroy` method should be implemented if there are any low-level
    resources that we need to let go of.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可读接口的另外两个部分是`_destroy`方法以及如何使我们的流在无法处理的情况下出错。如果有任何低级资源需要释放，应该实现`_destroy`方法。
- en: This can be file handles opened with the `fs.open` command or sockets created
    with the `net` module. If an error occurred, we should also use that so we can
    emit an error event.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是使用`fs.open`命令打开的文件句柄，也可以是使用`net`模块创建的套接字。如果发生错误，我们也应该使用它来发出错误事件。
- en: To handle errors that may come up with our streams, we should emit our error
    through the `this.emit` system. If we throw an error, as per the documentation,
    it can lead to unexpected results. By emitting an error, we let the user of our
    stream deal with the error and handle it as they see fit.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理流可能出现的错误，我们应该通过`this.emit`系统发出错误。如果我们抛出错误，根据文档，可能会导致意外的结果。通过发出错误，我们让流的用户处理错误并根据他们的意愿处理它。
- en: Implementing the Readable stream
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现可读流
- en: From what we have learned here, let's implement the framing system that we talked
    about earlier. From our previous example, it should be obvious how we might handle
    this. We will hold the underlying resource, in this case, a socket. From there,
    we will find the `!!!BEGIN!!!` buffer and let it pass. We will then start to store
    the data that is held. Once we reach the `!!!END!!!` buffer, we will push out
    the data chunk.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在这里学到的知识，让我们实现我们之前讨论过的帧系统。从我们之前的示例中，我们应该清楚地知道我们如何处理这个问题。我们将持有底层资源，即套接字。然后，我们将找到`!!!BEGIN!!!`缓冲区并让其通过。然后我们将开始存储所持有的数据。一旦我们到达`!!!END!!!`缓冲区，我们将推出数据块。
- en: 'We are holding on to quite a bit of data in this case, but it showcases how
    we might handle framing. The duplex stream will showcase how we might handle a
    simple protocol. The example is seen as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们持有相当多的数据，但它展示了我们如何处理帧。双工流将展示我们如何处理一个简单的协议。示例如下：
- en: 'Import the `Readable` stream and create a class called `ReadMessagePassStream`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`Readable`流并创建一个名为`ReadMessagePassStream`的类：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Add some private variables to hold our internal state for the stream:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一些私有变量来保存流的内部状态：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create a `#data` method like the one before. We will now be looking for the
    beginning and end frame buffers that we set up before, `#bufBegin` and `#bufEnd`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个像之前那样的`#data`方法。我们现在将寻找之前设置的开始和结束帧缓冲区`#bufBegin`和`#bufEnd`：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the constructor for the class to initialize our private variables:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建类的构造函数以初始化我们的私有变量：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: One new piece of information is the `objectMode` property, which can be passed
    into our stream. This allows our streams to read objects instead of raw buffers.
    In our case, we do not want this to happen; we want to work with the raw data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的信息是`objectMode`属性，它可以传递到我们的流中。这允许我们的流读取对象而不是原始缓冲区。在我们的情况下，我们不希望发生这种情况；我们希望使用原始数据。
- en: 'Add the `_read` method to make sure that our stream will start up:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为确保我们的流将启动，请添加`_read`方法：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With this code, we now have a way of handling a socket without having to listen
    to the data event in our main code; it is now wrapped in this `Readable` stream.
    On top of this, we now have the capability of piping this stream into another.
    The following test harness code shows this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这段代码，我们现在有了一种处理套接字的方法，而不必在主代码中监听数据事件；它现在包装在这个`Readable`流中。除此之外，我们现在有了将此流传输到另一个流的能力。以下是测试工具代码：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have a server being hosted on localhost at port `3333`. We create a `write`
    stream and pipe any data from our `ReadMessagePassStream` to that file. If we
    hook this up to the server in the test harness, we will notice that an output
    file is created and holds only the data that we sent over it, not the framing
    code.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本地主机的端口`3333`上托管了一个服务器。我们创建一个`write`流，并将任何数据从我们的`ReadMessagePassStream`传输到该文件。如果我们将其连接到测试工具中的服务器，我们会注意到创建了一个输出文件，其中只包含我们发送的数据，而不包含帧代码。
- en: The framing technique that we are utilizing is not always going to work. Just
    as it has been shown in the `lorem` example that our data can get chunked at any
    point, we could have our `!!!START!!!` and `!!!END!!!` end up on one of the chunk
    boundaries. If this happened, our streams would fail. There is additional code
    that we would need to handle these cases, but these examples should provide all
    the necessary ideas to implement the streaming code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的帧技术并不总是有效。就像在`lorem`示例中展示的那样，我们的数据可能在任何时候被分块，我们的`!!!START!!!`和`!!!END!!!`可能会出现在其中一个块的边界上。如果发生这种情况，我们的流将失败。我们需要额外的代码来处理这些情况，但这些示例应该提供了实现流代码所需的所有必要思路。
- en: Next, we will take a look at the `Writable` stream interface.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下`Writable`流接口。
- en: Building a Writable stream
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建可写流
- en: A `Writable` stream is one that we write data to and it can pipe into a `Readable`,
    `Duplex`, or `Transform` stream. We can use these streams to write data in a chunked
    manner so a consuming stream can process the data in chunks instead of all at
    once. The API for a writable stream is quite similar to that of a `Readable` stream
    except for the methods that are available to us.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`Writable`流是我们写入数据的流，它可以连接到`Readable`、`Duplex`或`Transform`流。我们可以使用这些流以分块的方式写入数据，以便消费流可以以分块而不是一次性处理数据。可写流的API与`Readable`流非常相似，除了可用的方法。'
- en: Understanding the Writable stream interface
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解可写流接口
- en: 'A writable stream gives us nearly the same options that are available to a
    `Readable` stream so we will not go into that. Instead, we will take a look at
    the four methods that are available to us—one that we *must* implement and the
    rest that we *can* implement:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 可写流为我们提供了几乎与`Readable`流相同的选项，因此我们不会深入讨论。相反，我们将看一下可用于我们的四种方法——一种我们*必须*实现的方法和其余我们*可以*实现的方法：
- en: The `_write` method allows us to perform any type of transformations or data
    manipulations that we need to and provides us with the ability to use a callback.
    This callback is what signals that the write stream is able to take in more data.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_write`方法允许我们执行任何类型的转换或数据操作，并为我们提供使用回调的能力。这个回调是信号，表明写流能够接收更多数据。'
- en: While not inherently true, it is what pops data off the internal buffer if there
    is any. However, for our purposes, it is best to think of the callback as a way
    to process more data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是固有的真实情况，但它会从内部缓冲区中弹出数据。然而，对于我们的目的，最好将回调视为处理更多数据的一种方式。
- en: We can utilize this to wrap a more primitive stream and add our own data before
    or after the main data chunk. We will see this with the practical counterpart
    to our `Readable` stream.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这一点来包装一个更原始的流，并在主数据块之前或之后添加我们自己的数据。我们将在我们的`Readable`流的实际对应物中看到这一点。
- en: The `_final` method allows us to perform any actions that we need to before
    the writable stream closes. This could be a cleanup of resources or sending out
    any data that we may have been holding on to. We will usually not implement this
    method unless we are holding on to something such as a file descriptor.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_final`方法允许我们在可写流关闭之前执行任何必要的操作。这可能是清理资源或发送我们可能一直保留的任何数据。除非我们保留了诸如文件描述符之类的东西，我们通常不会实现这个方法。'
- en: The `_destroy` method is the same as the `Readable` stream and should be treated
    similar to the `_final` method, except we can potentially get errors on this method.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_destroy`方法与`Readable`流相同，应该类似于`_final`方法，只是我们可能会在这个方法上出现错误。'
- en: The `_writev` method gives us the capability of handling multiple chunks at
    the same time. We can implement this if we have some sort of ordering system for
    the chunks or we do not care what order the chunks come in. While this may not
    be apparent now, we will implement this method when we implement the duplex stream
    next. The use cases can be somewhat limited, but it still can be beneficial.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_writev`方法使我们能够同时处理多个块。如果我们对块有某种排序系统，或者我们不在乎块的顺序，我们可以实现这一点。虽然现在可能不明显，但我们将在实现双工流时实现这个方法。用例可能有些有限，但仍然可能有益。'
- en: Implementing the Writable stream
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现可写流
- en: 'The following `Writable` stream implementation showcases our framing method
    and how we can use it to put the `!!!START!!!` and `!!!END!!!` frames on our data.
    While simplistic, it does showcase the power of framing and building more complex
    streams around the primitive ones:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`Writable`流实现展示了我们的帧方法以及我们如何使用它在我们的数据上放置`!!!START!!!`和`!!!END!!!`帧。虽然简单，但它展示了帧的强大和如何在原始流周围构建更复杂的流：
- en: 'Import the `Writable` class from the stream module and create the shell for `WriteMessagePassStream`.
    Set this as the default export for this file:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从流模块导入`Writable`类，并为`WriteMessagePassStream`创建外壳。将其设置为此文件的默认导出：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add the private state variables and the constructor. Make sure not to allow
    `objectMode` through since we want to work on the raw data:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加私有状态变量和构造函数。确保不允许`objectMode`通过，因为我们要处理原始数据：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add the `_write` method to our class. It will be explained as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向我们的类添加`_write`方法。将如下解释：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With this code, we can see some similar points to how we handled the readable
    side. Some notable exceptions include the following items:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这段代码，我们可以看到一些与我们处理可读端类似的地方。一些值得注意的例外包括以下项目：
- en: We implement the `_write` method. We are ignoring, again, the encoding parameter
    of this function, but we should check this in case we get an encoding that we
    are not expecting. The chunk is the data that is being written, and the callback
    is what we call when we are finished processing the write for this chunk.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实现`_write`方法。再次忽略这个函数的编码参数，但我们应该检查这一点，以防我们得到一个意料之外的编码。chunk是正在写入的数据，回调是在我们完成对这个块的写入处理时调用的。
- en: Since we are wrapping a socket and we do not want to kill it once we are done
    sending the data, we need to send some type of stop signal to our stream. In our
    case, we are using the simple `0x00` byte. In a more robust implementation, we
    would utilize something else, but this should work for now.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们正在包装一个套接字，并且我们不希望在发送数据完成后关闭它，我们需要向我们的流发送某种停止信号。在我们的情况下，我们使用简单的`0x00`字节。在更健壮的实现中，我们会利用其他东西，但现在这应该可以工作。
- en: No matter what, we either use the framing or we just write to the underlying
    socket.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论如何，我们要么使用帧，要么直接写入底层套接字。
- en: We call the callback once we are finished with our processing. In our case,
    if we have the `writing` flag set, this means we are still in a frame and we want
    to return early, otherwise, we want to put our stream into writing mode and write
    out the `!!!START!!!` and then the chunk. Again, if we never use the callback,
    our stream will be infinitely paused. The callback is what tells the internal
    mechanism to pull more data from the internal buffer for us to consume.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在处理完成后调用回调。在我们的情况下，如果我们设置了`writing`标志，这意味着我们仍然处于一个帧中，我们希望提前返回，否则，我们希望将我们的流置于写入模式，并写出`!!!START!!!`，然后是块。同样，如果我们从不使用回调，我们的流将被无限暂停。回调告诉内部机制从内部缓冲区中拉取更多数据供我们消耗。
- en: 'With this code, we can now look at the test harness and how we are utilizing
    it to create a server and handle incoming `Readable` streams that implement our
    framing context:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这段代码，我们现在可以看一下测试工具和我们如何利用它来创建一个服务器并处理实现我们帧上下文的传入`Readable`流：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We create a server and listen in on port `3333` for localhost. Whenever we
    receive a connection, we wrap it with our `Writable` stream. We then send down
    a bunch of test data and, once that is finished, we write out the `0x00` signal
    to tell our stream this frame is done, and we then call the `end` method to say
    we are finished with this socket. If we added another test run after our first,
    we can see how our framing system works. Let''s go ahead and do just that. Add
    the following code after `wrapped.write(Buffer.from([0x00]))`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个服务器，并在本地端口`3333`上监听。每当我们接收到一个连接时，我们用我们的`Writable`流包装它。然后我们发送一堆测试数据，一旦完成，我们写出`0x00`信号告诉我们的流这个帧已经完成，然后我们调用`end`方法告诉我们已经完成了这个套接字。如果我们在第一次之后添加了另一个测试运行，我们可以看到我们的帧系统是如何工作的。让我们继续做这件事。在`wrapped.write(Buffer.from([0x00]))`之后添加以下代码：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If we ever hit the `highWaterMark` of our stream, the write stream will pause
    until the read stream has started to consume from it.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们达到流的`highWaterMark`，写入流将暂停，直到读取流开始从中消耗。
- en: If we now run the test harness with our `Readable` stream from before, we will
    see that we are processing all of this data and writing out to our file without
    any of the framing passing through. With these two stream implementations, we
    are now able to pipe data across a socket with a custom framing option. We would
    now be able to use this system to implement our data-passing system from the previous
    chapter. However, we will instead implement a `Duplex` stream that will improve
    on this system and allow us to work with multiple writable chunks, which is what
    we will see in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在使用之前的`Readable`流运行测试工具，我们将看到我们正在处理所有这些数据并将其写入文件，而没有任何传输。有了这两种流实现，我们现在可以通过套接字传输数据，而不需要传输任何帧。我们现在可以使用这个系统来实现前一章中的数据传递系统。然而，我们将实现一个`Duplex`流，它将改进这个系统，并允许我们处理多个可写块，这将在下一节中看到。
- en: Implementing a Duplex stream
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现双工流
- en: A duplex stream is just that, one that works both ways. It combines a `Readable`
    and `Writable` stream into a single interface. With this type of stream, we can
    now just pipe from the socket into our custom stream instead of wrapping the stream
    like we have been (even though we will still implement it as a wrapped stream).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 双工流就是这样，可以双向工作。它将`Readable`和`Writable`流合并为一个单一的接口。有了这种类型的流，我们现在可以直接从套接字中导入到我们的自定义流中，而不是像以前那样包装流（尽管我们仍然将其实现为包装流）。
- en: 'There is not much more to talk about with `Duplex` streams other than one fact
    that trips up newcomers to the stream type. There are two separate buffers: one
    for `Readable` and one for `Writable`. We need to make sure to treat them as separate
    instances. This means whatever we use for the `_read` method in terms of variables,
    should not be used for the `_write` and `_writev` method implementations, otherwise,
    we could run into bad bugs.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`Duplex`流没有更多可以谈论的了，除了一个让新手对流类型感到困惑的事实。有两个单独的缓冲区：一个用于`Readable`，一个用于`Writable`。我们需要确保将它们视为单独的实例。这意味着我们在`_read`方法中使用的变量，在`_write`和`_writev`方法的实现中不应该使用，否则我们可能会遇到严重的错误。
- en: 'As stated before, the following code implements a `Duplex` stream along with
    a counting mechanism so, that way, we can utilize the `_writev` method. As mentioned
    in the *Understanding the Writable stream interface* section, the `_writev` method
    allows us to work on multiple chunks of data at a time:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，以下代码实现了一个`Duplex`流，以及一个计数机制，这样我们就可以利用`_writev`方法。正如在*理解可写流接口*部分所述，`_writev`方法允许我们一次处理多个数据块：
- en: 'Import the `Duplex` class from the `stream` module and add the shell for our
    `MessageTranslator` class. Export this class:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`stream`模块导入`Duplex`类，并为我们的`MessageTranslator`类添加外壳。导出这个类：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Add all the internal state variables. Each of them will be explained in the
    following:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加所有内部状态变量。每个变量将在接下来的部分中解释：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Add the constructor for our class. We will handle the data event for our `#socket`
    inside of this constructor instead of creating another method as we have in the
    past:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为我们的类添加构造函数。我们将在这个构造函数中处理我们的`#socket`的数据事件，而不是像以前那样创建另一个方法：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We will automatically assume we have a single message per chunk. This makes
    the processing much easier. When we do get data, we will read in the packet number,
    which should be the first four bytes of data. We then read in the size of the
    message, which is the next `4` bytes of data. Finally, we push the rest of the
    data into our internal buffer. Once we have finished reading the entire message,
    we will put all the internal chunks together and push them out. Finally, we will
    reset our internal buffer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将自动假设每个块中有一条消息。这样处理会更容易。当我们获取数据时，我们将读取数据包编号，这应该是数据的前四个字节。然后我们读取消息的大小，这是接下来的`4`个字节数据。最后，我们将剩余的数据推入我们的内部缓冲区。一旦我们完成读取整个消息，我们将把所有内部块放在一起并推送它们出去。最后，我们将重置我们的内部缓冲区。
- en: 'Add the `_writev` and `_write` methods to our class. Remember that the `_writev`
    method is utilized for multiple chunks, so we will have to loop through them and
    write each one out:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向我们的类添加`_writev`和`_write`方法。记住，`_writev`方法用于多个数据块，所以我们需要循环遍历它们并将每个写出去：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Add helper methods to process the chunks and to actually write them out. We
    will utilize the number `-1` as a `4`-byte message to state we are done with this
    message:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加处理块和实际写出的辅助方法。我们将使用数字`-1`作为`4`字节消息，表示我们已经完成了这条消息。
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Our `#processChunkHelper` method checks to see whether we hit the magical `-1`
    `4`-byte message to say we have finished writing our message. If we do not, we
    keep adding to our internal buffer (the array). Once we have reached the end,
    we will put all of the data together and then move onto the next packet of data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`#processChunkHelper`方法检查我们是否达到了神奇的`-1` `4`字节消息，表示我们已经完成了消息的写入。如果没有，我们将继续向我们的内部缓冲区（数组）添加。一旦我们到达末尾，我们将把所有数据放在一起，然后转移到下一个数据包。
- en: Our `#writeHelper` method will loop through all of those packets and check to
    see whether any of them are finished. If they are, it will get the packet number,
    the size of the buffer, the data itself, and concatenate it all together. Once
    it has done this, it will reset the internal buffer to make sure we are not leaking
    memory. We will write all of this data to the socket and then call our callback
    to state that we are done writing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`#writeHelper`方法将循环遍历所有这些数据包，并检查它们是否有任何一个已经完成。如果有，它将获取数据包编号、缓冲区的大小、数据本身，并将它们全部连接在一起。一旦完成这些操作，它将重置内部缓冲区，以确保我们不会泄漏内存。我们将把所有这些数据写入套接字，然后调用回调函数表示我们已经完成写入。
- en: 'Finish up the `Duplex` stream by implementing our `_read` method as we have
    before. The `_final` method should just call the callback since there is no processing
    left:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实现我们之前的`_read`方法来完成`Duplex`流。`_final`方法应该只是调用回调函数，因为没有剩余的处理：
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`_writev` should really be used when order does not matter and we are just
    processing the data and possibly turning it into something else. This could be
    a hashing algorithm or something similar to that. In almost all cases, the `_write`
    method should be used.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当顺序不重要且我们只是处理数据并可能将其转换为其他形式时，应该真正使用`_writev`。这可能是一个哈希算法或类似的东西。在几乎所有情况下，应该使用`_write`方法。
- en: While this implementation has quite a few flaws (one being that we do not look
    for possible other packets if we reach the `-1` number), it does showcase how
    we can build a `Duplex` stream and also another way of handling messages. It is
    not recommended to come up with our own schemes of moving data across a socket
    (as we will see in the next chapter), but if there is a new specification that
    comes out, we could always write for it utilizing the `Duplex` socket.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个实现有一些缺陷（其中一个是如果我们达到`-1`数字时没有寻找可能的其他数据包），但它展示了我们如何构建一个`Duplex`流，以及处理消息的另一种方式。不建议自己设计在套接字之间传输数据的方案（正如我们将在下一章中看到的），但如果有一个新的规范出来，我们总是可以利用`Duplex`套接字来编写它。
- en: If we test this implementation with our test harness, we should get a file called
    `output.txt` that has the duplex plus the number message written 100,000 times,
    plus a trailing end-of-line character. Again, a `Duplex` stream is just a separate
    `Readable` and `Writable` stream put together and should be used when implementing
    a data transmission protocol.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用我们的测试工具测试这个实现，我们应该得到一个名为`output.txt`的文件，其中包含了双工加上数字消息被写入了10万次，以及一个尾随的换行符。再次强调，`Duplex`流只是一个单独的`Readable`和`Writable`流组合在一起，应该在实现数据传输协议时使用。
- en: The final stream that we will take a look at is the `Transform` stream.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看的最后一个流是`Transform`流。
- en: Implementing a Transform stream
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Transform流
- en: Out of the four streams, this may be the most useful and possibly the most used
    stream out of the group. A `Transform` stream hooks up the readable and writable
    portions of a stream and allows us to manipulate the data that comes across it.
    This may sound similar to a `Duplex`. Well, a `Transform` stream is a special
    type of `Duplex` stream!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这四个流中，这可能是最有用的，也可能是最常用的流之一。`Transform`流连接了流的可读和可写部分，并允许我们操纵流中传输的数据。这听起来可能类似于`Duplex`。嗯，`Transform`流是`Duplex`流的一种特殊类型！
- en: Built-in implementations of `Transform` streams include any of the streams implemented
    in the `zlib` module. The basic idea is that we are not just trying to pass information
    from one end to the other; we are trying to manipulate that data and turn it into
    something else. That is what the `zlib` streams give us. They compress and decompress
    the data. `Transform` streams change the data into another form. This also means
    that we can make a transform stream be a one-way transformation; whatever is output
    from the transform stream cannot be undone. We will create one of these `Transform`
    streams here, specifically creating a hash of a string.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transform`流的内置实现包括`zlib`模块中实现的任何流。基本思想是我们不仅仅是试图将信息从一端传递到另一端；我们试图操纵这些数据并将其转换为其他形式。这就是`zlib`流给我们的。它们压缩和解压数据。`Transform`流将数据转换为另一种形式。这也意味着我们可以使一个转换流成为单向转换；从转换流输出的任何东西都无法被撤销。我们将在这里创建一个这样的`Transform`流，具体地创建一个字符串的哈希。'
- en: First, let's go through the interface for a `Transform` stream.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下`Transform`流的接口。
- en: Understanding the Transform stream interface
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Transform流接口
- en: 'We have access to two methods that we almost always want to implement, no matter
    what. One gives us access to the underlying chunk of data and allows us to perform
    the transformation on it. We implement this one with the `_transform` method.
    It takes three arguments: the chunk of data that we are working on, the encoding,
    and a callback to let the underlying system know that we are ready to process
    more information.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以访问两种方法，几乎无论如何我们都想要实现。其中一个让我们可以访问底层数据块，并允许我们对其进行转换。我们使用`_transform`方法来实现这一点。它接受三个参数：我们正在处理的数据块，编码和一个回调，让底层系统知道我们已经准备好处理更多信息。
- en: One special thing about the callback function that differs from the `_write`
    callback for a `Writable` stream is that we can pass data to it to emit data on
    the readable side of the `Transform` stream, or we can pass nothing to it to signal
    that we want to process more data. This allows us to only send out data events
    when we want to instead of almost always needing to pass them out.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与`Writable`流的`_write`回调不同的是，回调函数的一个特殊之处是我们可以向其传递数据，以在`Transform`流的可读端发出数据，或者我们可以不传递任何数据，以表示我们想要处理更多数据。这使我们只在需要时发送数据事件，而不是几乎总是需要传递它们。
- en: The other method is the `_flush` method. This allows us to finish any processing
    of any data that we may still be holding. Or, it will allow us to output once all
    of the data that has been sent into the stream. This is what we will implement
    with our string hashing function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是`_flush`方法。这允许我们完成可能仍在持有的任何数据的处理。或者，它将允许我们在流中发送的所有数据都输出一次。这就是我们将用字符串哈希函数实现的功能。
- en: Implementing a Transform stream
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Transform流
- en: Our `Transform` stream is going to take in string data and keep running a hashing
    algorithm. Once it has finished, it will output the final hash that was calculated.
    A hashing function is one where we take some form of input and output a unique
    piece of data. This unique piece of data (in our case, a number) should not be
    vulnerable to collisions. Collisions is the concept that two values that differ
    from each other could come to the exact same hash value. In our case, we are converting
    the string to a 32-bit integer in JavaScript so we have a low chance of collision,
    but not an impossible chance of it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Transform`流将接收字符串数据并继续运行哈希算法。一旦完成，它将输出计算出的最终哈希值。哈希函数是一种我们将某种形式的输入转换为唯一数据的函数。这个唯一的数据（在我们的例子中是一个数字）不应该容易发生碰撞。碰撞是两个不同值可能得到相同哈希值的概念。在我们的情况下，我们将字符串转换为JavaScript中的32位整数，因此我们很少发生碰撞，但并非不可能。
- en: 'The example is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是示例：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Each function of the previous stream is explained below:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个流的每个函数都在下面解释：
- en: The only thing that we need to persist until the stream is destroyed is the
    current hash code. This will allow the hash function to keep track of what we
    have already passed into it and work off of the data after each write.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要持久化的唯一一件事是直到流被销毁的当前哈希码。这将允许哈希函数跟踪我们已经传递给它的内容，并在每次写入后处理数据。
- en: We do a check here to see whether the chunk we received is a `Buffer`. Since
    we made sure to turn the option of `decodeStrings` on, this means that we should
    always get buffers, but it still helps to check.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里进行检查，看我们收到的块是否是一个`Buffer`。由于我们确保打开了`decodeStrings`选项，这意味着我们应该总是得到缓冲区，但检查仍然有帮助。
- en: While the contents of the hash function can be seen at the URL provided, the
    only major thing that we need to worry about is that we call our callback, just
    like we had to do when we were implementing our `Writable` stream.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然哈希函数的内容可以在提供的URL中看到，但我们需要担心的唯一重要事情是，我们要调用我们的回调，就像我们在实现`Writable`流时所做的那样。
- en: Once we are ready to produce data, we utilize the `push` method, just like we
    did with the `Readable` stream. Remember, `Transform` streams are just special
    `Duplex` streams that allow us to manipulate the data that is being input and
    change it into something for the output. We can also change the last two lines
    of code to just do `callback(null, buf)`; this is just the shorthand of what we've
    seen previously.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们准备生成数据，我们就使用`push`方法，就像我们在`Readable`流中所做的那样。记住，`Transform`流只是允许我们操纵输入数据并将其转换为输出的特殊`Duplex`流。我们还可以将代码的最后两行更改为`callback(null,
    buf)`；这只是我们之前看到的简写。
- en: Now, if we run some test cases on the previous code, we will see that we do
    get a unique hash code for each unique string that we enter, but we get the same
    hash code when we input the exact same thing. This means that our hashing function
    is good and we can hook it up to a streaming application.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们对前面的代码运行一些测试用例，我们会发现每个唯一字符串输入都会得到一个唯一的哈希码，但当我们输入完全相同的内容时，我们会得到相同的哈希码。这意味着我们的哈希函数很好，我们可以将其连接到流应用程序中。
- en: Using generators with streams
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流生成器
- en: Everything that we have seen up to this point showcases how we can utilize all
    of the built-in systems in Node.js to create streaming applications. However,
    for those that have been following sequentially in the book, we have discussed
    generators. Those that have been keen to think about them would notice a strong
    correlation between streams and generators. This is actually the case! We can
    utilize generators to hook into the Streaming API.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所看到的一切都展示了我们如何利用Node.js中的所有内置系统来创建流应用程序。然而，对于那些一直在按顺序阅读本书的人来说，我们已经讨论了生成器。那些一直在思考它们的人会注意到流和生成器之间有很强的相关性。事实上就是这样！我们可以利用生成器来连接到流API。
- en: With this concept, we could build generators that can both work in the browser
    and inside of Node.js without that much overhead. We have even seen in a [Chapter
    6](7dc2a5fd-0c78-49e7-9e84-f789eab14ca5.xhtml), *Message Passing – Learning about
    the Different Types*,how we can get at the underlying stream for the Fetch API.
    Now, we can write a generator that can work with both of these subsystems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个概念，我们可以构建既可以在浏览器中工作又可以在Node.js中工作的生成器，而不需要太多的开销。我们甚至在[第6章](7dc2a5fd-0c78-49e7-9e84-f789eab14ca5.xhtml)中看到了如何使用Fetch
    API获取底层流。现在，我们可以编写一个可以与这两个子系统一起工作的生成器。
- en: 'For now, let''s just look at an example of an `async` generator and how we
    can hook them into the Node.js streaming system. The example will be to see how
    we can have a generator as the input for a `Readable` stream:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们只看一个`async`生成器的示例，以及我们如何将它们连接到Node.js流系统中。示例将是看看我们如何将生成器作为`Readable`流的输入：
- en: 'We are going to set up a `Readable` stream to read out the 26 lowercase characters
    of the English alphabet. We can do this fairly easily by writing the following
    generator:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将建立一个`Readable`流来读取英语字母表的26个小写字符。我们可以通过编写以下生成器来轻松实现这一点：
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'While the character code is below `123`, we keep sending data. We can then
    wrap this in a `Readable` stream, like so:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当字符代码低于`123`时，我们继续发送数据。然后我们可以将其包装在`Readable`流中，如下所示：
- en: '[PRE29]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If we now run this code, we will see the characters *a* through *z* appear in
    our console. The `Readable` stream knows that it is the end because a generator
    produces an object with two keys. The `value` field gives us the value from the
    `yield` expression and the `done` tells us if the generator has finished running.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在运行这段代码，我们会看到控制台中出现字符*a*到*z*。`Readable`流知道它已经结束，因为生成器生成了一个具有两个键的对象。`value`字段给出了`yield`表达式的值，`done`告诉我们生成器是否已经完成运行。
- en: 'This lets the readable interface know when to send out `data` events (through
    us yielding a value) and when to close the stream (through the `done` key being
    set to `true`). We could also pipe the output of our readable system into that
    of the writable to chain the process. This can easily be seen with the following
    code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这让可读接口知道何时发送`data`事件（通过我们产生一个值）以及何时关闭流（通过将`done`键设置为`true`）。我们还可以将可读系统的输出管道到可写系统的输出，以链接整个过程。这可以很容易地通过以下代码看到：
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Implementing streams through generators and `async`/`await` may seem like a
    good idea, but we should only utilize this if we are trying to put an already
    `async`/`await` piece of code with a stream. Always try to go for readability;
    utilizing the generator or `async`/`await` method will most likely lead to something
    unreadable.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成器和`async`/`await`实现流可能看起来是一个好主意，但只有在我们试图将一个已经是`async`/`await`的代码片段与流结合时，我们才应该利用它。始终要追求可读性；利用生成器或`async`/`await`方法很可能会导致代码难以阅读。
- en: With the previous example, we have combined the readable from a generator and
    we have utilized the piping mechanism to send it to a file. With `async`/`await`
    and generators becoming constructs in the JavaScript language, it won't be long
    before we have streaming as a first-class concept.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的例子，我们已经将生成器的可读性与利用管道机制发送到文件相结合。随着`async`/`await`和生成器成为JavaScript语言中的构造，流很快就会成为一个一流的概念。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Streaming is one of the pillars of writing highly performant Node.js code. It
    allows us to not block the main thread, while still being able to process data.
    The Streaming API allows us to write different types of streams for our purposes.
    While most of these streams will be in the form of transform streams, it is nice
    to see how we could implement the other three.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 流是编写高性能Node.js代码的支柱之一。它允许我们不阻塞主线程，同时仍然能够处理数据。流API允许我们为我们的目的编写不同类型的流。虽然这些流大多数将是转换流的形式，但看到我们如何实现其他三种流也是很好的。
- en: The final topic we will look at in the next chapter is data formats. Handling
    different data formats besides JSON will allow us to interface with many big data
    providers and be able to handle the data formats that they like to use. We will
    see how they utilize streaming to implement all of their format specifications.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中看到的最后一个主题是数据格式。处理除了JSON之外的不同数据格式将使我们能够与许多大数据提供商进行接口，并能够处理他们喜欢使用的数据格式。我们将看到他们如何利用流来实现所有的格式规范。
