- en: Elastic Kubernetes Service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性Kubernetes服务
- en: '**Kubernetes** is a popular open source container management platform originally
    developed by Google, who based Kubernetes on Google''s own internal **Borg** ([https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/))
    container platform. Kubernetes draws on Google''s extensive experience of running
    containers at scale, and is now supported by all of the major cloud platform providers
    with the release of the AWS Elastic Kubernetes Service (EKS). EKS provides a managed
    Kubernetes cluster to which you can deploy your container applications, without
    having to worry about day-to-day operational overheads and the complexities of
    cluster management. AWS has performed all the heavy lifting of establishing a
    robust and scalable platform, making it easier than ever to get up and running
    with Kubernetes.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一种流行的开源容器管理平台，最初由谷歌开发，基于谷歌自己内部的Borg容器平台。Kubernetes借鉴了谷歌在大规模运行容器方面的丰富经验，现在得到了所有主要云平台提供商的支持，包括AWS
    Elastic Kubernetes Service（EKS）的发布。EKS提供了一个托管的Kubernetes集群，您可以在其中部署容器应用程序，而无需担心日常运营开销和集群管理的复杂性。AWS已经完成了建立一个强大和可扩展平台的大部分工作，使得使用Kubernetes变得比以往更容易。
- en: In this chapter, you will be introduced to the world of Kubernetes, we will
    work through how we can configure Kubernetes to ensure we are able to successfully
    deploy and operate the sample application we have used through this book, and
    then establish an EKS cluster in AWS where you will deploy the application using
    the configuration you have developed locally. This will provide practical, real-world
    insights into how, as an application owner, you can deploy your container workloads
    to Kubernetes, and how you can quickly get up and running with EKS.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将被介绍到Kubernetes的世界，我们将通过如何配置Kubernetes来确保我们能够成功部署和操作本书中使用的示例应用程序，并在AWS中建立一个EKS集群，您将使用本地开发的配置部署应用程序。这将为您提供实际的、现实世界的见解，作为应用程序所有者，您可以将您的容器工作负载部署到Kubernetes，并且您可以快速地开始使用EKS。
- en: We will first learn how you can work with the platform locally, using the native
    support that Docker for Mac and Docker for Windows now include for Kubernetes.
    You can spin up a local single-node cluster straight out of the box, reducing
    much of the manual configuration you would typically require to get a local environment
    up and running. You will learn how to create the various types of resources required
    to run the sample application in Kubernetes, addressing key operational challenges
    such as providing persistent storage for your application database, secrets management,
    and running one-shot tasks such as database migrations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先学习如何在本地使用Docker for Mac和Docker for Windows对Kubernetes进行本地支持。您可以直接启动一个本地单节点集群，减少了通常需要进行的大量手动配置，以便快速启动本地环境。您将学习如何创建运行Kubernetes中示例应用程序所需的各种资源，解决关键的运营挑战，如为应用程序数据库提供持久存储、管理密钥和运行一次性任务，如数据库迁移。
- en: Once you have established a working configuration to get the sample application
    up and running locally in Kubernetes, we will turn our attention to getting started
    with EKS, creating an EKS cluster, and establishing an EC2 auto scaling group
    where worker nodes that run your container workloads are managed. You will learn
    how to set up access to your cluster from your local environment and proceed to
    deploy the Kubernetes Dashboard, which provides a rich management user interface
    from which you can deploy and manage your applications. Finally, you will set
    up integrations with other AWS services including Elastic Block Store (EBS) and
    Elastic Load Balancing (ELB), and proceed to deploy the sample application to
    your EKS cluster.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您建立了一个工作配置，可以在Kubernetes中本地运行示例应用程序，我们将把注意力转向开始使用EKS，创建EKS集群，并建立一个EC2自动扩展组，管理运行容器工作负载的工作节点。您将学习如何从本地环境设置对集群的访问，并继续部署Kubernetes仪表板，该仪表板提供了丰富的管理用户界面，您可以从中部署和管理应用程序。最后，您将设置与其他AWS服务的集成，包括弹性块存储（EBS）和弹性负载均衡（ELB），并将示例应用程序部署到您的EKS集群。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes简介
- en: Kubernetes architecture
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: Getting started with Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用Kubernetes
- en: Installing Kubernetes using Docker Desktop
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker Desktop安装Kubernetes
- en: Creating core Kubernetes resources including pods, deployments, and services
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建核心Kubernetes资源，包括pod、部署和服务
- en: Creating Persistent volumes
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建持久卷
- en: Creating Kubernetes secrets
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建Kubernetes secrets
- en: Running Kubernetes jobs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Kubernetes作业
- en: Creating an EKS cluster
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建EKS集群
- en: Establishing access to an EKS cluster
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立对EKS集群的访问
- en: Deploying applications to EKS
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将应用程序部署到EKS
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the technical requirements for this chapter:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章的技术要求：
- en: Administrator access to an AWS account
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS账户的管理员访问权限
- en: A local AWS profile, configured as per the instructions in Chapter 3
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地AWS配置文件，按照第3章的说明进行配置
- en: AWS CLI version 1.15.71 or higher
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CLI版本1.15.71或更高版本
- en: Docker 18.06 or higher
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 18.06或更高版本
- en: Docker Compose 1.22 or higher
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Compose 1.22或更高版本
- en: GNU Make 3.82 or higher
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GNU Make 3.82或更高版本
- en: This chapter assumes you have completed all of the preceding chapters in this
    book
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章假设您已经完成了本书中的所有前几章。
- en: The following GitHub URL contains the code samples used in this chapter: [https://github.com/docker-in-aws/docker-in-aws/tree/master/ch17](https://github.com/docker-in-aws/docker-in-aws/tree/master/ch17).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下GitHub网址包含本章中使用的代码示例：[https://github.com/docker-in-aws/docker-in-aws/tree/master/ch17](https://github.com/docker-in-aws/docker-in-aws/tree/master/ch17)。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 观看以下视频，了解代码的实际操作：
- en: '[http://bit.ly/2LyGtSY](http://bit.ly/2LyGtSY)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2LyGtSY](http://bit.ly/2LyGtSY)'
- en: Introduction to Kubernetes
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes简介
- en: '**Kubernetes **is an open source container management platform that was open
    sourced by Google in 2014, and achieved production readiness in 2015 with its
    1.0 release. In the space of three years, it has established itself as the most
    popular container management platform, and is very popular for larger organizations
    that are looking to run their applications as container workloads. Kubernetes
    is one of the most popular open source projects ([https://github.com/cncf/velocity/blob/master/docs/top30_chart_creation.md](https://github.com/cncf/velocity/blob/master/docs/top30_chart_creation.md))
    on GitHub, and according to [Redmonk](https://redmonk.com/fryan/2017/09/10/cloud-native-technologies-in-the-fortune-100/),
    Kubernetes is used at 54% of Fortune 100 companies as of late 2017.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes**是一个开源的容器管理平台，由Google在2014年开源，并在2015年通过1.0版本实现了生产就绪。在短短三年的时间里，它已经成为最受欢迎的容器管理平台，并且非常受大型组织的欢迎，这些组织希望将他们的应用程序作为容器工作负载来运行。Kubernetes是GitHub上最受欢迎的开源项目之一（[https://github.com/cncf/velocity/blob/master/docs/top30_chart_creation.md](https://github.com/cncf/velocity/blob/master/docs/top30_chart_creation.md)），根据[Redmonk](https://redmonk.com/fryan/2017/09/10/cloud-native-technologies-in-the-fortune-100/)的说法，截至2017年底，Kubernetes在财富100强公司中被使用率达到了54%。'
- en: 'Key features of Kubernetes include the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的关键特性包括以下内容：
- en: '**Platform agnostic**: Kubernetes can run anywhere, from your local machine
    to your data centre and in cloud providers such as AWS, Azure, and Google Cloud,
    whom all now offer integrated managed Kubernetes offerings.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台无关**：Kubernetes可以在任何地方运行，从您的本地机器到数据中心，以及在AWS、Azure和Google Cloud等云提供商中，它们现在都提供集成的托管Kubernetes服务。'
- en: '**Open source**: Kubernetes'' greatest strength is its community and open source
    nature, which has seen Kubernetes become one of the leading open source projects
    on the planet. Major organizations and vendors are investing significant time
    and resources contributing to the platform, ensuring that the entire community
    benefits from these ongoing enhancements.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：Kubernetes最大的优势在于其社区和开源性质，这使得Kubernetes成为了全球领先的开源项目之一。主要组织和供应商正在投入大量时间和资源来为平台做出贡献，确保整个社区都能从这些持续的增强中受益。'
- en: '**Pedigree**: Kubernetes'' roots are from Google''s internal Borg platform,
    which has been running containers at scale since the early 2000s. Google is one
    of the pioneers of container technology and is without a doubt one of, if not
    the largest, adopters of containers – back in 2014, Google indicated that they
    were running 2 billion containers every week, at a time when most enterprises
    had only just heard about containers through a new project called Docker that
    was taking the tech industry by storm. This pedigree and heritage ensures many
    of the lessons Google has learned over its many years of running containers at
    scale are encapsulated in the Kubernetes platform.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**血统**：Kubernetes的根源来自Google内部的Borg平台，自从2000年代初以来一直在大规模运行容器。Google是容器技术的先驱之一，毫无疑问是容器的最大采用者之一，如果不是最大的采用者。在2014年，Google表示他们每周运行20亿个容器，而当时大多数企业刚刚通过一个名为Docker的新项目听说了容器技术。这种血统和传统确保了Google在多年大规模运行容器中所学到的许多经验教训都被包含在Kubernetes平台中。'
- en: '**Production grade container management features**: Kubernetes offers all of
    the container management features that you would expect to see and will come across
    with other competing platforms. This includes cluster management, multi-host networking,
    pluggable storage, health checks, service discovery and load balancing, service
    scaling and rolling updates, desired stage configuration, role-based access control,
    and secret management to name a few. All of these features are implemented in
    a modular building-block fashion that allows you to tune the system to meet the
    specific requirements of your organization, and is one of the reasons Kubernetes
    is now considered the gold standard for enterprise-grade container management.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产级容器管理功能**：Kubernetes提供了您在其他竞争平台上期望看到并会遇到的所有容器管理功能。这包括集群管理、多主机网络、可插拔存储、健康检查、服务发现和负载均衡、服务扩展和滚动更新、期望阶段配置、基于角色的访问控制以及秘密管理等。所有这些功能都以模块化的构建块方式实现，使您可以调整系统以满足组织的特定要求，这也是Kubernetes现在被认为是企业级容器管理的黄金标准的原因之一。'
- en: Kubernetes versus Docker Swarm
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes与Docker Swarm
- en: In the previous chapter, I provided my own thoughts on Docker Swarm versus Kubernetes,
    and here I will continue, this time with more of a focus on why you would choose
    Kubernetes over Docker Swarm. As you work through this chapter, it should become
    apparent that Kubernetes has a more elaborate architecture that means there is
    a higher learning curve, and what I cover in this chapter only really scratches
    the surface of what is possible with Kubernetes. That said, once you get your
    head around these concepts, at least from my perspective, you should see that
    ultimately Kubernetes is more powerful with greater flexibility, and arguably
    it's probably fair to state that Kubernetes certainly feels more "enterprise-grade"
    than Docker Swarm, with many more knobs you can tune to tailor Kubernetes to your
    specific needs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我提出了关于Docker Swarm与Kubernetes的个人看法，这一次我将继续，这次更加关注为什么选择Kubernetes而不是Docker
    Swarm。当您阅读本章时，应该会发现Kubernetes具有更为复杂的架构，这意味着学习曲线更高，而我在本章中涵盖的内容只是Kubernetes可能实现的一小部分。尽管如此，一旦您理解了这些概念，至少从我的角度来看，您应该会发现最终Kubernetes更加强大、更加灵活，可以说Kubernetes肯定比Docker
    Swarm更具“企业级”感觉，您可以调整更多的参数来定制Kubernetes以满足您的特定需求。
- en: Probably the biggest advantage Kubernetes has over Docker Swarm and other competitors
    is its community, which is significant and means that information about almost
    any configuration scenario you can think of, can be readily found across the wider
    Kubernetes community and ecosystem. There has been a lot of momentum behind the
    Kubernetes movement, and this only seems to be growing as leading vendors and
    providers such as AWS embrace Kubernetes with their own offerings and solutions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes相对于Docker Swarm和其他竞争对手最大的优势可能是其庞大的社区，这意味着几乎可以在更广泛的Kubernetes社区和生态系统中找到关于几乎任何配置方案的信息。Kubernetes运动背后有很多动力，随着AWS等领先供应商和提供商采用Kubernetes推出自己的产品和解决方案，这一趋势似乎正在不断增长。
- en: Kubernetes architecture
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: 'Architecturally, Kubernetes organizes itself in the form of a cluster, where
    master nodes form the cluster control plane, and worker nodes run your actual
    container workloads:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在架构上，Kubernetes以集群的形式组织自己，其中主节点形成集群控制平面，工作节点运行实际的容器工作负载：
- en: '![](assets/64ef6b49-8e21-40b1-a9e2-3473309890f6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/64ef6b49-8e21-40b1-a9e2-3473309890f6.png)'
- en: Kubernetes architecture
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: 'Within each master node, a number of components exist:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个主节点中，存在许多组件：
- en: '**kube-apiserver**: This exposes the Kubernetes API and is the frontend component
    that you use to interact with the Kubernetes control plane.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-apiserver**：这个组件公开Kubernetes API，是您用来与Kubernetes控制平面交互的前端组件。'
- en: '**etcd**: This provides a distributed and highly available key/value store
    across the cluster that is used to store Kubernetes configuration and operational
    data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd**：这提供了一个跨集群的分布式和高可用的键/值存储，用于存储Kubernetes配置和操作数据。'
- en: '**kube-scheduler**: This schedules pods onto worker nodes, taking into consideration
    resource requirements, constraints, data locality, and other factors. You will
    learn more about pods later on, but for now you can think of them as a collection
    of associated containers and volumes that collectively need to be create created,
    updated, and deployed together.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-scheduler**：这将pod调度到工作节点上，考虑资源需求、约束、数据位置和其他因素。稍后您将了解更多关于pod的信息，但现在您可以将它们视为一组相关的容器和卷，需要一起创建、更新和部署。'
- en: '**kube-controller-manager**: This is responsible for managing controllers,
    which consists of a number of components that detects when your nodes go down,
    ensures that the correct number of instances or replicas of your pods are running,
    publishes service endpoints for the applications running in your pods, and manages
    service accounts and API access tokens for the cluster.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-controller-manager**：这负责管理控制器，包括一些组件，用于检测节点何时宕机，确保pod的正确数量的实例或副本正在运行，为在pod中运行的应用程序发布服务端点，并管理集群的服务帐户和API访问令牌。'
- en: '**cloud-controller-manager**: This provides controllers that interact with
    an underlying cloud provider, enabling cloud providers to support features specific
    to their platform. Examples of cloud controllers include service controllers,
    which create, update, and delete cloud provider load balancers, and volume controllers,
    which create, attach, detach, and delete the various storage volume technologies
    supported by the cloud provider.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cloud-controller-manager**：这提供与底层云提供商交互的控制器，使云提供商能够支持特定于其平台的功能。云控制器的示例包括服务控制器，用于创建、更新和删除云提供商负载均衡器，以及卷控制器，用于创建、附加、分离和删除云提供商支持的各种存储卷技术。'
- en: '**Add-ons**: A number of add-ons are available that extend the functionality
    of your cluster. These run in the form of pods and services that provide cluster
    features. One add-on that is typically deployed on most installations is the cluster
    DNS add-on, which provides automatic DNS naming and resolution for services and
    pods running on the cluster.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件**：有许多可用的插件可以扩展集群的功能。这些以pod和服务的形式运行，提供集群功能。在大多数安装中通常部署的一个插件是集群DNS插件，它为在集群上运行的服务和pod提供自动DNS命名和解析。'
- en: 'On all nodes, the following components exist:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有节点上，存在以下组件：
- en: '**kubelet**: An agent that runs on each node in the cluster and ensures all
    containers in a pod are healthy and running. The kubelet can also collect container
    metrics that can be published to monitoring systems.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kubelet**：这是在集群中每个节点上运行的代理，确保pod中的所有容器健康运行。kubelet还可以收集容器指标，可以发布到监控系统。'
- en: '**kube-proxy**: This manages network communications, port mappings, and routing
    rules required on each node to support the various service abstractions that Kubernetes
    supports.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-proxy**：这管理每个节点上所需的网络通信、端口映射和路由规则，以支持Kubernetes支持的各种服务抽象。'
- en: '**Container runtime**: This provides the container engine that runs containers.
    The most popular container runtime that''s supported is Docker, however under
    container runtimes such as rkt (Rocket) or any OCI runtime-spec implementation
    can be supported.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：提供运行容器的容器引擎。最受欢迎的容器运行时是Docker，但是也支持rkt（Rocket）或任何OCI运行时规范实现。'
- en: '**Pods**: A pod is the core unit of work for deploying your container applications.
    Each pod consists of one more containers and associated resources, and a single
    network interface, meaning each container in a given pod shares the same network
    stack.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pods**：Pod是部署容器应用程序的核心工作单元。每个Pod由一个或多个容器和相关资源组成，并且一个单一的网络接口，这意味着给定Pod中的每个容器共享相同的网络堆栈。'
- en: Note that worker nodes only run the components previously listed directly, while
    master nodes run all of the components we have discussed to date, allowing master
    nodes to also run container workloads for scenarios such as a single-node cluster.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，工作节点只直接运行先前列出的组件，而主节点运行到目前为止我们讨论的所有组件，允许主节点也运行容器工作负载，例如单节点集群的情况。
- en: Kubernetes also provides a client component called **kubectl**, which provides
    the ability to manage your clusters via the Kubernetes API. **kubectl** is supported
    on Windows, macOS, and Linux, and allows you to easily manage and switch between
    multiple clusters, running both locally and remotely.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还提供了一个名为**kubectl**的客户端组件，它提供了通过Kubernetes API管理集群的能力。**kubectl**支持Windows、macOS和Linux，并允许您轻松管理和在本地和远程之间切换多个集群。
- en: Getting started with Kubernetes
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Kubernetes
- en: Now that you have been briefly introduced to Kubernetes, let's focus on getting
    up and running with Kubernetes in your local environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经简要介绍了Kubernetes，让我们专注于在本地环境中启动和运行Kubernetes。
- en: Earlier in this book when you set up your local development environment, if
    you are using macOS or Windows, you installed the community edition (CE) versions
    of Docker Desktop (Docker for Mac or Docker for Windows, which I may refer to
    collectively as Docker Desktop in this chapter), which includes native support
    for Kubernetes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中的早期，当您设置本地开发环境时，如果您使用的是macOS或Windows，您安装了Docker Desktop的社区版（CE）版本（Docker
    for Mac或Docker for Windows，在本章中我可能统称为Docker Desktop），其中包括对Kubernetes的本地支持。
- en: If you are using a variant of Docker for Mac/Windows that does not support Kubernetes,
    or are using Linux, you can install minikube by following the instructions at [https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube).
    Most of the examples included in this section should work with minikube, although
    features such as load balancing and dynamic host path provisioning may not be
    directly supported and require some additional configuration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是不支持Kubernetes的Docker for Mac/Windows的变体，或者使用Linux，您可以按照以下说明安装minikube：[https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube)。本节中包含的大多数示例应该可以在minikube上运行，尽管诸如负载平衡和动态主机路径配置等功能可能不会直接支持，需要一些额外的配置。
- en: 'To enable Kubernetes, select **Kubernetes** in your local Docker Desktop settings,
    and check the **Enable Kubernetes** option. Once you click **Apply**, Kubernetes
    will be installed and will take a few minutes to get up and running:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用Kubernetes，请在本地Docker Desktop设置中选择**Kubernetes**，并勾选**启用Kubernetes**选项。一旦您点击**应用**，Kubernetes将被安装，并需要几分钟来启动和运行：
- en: '![](assets/9c033101-8ec4-4dba-bfaf-dd99d43ed4e2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9c033101-8ec4-4dba-bfaf-dd99d43ed4e2.png)'
- en: Enabling Kubernetes using Docker for Mac
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker for Mac启用Kubernetes
- en: 'Docker Desktop also installs and configures the Kubernetes command-line utility
    `kubectl` automatically for you, which can be used to verify your installation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Desktop还会自动为您安装和配置Kubernetes命令行实用程序`kubectl`，该实用程序可用于验证您的安装：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you are using Docker for Windows in conjunction with the Linux subsystem
    for Windows, you will need to install `kubectl` into the subsystem by running
    the following commands (see [https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-native-package-management](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-native-package-management) for
    more details):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Windows的Docker与Linux子系统配合使用，您需要通过运行以下命令将`kubectl`安装到子系统中（有关更多详细信息，请参见[https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-native-package-management](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-native-package-management)）：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After installing `kubectl` , if you previously changed your Linux subsystem
    home folder to your Windows home folder, you should be able now interact with
    your local Kubernetes cluster without further configuration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`kubectl`后，如果您之前将Linux子系统的主文件夹更改为Windows主文件夹，则现在应该能够与本地Kubernetes集群进行交互，无需进一步配置。
- en: 'If your home folder is different from the Windows home folder (this is the
    case by default), then you will need to set up a symbolic link that points to
    the `kubectl` config file in your Windows home folder, after which you should
    be able to use `kubectl` to interact with your local Kubernetes installation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的主文件夹与Windows主文件夹不同（默认情况下是这种情况），那么您将需要设置一个符号链接，指向Windows主文件夹中的`kubectl`配置文件，之后您应该能够使用`kubectl`与本地Kubernetes安装进行交互：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Linux subsytem for Windows also allows you to run Windows command-line programs,
    so alternatively you can run `kubectl.exe` to invoke the Windows kubectl component.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Windows的Linux子系统还允许您运行Windows命令行程序，因此您也可以运行`kubectl.exe`来调用Windows kubectl组件。
- en: Creating a pod
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个pod
- en: In Kubernetes, you deploy your applications as *pods*, which refer to one or
    more containers and other resources that are closely related to each other and
    collectively represent your application. A **pod** is the core unit of work in
    Kubernetes and is conceptually similar to an ECS task definition, although under
    the hood they work in a completely different way.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，您将应用程序部署为*pods*，这些pods指的是一个或多个容器和其他与之密切相关的资源，共同代表您的应用程序。**pod**是Kubernetes中的核心工作单元，概念上类似于ECS任务定义，尽管在底层它们以完全不同的方式工作。
- en: A common shorthand code for Kubernetes is k8s, where the "ubernete" portion
    of the name Kubernetes is replaced with the digit 8, representing the number of
    characters in "ubernete".
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的常用简写代码是k8s，其中名称Kubernetes中的“ubernete”部分被数字8替换，表示“ubernete”中的字符数。
- en: 'Before we create our first pod, let''s establish a folder called `k8s` in the
    todobackend repository that will hold all Kubernetes configuration for the todobackend
    application, and then create a folder called `app`, which will store all resource
    definitions that relate to the core todobackend applications:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建我们的第一个pod之前，让我们在todobackend存储库中建立一个名为`k8s`的文件夹，该文件夹将保存todobackend应用程序的所有Kubernetes配置，然后创建一个名为`app`的文件夹，该文件夹将存储与核心todobackend应用程序相关的所有资源定义：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following code demonstrates a basic pod definition for the todobackend
    application, which we will save to the `k8s/app/deployment.yaml` file:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了todobackend应用程序的基本pod定义，我们将其保存到`k8s/app/deployment.yaml`文件中：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The format of the pod configuration file is easy to follow, and in general most
    of the parameters that you see map to parameters of the same name if you are used
    to defining your containers using Docker Compose. One important difference that
    does tend to cause confusion is the `command` parameter – in Kubernetes, this
    parameter is the equivalent of the `ENTRYPOINT` Dockerfile directive and `entrypoint`
    parameter in a Docker Compose service specification, while the `args` parameter
    in Kubernetes is equivalent to the CMD directive (Dockerfile) and `command` service
    parameter (Docker Compose). This means that in the preceding configuration, the
    default entrypoint script in our container is bypassed, and instead the uwsgi
    web server will be run directly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: pod配置文件的格式很容易遵循，通常情况下，您看到的大多数参数都与使用Docker Compose定义容器时的同名参数相对应。一个经常引起混淆的重要区别是`command`参数-在Kubernetes中，此参数相当于`ENTRYPOINT`
    Dockerfile指令和Docker Compose服务规范中的`entrypoint`参数，而在Kubernetes中，`args`参数相当于CMD指令（Dockerfile）和Docker
    Compose中的`command`服务参数。这意味着在前面的配置中，我们的容器中的默认入口脚本被绕过，而是直接运行uwsgi web服务器。
- en: The `imagePullPolicy` property value of `IfNotPresent` configures Kubernetes
    to only pull an image if one is not available in the local Docker Engine registry,
    which means you must ensure the existing todobackend Docker Compose workflow has
    been run to build and tag the todobackend image locally, before attempting to
    create the pod. This is required as Kubernetes only includes native support for
    ECR when you are running Kubernetes on AWS EC2 instances, and does not natively
    support ECR when you are running Kubernetes outside of AWS.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`imagePullPolicy`属性值为`IfNotPresent`配置了Kubernetes只有在本地Docker Engine注册表中没有可用的镜像时才拉取镜像，这意味着在尝试创建pod之前，您必须确保已运行现有的todobackend
    Docker Compose工作流以在本地构建和标记todobackend镜像。这是必需的，因为当您在AWS EC2实例上运行Kubernetes时，Kubernetes只包括对ECR的本机支持，并且在您在AWS之外运行Kubernetes时，不会本地支持ECR。'
- en: There are a number of third-party plugins available that allow you to manage
    AWS credentials and pull ECR images. A popular example can be found at [https://github.com/upmc-enterprises/registry-creds](https://github.com/upmc-enterprises/registry-creds)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多第三方插件可用，允许您管理AWS凭据并拉取ECR镜像。一个常见的例子可以在[https://github.com/upmc-enterprises/registry-creds](https://github.com/upmc-enterprises/registry-creds)找到。
- en: 'To create our pod and verify that it is running, you can run the `kubectl apply`
    command, with the `-f` flag referencing the deployment file you just created,
    followed by the `kubectl get pods` command:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的pod并验证它是否正在运行，您可以运行`kubectl apply`命令，使用`-f`标志引用您刚刚创建的部署文件，然后运行`kubectl
    get pods`命令：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can see that the status of the pod is `Running` and that a container has
    been deployed to the single-node Kubernetes cluster running in your local Docker
    Desktop environment. One important point to note is that the todobackend container
    that has been deployed has no means of communicating with the outside world, as
    there are no networks ports that have been published from the pod and its associated
    container.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到pod的状态为`Running`，并且已经部署了一个容器到在您的本地Docker Desktop环境中运行的单节点Kubernetes集群。一个重要的要注意的是，已部署的todobackend容器无法与外部世界通信，因为从pod及其关联的容器中没有发布任何网络端口。
- en: 'An interesting aspect of Kubernetes is that you can use the Kubernetes API
    to interact with your pods. To demonstrate this, you first run the `kubectl proxy` command,
    which sets up a local HTTP proxy that exposes the API via a plain old HTTP interface:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的一个有趣之处是您可以使用Kubernetes API与您的pod进行交互。为了演示这一点，首先运行`kubectl proxy`命令，它会设置一个本地HTTP代理，通过普通的HTTP接口公开API：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can now access the container port 8000 on your pod via the URL `http://localhost:8001/api/v1/namespaces/default/pods/todobackend:8000/proxy/`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以通过 URL `http://localhost:8001/api/v1/namespaces/default/pods/todobackend:8000/proxy/`
    访问 pod 上的容器端口 8000：
- en: '![](assets/30a20671-faac-4e12-af03-adfd67e9629a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/30a20671-faac-4e12-af03-adfd67e9629a.png)'
- en: Running the kubectl proxy
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 kubectl 代理
- en: As you can see, the todobackend application is running, although it is missing
    static content as we haven't generated this yet. Notice also that the todos link
    at the bottom of the page (`http://localhost:8001/todos`) is invalid, as the todobackend
    application has no knowledge of the API path that is called to access the application
    via the proxy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，todobackend 应用正在运行，尽管它缺少静态内容，因为我们还没有生成它。还要注意页面底部的 todos 链接（`http://localhost:8001/todos`）是无效的，因为
    todobackend 应用程序不知道通过代理访问应用程序的 API 路径。
- en: 'Another interesting feature of Kubernetes is the ability to expose a port from
    your Kubernetes client to the application by running the `kubectl port-forward`
    command, which publishes a local port on the client and connects it to a specified
    pod using the Kubernetes API:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的另一个有趣特性是通过运行 `kubectl port-forward` 命令，将 Kubernetes 客户端的端口暴露给应用程序，从而连接到指定的
    pod，这样可以实现从 Kubernetes 客户端到应用程序的端口转发：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you now attempt to access `http://localhost:8000`, you should see the todobackend
    home page, and the todos link at the bottom of the page should now be accessible:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在尝试访问 `http://localhost:8000`，您应该能看到 todobackend 的主页，并且页面底部的 todos 链接现在应该是可访问的：
- en: '![](assets/6ccde81a-5c0f-4f36-bdb8-79dfb0de4d8f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6ccde81a-5c0f-4f36-bdb8-79dfb0de4d8f.png)'
- en: Accessing a port forwarded pod
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 访问一个端口转发的 pod
- en: You can see that, once again, our application is not in a fully functional state,
    given we haven't configured any database settings as yet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，再次，我们的应用程序并不处于完全功能状态，因为我们还没有配置任何数据库设置。
- en: Creating a deployment
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个部署
- en: Although we have been able to publish our todobackend application, the mechanism
    that we have used to do this is not suitable for real-world production use, and
    is only really useful for limited local development scenarios.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经能够发布我们的 todobackend 应用程序，但我们用来做这件事的机制并不适合实际的生产使用，而且只对有限的本地开发场景真正有用。
- en: One key requirement for running our application in the real-world is the ability
    to scale up or down the number of instances or *replicas* of the application container.
    To achieve this, Kubernetes supports a class of resources referred to as *controllers*,
    which are responsible for coordinating, orchestrating, and managing multiple replicas
    of a given pod. One popular type of controller is the *deployment* resource, which
    as the name suggests includes support for creating and updating new versions of
    your pods along with features such as rolling upgrades and support for rollbacks
    should a deployment fail.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中运行我们的应用程序的一个关键要求是能够扩展或缩减应用程序容器的实例或*副本*数量。为了实现这一点，Kubernetes 支持一类资源，称为*控制器*，它负责协调、编排和管理给定
    pod 的多个副本。一种流行的控制器类型是*部署*资源，正如其名称所示，它包括支持创建和更新 pod 的新版本，以及滚动升级和在部署失败时支持回滚等功能。
- en: 'The following example demonstrates updating the `k8s/app/deployment.yaml` file
    in the `todobackend` repository to define a deployment resource:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何更新 `todobackend` 仓库中的 `k8s/app/deployment.yaml` 文件来定义一个部署资源：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We update the previous pod resource to now be a deployment resource, with the
    `template` property of the top-level `spec` property (i.e. `spec.template`) defining
    inline the pod that should be deployed. A key concept of deployments and Kubernetes
    in general is the use of set-based label selector matching ([https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors))
    to determine the resources or pods that the deployment applies to. In the preceding
    example, the deployment resource `spec` specifies two `replicas` and uses `selectors.matchLabels`
    to match the deployment to a pod that includes the label `app` with a value of
    `todobackend`. This is a simple yet powerful paradigm that allows you to create
    your own structures and relationships between resources in a flexible and loosely
    coupled manner. Notice that we also add the `readinessProbe` and `livenessProbe`
    properties to the container definition, which create a readiness probe and liveness
    probe, respectively. A readiness probe defines an action that should be performed
    by Kubernetes to determine if a container is ready, while a liveness probe is
    used to determine if a container is still healthy. In the preceding example, the
    readiness probe uses HTTP GET requests to port 8000 to determine when the deployment
    controller should permit connections to be forwarded to the container, while the
    liveness probe is used to restart the container in the event it no longer responds
    to the liveness probes. Refer to [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
    for further information on the different types of probes and how they can be used.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将之前的 pod 资源更新为现在的 deployment 资源，使用顶级 spec 属性（即 spec.template）的 template 属性内联定义应该部署的
    pod。部署和 Kubernetes 的一个关键概念是使用基于集合的标签选择器匹配来确定部署适用于哪些资源或 pod。在前面的示例中，部署资源的 spec
    指定了两个副本，并使用 selectors.matchLabels 来将部署与包含标签 app 值为 todobackend 的 pod 匹配。这是一个简单但强大的范例，可以以灵活和松散耦合的方式创建自己的结构和资源之间的关系。请注意，我们还向容器定义添加了
    readinessProbe 和 livenessProbe 属性，分别创建了 readiness probe 和 liveness probe。readiness
    probe 定义了 Kubernetes 应执行的操作，以确定容器是否准备就绪，而 liveness probe 用于确定容器是否仍然健康。在前面的示例中，readiness
    probe 使用 HTTP GET 请求到端口 8000 来确定部署控制器何时应允许连接转发到容器，而 liveness probe 用于在容器不再响应 liveness
    probe 时重新启动容器。有关不同类型的探针及其用法的更多信息，请参阅 https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/。
- en: 'To create the new deployment resource, we can first remove the existing pod
    and then apply the `k8s/app/deployment.yaml` file in the `todobackend` repository
    using `kubectl`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建新的部署资源，我们可以首先删除现有的 pod，然后使用 kubectl 应用 todobackend 仓库中的 k8s/app/deployment.yaml
    文件：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After creating the deployment, you can see that the configured number of replicas
    are deployed in the form of two pods, each with a unique name.  The state for
    each pod will transition to ready as soon as the readiness probe you configured
    is successful.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 创建部署后，您可以看到配置的副本数量以两个 pod 的形式部署，每个都有一个唯一的名称。只要您配置的 readiness probe 成功，每个 pod
    的状态就会立即转换为 ready。
- en: Creating a service
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建服务
- en: At this point, we have defined a pod for our application and deployed multiple
    replicas of our application using a deployment resource, and we now need to ensure
    external clients can connect to our application. Given that we have multiple replicas
    of our application running, we require a component that is able to provide a stable
    service endpoint, track the location of each replica, and load balance incoming
    connections across all replicas.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经为我们的应用程序定义了一个pod，并使用部署资源部署了多个应用程序副本，现在我们需要确保外部客户端可以连接到我们的应用程序。鉴于我们有多个应用程序副本正在运行，我们需要一个能够提供稳定服务端点、跟踪每个副本位置并在所有副本之间负载平衡传入连接的组件。
- en: '*Services* are the Kubernetes resources that provide such features, where each
    service is assigned a virtual IP address that can be used to access a given set
    of pods, and incoming connections to the virtual IP address are load balanced
    to each pod replica, based upon iptables rules that are managed and updated via
    a standard Kubernetes system resource called the kube-proxy:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务*是提供此类功能的Kubernetes资源，每个服务都被分配一个虚拟IP地址，可以用来访问一组pod，并且对虚拟IP地址的传入连接进行负载平衡到每个pod副本，基于通过一个名为kube-proxy的标准Kubernetes系统资源管理和更新的iptables规则：'
- en: '![](assets/869fe4cb-aa5f-4772-935c-4f14ca899e43.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/869fe4cb-aa5f-4772-935c-4f14ca899e43.png)'
- en: Services and endpoints in Kubernetes
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的服务和端点
- en: In the preceding diagram, a client pod is attempting to communicate with the
    application pod using a virtual IP address of `10.1.1.1` on port `80` (`10.1.1.1:80`).
    Note that the service virtual IP address is published on every node in the cluster,
    with the **kube-proxy** component responsible for updating iptables rules that
    select an appropriate endpoint that client connections should be routed to in
    a round robin fashion. Because the virtual IP address is published on every node
    in the cluster, any client on any node can communicate with the service, and traffic
    is distributed across the cluster in an even manner.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，一个客户端pod正试图使用虚拟IP地址`10.1.1.1`的端口`80`(`10.1.1.1:80`)与应用程序pod进行通信。请注意，服务虚拟IP地址在集群中的每个节点上都是公开的，**kube-proxy**组件负责更新iptables规则，以循环方式选择适当的端点，将客户端连接路由到。由于虚拟IP地址在集群中的每个节点上都是公开的，因此任何节点上的任何客户端都可以与服务通信，并且流量会均匀地分布在整个集群中。
- en: 'Now that you have a high level understanding of how a service works, let''s
    actually define a new service in the `k8s/app/deployment.yaml` file that''s located
    within the `todobackend` repository:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经对服务的工作原理有了高层次的理解，让我们实际在`k8s/app/deployment.yaml`文件中定义一个新的服务，该文件位于`todobackend`存储库中：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice that you can define multiple resources in a single YAML file by using
    the `---` separator to separate each resource, and that we can create a service
    called todobackend that uses label matching to bind the service to any pods with
    a label of `app=todobackend`. In the `spec.ports` section, we configure port 80
    as the incoming or listener port on the service, which load balances connections
    to a `targetPort` of 8000 on each pod.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以使用`---`分隔符在单个YAML文件中定义多个资源，并且我们可以创建一个名为todobackend的服务，该服务使用标签匹配将服务绑定到具有`app=todobackend`标签的任何pod。在`spec.ports`部分，我们将端口80配置为服务的传入或监听端口，该端口将连接负载平衡到每个pod上的8000端口。
- en: 'With the definition of our service in place, you can now deploy the service
    using the `kubectl apply` command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务定义已经就位，现在您可以使用`kubectl apply`命令部署服务：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can use the `kubectl get svc` command to view current services, and notice
    that each service includes a unique cluster IP address, which is the virtual IP
    address that other resources in the cluster can use to communicate with the pods
    associated with the service. The `kubectl get endpoints` command shows the actual
    endpoints associated with each service, and you can see that connections to the
    `todobackend` service virtual IP address of `10.103.210.17:80` will be load balanced
    to `10.1.0.27:8000` and `10.1.0.30:8000`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`kubectl get svc`命令查看当前服务，并注意到每个服务都包括一个唯一的集群IP地址，这是集群中其他资源可以用来与与服务关联的pod进行通信的虚拟IP地址。`kubectl
    get endpoints`命令显示与每个服务关联的实际端点，您可以看到对`todobackend`服务虚拟IP地址`10.103.210.17:80`的连接将负载均衡到`10.1.0.27:8000`和`10.1.0.30:8000`。
- en: 'Each service is also allocated a unique DNS name in the form of `<service-name>.<namespace>.svc.cluster.local`.
    The default namespace in Kubernetes is called `default`, so for our todobackend
    application, it will be assigned a name of `todobackend.default.svc.cluster.local`,
    which you can verify is reachable within the cluster by using the `kubectl run`
    command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务还分配了一个唯一的DNS名称，格式为`<service-name>.<namespace>.svc.cluster.local`。Kubernetes中的默认命名空间称为`default`，因此对于我们的todobackend应用程序，它将被分配一个名为`todobackend.default.svc.cluster.local`的名称，您可以使用`kubectl
    run`命令验证在集群内是否可访问：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding example, you can simply query for todobackend, as Kubernetes
    sends the DNS search domain to `<namespace>.svc.cluster.local` (`default.svc.cluster.local`,
    in our use case), and you can see that this resolves to the cluster IP address
    of the todobackend service.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，您可以简单地查询todobackend，因为Kubernetes将DNS搜索域发送到`<namespace>.svc.cluster.local`（在我们的用例中为`default.svc.cluster.local`），您可以看到这将解析为todobackend服务的集群IP地址。
- en: It's important to note that the cluster IP address is only reachable within
    the Kubernetes cluster – we can't reach this service externally without further
    configuration.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，集群IP地址只能在Kubernetes集群内访问 - 如果没有进一步的配置，我们无法从外部访问此服务。
- en: Exposing a service
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 暴露服务
- en: 'To allow external clients and systems to communicate with Kubernetes services,
    you must expose the service to the outside world. In true Kubernetes style, there
    are a variety of options available to achieve this, which are controlled by Kubernetes
    `ServiceTypes`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许外部客户端和系统与Kubernetes服务通信，您必须将服务暴露给外部世界。按照Kubernetes的风格，有多种选项可用于实现这一点，这些选项由Kubernetes的`ServiceTypes`控制：
- en: '**Node ports**: This service type maps an external port on each Kubernetes
    node to the internal cluster IP and port configured for the service. This creates
    several external connection points for your service that may change as nodes come
    and go, making it difficult to create a stable external service endpoint.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点端口：此服务类型将Kubernetes每个节点上的外部端口映射到为服务配置的内部集群IP和端口。这为您的服务创建了几个外部连接点，随着节点的进出可能会发生变化，这使得创建稳定的外部服务端点变得困难。
- en: '**Load balancer**: Represents an external dedicated Layer 4 (TCP or UDP) load
    balancer that is mapped exclusively to your service. The actual load balancer
    that is deployed depends on your target platform – for example, with AWS, a classic
    Elastic Load Balancer is created. This is a very popular option, however one significant
    limitation is that a load balancer is created per service, meaning that this option
    can become quite expensive if you have a lot of services.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器：表示专用的外部第4层（TCP或UDP）负载均衡器，专门映射到您的服务。部署的实际负载均衡器取决于您的目标平台 - 例如，对于AWS，将创建一个经典的弹性负载均衡器。这是一个非常受欢迎的选项，但一个重要的限制是每个服务都会创建一个负载均衡器，这意味着如果您有很多服务，这个选项可能会变得非常昂贵。
- en: '**Ingress**: This is a shared Layer 7 (HTTP) load balancer resource that works
    in a similar fashion to the AWS Application Load Balancer, where connections to
    a single HTTP/HTTPS endpoint can be routed to multiple services based upon host
    header or URL path patterns. This is considered the best option for HTTP-based
    services, given you can share one load balancer across multiple services.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ingress**：这是一个共享的第7层（HTTP）负载均衡器资源，其工作方式类似于AWS应用程序负载均衡器，其中对单个HTTP/HTTPS端点的连接可以根据主机标头或URL路径模式路由到多个服务。鉴于您可以跨多个服务共享一个负载均衡器，因此这被认为是基于HTTP的服务的最佳选择。'
- en: 'The most popular method to publish your services externally is to use the load
    balancer method, which works as illustrated in the following diagram:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 发布您的服务的最流行的方法是使用负载均衡器方法，其工作方式如下图所示：
- en: '![](assets/95505204-2d0d-4d38-9188-5741bbd5bfc6.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/95505204-2d0d-4d38-9188-5741bbd5bfc6.png)'
- en: Load balancing in Kubernetes
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的负载均衡
- en: The external load balancer publishes the external service endpoint that clients
    will connect to, which in the preceding example is `192.0.2.43:80`. The load balancer
    service endpoint will be associated with the nodes in your cluster that have active
    pods associated with the service, who each have a node port mapping set up via
    the **kube-proxy** component. The node port mapping is then mapped to each of
    the local endpoints on the node, allowing traffic to be load balanced efficiently
    and evenly across the cluster.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器发布客户端将连接到的外部服务端点，在前面的示例中是`192.0.2.43:80`。负载均衡器服务端点将与具有与服务关联的活动pod的集群中的节点相关联，每个节点都通过**kube-proxy**组件设置了节点端口映射。然后，节点端口映射将映射到节点上的每个本地端点，从而实现在整个集群中高效均匀地进行负载平衡。
- en: For communications from internal clients within the cluster, communications
    still take place using the service cluster IP address, as described earlier in
    this chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集群内部客户端的通信，通信仍然使用服务集群IP地址，就像本章前面描述的那样。
- en: 'Later on in this chapter, we will see how to integrate AWS load balancers with
    EKS, however for now your local Docker Desktop environment includes support for
    its own load balancer resource, which publishes an external endpoint on your host
    for your service. Adding an external load balancer to a service is very simple,
    as demonstrated in the following example, where we modify the configuration of
    the `k8s/app/deployments.yaml` file in the todobackend repository:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，我们将看到如何将AWS负载均衡器与EKS集成，但是目前您的本地Docker桌面环境包括对其自己的负载均衡器资源的支持，该资源会在您的主机上发布一个外部端点供您的服务使用。向服务添加外部负载均衡器非常简单，就像在以下示例中演示的那样，我们修改了`k8s/app/deployments.yaml`文件中的配置，该文件位于todobackend存储库中：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All that is required to deploy the appropriate load balancer for your environment
    is to set the `spec.type` property to `LoadBalancer`, and Kubernetes will automatically
    create an external load balancer. You can test this by applying your updated configuration
    and running the `kubectl get svc` command:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在您的环境中部署适当的负载均衡器，所需的全部就是将`spec.type`属性设置为`LoadBalancer`，Kubernetes将自动创建一个外部负载均衡器。您可以通过应用更新后的配置并运行`kubectl
    get svc`命令来测试这一点：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice that the `kubectl get svc` output now shows that the external IP address
    of the todobackend service is localhost (localhost is always the external interface
    that's reachable by your Docker client when using Docker Desktop) and that it
    is published externally on port 80, which you can verify is true by running the
    `curl localhost` command. The external port maps to port 31417 on a single node
    cluster, which is the port that the **kube-proxy** component listens on in order
    to support the load balancer architecture we described earlier.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`kubectl get svc`输出现在显示todobackend服务的外部IP地址为localhost（当使用Docker Desktop时，localhost始终是Docker客户端可访问的外部接口），并且它在端口80上外部发布，您可以通过运行`curl
    localhost`命令来验证这一点。外部端口映射到单节点集群上的端口31417，这是**kube-proxy**组件监听的端口，以支持我们之前描述的负载均衡器架构。
- en: Adding volumes to your pods
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向您的pods添加卷
- en: Now that we have an understanding of how to publish our application both internally
    within the Kubernetes cluster and externally to the outside world, we can focus
    on making the todobackend application fully functional by adding support for the
    various deployment activities and dependencies of the todobackend application.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在Kubernetes集群内部和外部发布我们的应用程序，我们可以专注于通过添加对todobackend应用程序的各种部署活动和依赖项的支持，使todobackend应用程序完全功能。
- en: We will first tackle the issue of serving static content for the todobackend
    application – as you know from previous chapters, we need to run a **collectstatic**
    task that ensures static content is available for the **todobackend** application,
    and this should be run any time the **todobackend** application is deployed. The
    **collectstatic** task needs to write static content to a volume that is then
    mounted by the main application container, so let's discuss how we can add volumes
    to Kubernetes pods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将解决为todobackend应用程序提供静态内容的问题 - 正如您从之前的章节中了解的那样，我们需要运行**collectstatic**任务，以确保todobackend应用程序的静态内容可用，并且应该在部署todobackend应用程序时运行。**collectstatic**任务需要将静态内容写入一个卷，然后由主应用程序容器挂载，因此让我们讨论如何向Kubernetes
    pods添加卷。
- en: Kubernetes has a powerful storage subsystem that supports a variety of volume
    types, which you can read more about at [https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes).
    For the **collectstatic** use case, the [emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)
    volume type is suitable, which is a volume that follows the lifecycle of each
    pod – it is created and destroyed dynamically with the pod – hence it is suitable
    as an ephemeral storage type for use cases such as caching and serving static
    content that can be easily regenerated on pod creation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes具有强大的存储子系统，支持各种卷类型，您可以在[https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes)上阅读更多信息。对于**collectstatic**用例，[emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)卷类型是合适的，这是一个遵循每个pod生命周期的卷
    - 它会随着pod的创建和销毁而动态创建和销毁 - 因此它适用于诸如缓存和提供静态内容之类的用例，这些内容在pod创建时可以轻松重新生成。
- en: 'The following example demonstrates adding a public `emptyDir` volume to the
    `k8s/app/deployment.yaml` file:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了向`k8s/app/deployment.yaml`文件添加公共`emptyDir`卷：
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We define a volume called `public` in the `spec.Volumes` property of the pod
    template, and then use the `volumeMounts` property in the todobackend container
    definition to mount the `public` volume to `/public`. One important configuration
    requirement for our use case is setting the `spec.securityContext.fsGroup` property,
    which defines the group ID that will be configured as the group owner for any
    filesystem mounts in the pod. We set this value to `1000`; recall from earlier
    chapters that the todobackend image runs as the `app` user, which has a user/group
    ID of 1000\. This configuration ensures that the todobackend container is able
    to read and write static content to the `public` volume.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 pod 模板的 `spec.Volumes` 属性中定义了一个名为 `public` 的卷，然后在 todobackend 容器定义中使用 `volumeMounts`
    属性将 `public` 卷挂载到 `/public`。我们的用例的一个重要配置要求是设置 `spec.securityContext.fsGroup` 属性，该属性定义了将配置为文件系统挂载点的组所有者的组
    ID。我们将此值设置为 `1000`；回想一下前几章中提到的，todobackend 映像以 `app` 用户运行，其用户/组 ID 为 1000。此配置确保
    todobackend 容器能够读取和写入 `public` 卷的静态内容。
- en: 'If you now deploy your configuration changes, you should be able to use the
    `kubectl exec` command to inspect the todobackend container filesystem and verify
    that we can read and write to the `/public` mount:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在部署配置更改，您应该能够使用 `kubectl exec` 命令来检查 todobackend 容器文件系统，并验证我们能够读取和写入 `/public`
    挂载点：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `kubectl exec` command is similar to the `docker exec` command, in that
    it allows you to execute a command within a currently running pod container. This
    command must reference the name of the pod, and we use the `kubectl get pods`
    command along with a JSON path query to extract this name. As you can see, the
    `app` user within the **todobackend** container is able to read and write to the
    `/public` mount.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl exec` 命令类似于 `docker exec` 命令，允许您在当前运行的 pod 容器中执行命令。此命令必须引用 pod 的名称，我们使用
    `kubectl get pods` 命令以及 JSON 路径查询来提取此名称。正如您所看到的，**todobackend** 容器中的 `app` 用户能够读取和写入
    `/public` 挂载点。'
- en: Adding init containers to your pods
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向您的 pod 添加初始化容器
- en: With an ephemeral volume in place for static content, we can now focus on scheduling
    the **collectstatic** task to generate static content for our application. Kubernetes
    has support for [init containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/),
    which are a special type of container within a pod that are executed before your
    main application container(s) are started. Kubernetes will ensure that your init
    containers run to completion and complete successfully before starting your application,
    and if you specify multiple init containers, Kubernetes will execute them in order,
    one by one, until all init containers have completed.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在为静态内容准备了临时卷后，我们现在可以专注于安排 **collectstatic** 任务来为我们的应用程序生成静态内容。Kubernetes 支持
    [初始化容器](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)，这是一种特殊类型的容器，在
    pod 中启动主应用程序容器之前执行。Kubernetes 将确保您的初始化容器运行完成并成功完成，然后再启动您的应用程序，如果您指定了多个初始化容器，Kubernetes
    将按顺序执行它们，直到所有初始化容器都完成。
- en: 'The following code demonstrates adding an init container to the `k8s/app/deployment.yaml`
    file:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了向 `k8s/app/deployment.yaml` 文件添加初始化容器：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can now deploy your changes and use the `kubectl logs` command to verify
    that the collectstatic init container executed successfully:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以部署您的更改，并使用 `kubectl logs` 命令来验证 collectstatic 初始化容器是否成功执行：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you now browse to `http://localhost` in your browser, you should be able
    to verify that the static content is now rendering correctly:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在在浏览器中浏览 `http://localhost`，您应该能够验证静态内容现在正确呈现：
- en: '![](assets/6dce9b13-5f6f-4d61-a6a3-6dc3ac1017b6.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6dce9b13-5f6f-4d61-a6a3-6dc3ac1017b6.png)'
- en: The todobackend application with correct static content
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: todobackend应用程序具有正确的静态内容
- en: Adding a database service
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加数据库服务
- en: The next step in getting the **todobackend** application fully functional is
    to add a database service that will host the **todobackend** application database.
    We will run this service within our Kubernetes cluster, however in a real-world
    production use case in AWS, I would typically recommend using the Relational Database
    Service (RDS).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使todobackend应用程序完全功能的下一步是添加一个数据库服务，该服务将托管todobackend应用程序数据库。我们将在我们的Kubernetes集群中运行此服务，但是在AWS中的真实生产用例中，我通常建议使用关系数据库服务（RDS）。
- en: 'Defining the database service requires two main configuration tasks:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 定义数据库服务需要两个主要的配置任务：
- en: Creating persistent storage
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建持久存储
- en: Creating a database service
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据库服务
- en: Creating persistent storage
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建持久存储
- en: A key requirement of our database service is persistent storage, and for our
    single-node local Kubernetes development environment, the [**hostPath**](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) volume
    type represents the standard option for providing simple persistent storage requirements.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据库服务的一个关键要求是持久存储，在我们的单节点本地Kubernetes开发环境中，[hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)卷类型代表提供简单持久存储需求的标准选项。
- en: Although you can create a **hostPath** volume very easily by specifying a path
    directly in your volume definition (see the example pod definition at [https://kubernetes.io/docs/concepts/storage/volumes/#hostpath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)),
    one problem with such an approach is that it creates a hard dependency on the
    underlying volume type, and also requires manual cleanup if you ever want to delete
    the pod and the data associated with the volumes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以通过在卷定义中直接指定路径来轻松创建hostPath卷（请参阅[https://kubernetes.io/docs/concepts/storage/volumes/#hostpath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)中的示例pod定义），但这种方法的一个问题是它对底层卷类型创建了硬依赖，并且如果您想要删除pod和与卷关联的数据，则需要手动清理。
- en: 'A very useful feature of the Docker Desktop Kubernetes support is the inclusion
    of a dynamic volume provisioner called `docker.io/hostpath` that automatically
    creates volumes of type **hostPath** for you, which is available via the default
    *storage class* that you can view by running the `kubectl get sc` command:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Desktop Kubernetes支持的一个非常有用的功能是包含一个名为`docker.io/hostpath`的动态卷提供程序，它会自动为您创建hostPath类型的卷，该卷可通过运行`kubectl
    get sc`命令查看的默认*storage class*来使用：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: A storage class provides an abstraction over the underlying volume type, meaning
    your pods can request storage from a specific class. This includes generic requirements
    such as volume size, without needing to worry about the underlying volume type.
    In the case of Docker Desktop, a default storage class is included out of the
    box, which provisions storage requests using the **hostPath** volume type.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类提供了对底层卷类型的抽象，这意味着您的pod可以从特定类中请求存储。这包括通用要求，如卷大小，而无需担心底层卷类型。在Docker Desktop的情况下，开箱即用包含了一个默认的存储类，它使用hostPath卷类型来提供存储请求。
- en: However, later on when we set up a Kubernetes cluster in AWS using EKS, we will
    configure a default storage class that uses the AWS Elastic Block Store (EBS)
    as the underlying volume type. Taking this approach means that we don't need to
    change our pod definitions, as we will be referring to the same storage class
    in each environment.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们稍后在AWS中使用EKS设置Kubernetes集群时，我们将配置一个使用AWS Elastic Block Store（EBS）作为底层卷类型的默认存储类。采用这种方法意味着我们不需要更改我们的pod定义，因为我们将在每个环境中引用相同的存储类。
- en: If you are using minikube, a dynamic provisioner called `k8s.io/minikube-hostpath`
    provides similar functionality to the Docker hostpath provisioner, but mounts
    volumes under `/tmp/hostpath-provisioner`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用minikube，名为`k8s.io/minikube-hostpath`的动态provisioner提供了类似于Docker hostpath
    provisioner的功能，但是将卷挂载在`/tmp/hostpath-provisioner`下。
- en: 'To use storage classes rather than specify your volume types directly with
    your pod definitions, you need to create a *persistent volume claim*, which provides
    a logical definition of storage requirements such as volume size and access mode.
    Let''s define a persistent volume claim, but before we do this we need to establish
    a new folder called `k8s/db` in the todobackend repository that will store our
    database service configuration:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用存储类而不是直接在pod定义中指定卷类型，您需要创建*持久卷索赔*，它提供了存储需求的逻辑定义，如卷大小和访问模式。让我们定义一个持久卷索赔，但在此之前，我们需要在todobackend存储库中建立一个名为`k8s/db`的新文件夹，用于存储我们的数据库服务配置：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Within this folder, we will create a file called `k8s/db/storage.yaml`, in
    which we will define a persistent volume claim:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件夹中，我们将创建一个名为`k8s/db/storage.yaml`的文件，在其中我们将定义一个持久卷索赔。
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We create the claim (called `todobackend-data`) in a dedicated file, as this
    will allow us to independently manage the life cycle of the claim. One property
    that is not included in the preceding example is the `spec.storageClassName` property
    – if this is omitted, the default storage class is used, however bear in mind
    that you can create and reference your own storage classes. The `spec.accessModes`
    property specifies how the storage should be mounted – in the case of both local
    storage and EBS storage in AWS, we only want a single container at a time to be
    able to read and write to the volume, which is encompassed by the `ReadWriteOnce`
    access mode.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个专用文件中创建索赔（称为`todobackend-data`），因为这样可以让我们独立管理索赔的生命周期。在前面的示例中未包括的一个属性是`spec.storageClassName`属性
    - 如果省略此属性，将使用默认的存储类，但请记住您可以创建和引用自己的存储类。`spec.accessModes`属性指定存储应该如何挂载 - 在本地存储和AWS中的EBS存储的情况下，我们只希望一次只有一个容器能够读写卷，这由`ReadWriteOnce`访问模式包含。
- en: The `spec.resources.requests.storage` property specifies the size of the persistent
    volume, which in this case we configure as 8 GB.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec.resources.requests.storage`属性指定持久卷的大小，在这种情况下，我们配置为8GB。'
- en: If you are using Docker for Windows, you will be prompted to share your C:\
    with Docker the first time you attempt to use the Docker hostPath provisioner.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Windows版的Docker，第一次尝试使用Docker hostPath provisioner时，将提示您与Docker共享C:\。
- en: 'If you now deploy the persistent volume claim using `kubectl`, you can use
    the `kubectl get pvc` command to view your newly created claim:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在使用`kubectl`部署持久卷索赔，可以使用`kubectl get pvc`命令查看您新创建的索赔：
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can see that when you create a persistent volume claim, a persistent volume
    is dynamically created. When using Docker Desktop, this is actually created in
    the path `~/.docker/Volumes/<persistent-volume-claim>/<volume>`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，当您创建持久卷索赔时，会动态创建一个持久卷。在使用Docker Desktop时，实际上是在路径`~/.docker/Volumes/<persistent-volume-claim>/<volume>`中创建的。
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you are using Docker for Windows and you are using the Windows Subsystem
    for Linux, you can create a symbolic link to the `.docker` folder on your Windows
    host:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Windows版的Docker并且正在使用Windows子系统用于Linux，您可以在Windows主机上创建一个符号链接到`.docker`文件夹：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that if you followed the instructions in [Chapter 1](3d8c99a9-f463-4891-ad1a-bc2450d91251.xhtml),
    *Container and Docker Fundamentals*, for setting up the Windows Subsystem for
    Linux, you already configured `/mnt/c/Users/<user-name>/` as your home directory
    so you don't need to perform the configuration above.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您按照[第1章](3d8c99a9-f463-4891-ad1a-bc2450d91251.xhtml)中的说明进行了设置，*容器和Docker基础知识*，为了设置
    Windows Subsystem for Linux，您已经将 `/mnt/c/Users/<user-name>/` 配置为您的主目录，因此您不需要执行上述配置。
- en: Creating a database service
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据库服务
- en: 'Now that we have created a persistent volume claim, we can define the database
    service. We will define the database service within a new file called `k8s/db/deployment.yaml` in
    the `todobackend` repository, where we create a service and deployment definition:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个持久卷索赔，我们可以定义数据库服务。我们将在 `todobackend` 仓库中的一个新文件 `k8s/db/deployment.yaml`
    中定义数据库服务，其中我们创建了一个服务和部署定义：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We first define a service called `todobackend-db`, which publishes the default
    MySQL TCP port `3306`. Notice that we specify a `spec.clusterIP` value of `None`,
    which creates a headless service. Headless services are useful for single instance
    services and allow the IP address of the pod to be used as the service endpoint,
    rather than using the **kube-proxy** component with a virtual IP address that
    would only ever load balance to a single endpoint. Defining a headless service
    will still publish a DNS record for the service but will associate the record
    with the pod IP address, ensuring that the **todobackend** application can connect
    to the `todobackend-db` service by name. We then create a deployment for the `todobackend-db`
    service, and define a volume called `data` which is mapped to the persistent volume
    claim we created earlier and mounted to the database data directory (`/var/lib/mysql`)
    in the MySQL container. Notice that we specify the `args` property (the equivalent
    of the CMD/command directive in Docker/Docker Compose), which configures MySQL
    to ignore the `lost+found` directory if it is present. Although this won't be
    a problem when using Docker Desktop, it will be a problem in AWS for the same
    reasons discussed in the previous Docker Swarm chapter. Finally, we create a liveness
    probe of type `exec` that executes the `mysqlshow` command to check that a connection
    to the MySQL database can be made locally within the MySQL container. Because
    the MySQL secret is located in a file, we wrap the MySQL command in a shell process
    (`/bin/sh`), which allows us to use the `$(cat /tmp/secrets/MYSQL_PASSWORD)` command
    substitution.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个名为 `todobackend-db` 的服务，它发布默认的 MySQL TCP 端口 `3306`。请注意，我们指定了 `spec.clusterIP`
    值为 `None`，这将创建一个无头服务。无头服务对于单实例服务非常有用，并允许使用 pod 的 IP 地址作为服务端点，而不是使用 **kube-proxy**
    组件与虚拟 IP 地址进行负载均衡到单个端点。定义无头服务仍将发布服务的 DNS 记录，但将该记录与 pod IP 地址关联，确保 **todobackend**
    应用可以通过名称连接到 `todobackend-db` 服务。然后，我们为 `todobackend-db` 服务创建一个部署，并定义一个名为 `data`
    的卷，该卷映射到我们之前创建的持久卷索赔，并挂载到 MySQL 容器中的数据库数据目录 (`/var/lib/mysql`)。请注意，我们指定了 `args`
    属性（在 Docker/Docker Compose 中相当于 CMD/command 指令），它配置 MySQL 忽略 `lost+found` 目录（如果存在的话）。虽然在使用
    Docker Desktop 时这不会成为问题，但在 AWS 中会成为问题，原因与前面的 Docker Swarm 章节中讨论的原因相同。最后，我们创建了一个类型为
    `exec` 的活动探针，执行 `mysqlshow` 命令来检查在 MySQL 容器内部可以本地进行与 MySQL 数据库的连接。由于 MySQL 密钥位于文件中，我们将
    MySQL 命令包装在一个 shell 进程 (`/bin/sh`) 中，这允许我们使用 `$(cat /tmp/secrets/MYSQL_PASSWORD)`
    命令替换。
- en: Kubernetes allows you resolve environment variables at execution time by using
    the syntax `$(<environment variable>)`. For example, the `$(MYSQL_USER)` value
    included in the preceding liveness probe will be resolved to the environment variable
    `MYSQL_USER` when the probe is executed. See [https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments)
    for more details.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes允许您在执行时使用语法`$(<environment variable>)`来解析环境变量。例如，前面存活探针中包含的`$(MYSQL_USER)`值将在执行探针时解析为环境变量`MYSQL_USER`。有关更多详细信息，请参阅[https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments)。
- en: 'If you now deploy the database service and deployment resources, you can use
    the `kubectl get svc` and `kubectl get endpoints` commands to verify your headless
    service configuration:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在部署数据库服务和部署资源，可以使用`kubectl get svc`和`kubectl get endpoints`命令来验证无头服务配置：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice that the `todobackend-db` service is deployed with a cluster IP of none,
    which means that the published endpoint of the service is the IP address of the
    `todobackend-db` pod.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`todobackend-db`服务部署时的集群IP为none，这意味着服务的发布端点是`todobackend-db` pod的IP地址。
- en: 'You can also verify that the data volume was created correctly by listing the
    contents of the physical volume directory in `~/.docker/Volumes/todobackend-data` on
    your local host:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过列出本地主机上`~/.docker/Volumes/todobackend-data`目录中物理卷的内容来验证数据卷是否正确创建：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you now delete just the database service and deployment, you should be able
    to verify that the persistent volume is not removed and persists, meaning that
    you can then recreate the database service and reattach to the `data` volume with
    no data loss:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在只删除数据库服务和部署，您应该能够验证持久卷未被删除并持续存在，这意味着您随后可以重新创建数据库服务并重新附加到`data`卷而不会丢失数据。
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The preceding code is a good example of why we separated out the persistent
    volume claim into its own file – doing this means that we can easily manage the
    life cycle of the database service without any data loss. In the event that you
    do want to destroy the database service and its data, you can choose to remove
    the persistent volume claim, in which case the Docker Desktop **hostPath** provisioner
    will automatically remove the persistent volume and any stored data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码很好地说明了为什么我们将持久卷索赔分离成自己的文件的原因 - 这样做意味着我们可以轻松地管理数据库服务的生命周期，而不会丢失任何数据。如果您确实想要销毁数据库服务及其数据，您可以选择删除持久卷索赔，这样Docker
    Desktop **hostPath**提供程序将自动删除持久卷和任何存储的数据。
- en: Kubernetes also supports a controller called a StatefulSet, which is specifically
    designed for stateful applications such as databases. You can read more about
    StatefulSets at [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还支持一种称为StatefulSet的控制器，专门用于有状态的应用程序，如数据库。您可以在[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)上阅读更多关于StatefulSets的信息。
- en: Creating and consuming secrets
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和使用秘密
- en: Kubernetes supports *secret* objects, which allow sensitive data such as a password
    or token to be stored securely in an encrypted format, and then exposed privately
    as required to your containers. Kubernetes secrets are stored in a key/value map
    or dictionary format, which is different from Docker secrets, which as you saw
    in the previous chapter typically just store the secret value.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持*secret*对象，允许将诸如密码或令牌之类的敏感数据以加密格式安全存储，然后根据需要私密地暴露给您的容器。Kubernetes秘密以键/值映射或字典格式存储，这与Docker秘密不同，正如您在上一章中看到的，Docker秘密通常只存储秘密值。
- en: 'You can create secrets manually using literal values, or by including your
    secret values in a file and applying the file. I recommend creating your secrets
    using literal values to avoid storing your secrets in your configuration files,
    which may be inadvertently committed and pushed to your source code repositories:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用文字值手动创建秘密，也可以将秘密值包含在文件中并应用该文件。我建议使用文字值创建您的秘密，以避免将您的秘密存储在配置文件中，这可能会意外地提交并推送到您的源代码存储库中。
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding example, you use the `kubectl create secret generic` command
    to create a secret called `todobackend-secret` that stores three secret values.
    Notice that each value is stored with a key that has the same name as the expected
    environment variable, which will make configuration of these values easy to consume.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，您使用`kubectl create secret generic`命令创建了一个名为`todobackend-secret`的秘密，其中存储了三个秘密值。请注意，每个值都使用与预期环境变量相同的键存储，这将使这些值的配置易于消耗。
- en: With the secret now created, you can configure the `todobackend` and `db` deployments
    to consume the secret. Kubernetes includes a special volume type called secret
    that allows you to mount your secrets at a configurable location in your containers,
    which your applications can then read securely and privately.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建了秘密，您可以配置`todobackend`和`db`部署以使用该秘密。Kubernetes包括一种特殊的卷类型，称为秘密，允许您在容器中的可配置位置挂载您的秘密，然后您的应用程序可以安全和私密地读取。
- en: Consuming secrets for the database service
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为数据库服务使用秘密
- en: 'Let''s first update the database deployment resource that is defined in the
    `k8s/db/deployment.yaml` file to consume the `todobackend-secret`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先更新`k8s/db/deployment.yaml`文件中定义的数据库部署资源，以使用`todobackend-secret`：
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You first create a volume called `secrets` that is of type `secret` which references
    the `todobackend-secret` we created earlier. By default, all secret items will
    be available, however you can control which items are published to the volume
    via the optional `items` property. Because the `todobackend-secret` includes the
    `SECRET_KEY` secret that is specific to the todobackend application, we configure
    the `items` list to exclude this item and only present the `MYSQL_PASSWORD` and
    `MYSQL_ROOT_PASSWORD` keys. Note that the specified `path` is required and is
    expressed as a relative path based from where the secret volume is mounted in
    each of your containers.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个名为`secrets`的卷，类型为`secret`，引用我们之前创建的`todobackend-secret`。默认情况下，所有秘密项目都将可用，但是您可以通过可选的`items`属性控制发布到卷的项目。因为`todobackend-secret`包含特定于todobackend应用程序的`SECRET_KEY`秘密，我们配置`items`列表以排除此项目，并仅呈现`MYSQL_PASSWORD`和`MYSQL_ROOT_PASSWORD`键。请注意，指定的`path`是必需的，并且表示为相对路径，基于秘密卷在每个容器中挂载的位置。
- en: You then mount the `secrets` volume as read only to `/tmp/secrets` in the `db`
    container, and update the password-related environment variables to reference
    the secret files rather than using values directly from the environment. Notice
    that each secret value will be created in a file that is named based upon the
    key within the folder that the secret volume is mounted to.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将`secrets`卷作为只读挂载到`/tmp/secrets`中的`db`容器，并更新与密码相关的环境变量，以引用秘密文件，而不是直接使用环境中的值。请注意，每个秘密值将被创建在基于秘密卷挂载到的文件夹中的键命名的文件中。
- en: 'To deploy our new configuration, you first need to delete the database service
    and its associated persistent volume as this includes the previous credentials,
    and then redeploy the database service. You can do this very easily by referencing
    the entire `k8s/db` directory when you execute the delete and apply actions, rather
    than specifying each file individually:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署我们的新配置，您首先需要删除数据库服务及其关联的持久卷，因为这包括了先前的凭据，然后重新部署数据库服务。您可以通过在执行删除和应用操作时引用整个`k8s/db`目录来轻松完成此操作，而不是逐个指定每个文件：
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once you have recreated the `db` service, you can use the `kubectl exec` command
    to verify that the `MYSQL_PASSWORD` and `MYSQL_ROOT_PASSWORD` secret items have
    been written to `/tmp/secrets`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您重新创建了`db`服务，您可以使用`kubectl exec`命令来验证`MYSQL_PASSWORD`和`MYSQL_ROOT_PASSWORD`秘密项目是否已写入`/tmp/secrets`：
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Consuming secrets for the application
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为应用程序使用秘密
- en: 'We now need to update the todobackend service to consume our secrets by modifying
    the `k8s/app/deployment.yaml` file:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要通过修改`k8s/app/deployment.yaml`文件来更新todobackend服务以使用我们的秘密：
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You must define the `secrets` volume and ensure that only the `MYSQL_PASSWORD`
    and `SECRET_KEY` items are exposed to the **todobackend** container. After mounting
    the volume as read only in the **todobackend** application container, you must
    then configure the `SECRETS_ROOT` environment variable with the path to the `secrets`
    mount. Recall in the last chapter that we added support to the **todobackend**
    application for consuming Docker secrets, which by default expects your secrets
    to be located at `/run/secrets`. However, because `/run` is a special tmpfs filesystem,
    you cannot mount your secrets using a regular file system mount at this location,
    hence we need to configure the `SECRETS_ROOT` environment variable, which reconfigure
    the secrets location the application will look in. We must also configure the
    `MYSQL_HOST` and `MYSQL_USER` environment variables, so that along with the `MYSQL_PASSWORD`
    secret, the **todobackend** application has the required information to connect
    to the database service.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须定义`secrets`卷，并确保只有`MYSQL_PASSWORD`和`SECRET_KEY`项目暴露给**todobackend**容器。在**todobackend**应用程序容器中只读挂载卷后，您必须使用`SECRETS_ROOT`环境变量配置到`secrets`挂载的路径。回想一下，在上一章中，我们为**todobackend**应用程序添加了对Docker秘密的支持，默认情况下，它期望您的秘密位于`/run/secrets`。但是，因为`/run`是一个特殊的tmpfs文件系统，您不能在此位置使用常规文件系统挂载您的秘密，因此我们需要配置`SECRETS_ROOT`环境变量，重新配置应用程序将查找的秘密位置。我们还必须配置`MYSQL_HOST`和`MYSQL_USER`环境变量，以便与`MYSQL_PASSWORD`秘密一起，**todobackend**应用程序具有连接到数据库服务所需的信息。
- en: 'If you now deploy the changes, you should be able to verify that the correct
    secret items are mounted in the **todobackend** container:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在部署更改，您应该能够验证**todobackend**容器中挂载了正确的秘密项目：
- en: '[PRE35]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If you browse to `http://localhost/todos`, you should receive an error indicating
    that a database table does not exist, which means that the application is now
    successfully connecting and authenticating to the database, but is missing the
    required schema and tables that the application expects.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您浏览`http://localhost/todos`，您应该会收到一个错误，指示数据库表不存在，这意味着应用程序现在成功连接和验证到数据库，但缺少应用程序所需的模式和表。
- en: Running jobs
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行作业
- en: Our **todobackend** application is almost fully functional, however there is
    one key deployment task that we need to perform, which is to run database migrations
    to ensure that the correct schema and tables are present in the **todobackend**
    database. As you have seen throughout this book, database migrations should only
    be executed once per deployment, regardless of the number of instances running
    of our application. Kubernetes supports tasks of this nature via a special controller
    of type *job*, which as the name suggests, runs a task or process (in the form
    of a pod) until the job completes successfully.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的**todobackend**应用程序几乎完全功能，但是有一个关键的部署任务，我们需要执行，那就是运行数据库迁移，以确保**todobackend**数据库中存在正确的模式和表。正如您在本书中所看到的，数据库迁移应该在每次部署时只执行一次，而不管我们的应用程序运行的实例数量。Kubernetes通过一种特殊类型的控制器*job*支持这种性质的任务，正如其名称所示，运行一个任务或进程（以pod的形式）直到作业成功完成。
- en: 'To create a job for the required database migrations task, we will create a
    new file called `k8s/app/migrations.yaml` that''s located in the `todobackend`
    repository, which allows you to run the job independently of the other application
    resources that are defined in the co-located `deployment.yaml` file:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建所需的数据库迁移任务作业，我们将创建一个名为`k8s/app/migrations.yaml`的新文件，该文件位于`todobackend`存储库中，这样可以独立于在同一位置定义的`deployment.yaml`文件中的其他应用程序资源来运行作业。
- en: '[PRE36]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You must specify a kind of `Job` to configure this resource as a job, and for
    the most part, the configuration is very similar to the pod/deployment template
    we created earlier, except for the `spec.backoffLimit` property, which defines
    how many times Kubernetes should attempt to re-run the job should it fail, and
    the template `spec.restartPolicy` property, which should always be set to `Never`
    for a job.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须指定一种`Job`的类型来配置此资源作为作业，大部分情况下，配置与我们之前创建的pod/deployment模板非常相似，除了`spec.backoffLimit`属性，它定义了Kubernetes在失败时应尝试重新运行作业的次数，以及模板`spec.restartPolicy`属性，它应始终设置为`Never`以用于作业。
- en: 'If you now run the job, you should be to verify that database migrations ran
    successfully:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在运行作业，您应该能够验证数据库迁移是否成功运行：
- en: '[PRE37]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: At this point, you have successfully deployed the todobackend application in
    a fully functional state, and you should be able to connect to the todobackend
    application and create, update, and delete todo items.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您已经成功地部署了todobackend应用程序，处于完全功能状态，您应该能够连接到todobackend应用程序，并创建、更新和删除待办事项。
- en: Creating an EKS cluster
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建EKS集群
- en: Now that you have a solid understanding of Kubernetes and have defined the core
    resources required to deploy and run the todobackend application locally, it is
    time to shift our attention to the Elastic Kubernetes Service (EKS).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经对Kubernetes有了扎实的了解，并且已经定义了部署和本地运行todobackend应用程序所需的核心资源，是时候将我们的注意力转向弹性Kubernetes服务（EKS）了。
- en: The core resource supported by EKS is the EKS cluster, which represents a fully
    managed, highly available cluster of Kubernetes managers that take care of the
    Kubernetes control plane for you. In this section, we will focus on creating an
    EKS cluster in AWS, establishing authentication and access to the cluster, and
    deploying the Kubernetes dashboard.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: EKS支持的核心资源是EKS集群，它代表了一个完全托管、高可用的Kubernetes管理器集群，为您处理Kubernetes控制平面。在本节中，我们将重点关注在AWS中创建EKS集群，建立对集群的认证和访问，并部署Kubernetes仪表板。
- en: 'Creating an EKS cluster consists of the following primary tasks:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 创建EKS集群包括以下主要任务：
- en: '**Install client components**: In order to manager your EKS cluster, you need
    to install various client components, including `kubectl` (which you have already
    installed) and the AWS IAM authenticator for Kubernetes tool.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装客户端组件：为了管理您的EKS集群，您需要安装各种客户端组件，包括`kubectl`（您已经安装了）和AWS IAM认证器用于Kubernetes工具。
- en: '**Create cluster resources**: This establishes the control plane component
    of Kubernetes, which consists of Kubernetes masters. When using EKS, the masters
    are provided as a fully managed service.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建集群资源：这建立了Kubernetes的控制平面组件，包括Kubernetes主节点。在使用EKS时，主节点作为一个完全托管的服务提供。
- en: '**Configure kubectl for EKS**: This allows you to manage EKS using your local
    kubectl client.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为EKS配置kubectl：这允许您使用本地kubectl客户端管理EKS。
- en: '**Create worker nodes**: This consists of Kubernetes nodes that are intended
    to run your container workloads. When using EKS, you are responsible for creating
    your own worker nodes, which you will typically deploy in the form of EC2 auto
    scaling groups. Just like for the ECS service, AWS provides an EKS-optimized AMI
    ([https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html))
    that includes all of the necessary software components for a worker node to join
    your EKS clusters.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建工作节点：这包括用于运行容器工作负载的Kubernetes节点。在使用EKS时，您需要负责创建自己的工作节点，通常会以EC2自动扩展组的形式部署。就像对于ECS服务一样，AWS提供了一个EKS优化的AMI（[https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html)），其中包括所有必要的软件组件，使工作节点能够加入您的EKS集群。
- en: '**Deploy the Kubernetes dashboard**: The Kubernetes dashboard provides you
    with a web-based management interface to manage and monitor your cluster and container
    applications.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Kubernetes仪表板：Kubernetes仪表板为您提供了一个基于Web的管理界面，用于管理和监视您的集群和容器应用程序。
- en: At the time of writing, EKS clusters are not part of the AWS free tier and cost
    $0.20 USD per minute to run, so bear this in mind before you continue (see [https://aws.amazon.com/eks/pricing/](https://aws.amazon.com/eks/pricing/)
    for latest pricing information). We will be using CloudFormation templates to
    deploy both the EKS cluster and EKS worker nodes, so you can easily tear down
    and recreate your EKS cluster and worker nodes as required to reduce costs.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，EKS集群不属于AWS免费套餐，并且每分钟收费0.20美元，因此在继续之前请记住这一点（请参阅[https://aws.amazon.com/eks/pricing/](https://aws.amazon.com/eks/pricing/)获取最新定价信息）。我们将使用CloudFormation模板来部署EKS集群和EKS工作节点，因此您可以根据需要轻松拆除和重新创建EKS集群和工作节点，以减少成本。
- en: Installing client components
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装客户端组件
- en: To manage your EKS cluster, you must have `kubectl` installed, as well as the
    AWS IAM authenticator for Kubernetes components, which allows `kubectl` to authenticate
    to your EKS cluster using your IAM credentials.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要管理您的EKS集群，您必须安装`kubectl`，以及AWS IAM认证器用于Kubernetes组件，它允许`kubectl`使用您的IAM凭据对您的EKS集群进行身份验证。
- en: 'You already have `kubectl` installed, so to install the AWS IAM authenticator
    for Kubernetes, you need to install a binary called `aws-iam-authenticator` that
    is published by AWS as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经安装了`kubectl`，因此要安装用于Kubernetes的AWS IAM认证器，您需要安装一个名为`aws-iam-authenticator`的二进制文件，该文件由AWS发布如下：
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Creating cluster resources
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群资源
- en: 'Before creating your an EKS cluster, you need to ensure that your AWS account
    meets the following prerequisites:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建您的EKS集群之前，您需要确保您的AWS账户满足以下先决条件：
- en: '**VPC resources**: EKS resources must be deployed into a VPC with a minimum
    of three subnets. AWS recommends that you create your own dedicated VPC and subnets
    per EKS cluster, however in this chapter we will use the default VPC and subnets
    that are automatically created in your AWS account. Note that when you create
    a VPC and define the subnets that your cluster will use, you must specify *all*
    subnets where you expect your worker nodes *and *load balancers will be deployed.
    A recommended pattern is to deploy your worker nodes in private subnets, and ensure
    that you have also included public subnets so that EKS can create public facing
    load balancers as required.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VPC资源**：EKS资源必须部署到具有至少三个子网的VPC中。AWS建议您为每个EKS集群创建自己的专用VPC和子网，但是在本章中，我们将使用在您的AWS账户中自动创建的默认VPC和子网。请注意，当您创建VPC并定义集群将使用的子网时，您必须指定*所有*子网，您期望您的工作节点*和*负载均衡器将被部署在其中。一个推荐的模式是在私有子网中部署您的工作节点，并确保您还包括了公共子网，以便EKS根据需要创建面向公众的负载均衡器。'
- en: '**EKS service role**: When creating an EKS cluster, you must specify an IAM
    role that grants access to the EKS service to manage your clusters.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EKS服务角色**：在创建EKS集群时，您必须指定一个IAM角色，该角色授予EKS服务管理您的集群的访问权限。'
- en: '**Control plane security group**: You must provide a security group that is
    used for control plane communications between your EKS cluster managers and worker
    nodes. The security group rules will be modified by the EKS service, so you should
    create a new, empty security group for this requirement.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面安全组**：您必须提供一个用于EKS集群管理器和工作节点之间的控制平面通信的安全组。安全组规则将由EKS服务修改，因此您应为此要求创建一个新的空安全组。'
- en: 'The AWS documentation includes a Getting Started ([https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html))
    section, which provides details on how to create EKS clusters using the AWS console.
    Given that EKS is supported by CloudFormation and the infrastructure as code approach
    we have used throughout this book, we need to create a folder called `eks` in
    the `todobackend-aws` repository and define our EKS cluster and the associated
    EKS service role in a new CloudFormation template file called `todobackend-aws/eks/stack.yml`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: AWS文档包括一个入门（[https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html](https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html)）部分，其中提供了如何使用AWS控制台创建EKS集群的详细信息。鉴于EKS受CloudFormation支持，并且我们在本书中一直使用的基础设施即代码方法，我们需要在`todobackend-aws`存储库中创建一个名为`eks`的文件夹，并在一个名为`todobackend-aws/eks/stack.yml`的新CloudFormation模板文件中定义我们的EKS集群和相关的EKS服务角色：
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The template requires two input parameters – the target VPC ID and target Subnet
    IDs. The `EksServiceRole` resource creates an IAM role that grants the `eks.awsamazon.com`
    service the ability to manage EKS clusters on your behalf, as specified by the
    managed policies referenced in the `ManagedPolicyArns` property. You must then
    define an empty security group for control plane communications, and finally define
    the EKS cluster resource, referencing the `EksServiceRole` resource for the `RoleArn`
    property, and defining a VPC configuration that targets the input `ApplicationSubnets`
    and uses the `EksClusterSecurityGroup` resource.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 模板需要两个输入参数 - 目标VPC ID和目标子网ID。`EksServiceRole`资源创建了一个IAM角色，授予`eks.awsamazon.com`服务代表您管理EKS集群的能力，如`ManagedPolicyArns`属性中引用的托管策略所指定的。然后，您必须为控制平面通信定义一个空安全组，并最后定义EKS集群资源，引用`EksServiceRole`资源的`RoleArn`属性，并定义一个针对输入`ApplicationSubnets`的VPC配置，并使用`EksClusterSecurityGroup`资源。
- en: 'You can now deploy this template using the `aws cloudformation deploy` command,
    as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用`aws cloudformation deploy`命令部署此模板，如下所示：
- en: '[PRE40]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The cluster will take approximately 10 minutes to create and, once created,
    you can use the AWS CLI to obtain further information about the cluster:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 集群将大约需要10分钟来创建，一旦创建完成，您可以使用AWS CLI获取有关集群的更多信息：
- en: '[PRE41]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The cluster endpoint and certificate authority data are both required for later
    on in this chapter, so take note of these values.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 集群端点和证书颁发机构数据在本章后面都是必需的，因此请注意这些值。
- en: Configuring kubectl for EKS
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为EKS配置kubectl
- en: With your EKS cluster created, you now need to add the new cluster to your local
    `kubectl` configuration. All clusters that `kubectl` is aware of are defined in
    a file called `~/.kube/config` by default, which at the moment will include a
    single cluster called `docker-for-desktop-cluster` if you are using Docker for
    Mac or Docker for Windows.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您创建的EKS集群，现在需要将新集群添加到本地的`kubectl`配置中。`kubectl`知道的所有集群默认都在一个名为`~/.kube/config`的文件中定义，目前如果您使用Docker
    for Mac或Docker for Windows，则该文件将包括一个名为`docker-for-desktop-cluster`的单个集群。
- en: 'The following code demonstrates adding your EKS cluster and associated configuration
    to the `~/.kube/config` file:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了将您的EKS集群和相关配置添加到`~/.kube/config`文件中：
- en: '[PRE42]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You must first add a new cluster called `eks-cluster` to the `clusters` property,
    specifying the certificate authority data and server endpoint you captured earlier
    after you created the EKS cluster. You must then add a context called `eks`, which
    will allow you to switch between your local Kubernetes server and your EKS cluster,
    and finally add a new user called `aws` to the users section which is used by
    the `eks` context to authenticate to your EKS cluster. The `aws` user configuration
    configures kubectl to execute the `aws-iam-authenticator` component you previously
    installed, passing the argument `token -i eks-cluster` and using your local `docker-in-aws`
    profile to authenticate access. Executing this command will automatically return
    an authentication token to `kubectl` that can then be used to authenticate to
    your EKS cluster.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在`clusters`属性中首先添加一个名为`eks-cluster`的新集群，指定您在创建EKS集群后捕获的证书颁发机构数据和服务器端点。然后添加一个名为`eks`的上下文，这将允许您在本地Kubernetes服务器和EKS集群之间切换，并最后在用户部分添加一个名为`aws`的新用户，该用户由`eks`上下文用于对EKS集群进行身份验证。`aws`用户配置配置kubectl执行您之前安装的`aws-iam-authenticator`组件，传递参数`token
    -i eks-cluster`，并使用您本地的`docker-in-aws`配置文件进行身份验证访问。执行此命令将自动返回一个身份验证令牌给`kubectl`，然后可以用于对EKS集群进行身份验证。
- en: 'With the preceding configuration in place, you should now be able to access
    a new context called `eks` and verify connectivity to your EKS cluster, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述配置就位后，您现在应该能够访问一个名为`eks`的新上下文，并验证连接到您的EKS集群，如下所示：
- en: '[PRE43]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that if you are using the **multi-factor authentication** (**MFA**) configuration
    we set up in earlier chapters, you will be prompted to enter an MFA token every
    single time you run a `kubectl` command against your EKS cluster, which will quickly
    become tiresome.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您在前几章中设置了**多因素身份验证**（**MFA**）配置，每次对您的EKS集群运行`kubectl`命令时，都会提示您输入MFA令牌，这将很快变得烦人。
- en: 'To disable MFA temporarily, you can remove your user account from the Users
    group using the `aws iam remove-user-from-group` command:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要暂时禁用MFA，您可以使用`aws iam remove-user-from-group`命令将用户帐户从用户组中移除：
- en: '[PRE44]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And then comment the `mfa_serial` line for your local AWS profile in the `~/.aws/config`
    file:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在`~/.aws/config`文件中为您的本地AWS配置文件注释掉`mfa_serial`行：
- en: '[PRE45]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Creating worker nodes
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建工作节点
- en: 'The next step in setting up EKS is creating worker nodes that will join your
    EKS cluster. Unlike the Kubernetes master nodes that are fully managed by AWS,
    you are responsible for creating and managing your worker nodes. AWS provide an
    EKS-optimized AMI that includes all of the software required to join an EKS cluster
    and operate as an EKS worker. You can browse to [https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html)
    to obtain the latest AMI ID for your region:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 设置EKS的下一步是创建将加入您的EKS集群的工作节点。与由AWS完全管理的Kubernetes主节点不同，您负责创建和管理您的工作节点。AWS提供了一个EKS优化的AMI，其中包含加入EKS集群并作为EKS工作节点运行所需的所有软件。您可以浏览[https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html)来获取您所在地区的最新AMI
    ID：
- en: '![](assets/75c0a503-604b-4852-bf2b-bc5234c54df1.png)Amazon EKS-Optimized AMI'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/75c0a503-604b-4852-bf2b-bc5234c54df1.png)Amazon EKS-Optimized AMI'
- en: 'At the time of writing this book, the EKS-Optimized AMI requires extensive
    configuration using the **cfn-init** framework that we learned about in earlier
    chapters. The recommended approach to create your worker nodes is to use a predefined
    CloudFormation template which is published by AWS that already includes the required
    configuration specified at [https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html](https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本书时，EKS-Optimized AMI需要使用我们在前几章中学到的**cfn-init**框架进行广泛配置。创建工作节点的推荐方法是使用由AWS发布的预定义CloudFormation模板，该模板已经包含了在[https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html](https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html)中指定的所需配置：
- en: '![](assets/86d253f0-e18a-494d-9ca2-56ee21b93408.png)Worker CloudFormation template
    URL'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/86d253f0-e18a-494d-9ca2-56ee21b93408.png)工作节点CloudFormation模板URL'
- en: 'You can now create a new CloudFormation stack for your worker nodes by selecting
    **Services** | **CloudFormation** in the AWS console, clicking the **Create Stack**
    button, and pasting the worker template URL you obtained previously in the **Choose
    a template** section:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以通过在AWS控制台中选择**服务** | **CloudFormation**，单击**创建堆栈**按钮，并粘贴您之前在**选择模板**部分获取的工作模板URL来为您的工作节点创建新的CloudFormation堆栈：
- en: '![](assets/cafa3d9c-5393-4aa9-9be5-7aaeec57b1ec.png)Creating a worker node
    CloudFormation stack'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/cafa3d9c-5393-4aa9-9be5-7aaeec57b1ec.png)创建工作节点CloudFormation堆栈'
- en: 'After clicking **Next**, you will be prompted to enter a stack name (you can
    specify a name of `eks-cluster-workers` or similar) and provide the following
    parameters:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**下一步**后，您将被提示输入堆栈名称（您可以指定一个类似`eks-cluster-workers`的名称）并提供以下参数：
- en: '**ClusterName**: Specifies the name of your EKS cluster (`eks-cluster`, in
    our example).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterName**：指定您的EKS集群的名称（在我们的示例中为`eks-cluster`）。'
- en: '**ClusterControlPlaneSecurityGroup**: The name of the control plane security
    group. In our example, we previously created a security group called `eks-cluster-control-plane-sg`
    when we created our EKS cluster.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterControlPlaneSecurityGroup**：控制平面安全组的名称。在我们的示例中，我们在创建EKS集群时先前创建了一个名为`eks-cluster-control-plane-sg`的安全组。'
- en: '**NodeGroupName**: This defines part of the name of the EC2 auto scaling group
    that will be created for your workers. For our scenario, you can specify a name
    of `eks-cluster-workers` or similar.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeGroupName**：这定义了将为您的工作节点创建的EC2自动扩展组名称的一部分。对于我们的情况，您可以指定一个名为`eks-cluster-workers`或类似的名称。'
- en: '**NodeAutoScalingGroupMinSize** **and** **NodeAutoScalingGroupMaxSize**: By,
    default these are set to 1 and 3, respectively. Note that the CloudFormation template
    sets the desired size of the auto scaling group to the value of the `NodeAutoScalingGroupMaxSize`
    parameter, so you may want to lower this value.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeAutoScalingGroupMinSize**和**NodeAutoScalingGroupMaxSize**：默认情况下，分别设置为1和3。请注意，CloudFormation模板将自动缩放组的期望大小设置为`NodeAutoScalingGroupMaxSize`参数的值，因此您可能希望降低此值。'
- en: '**NodeInstanceType**: The smallest instance type you can specify using the
    predefined worker CloudFormation template is `t2.small`. For EKS, the node instance
    type is not just important in terms of CPU and memory resources, but also has
    implications on pod capacity in terms on networking requirements. The EKS networking
    model ([https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html](https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html))
    exposes each pod in your EKS cluster as an IP address that''s reachable within
    your VPC, using a combination of elastic network interfaces (ENI) and secondary
    IP addresses running on each ENI. You can refer to [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI),
    which describes the maximum number of ENIs and secondary IP addresses per interface
    for the various EC2 instance types and ultimately determines the maximum number
    of pods you can run per node.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeInstanceType**：您可以使用预定义的工作节点CloudFormation模板指定的最小实例类型是`t2.small`。对于EKS，节点实例类型不仅在CPU和内存资源方面很重要，而且还对网络要求的Pod容量有影响。EKS网络模型（[https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html](https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html)）将EKS集群中的每个Pod公开为可在您的VPC内访问的IP地址，使用弹性网络接口（ENI）和运行在每个ENI上的次要IP地址的组合。您可以参考[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI)，其中描述了各种EC2实例类型的每个接口的最大ENI和次要IP地址的数量，并最终确定了每个节点可以运行的最大Pod数量。'
- en: '**NodeImageId**: Specifies the ID of the EKS-Optimized AMI for your region
    (see the previous screenshots).'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeImageId**：指定您所在地区的EKS优化AMI的ID（请参阅上面的截图）。'
- en: '**KeyName**: Specifies an existing EC2 key pair in your account (for example,
    the admin keypair you created earlier in this book).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KeyName**：指定您帐户中现有的EC2密钥对（例如，您在本书中之前创建的管理员密钥对）。'
- en: '**VpcId**: Specifies the VPC ID where your EKS cluster is located.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VpcId**：指定您的EKS集群所在的VPC ID。'
- en: '**Subnets**: Specifies the subnets where you would like to place your workers.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Subnets**：指定您希望放置工作节点的子网。'
- en: 'Once you have configured the various parameters that are required, click on
    the **Next** button twice and finally acknowledge that CloudFormation may create
    IAM resources before clicking the **Create** button to deploy your worker nodes.
    When your stack has been created successfully, open the **Outputs** tab for the
    stack and take a note of the `NodeInstanceRole` output, which is required for
    the next configuration step:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您配置了所需的各种参数，点击**下一步**按钮两次，最后确认CloudFormation可能会在点击**创建**按钮之前创建IAM资源，以部署您的工作节点。当您的堆栈成功创建后，打开堆栈的**输出**选项卡，并记录`NodeInstanceRole`输出，这是下一个配置步骤所需的：
- en: '![](assets/b95ee581-3246-49f3-89ed-40a29347f66e.png)Obtaining the NodeInstanceRole
    output'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/b95ee581-3246-49f3-89ed-40a29347f66e.png)获取NodeInstanceRole输出'
- en: Joining worker nodes to your EKS cluster
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将工作节点加入您的EKS集群
- en: After the CloudFormation stack has deployed successfully, your worker nodes
    will attempt to join your cluster, however before they can do this you need to
    grant access to the EC2 instance role of your worker nodes by applying an AWS
    authenticator `ConfigMap` resource called `aws-auth` to your cluster.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: CloudFormation堆栈成功部署后，您的工作节点将尝试加入您的集群，但是在此之前，您需要通过将名为`aws-auth`的AWS认证器`ConfigMap`资源应用到您的集群来授予对工作节点的EC2实例角色的访问权限。
- en: A ConfigMap is simply a key/value data structure used to store configuration
    data that can be used by different resources in your cluster. The `aws-auth` ConfigMap
    is used by EKS to grant AWS users the ability to interact with your cluster, which
    you can read more about at [https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html).
    You can also download a sample `aws-auth` ConfigMap from [https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/aws-auth-cm.yaml](https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/aws-auth-cm.yaml).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ConfigMap只是一个键/值数据结构，用于存储配置数据，可以被集群中的不同资源使用。 `aws-auth` ConfigMap被EKS用于授予AWS用户与您的集群进行交互的能力，您可以在[https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html)上了解更多信息。您还可以从[https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/aws-auth-cm.yaml](https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-06-05/aws-auth-cm.yaml)下载一个示例`aws-auth`
    ConfigMap。
- en: 'To create the `aws-auth` ConfigMap, create a file called `aws-auth-cm.yaml`
    in the `todobackend-aws/eks` folder:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`aws-auth` ConfigMap， 在`todobackend-aws/eks`文件夹中创建一个名为`aws-auth-cm.yaml`的文件：
- en: '[PRE46]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In the preceding example, you need to paste the value of the `NodeInstanceRole`
    output you obtained when you created your workers CloudFormation stack. Once you
    have created this file, you can now apply it to your EKS cluster using the `kubectl
    apply` command, and then wait for your worker nodes to join the cluster by running `kubectl
    get nodes --watch`:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，您需要粘贴在创建工作节点CloudFormation堆栈时获得的`NodeInstanceRole`输出的值。创建此文件后，您现在可以使用`kubectl
    apply`命令将其应用到您的EKS集群，然后通过运行`kubectl get nodes --watch`等待您的工作节点加入集群：
- en: '[PRE47]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Once all of your workers have a status of `Ready`, you have successfully joined
    your worker nodes to your EKS cluster.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的所有工作节点的状态都为`Ready`，您已成功将工作节点加入您的EKS集群。
- en: Deploying the Kubernetes dashboard
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Kubernetes仪表板
- en: The final step in setting up your EKS cluster is to deploy the Kubernetes dashboard
    to your cluster. The Kubernetes dashboard is a powerful and comprehensive web-based
    management interface for managing and monitoring your cluster and container applications,
    and is deployed as a container-based application within the `kube-system` namespace
    of your Kubernetes cluster. The dashboard consists of a number of components that
    I won't go into detail about here, but you can read more about the dashboard at [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 EKS 集群的最后一步是将 Kubernetes 仪表板部署到您的集群。Kubernetes 仪表板是一个功能强大且全面的基于 Web 的管理界面，用于管理和监视集群和容器应用程序，并部署为
    Kubernetes 集群的 `kube-system` 命名空间中的基于容器的应用程序。仪表板由许多组件组成，我在这里不会详细介绍，但您可以在 [https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard)
    上阅读更多关于仪表板的信息。
- en: 'To deploy the dashboard, we will first create a folder called `todobackend-aws/eks/dashboard` and
    proceed to download and apply the various components that comprise the dashboard
    to this folder:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署仪表板，我们将首先创建一个名为 `todobackend-aws/eks/dashboard` 的文件夹，并继续下载和应用组成该仪表板的各种组件到此文件夹：
- en: '[PRE48]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You then need to create a file called `eks-admin.yaml` that creates a service
    account and cluster role binding with full cluster-admin privileges:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要创建一个名为 `eks-admin.yaml` 的文件，该文件创建一个具有完整集群管理员特权的服务帐户和集群角色绑定：
- en: '[PRE49]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'After creating this file, you need to apply it to your EKS cluster:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 创建此文件后，您需要将其应用于您的 EKS 集群：
- en: '[PRE50]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'With the `eks-admin` service account in place, you can retrieve an authentication
    token for this account by running the following command:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 有了 `eks-admin` 服务帐户，您可以通过运行以下命令检索此帐户的身份验证令牌：
- en: '[PRE51]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The key piece of information in the preceding example is the token value, which
    you need to copy and paste when you connect to the dashboard. To connect to the
    dashboard, you need to start the kubectl proxy, which provides HTTP access to
    the Kubernetes API:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，关键信息是令牌值，连接到仪表板时需要复制和粘贴。要连接到仪表板，您需要启动 kubectl 代理，该代理提供对 Kubernetes API
    的 HTTP 访问：
- en: '[PRE52]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If you now browse to `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`,
    you will be prompted to sign in to the dashboard, where you need to paste the
    token that you retrieved previously for the `eks-admin` service account:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在浏览到 `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`，您将被提示登录到仪表板，您需要粘贴之前为
    `eks-admin` 服务帐户检索的令牌：
- en: '![](assets/8315bcd4-56a5-44a0-b7a5-52bee1895ec2.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8315bcd4-56a5-44a0-b7a5-52bee1895ec2.png)'
- en: Signing in to the Kubernetes dashboard
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 登录 Kubernetes 仪表板
- en: 'Once you have signed in, if you change the Namespace to **kube-system** and
    select **Workloads** | **Deployments**, it is possible that you may be shown an
    error indicating that the image for the **monitoring-influxdb** deployment could
    not be found:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您登录，如果将 Namespace 更改为 **kube-system** 并选择 **Workloads** | **Deployments**，可能会显示一个错误，指示找不到
    **monitoring-influxdb** 部署的图像：
- en: '![](assets/c029f6d5-044b-49cc-99ad-afb86d5b0fbd.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c029f6d5-044b-49cc-99ad-afb86d5b0fbd.png)'
- en: Kubernetes dashboard deployment failure
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 仪表板部署失败
- en: 'If this is the case, you will need to update the `todobackend-aws/eks/dashboard/influxdb.yml`
    file that you downloaded earlier to reference `k8s.gcr.io/heapster-influxdb-amd64:v1.3.3`
    (this is a known issue (`https://github.com/kubernetes/heapster/issues/2059`))
    that may or may not exist when you are reading this chapter):'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这种情况，您需要更新之前下载的 `todobackend-aws/eks/dashboard/influxdb.yml` 文件，以引用 `k8s.gcr.io/heapster-influxdb-amd64:v1.3.3`（这是一个已知问题(`https://github.com/kubernetes/heapster/issues/2059`）可能在您阅读本章时存在或不存在）：
- en: '[PRE53]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: If you now re-apply the file by running `kubectl apply -f influxdb.yml`, the
    dashboard should now show all services running as expected.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在通过运行`kubectl apply -f influxdb.yml`重新应用文件，则仪表板应该显示所有服务都按预期运行。
- en: Deploying the sample application to EKS
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将示例应用程序部署到EKS
- en: Now that our EKS cluster and worker nodes are in place and we have confirmed
    that we can deploy to the cluster, it's time to deploy the todobackend application
    to EKS. You have already performed the majority of the hard work earlier when
    you defined the various resources required to run your application locally in
    Kubernetes, and all that is required is to adapt some of the external resources
    such as the load balancer and persistent volume for the database service to use
    AWS native services.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的EKS集群和工作节点已经就位，并且我们已经确认可以向集群部署，是时候将todobackend应用程序部署到EKS了。在本地定义了在Kubernetes中运行应用程序所需的各种资源时，您已经在之前完成了大部分艰苦的工作，现在所需的只是调整一些外部资源，例如负载均衡器和数据库服务的持久卷，以使用AWS原生服务。
- en: 'You are now required to perform the following configuration tasks:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要执行以下配置任务：
- en: Configuring support for persistent volumes using the AWS Elastic Block Store
    (EBS)
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS Elastic Block Store（EBS）配置持久卷支持
- en: Configuring support for AWS Elastic Load Balancers
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置支持AWS Elastic Load Balancers
- en: Deploying the sample application
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署示例应用程序
- en: Configuring support for persistent volumes using AWS EBS
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS EBS配置持久卷支持
- en: Earlier in this chapter, we discussed the concepts of persistent volume claims
    and storage classes, which allow you to abstract the details of your storage infrastructure
    away from your applications. We learned that when using Docker Desktop, a default
    storage class is provided that will automatically create persistent volumes of
    type hostPath that are accessible from your local operating system at `~/.docker/Volumes`,
    which makes it easy to provision, manage, and maintain persistent volumes when
    using Docker Desktop with Kubernetes.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们讨论了持久卷索赔和存储类的概念，这使您可以将存储基础设施的细节与应用程序分离。我们了解到，在使用Docker Desktop时，提供了一个默认的存储类，它将自动创建类型为hostPath的持久卷，这些持久卷可以从本地操作系统的`~/.docker/Volumes`访问，这样在使用Docker
    Desktop与Kubernetes时就可以轻松地提供、管理和维护持久卷。
- en: When using EKS, it is important to understand that unlike Docker Desktop, by
    default, no storage classes are created for you. This requires you to create at
    least one storage class if you want to support persistent volume claims, and in
    most use cases, you would typically define a default storage class that provides
    a standard default storage medium and volume type for your cluster. When using
    EKS, a good candidate for these storage classes is the Elastic Block Store (EBS),
    which provides a standard integrated mechanism to support block-based volume storage
    for the EC2 instances that run as worker nodes in your cluster. Kubernetes supports
    a volume type called `AWSElasticBlockStore`, which allows you to access and mount
    EBS volumes from your worker nodes, and also includes support for a storage provisioner
    called `aws-ebs`, which provides dynamic provisioning and management of EBS volumes.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用EKS时，重要的是要了解，默认情况下，不会为您创建任何存储类。这要求您至少创建一个存储类，如果要支持持久卷索赔，并且在大多数用例中，您通常会定义一个提供标准默认存储介质和卷类型的默认存储类，以支持您的集群。在使用EKS时，这些存储类的一个很好的候选者是弹性块存储（EBS），它为在集群中作为工作节点运行的EC2实例提供了一种标准的集成机制来支持基于块的卷存储。Kubernetes支持一种名为`AWSElasticBlockStore`的卷类型，它允许您从工作节点访问和挂载EBS卷，并且还包括对名为`aws-ebs`的存储供应商的支持，该供应商提供EBS卷的动态提供和管理。
- en: 'With this native support for AWS EBS included out of the box, it is very easy
    to create a default storage class that will automatically provision EBS storage,
    which we will define in a file called `todobackend-aws/eks/gp2-storage-class.yaml`:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个原生支持AWS EBS的基础上，非常容易创建一个默认的存储类，它将自动提供EBS存储，我们将在名为`todobackend-aws/eks/gp2-storage-class.yaml`的文件中定义它。
- en: '[PRE54]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We will create a storage class called `gp2`, which, as the name suggests, will
    provision EBS storage of type `gp2`, or SSD, from AWS, using the `kubernetes.io/aws-ebs`
    storage provisioner. The `parameters` section controls this storage selection,
    and depending on the type of storage, there may be other configuration options
    available, which you can read more about at [https://kubernetes.io/docs/concepts/storage/storage-classes/#aws](https://kubernetes.io/docs/concepts/storage/storage-classes/#aws).
    The value for `reclaimPolicy` can be either `Retain` or `Delete`, which controls
    whether or not the storage provisioner should retain or delete the associated
    EBS volume whenever a persistent volume claim associated with the storage class
    is deleted from Kubernetes. For production use cases, you would typically set
    this to `Retain`, but for non-production environments, you may want to set this
    to the default reclaim policy of `Delete` to save you from having to manually
    clean up EBS volumes that are no longer used by your cluster.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个名为`gp2`的存储类，顾名思义，它将使用`kubernetes.io/aws-ebs`存储供应程序从AWS提供`gp2`类型或SSD的EBS存储。`parameters`部分控制此存储选择，根据存储类型，可能有其他配置选项可用，您可以在[https://kubernetes.io/docs/concepts/storage/storage-classes/#aws](https://kubernetes.io/docs/concepts/storage/storage-classes/#aws)了解更多信息。`reclaimPolicy`的值可以是`Retain`或`Delete`，它控制存储供应程序在从Kubernetes中删除与存储类关联的持久卷索赔时是否保留或删除关联的EBS卷。对于生产用例，您通常会将其设置为`Retain`，但对于非生产环境，您可能希望将其设置为默认的回收策略`Delete`，以免手动清理不再被集群使用的EBS卷。
- en: 'Now, let''s create this storage class in our EKS cluster, after which we can
    configure the new storage class to be the default storage class for the cluster:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在我们的EKS集群中创建这个存储类，之后我们可以配置新的存储类为集群的默认存储类。
- en: '[PRE55]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: After creating the storage class, you use the `kubectl patch` command to add
    an annotation to the storage class, which configures the class as the default
    class. You can see that when you run the `kubectl describe sc/gp2` command to
    view details about the storage class, the `IsDefaultClass` attribute is set to
    `Yes`, confirming that the newly created class is the default storage class for
    the cluster.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 创建存储类后，您可以使用`kubectl patch`命令向存储类添加注释，将该类配置为默认类。当您运行`kubectl describe sc/gp2`命令查看存储类的详细信息时，您会看到`IsDefaultClass`属性设置为`Yes`，确认新创建的类是集群的默认存储类。
- en: With this in place, the Kubernetes configuration for the **todobackend** application
    now has a default storage class that can be applied for the `todobackend-data`
    persistent volume claim, which will provision an EBS volume of type `gp2` based
    upon the storage class parameters.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，**todobackend**应用程序的Kubernetes配置现在有了一个默认的存储类，可以应用于`todobackend-data`持久卷索赔，它将根据存储类参数提供一个`gp2`类型的EBS卷。
- en: The `eksServiceRole` IAM role that you created earlier in this chapter includes
    the `AmazonEKSClusterPolicy` managed policy, which grants your EKS cluster the
    ability to manage EBS volumes. If you choose to implement your own custom IAM
    policies for the EKS service role, you must ensure that you include the various
    EC2 IAM permissions for managing volumes, such as `ec2:AttachVolume`, `ec2:DetachVolume`,
    `ec2:CreateVolumes`, `ec2:DeleteVolumes`, `ec2:DescribeVolumes`, and `ec2:ModifyVolumes`
    (this is not an exhaustive list). See [https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html](https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html)
    for details on the full list of IAM permissions that are granted by AWS-defined
    EKS service roles and managed policies.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面创建的`eksServiceRole` IAM角色包括`AmazonEKSClusterPolicy`托管策略，该策略授予您的EKS集群管理EBS卷的能力。如果您选择为EKS服务角色实现自定义IAM策略，您必须确保包括用于管理卷的各种EC2
    IAM权限，例如`ec2:AttachVolume`、`ec2:DetachVolume`、`ec2:CreateVolumes`、`ec2:DeleteVolumes`、`ec2:DescribeVolumes`和`ec2:ModifyVolumes`（这不是详尽的清单）。有关由AWS定义的EKS服务角色和托管策略授予的IAM权限的完整清单，请参阅[https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html](https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html)。
- en: Configuring support for AWS Elastic Load Balancers
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置对AWS弹性负载均衡器的支持
- en: Earlier in this chapter, when you defined your Kubernetes configuration for
    the todobackend application, you created a service for the todobackend application
    of type `LoadBalancer`. We discussed that the implementation details of the load
    balancer are specific to the platform that your Kubernetes cluster is deployed
    to, and in the case of Docker Desktop, Docker provides their own load balancer
    component that allows the service to be exposed to the local network interface
    on your development machine.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，当您为todobackend应用程序定义Kubernetes配置时，您创建了一个类型为`LoadBalancer`的todobackend应用程序的服务。我们讨论了负载均衡器的实现细节是特定于部署到的Kubernetes集群的平台的，并且在Docker
    Desktop的情况下，Docker提供了自己的负载均衡器组件，允许服务暴露给开发机器上的本地网络接口。
- en: When using EKS, the good news is that you don't need to do anything to support
    services of type `LoadBalancer` – your EKS cluster will automatically create and
    associate an AWS Elastic Load Balancer with each service endpoint, with the `AmazonEKSClusterPolicy`
    managed policy granting the required IAM permissions for this.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用EKS时，好消息是您不需要做任何事情来支持`LoadBalancer`类型的服务 - 您的EKS集群将自动为每个服务端点创建并关联一个AWS弹性负载均衡器，`AmazonEKSClusterPolicy`托管策略授予了所需的IAM权限。
- en: 'Kubernetes does allow you configure vendor-specific features of the `LoadBalancer`
    type by configuring *annotations*, which are a metadata property that will be
    understood by a given vendor on their target platform and ignored if deploying
    on a different platform, such as your local Docker Desktop environment. You can
    read more about these annotations at [https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types),
    and the following example demonstrates adding several annotations that are specific
    to the AWS Elastic Load Balancer to the service definition within the `todobackend/k8s/app/deployment.yaml`
    file:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes确实允许您通过配置*注释*来配置`LoadBalancer`类型的供应商特定功能，这是一种元数据属性，将被给定供应商在其目标平台上理解，并且如果在不同平台上部署，比如您的本地Docker
    Desktop环境，将被忽略。您可以在[https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)了解更多关于这些注释的信息，以下示例演示了向`todobackend/k8s/app/deployment.yaml`文件中的服务定义添加了几个特定于AWS弹性负载均衡器的注释：
- en: '[PRE56]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In the preceding example, we added the following annotations:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们添加了以下注释：
- en: '`service.beta.kubernetes.io/aws-load-balancer-backend-protocol`: This configures
    the backend protocol. A value of `http` ensures that the `X-Forward-For` header
    is set on incoming requests so that your web applications can track client IP
    addresses.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.beta.kubernetes.io/aws-load-balancer-backend-protocol`: 这将配置后端协议。值为`http`可确保在传入请求上设置`X-Forward-For`标头，以便您的Web应用程序可以跟踪客户端IP地址。'
- en: '`service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled`:
    This enables connection draining.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled`:
    这将启用连接排空。'
- en: '`service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout`:
    This specifies the connection draining timeout.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout`:
    这指定了连接排空超时。'
- en: One important point to note is that the annotations expect every value to be
    a string value, so ensure that you quote boolean values such as `"true"` and `"false"`,
    as well as any numeric values such as `"60"`, as demonstrated in the preceding
    code.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的要点是，注释期望每个值都是字符串值，因此请确保引用布尔值，如`"true"`和`"false"`，以及任何数值，如`"60"`，如前面的代码所示。
- en: Deploying the sample application
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署示例应用程序
- en: 'You are now ready to deploy the sample application to AWS, which you can do
    by first switching over to the todobackend repository and ensuring that use are
    using the `eks` context you created earlier in this chapter:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以准备将示例应用程序部署到AWS，首先切换到todobackend存储库，并确保您正在使用本章前面创建的`eks`上下文：
- en: '[PRE57]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Creating secrets
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建秘密
- en: 'Recall that both the application and database services rely on secrets that
    we manually created in our local Docker Desktop context, so you first need to
    create these secrets in your EKS context:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，应用程序和数据库服务都依赖于我们在本地Docker Desktop上手动创建的秘密，因此您首先需要在EKS上下文中创建这些秘密：
- en: '[PRE58]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Deploying the database service
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署数据库服务
- en: 'You can now deploy the database service, which should create a new persistent
    volume backed by EBS as per the configuration of the default storage class you
    created earlier:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以部署数据库服务，这应该根据您之前创建的默认存储类的配置创建一个新的由EBS支持的持久卷：
- en: '[PRE59]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You can see that a persistent volume was created, and if you browse to **Services**
    | **EC2** in the AWS console and select **Volumes** from the left ELASTIC BLOCK
    STORAGE menu, you should be able to see a corresponding EBS volume for the persistent
    value:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到已创建了持久卷，如果您在AWS控制台中浏览**服务** | **EC2**并从左侧ELASTIC BLOCK STORAGE菜单中选择**卷**，您应该能够看到持久值的相应EBS卷：
- en: '![](assets/30dc1bb5-7f02-4b95-9203-d31dc5359383.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/30dc1bb5-7f02-4b95-9203-d31dc5359383.png)'
- en: Viewing EBS volumes
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 查看EBS卷
- en: Notice that Kubernetes tags the EBS volume with a number of tags that allow
    easy identification of which persistent volume and persistent volume claim a given
    EBS volume it is associated with.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Kubernetes使用多个标签标记EBS卷，以便轻松识别与给定EBS卷关联的哪个持久卷和持久卷索赔。
- en: In the Kubernetes dashboard, you can verify that the `todobackend-db` deployment
    is running by selecting **Workloads** | **Deployments:**
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes仪表板中，您可以通过选择**工作负载** | **部署**来验证`todobackend-db`部署是否正在运行：
- en: '![](assets/112c5cad-3258-4358-b836-c1fe352f41fc.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/112c5cad-3258-4358-b836-c1fe352f41fc.png)'
- en: Viewing EBS volumes
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 查看EBS卷
- en: Deploying the application service
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署应用程序服务
- en: 'With the database service in place, you can now proceed to deploy the application:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 有了数据库服务，现在可以继续部署应用程序：
- en: '[PRE60]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Deploying the application will perform the following tasks:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 部署应用程序将执行以下任务：
- en: Create the `todobackend-migrate` job, which runs database migrations
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建`todobackend-migrate`作业，运行数据库迁移
- en: Create the `todobackend` deployment, which runs a collectstatic initContainer
    and then runs the main todobackend application container
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 `todobackend` 部署，其中运行一个 collectstatic initContainer，然后运行主要的 todobackend 应用程序容器
- en: Create the `todobackend` service, which will deploy a new service with an AWS
    ELB frontend
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 `todobackend` 服务，将部署一个带有 AWS ELB 前端的新服务
- en: 'In the Kubernetes dashboard, if you select **Discovery and Load Balancing**
    | **Services** and select the **todobackend** service, you can view each of the
    internal endpoints for the service, as well as the external load balancer endpoint:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 仪表板中，如果选择 **发现和负载均衡** | **服务** 并选择 **todobackend** 服务，您可以查看服务的每个内部端点，以及外部负载均衡器端点：
- en: '![](assets/caeb39a5-d2a2-4864-b707-b2822a511d4f.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/caeb39a5-d2a2-4864-b707-b2822a511d4f.png)'
- en: Viewing the todobackend service in the Kubernetes dashboardYou can also obtain
    the external endpoint URL by running the `kubectl describe svc/todobackend` command.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 仪表板中查看 todobackend 服务您还可以通过运行 `kubectl describe svc/todobackend`
    命令来获取外部端点 URL。
- en: 'If you click on the external endpoint URL, you should be able to verify that
    the todobackend application is fully functional, with all static content being
    displayed correctly and the ability to add, remove, and update Todo items in the
    application database:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您单击外部端点 URL，您应该能够验证 todobackend 应用程序是完全功能的，所有静态内容都正确显示，并且能够在应用程序数据库中添加、删除和更新待办事项项目：
- en: '![](assets/15a03619-bdd8-4854-875c-955fbad7f459.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/15a03619-bdd8-4854-875c-955fbad7f459.png)'
- en: Verifying the todobackend application
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 验证 todobackend 应用程序
- en: Tearing down down the sample application
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拆除示例应用程序
- en: 'Tearing down the sample application is very simple, as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 拆除示例应用程序非常简单，如下所示：
- en: '[PRE61]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Once this has been completed, you should be able to verify that the Elastic
    Load Balancer resource associated with the todobackend service has been deleted,
    along with the EBS volume for the todobackend database, given that you configured
    the reclaim policy for the default storage class as Delete. Of course you should
    also delete the worker node stack and EKS cluster stack you created earlier in
    this chapter, to avoid unnecessary charges.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您应该能够验证与 todobackend 服务关联的弹性负载均衡器资源已被删除，以及 todobackend 数据库的 EBS 卷已被删除，因为您将默认存储类的回收策略配置为删除。当然，您还应该删除本章前面创建的工作节点堆栈和
    EKS 集群堆栈，以避免不必要的费用。
- en: Summary
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to deploy Docker applications using Kubernetes
    and the AWS Elastic Kubernetes Service (EKS). Kubernetes has established itself
    as one of the leading container management platforms with a strong open source
    community, and with AWS now supporting Kubernetes customers with the EKS service,
    Kubernetes is certain to grow even more in popularity.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用 Kubernetes 和 AWS 弹性 Kubernetes 服务 (EKS) 部署 Docker 应用程序。Kubernetes
    已经成为了领先的容器管理平台之一，拥有强大的开源社区，而且现在 AWS 支持 Kubernetes 客户使用 EKS 服务，Kubernetes 肯定会更受欢迎。
- en: You first learned how to leverage the native support for Kubernetes in Docker
    Desktop, which makes it very easy to get up and running with Kubernetes locally.
    You learned how to create a variety of core Kubernetes resources including pods,
    deployments, services, secrets, and jobs, which provide the fundamental building
    blocks for running your applications in Kubernetes. You also learned how to configure
    support for persistent storage, leveraging persistent volume claims to abstract
    the application's storage requirements away from the underlying storage engine.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 您首先学会了如何在Docker Desktop中利用Kubernetes的本机支持，这使得在本地快速启动和运行Kubernetes变得非常容易。您学会了如何创建各种核心Kubernetes资源，包括pod、部署、服务、秘密和作业，这些为在Kubernetes中运行应用程序提供了基本的构建块。您还学会了如何配置对持久存储的支持，利用持久卷索赔来将应用程序的存储需求与底层存储引擎分离。
- en: You were then introduced to EKS and learned how to create an EKS cluster and
    associated supporting resources, including an EC2 auto scaling group that runs
    your worker nodes. You established access to the EKS cluster, and tested that
    the cluster was working correctly by deploying the Kubernetes dashboard, which
    provides a rich and powerful management user interface for your cluster.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您了解了EKS，并学会了如何创建EKS集群以及相关的支持资源，包括运行工作节点的EC2自动扩展组。您建立了对EKS集群的访问，并通过部署Kubernetes仪表板来测试集群是否正常工作，该仪表板为您的集群提供了丰富而强大的管理用户界面。
- en: Finally, you proceeded to deploy the todobackend application to EKS, which included
    integration with the AWS Elastic Load Balancer (ELB) service for external connectivity
    and the Elastic Block Store (EBS) for providing persist storage. An important
    consideration here is that we did not need to modify the Kubernetes configuration
    we created earlier when deploying locally to our Docker Desktop environment, other
    than adding a few annotations to control the configuration of todobackend service
    load balancer (these annotations are ignored when using Docker Desktop, so are
    considered "safe" vendor-specific configuration elements). You should always strive
    for this goal, as it ensures that your applications will have maximum portability
    across different Kubernetes environments, and can be easily deployed independently
    of the underlying Kubernetes platform, whether it be a local development environment,
    AWS EKS, or the Google Kubernetes Engine (GKE).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您开始部署todobackend应用程序到EKS，其中包括与AWS Elastic Load Balancer（ELB）服务集成以进行外部连接，以及使用Elastic
    Block Store（EBS）提供持久存储。这里的一个重要考虑因素是，当在Docker Desktop环境中部署时，我们不需要修改我们之前创建的Kubernetes配置，除了添加一些注释以控制todobackend服务负载均衡器的配置（在使用Docker
    Desktop时会忽略这些注释，因此被视为“安全”的特定于供应商的配置元素）。您应该始终努力实现这个目标，因为这确保了您的应用程序在不同的Kubernetes环境中具有最大的可移植性，并且可以轻松地独立部署，而不受基础Kubernetes平台的影响，无论是本地开发环境、AWS
    EKS还是Google Kubernetes Engine（GKE）。
- en: Well, all good things must come to an end, and it's time for me to say congratulations
    and thank you for completing this book! It was a great pleasure to write this
    book and I hope that you have learned how to leverage the power of Docker and
    AWS to test, build, deploy and operate your own container applications.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，所有美好的事情都必须结束了，现在是时候恭喜并感谢您完成了这本书！写这本书是一件非常愉快的事情，我希望您已经学会了如何利用Docker和AWS的力量来测试、构建、部署和操作自己的容器应用程序。
- en: Questions
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'True/false: Kubernetes is a native feature of Docker Desktop CE.'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'True/false: Kubernetes is a native feature of Docker Desktop CE.'
- en: You define a custom command string to run in a pod definition using the commands
    property, and notice that the entrypoint script container is no longer executed.
    How can you resolve this?
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用commands属性在pod定义中定义自定义命令字符串，并注意到entrypoint脚本容器不再被执行。您如何解决这个问题？
- en: 'True/false: Kubernetes includes three node types – manager, worker, and agent.'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确/错误：Kubernetes包括三种节点类型-管理节点、工作节点和代理节点。
- en: 'True/false: Kubernetes provides integration with AWS application load balancers.'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确/错误：Kubernetes提供与AWS应用负载均衡器的集成。
- en: 'True/false: Kubernetes supports relocating EBS volumes to other nodes in your
    cluster.'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确/错误：Kubernetes支持将EBS卷重新定位到集群中的其他节点。
- en: What component can you use to expose the Kubernetes API to web applications?
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用哪个组件将Kubernetes API暴露给Web应用程序？
- en: 'True/false: Kubernetes supports integration with the Elastic Container Registry.'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确/错误：Kubernetes支持与弹性容器注册表的集成。
- en: What Kubernetes resource provides a virtual IP address that can be used to connect
    to multiple instances of a given application?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么Kubernetes资源提供可用于连接到给定应用程序的多个实例的虚拟IP地址？
- en: What Kubernetes resource is suitable for running database migrations?
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么Kubernetes资源适合运行数据库迁移？
- en: 'True/false: EKS manages both Kubernetes manager nodes and worker nodes.'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确/错误：EKS管理Kubernetes管理节点和工作节点。
- en: What type of EBS storage does the default storage class provision when using
    EKS?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用EKS时，默认存储类提供什么类型的EBS存储？
- en: You want to run a task every time you deploy a pod that needs to run before
    starting the main application in the pod. How would you achieve this?
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您想在每次部署需要在启动Pod中的主应用程序之前运行的任务。您将如何实现这一点？
- en: Further reading
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can check the following links for more information about the topics covered
    in this chapter:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看以下链接，了解本章涵盖的主题的更多信息：
- en: What is Kubernetes?: [https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Kubernetes？：[https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)
- en: Kubernetes Tutorials: [https://kubernetes.io/docs/tutorials/](https://kubernetes.io/docs/tutorials/)
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes教程：[https://kubernetes.io/docs/tutorials/](https://kubernetes.io/docs/tutorials/)
- en: Kubernetes Pods: [https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes Pods：[https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)
- en: Kubernetes Deployments: [https://kubernetes.io/docs/concepts/workloads/controllers/deployment/](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes部署：[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
- en: Kubernetes Jobs: [https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/)
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes作业：[https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/)
- en: Kubernetes Services: [https://kubernetes.io/docs/concepts/services-networking/service/](https://kubernetes.io/docs/concepts/services-networking/service/)
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes服务：[https://kubernetes.io/docs/concepts/services-networking/service/](https://kubernetes.io/docs/concepts/services-networking/service/)
- en: DNS for Services and Pods: [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务和Pod的DNS：[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
- en: Kubernetes Secrets: [https://kubernetes.io/docs/concepts/configuration/secret/](https://kubernetes.io/docs/concepts/configuration/secret/)
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes秘密：[https://kubernetes.io/docs/concepts/configuration/secret/](https://kubernetes.io/docs/concepts/configuration/secret/)
- en: Kubernetes Volumes: [https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes卷：[https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)
- en: Kubernetes Persistent Volumes: [https://kubernetes.io/docs/concepts/storage/persistent-volumes/](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes持久卷：[https://kubernetes.io/docs/concepts/storage/persistent-volumes/](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
- en: Kubernetes Storage Classes: [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/)
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes存储类：[https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/)
- en: Dynamic Volume Provisioning: [https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/](https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/)
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态卷配置：[https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/](https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/)
- en: Kubectl Command Reference: [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubectl命令参考：[https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands)
- en: Amazon EKS User Guide: [https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon EKS用户指南：[https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html)
- en: EKS Optimized AMI: [https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html)
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EKS优化AMI：[https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html)
- en: EKS Cluster CloudFormation Resource Reference: [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-eks-cluster.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-eks-cluster.html)
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EKS集群CloudFormation资源参考：[https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-eks-cluster.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-eks-cluster.html)
