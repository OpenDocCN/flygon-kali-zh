- en: '11: Docker Networking'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11：Docker网络
- en: It’s always the network!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 总是网络的问题！
- en: Any time there’s a an infrastructure problem, we always blame the network. Part
    of the reason is that networks are at the center of everything — **no network,
    no app!**
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 每当出现基础设施问题时，我们总是责怪网络。部分原因是网络处于一切的中心位置 —— 没有网络，就没有应用程序！
- en: In the early days of Docker, networking was hard — really hard! These days it’s
    *almost* a pleasure ;-)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker早期，网络很难 —— 真的很难！如今，这几乎是一种愉悦 ;-)
- en: In this chapter, we’ll look at the fundamentals of Docker networking. Things
    like the Container Network Model (CNM) and `libnetwork`. We’ll also get our hands
    dirty building some networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看一下Docker网络的基础知识。像容器网络模型（CNM）和`libnetwork`这样的东西。我们还将动手构建一些网络。
- en: 'As usual, we’ll split the chapter into three parts:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们将把本章分为三个部分：
- en: The TLDR
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLDR
- en: The deep dive
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入挖掘
- en: The commands
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令
- en: Docker Networking - The TLDR
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker网络 - TLDR
- en: Docker runs applications inside of containers, and these need to communicate
    over lots of different networks. This means Docker needs strong networking capabilities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Docker在容器内运行应用程序，这些应用程序需要在许多不同的网络上进行通信。这意味着Docker需要强大的网络能力。
- en: Fortunately, Docker has solutions for container-to-container networks, as well
    as connecting to existing networks and VLANs. The latter is important for containerized
    apps that need to communicate with functions and services on external systems
    such as VM’s and physicals.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Docker为容器间网络提供了解决方案，以及连接到现有网络和VLAN的解决方案。后者对于需要与外部系统（如VM和物理系统）上的功能和服务进行通信的容器化应用程序非常重要。
- en: Docker networking is based on an open-source pluggable architecture called the
    Container Network Model (CNM). `libnetwork` is Docker’s real-world implementation
    of the CNM, and it provides all of Docker’s core networking capabilities. Drivers
    plug in to `libnetwork` to provide specific network topologies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Docker网络基于一个名为容器网络模型（CNM）的开源可插拔架构。`libnetwork`是Docker对CNM的真实实现，它提供了Docker的所有核心网络功能。驱动程序插入到`libnetwork`中以提供特定的网络拓扑。
- en: To create a smooth out-of-the-box experience, Docker ships with a set of native
    drivers that deal with the most common networking requirements. These include
    single-host bridge networks, multi-host overlays, and options for plugging into
    existing VLANs. Ecosystem partners extend things even further by providing their
    own drivers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个顺畅的开箱即用体验，Docker附带了一组处理最常见网络需求的本地驱动程序。这些包括单主机桥接网络、多主机覆盖网络以及插入到现有VLAN的选项。生态系统合作伙伴通过提供自己的驱动程序进一步扩展了这些功能。
- en: Last but not least, `libnetwork` provides a native service discovery and basic
    container load balancing solution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，`libnetwork`提供了本地服务发现和基本容器负载均衡解决方案。
- en: That’s this big picture. Let’s get into the detail.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大局。让我们进入细节。
- en: Docker Networking - The Deep Dive
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker网络 - 深入挖掘
- en: 'We’ll organize this section of the chapter as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下方式组织本章节的内容：
- en: The theory
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论
- en: Single-host bridge networks
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单主机桥接网络
- en: Multi-host overlay networks
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多主机覆盖网络
- en: Connecting to existing networks
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到现有网络
- en: Service Discovery
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现
- en: Ingress networking
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口网络
- en: The theory
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理论
- en: 'At the highest level, Docker networking comprises three major components:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，Docker网络包括三个主要组件：
- en: The Container Network Model (CNM)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器网络模型（CNM）
- en: '`libnetwork`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`libnetwork`'
- en: Drivers
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序
- en: The CNM is the design specification. It outlines the fundamental building blocks
    of a Docker network.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CNM是设计规范。它概述了Docker网络的基本构建模块。
- en: '`libenetwork` is a real-world implementation of the CNM, and is used by Docker.
    It’s written in Go, and implements the core components outlined in the CNM.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`libenetwork`是CNM的真实实现，被Docker使用。它是用Go编写的，并实现了CNM中概述的核心组件。'
- en: Drivers extend the model by implementing specific network topologies such as
    VXLAN-based overlay networks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序通过实现特定的网络拓扑，如基于VXLAN的覆盖网络，来扩展模型。
- en: Figure 11.1 shows how they fit together at a very high level.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1显示了它们在非常高的层次上是如何组合在一起的。
- en: '![Figure 11.1](images/figure11-1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1](images/figure11-1.png)'
- en: Figure 11.1
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1
- en: Let’s look a bit closer at each.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下每一个。
- en: The Container Network Model (CNM)
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 容器网络模型（CNM）
- en: Everything starts with a design!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都始于设计！
- en: 'The design guide for Docker networking is the CNM. It outlines the fundamental
    building blocks of a Docker network, and you can read the full spec here: https://github.com/docker/libnetwork/blob/master/docs/design.md'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Docker网络的设计指南是CNM。它概述了Docker网络的基本构建块，您可以在这里阅读完整的规范：https://github.com/docker/libnetwork/blob/master/docs/design.md
- en: 'I recommend reading the entire spec, but at a high level, it defines three
    building blocks:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议阅读整个规范，但在高层次上，它定义了三个构建块：
- en: Sandboxes
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沙盒
- en: Endpoints
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端点
- en: Networks
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络
- en: A ***sandbox*** is an isolated network stack. It includes; Ethernet interfaces,
    ports, routing tables, and DNS config.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 沙盒是一个隔离的网络堆栈。它包括以太网接口、端口、路由表和DNS配置。
- en: '***Endpoints*** are virtual network interfaces (E.g. `veth`). Like normal network
    interfaces, they’re responsible for making connections. In the case of the CNM,
    it’s the job of the *endpoint* to connect a *sandbox* to a *network*.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 端点是虚拟网络接口（例如`veth`）。与普通网络接口一样，它们负责建立连接。在CNM的情况下，端点的工作是将沙盒连接到网络。
- en: '***Networks*** are a software implementation of an 802.1d bridge (more commonly
    known as a switch). As such, they group together, and isolate, a collection of
    endpoints that need to communicate.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是802.1d桥的软件实现（更常见的称为交换机）。因此，它们将需要通信的一组端点组合在一起，并进行隔离。
- en: Figure 11.2 shows the three components and how they connect.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2显示了这三个组件以及它们的连接方式。
- en: '![Figure 11.2 The Container Network Model (CNM)](images/figure11-2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 容器网络模型（CNM）](images/figure11-2.png)'
- en: Figure 11.2 The Container Network Model (CNM)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 容器网络模型（CNM）
- en: The atomic unit of scheduling in a Docker environment is the container, and
    as the name suggests, the Container Network Model is all about providing networking
    to containers. Figure 11.3 shows how CNM components relate to containers — sandboxes
    are placed inside of containers to provide them with network connectivity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker环境中调度的原子单位是容器，正如其名称所示，容器网络模型的目的是为容器提供网络。图11.3显示了CNM组件如何与容器相关联——沙盒被放置在容器内，以为它们提供网络连接。
- en: '![Figure 11.3](images/figure11-3.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3](images/figure11-3.png)'
- en: Figure 11.3
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3
- en: Container A has a single interface (endpoint) and is connected to Network A.
    Container B has two interfaces (endpoints) and is connected to Network A **and**
    Network B. The containers will be able to communicate because they are both connected
    to Network A. However, the two *endpoints* in Container B cannot communicate with
    each other without the assistance of a layer 3 router.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 容器A有一个接口（端点），连接到网络A。容器B有两个接口（端点），连接到网络A和网络B。这些容器将能够通信，因为它们都连接到网络A。然而，容器B中的两个端点在没有第3层路由器的帮助下无法相互通信。
- en: It’s also important to understand that *endpoints* behave like regular network
    adapters, meaning they can only be connected to a single network. Therefore, if
    a container needs connecting to multiple networks, it will need multiple endpoints.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还要了解的重要一点是，端点的行为类似于常规网络适配器，这意味着它们只能连接到单个网络。因此，如果一个容器需要连接到多个网络，它将需要多个端点。
- en: Figure 11.4 extends the diagram again, this time adding a Docker host. Although
    Container A and Container B are running on the same host, their network stacks
    are completely isolated at the OS-level via the sandboxes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图11.4再次扩展了图表，这次添加了一个Docker主机。虽然容器A和容器B在同一主机上运行，但它们的网络堆栈在操作系统级别通过沙盒完全隔离。 '
- en: '![Figure 11.4](images/figure11-4.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4](images/figure11-4.png)'
- en: Figure 11.4
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4
- en: Libnetwork
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Libnetwork
- en: The CNM is the design doc, and `libnetwork` is the canonical implementation.
    It’s open-source, written in Go, cross-platform (Linux and Windows), and used
    by Docker.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CNM是设计文档，`libnetwork`是规范实现。它是开源的，用Go编写，跨平台（Linux和Windows），并被Docker使用。
- en: In the early days of Docker, all the networking code existed inside the daemon.
    This was a nightmare — the daemon became bloated, and it didn’t follow the Unix
    principle of building modular tools that can work on their own, but also be easily
    composed into other projects. As a result, it all got ripped out and refactored
    into an external library called `libnetwork`. Nowadays, all of the core Docker
    networking code lives in `libnetwork`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker早期，所有的网络代码都存在于守护进程中。这是一场噩梦——守护进程变得臃肿，而且它没有遵循构建模块化工具的Unix原则，这些工具可以独立工作，但也可以轻松地组合到其他项目中。因此，所有的核心Docker网络代码都被剥离出来，重构为一个名为`libnetwork`的外部库。如今，所有的核心Docker网络代码都存在于`libnetwork`中。
- en: As you’d expect, it implements all three of the components defined in the CNM.
    It also implements native *service discovery*, *ingress-based container load balancing*,
    and the network control plane and management plane functionality.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所期望的，它实现了CNM中定义的所有三个组件。它还实现了本地*服务发现*、*基于入口的容器负载均衡*，以及网络控制平面和管理平面功能。
- en: Drivers
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序
- en: If `libnetwork` implements the control plane and management plane functions,
    then drivers implement the data plane. For example, connectivity and isolation
    is all handled by drivers. So is the actual creation of network objects. The relationship
    is shown in Figure 11.5.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`libnetwork`实现了控制平面和管理平面功能，那么驱动程序就实现了数据平面。例如，连接和隔离都由驱动程序处理。网络对象的实际创建也是如此。这种关系如图11.5所示。
- en: '![Figure 11.5](images/figure11-5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5](images/figure11-5.png)'
- en: Figure 11.5
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5
- en: Docker ships with several built-in drivers, known as native drivers or *local
    drivers*. On Linux they include; `bridge`, `overlay`, and `macvlan`. On Windows
    they include; `nat`, `overlay`, `transparent`, and `l2bridge`. We’ll see how to
    use some of them later in the chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Docker附带了几个内置驱动程序，称为本地驱动程序或*本地驱动程序*。在Linux上，它们包括；`bridge`、`overlay`和`macvlan`。在Windows上，它们包括；`nat`、`overlay`、`transparent`和`l2bridge`。我们将在本章后面看到如何使用其中一些。
- en: 3rd-parties can also write Docker network drivers. These are known as *remote
    drivers*, and examples include `calico`, `contiv`, `kuryr`, and `weave`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方也可以编写Docker网络驱动程序。这些被称为*远程驱动程序*，例如`calico`、`contiv`、`kuryr`和`weave`。
- en: Each driver is in charge of the actual creation and management of all resources
    on the networks it is responsible for. For example, an overlay network called
    “prod-fe-cuda” will be owned and managed by the `overlay` driver. This means the
    `overlay` driver will be invoked for the creation, management, and deletion of
    all resources on that network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个驱动程序负责在其负责的网络上实际创建和管理所有资源。例如，名为“prod-fe-cuda”的覆盖网络将由`overlay`驱动程序拥有和管理。这意味着`overlay`驱动程序将被调用来创建、管理和删除该网络上的所有资源。
- en: In order to meet the demands of complex highly-fluid environments,`libnetwork`
    allows multiple network drivers to be active at the same time. This means your
    Docker environment can sport a wide range of heterogeneous networks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足复杂高度流动的环境的需求，`libnetwork`允许多个网络驱动程序同时处于活动状态。这意味着您的Docker环境可以支持各种异构网络。
- en: Single-host bridge networks
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单主机桥接网络
- en: The simplest type of Docker network is the single-host bridge network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的Docker网络类型是单主机桥接网络。
- en: 'The name tells us two things:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 名称告诉我们两件事：
- en: '**Single-host** tells us it only exists on a single Docker host and can only
    connect containers that are on the same host.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单主机**告诉我们它只存在于单个Docker主机上，并且只能连接位于同一主机上的容器。'
- en: '**Bridge** tells us that it’s an implementation of an 802.1d bridge (layer
    2 switch).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桥接**告诉我们它是802.1d桥接（第2层交换）的实现。'
- en: Docker on Linux creates single-host bridge networks with the built-in `bridge`
    driver, whereas Docker on Windows creates them using the built-in `nat` driver.
    For all intents and purposes, they work the same.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，Docker使用内置的`bridge`驱动程序创建单主机桥接网络，而在Windows上，Docker使用内置的`nat`驱动程序创建它们。就所有目的而言，它们的工作方式都是相同的。
- en: Figure 11.6 shows two Docker hosts with identical local bridge networks called
    “mynet”. Even though the networks are identical, they are independent isolated
    networks. This means the containers in the picture cannot communicate directly
    because they are on different networks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6显示了两个具有相同本地桥接网络“mynet”的Docker主机。尽管网络是相同的，但它们是独立的隔离网络。这意味着图片中的容器无法直接通信，因为它们位于不同的网络上。
- en: '![Figure 11.6](images/figure11-6.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6](images/figure11-6.png)'
- en: Figure 11.6
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6
- en: Every Docker host gets a default single-host bridge network. On Linux it’s called
    “bridge”, and on Windows it’s called “nat” (yes, those are the same names as the
    drivers used to create them). By default, this is the network that all new containers
    will attach to unless you override it on the command line with the `--network`
    flag.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Docker主机都会获得一个默认的单主机桥接网络。在Linux上，它被称为“bridge”，在Windows上被称为“nat”（是的，这些名称与用于创建它们的驱动程序的名称相同）。默认情况下，这是所有新容器将连接到的网络，除非您在命令行上使用`--network`标志进行覆盖。
- en: The following listing shows the output of a `docker network ls` command on newly
    installed Linux and Windows Docker hosts. The output is trimmed so that it only
    shows the default network on each host. Notice how the name of the network is
    the same as the driver that was used to create it — this is coincidence.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下清单显示了在新安装的Linux和Windows Docker主机上运行`docker network ls`命令的输出。输出被修剪，只显示每个主机上的默认网络。请注意，网络的名称与用于创建它的驱动程序的名称相同——这是巧合。
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`The `docker network inspect` command is a treasure trove of great information!
    I highly recommended reading through its output if you’re interested in low-level
    detail.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker network inspect`命令是一个极好的信息宝库！如果您对底层细节感兴趣，我强烈建议阅读它的输出。'
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Docker networks built with the `bridge` driver on Linux hosts are based on
    the battle-hardened *linux bridge* technology that has existed in the Linux kernel
    for over 15 years. This means they’re high performance and extremely stable! It
    also means you can inspect them using standard Linux utilities. For example.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux主机上使用`bridge`驱动程序构建的Docker网络基于已存在于Linux内核中超过15年的经过艰苦打磨的*Linux桥接*技术。这意味着它们具有高性能和极其稳定！这也意味着您可以使用标准的Linux实用程序来检查它们。例如。
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`The default “bridge” network, on all Linux-based Docker hosts, maps to an
    underlying *Linux bridge* in the kernel called “**docker0**”. We can see this
    from the output of `docker network inspect`.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`在所有基于Linux的Docker主机上，默认的“bridge”网络映射到内核中称为“**docker0**”的基础*Linux桥接*。我们可以从`docker
    network inspect`的输出中看到这一点。'
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`The relationship between Docker’s default “bridge” network and the “docker0”
    bridge in the Linux kernel is shown in Figure 11.7.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Docker默认“bridge”网络与Linux内核中的“docker0”桥接之间的关系如图11.7所示。
- en: '![Figure 11.7](images/figure11-7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7](images/figure11-7.png)'
- en: Figure 11.7
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7
- en: Figure 11.8 extends the diagram by adding containers at the top that plug into
    the “bridge” network. The “bridge” network maps to the “docker0” Linux bridge
    in the host’s kernel, which can be mapped back to an Ethernet interface on the
    host via port mappings.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8通过在顶部添加容器来扩展了图表，这些容器插入到“bridge”网络中。 “bridge”网络映射到主机内核中的“docker0”Linux桥接，可以通过端口映射将其映射回主机上的以太网接口。
- en: '![Figure 11.8](images/figure11-8.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图11.8](images/figure11-8.png)'
- en: Figure 11.8
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8
- en: Let’s use the `docker network create` command to create a new single-host bridge
    network called “localnet”.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`docker network create`命令创建一个名为“localnet”的新单主机桥接网络。
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`The new network is created, and will appear in the output of any future `docker
    network ls` commands. If you are using Linux, you will also have a new *Linux
    bridge* created in the kernel.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 新网络已创建，并将出现在任何未来的`docker network ls`命令的输出中。如果您使用的是Linux，还将在内核中创建一个新的*Linux桥接*。
- en: Let’s use the Linux `brctl` tool to look at the Linux bridges currently on the
    system. You may have to manually install the `brctl` binary using `apt-get install
    bridge-utils`, or the equivalent for your Linux distro.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Linux的`brctl`工具来查看系统上当前的Linux桥接。您可能需要手动安装`brctl`二进制文件，使用`apt-get install
    bridge-utils`，或者您的Linux发行版的等效命令。
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`The output shows two bridges. The first line is the “docker0” bridge that
    we already know about. This relates to the default “bridge” network in Docker.
    The second bridge (br-20c2e8ae4bbb) relates to the new `localnet` Docker bridge
    network. Neither of them have spanning tree enabled, and neither have any devices
    connected (`interfaces` column).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了两个桥接。第一行是我们已经知道的“docker0”桥接。这与Docker中的默认“bridge”网络相关。第二个桥接（br-20c2e8ae4bbb）与新的`localnet`
    Docker桥接网络相关。它们都没有启用生成树，并且都没有任何设备连接（`interfaces`列）。
- en: At this point, the bridge configuration on the host looks like Figure 11.9.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，主机上的桥接配置如图11.9所示。
- en: '![Figure 11.9](images/figure11-9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图11.9](images/figure11-9.png)'
- en: Figure 11.9
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9
- en: Let’s create a new container and attach it to the new `localnet` bridge network.
    If you’re following along on Windows, you should substitute “`alpine sleep 1d`”
    with “`microsoft/powershell:nanoserver pwsh.exe -Command Start-Sleep 86400`”.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的容器，并将其连接到新的`localnet`桥接网络。如果您在Windows上跟随操作，应该将“`alpine sleep 1d`”替换为“`microsoft/powershell:nanoserver
    pwsh.exe -Command Start-Sleep 86400`”。
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`This container will now be on the `localnet` network. You can confirm this
    with a `docker network inspect`.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个容器现在将位于`localnet`网络上。您可以通过`docker network inspect`来确认。
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`The output shows that the new “c1” container is on the `localnet` bridge/nat
    network.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示新的“c1”容器位于`localnet`桥接/网络地址转换网络上。
- en: It we run the Linux `brctl show` command again, we’ll see c1’s interface attached
    to the `br-20c2e8ae4bbb` bridge.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次运行Linux的`brctl show`命令，我们将看到c1的接口连接到`br-20c2e8ae4bbb`桥接上。
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`This is shown in Figure 11.10.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图11.10中显示。
- en: '![Figure 11.10](images/figure11-10.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图11.10](images/figure11-10.png)'
- en: Figure 11.10
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10
- en: If we add another new container to the same network, it should be able to ping
    the “c1” container by name. This is because all new containers are registered
    with the embedded Docker DNS service so can resolve the names of all other containers
    on the same network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将另一个新容器添加到相同的网络中，它应该能够通过名称ping通“c1”容器。这是因为所有新容器都已在嵌入式Docker DNS服务中注册，因此可以解析同一网络中所有其他容器的名称。
- en: '**Beware:** The default `bridge` network on Linux does not support name resolution
    via the Docker DNS service. All other *user-defined* bridge networks do!'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** Linux上的默认`bridge`网络不支持通过Docker DNS服务进行名称解析。所有其他*用户定义*的桥接网络都支持！'
- en: Let’s test it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下。
- en: Create a new interactive container called “c2” and put it on the same `localnet`
    network as “c1”.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为“c2”的新交互式容器，并将其放在与“c1”相同的`localnet`网络中。
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Your terminal will switch into the “c2” container.`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您的终端将切换到“c2”容器中。
- en: '`*   From within the “c2” container, ping the “c1” container by name.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*从“c2”容器内部，通过名称ping“c1”容器。'
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`It works! This is because the c2 container is running a local DNS resolver
    that forwards requests to an internal Docker DNS server. This DNS server maintains
    mappings for all containers started with the `--name` or `--net-alias` flag.``'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 成功了！这是因为c2容器正在运行一个本地DNS解析器，它会将请求转发到内部Docker DNS服务器。该DNS服务器维护了所有使用`--name`或`--net-alias`标志启动的容器的映射。`
- en: '``Try running some network-related commands while you’re still logged on to
    the container. It’s a great way of learning more about how Docker container networking
    works. The following snippet shows the `ipconfig` command ran from inside the
    “c2” Windows container previously created. You can match this IP address to the
    one shown in the `docker network inspect nat` output.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在仍然登录到容器的情况下运行一些与网络相关的命令。这是了解Docker容器网络工作原理的好方法。以下片段显示了先前在“c2”Windows容器内运行的`ipconfig`命令。您可以将此IP地址与`docker
    network inspect nat`输出中显示的IP地址进行匹配。
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`So far, we’ve said that containers on bridge networks can only communicate
    with other containers on the same network. However, you can get around this using
    *port mappings*.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经说过桥接网络上的容器只能与同一网络上的其他容器通信。但是，您可以使用*端口映射*来解决这个问题。
- en: Port mappings let you map a container port to a port on the Docker host. Any
    traffic hitting the Docker host on the configured port will be directed to the
    container. The high-level flow is shown in Figure 1.11
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 端口映射允许您将容器端口映射到Docker主机上的端口。命中配置端口的任何流量都将被重定向到容器。高级流程如图1.11所示
- en: '![Figure 11.11](images/figure11-11.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11](images/figure11-11.png)'
- en: Figure 11.11
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11
- en: In the diagram, the application running in the container is operating on port
    80\. This is mapped to port 5000 on the host’s `10.0.0.15` interface. The end
    result is all traffic hitting the host on `10.0.0.15:5000` being redirected to
    the container on port 80.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，容器中运行的应用程序正在端口80上运行。这被映射到主机的`10.0.0.15`接口上的端口5000。最终结果是所有命中主机`10.0.0.15:5000`的流量都被重定向到容器的端口80。
- en: Let’s walk through an example of mapping port 80 on a container running a web
    server, to port 5000 on the Docker host. The example will use NGINX on Linux.
    If you’re following along on Windows, you’ll need to substitute `nginx` with a
    Windows-based web server image.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来演示将运行Web服务器的容器上的端口80映射到Docker主机上的端口5000。该示例将在Linux上使用NGINX。如果您在Windows上跟随操作，您需要用基于Windows的Web服务器镜像替换`nginx`。
- en: Run a new web server container and map port 80 on the container to port 5000
    on the Docker host.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个新的Web服务器容器，并将容器上的端口80映射到Docker主机上的端口5000。
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`*   Verify the port mapping.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*验证端口映射。'
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`This shows that port 80 in the container is mapped to port 5000 on all interfaces
    on the Docker host.` `*   Test the configuration by pointing a web browser to
    port 5000 on the Docker host. To complete this step, you will need to know the
    IP or DNS name of your Docker host. If you’re using Docker for Windows or Docker
    for Mac, you’ll be able to use `localhost` or `127.0.0.1`.![Figure 11.12](images/figure11-12.png)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明容器中的端口80被映射到Docker主机上所有接口的端口5000。*通过将Web浏览器指向Docker主机上的端口5000来测试配置。要完成此步骤，您需要知道Docker主机的IP或DNS名称。如果您使用的是Windows版Docker或Mac版Docker，您可以使用`localhost`或`127.0.0.1`。![图11.12](images/figure11-12.png)
- en: Figure 11.12
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12
- en: External systems, can now access the NGINX container running on the `localnet`
    bridge network via a port mapping to TCP port 5000 on the Docker host.``
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，外部系统可以通过端口映射到Docker主机上的TCP端口5000访问运行在`localnet`桥接网络上的NGINX容器。``
- en: '``Mapping ports like this works, but it’s clunky and doesn’t scale. For example,
    only a single container can bind to any port on the host. This means no other
    containers will be able to use port 5000 on the host we’re running the NGINX container
    on. This is one of the reason’s that single-host bridge networks are only useful
    for local development and very small applications.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '``像这样映射端口是有效的，但它很笨拙，而且不具有扩展性。例如，只有一个容器可以绑定到主机上的任何端口。这意味着在我们运行NGINX容器的主机上，没有其他容器能够使用端口5000。这就是单主机桥接网络仅适用于本地开发和非常小的应用程序的原因之一。'
- en: Multi-host overlay networks
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多主机叠加网络
- en: We’ve got an entire chapter dedicated to multi-host overlay networks. So we’ll
    keep this section short.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一整章专门讲解多主机叠加网络。所以我们会把这一部分简短地介绍一下。
- en: Overlay networks are multi-host. They allow a single network to span multiple
    hosts so that containers on different hosts can communicate at layer 2\. They’re
    ideal for container-to-container communication, including container-only applications,
    and they scale well.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 叠加网络是多主机的。它们允许单个网络跨越多个主机，以便不同主机上的容器可以在第2层进行通信。它们非常适合容器间通信，包括仅容器应用程序，并且它们具有良好的扩展性。
- en: Docker provides a native driver for overlay networks. This makes creating them
    as simple as adding the `--d overlay` flag to the `docker network create` command.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Docker提供了一个用于叠加网络的本地驱动程序。这使得创建它们就像在`docker network create`命令中添加`--d overlay`标志一样简单。
- en: Connecting to existing networks
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连接到现有网络
- en: The ability to connect containerized apps to external systems and physical networks
    is vital. A common example is a partially containerized app — the containerized
    parts will need a way to communicate with the non-containerized parts still running
    on existing physical networks and VLANs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将容器化应用程序连接到外部系统和物理网络的能力至关重要。一个常见的例子是部分容器化的应用程序 - 容器化的部分需要一种方式与仍在现有物理网络和VLAN上运行的非容器化部分进行通信。
- en: The built-in `MACVLAN` driver (`transparent` on Windows) was created with this
    in mind. It makes containers first-class citizens on the existing physical networks
    by giving each one its own MAC and IP addresses. We show this in Figure 11.13.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的`MACVLAN`驱动程序（在Windows上是`transparent`）就是为此而创建的。它通过为每个容器分配自己的MAC和IP地址，使容器成为现有物理网络上的一等公民。我们在图11.13中展示了这一点。
- en: '![Figure 11.13](images/figure11-13.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13](images/figure11-13.png)'
- en: Figure 11.13
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13
- en: On the positive side, MACVLAN performance is good as it doesn’t require port
    mappings or additional bridges — you connect the container interface through to
    the hosts interface (or a sub-interface). However, on the negative side, it requires
    the host NIC to be in **promiscuous mode**, which isn’t allowed on most public
    cloud platforms. So MACVLAN is great for your corporate data center networks (assuming
    your network team can accommodate promiscuous mode), but it won’t work in the
    public cloud.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从积极的一面来看，MACVLAN的性能很好，因为它不需要端口映射或额外的桥接 - 您可以通过容器接口连接到主机接口（或子接口）。然而，从消极的一面来看，它需要主机网卡处于**混杂模式**，这在大多数公共云平台上是不允许的。因此，MACVLAN非常适合您的企业数据中心网络（假设您的网络团队可以适应混杂模式），但在公共云中不起作用。
- en: Let’s dig a bit deeper with the help of some pictures and a hypothetical example.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些图片和一个假设的例子深入了解一下。
- en: 'Assume we have an existing physical network with two VLANS:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个现有的物理网络，其中有两个VLAN：
- en: 'VLAN 100: 10.0.0.0/24'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'VLAN 100: 10.0.0.0/24'
- en: 'VLAN 200: 192.168.3.0/24'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'VLAN 200: 192.168.3.0/24'
- en: '![Figure 11.14](images/figure11-14.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14](images/figure11-14.png)'
- en: Figure 11.14
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14
- en: Next, we add a Docker host and connect it to the network.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个Docker主机并将其连接到网络。
- en: '![Figure 11.15](images/figure11-15.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15](images/figure11-15.png)'
- en: Figure 11.15
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15
- en: 'We then have a requirement for a container (app service) to be plumbed into
    VLAN 100\. To do this, we create a new Docker network with the `macvlan` driver.
    However, the `macvlan` driver needs us to tell it a few things about the network
    we’re going to associate it with. Things like:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将一个容器（应用服务）连接到VLAN 100。为此，我们使用`macvlan`驱动创建一个新的Docker网络。但是，`macvlan`驱动需要我们告诉它一些关于我们将要关联的网络的信息。比如：
- en: Subnet info
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子网信息
- en: Gateway
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网关
- en: Range of IP’s it can assign to containers
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以分配给容器的IP范围
- en: Which interface or sub-interface on the host to use
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主机上使用哪个接口或子接口
- en: The following command will create a new MACVLAN network called “macvlan100”
    that will connect containers to VLAN 100.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将创建一个名为“macvlan100”的新MACVLAN网络，将容器连接到VLAN 100。
- en: '[PRE14]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`This will create the “macvlan100” network and the eth0.100 sub-interface.
    The config now looks like this.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`这将创建“macvlan100”网络和eth0.100子接口。配置现在看起来像这样。'
- en: '![Figure 11.16](images/figure11-16.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图11.16](images/figure11-16.png)'
- en: Figure 11.16
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16
- en: MACVLAN uses standard Linux sub-interfaces, and you have to tag them with the
    ID of the VLAN they will connect to. In this example we’re connecting to VLAN
    100, so we tag the sub-interface with `.100` (`etho.100`).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MACVLAN使用标准的Linux子接口，并且您必须使用VLAN的ID对它们进行标记。在这个例子中，我们连接到VLAN 100，所以我们使用`.100`（`etho.100`）对子接口进行标记。
- en: We also used the `--ip-range` flag to tell the MACVLAN network which sub-set
    of IP addresses it can assign to containers. It’s vital that this range of addresses
    be reserved for Docker and not in use by other nodes or DHCP servers, as there
    is no management plane feature to check for overlapping IP ranges.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了`--ip-range`标志来告诉MACVLAN网络可以分配给容器的IP地址子集。这个地址范围必须保留给Docker使用，并且不能被其他节点或DHCP服务器使用，因为没有管理平面功能来检查重叠的IP范围。
- en: The `macvlan100` network is ready for containers, so let’s deploy one with the
    following command.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`macvlan100`网络已准备好用于容器，让我们使用以下命令部署一个。'
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`The config now looks like Figure 11.17\. But remember, the underlying network
    (VLAN 100) does not see any of the MACVLAN magic, it only sees the container with
    its MAC and IP addresses. And with that in mind, the “mactainer1” container will
    be able to ping and communicate with any other systems on VLAN 100\. Pretty sweet!'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`配置现在看起来像图11.17。但请记住，底层网络（VLAN 100）看不到任何MACVLAN的魔法，它只看到具有MAC和IP地址的容器。考虑到这一点，“mactainer1”容器将能够ping并与VLAN
    100上的任何其他系统通信。非常棒！'
- en: '**Note:** If you can’t get this to work, it might be because the host NIC is
    not in promiscuous mode. Remember that public cloud platforms do not allow promiscuous
    mode.'
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**如果无法使其工作，可能是因为主机网卡没有处于混杂模式。请记住，公共云平台不允许混杂模式。'
- en: '![Figure 11.17](images/figure11-17.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图11.17](images/figure11-17.png)'
- en: Figure 11.17
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17
- en: At this point, we’ve got a MACVLAN network and used it to connect a new container
    to an existing VLAN. However, it doesn’t stop there. The Docker MACVLAN driver
    is built on top of the tried-and-tested Linux kernel driver with the same name.
    As such, it supports VLAN trunking. This means we can create multiple MACVLAN
    networks and connect containers on the same Docker host to them as shown in Figure
    11.18.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了一个MACVLAN网络，并使用它将一个新容器连接到现有的VLAN。但事情并不止于此。Docker MACVLAN驱动是建立在经过验证的Linux内核驱动程序的基础上的。因此，它支持VLAN干线。这意味着我们可以创建多个MACVLAN网络，并将同一台Docker主机上的容器连接到它们，如图11.18所示。
- en: '![Figure 11.18](images/figure11-18.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图11.18](images/figure11-18.png)'
- en: Figure 11.18
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18
- en: That pretty much covers MACVLAN. Windows offers a similar solution with the
    `transparent` driver.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上涵盖了MACVLAN。Windows提供了一个类似的解决方案，使用`transparent`驱动。
- en: Container and Service logs for troubleshooting
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 容器和服务日志用于故障排除
- en: A quick note on troubleshooting connectivity issues before moving on to Service
    Discovery.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续服务发现之前，快速解决连接问题的说明。
- en: If you think you’re experiencing connectivity issues between containers, it’s
    worth checking the daemon logs and container logs (app logs).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为容器之间存在连接问题，值得检查守护程序日志和容器日志（应用程序日志）。
- en: 'On Windows systems, the daemon logs are stored under `~AppData\Local\Docker`,
    and you can view them in the Windows Event Viewer. On Linux, it depends what `init`
    system you’re using. If you’re running a `systemd`, the logs will go to `journald`
    and you can view them with the `journalctl -u docker.service` command. If you’re
    not running `systemd` you should look under the following locations:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows系统上，守护程序日志存储在`~AppData\Local\Docker`下，并且可以在Windows事件查看器中查看。在Linux上，这取决于您使用的`init`系统。如果您正在运行`systemd`，日志将进入`journald`，您可以使用`journalctl
    -u docker.service`命令查看它们。如果您没有运行`systemd`，您应该查看以下位置：
- en: 'Ubuntu systems running `upstart`: `/var/log/upstart/docker.log`'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`upstart`的Ubuntu系统：`/var/log/upstart/docker.log`
- en: 'RHEL-based systems: `/var/log/messages`'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于RHEL的系统：`/var/log/messages`
- en: 'Debian: `/var/log/daemon.log`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debian：`/var/log/daemon.log`
- en: 'Docker for Mac: `~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/console-ring`'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker for Mac: `~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/console-ring`'
- en: 'You can also tell Docker how verbose you want daemon logging to be. To do this,
    you edit the daemon config file (`daemon.json`) so that “`debug`” is set to “`true`”
    and “`log-level`” is set to one of the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以告诉Docker您希望守护程序日志记录的详细程度。要做到这一点，您需要编辑守护程序配置文件（`daemon.json`），以便将“`debug`”设置为“`true`”，并将“`log-level`”设置为以下之一：
- en: '`debug` The most verbose option'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`debug` 最详细的选项'
- en: '`info` The default value and second-most verbose option'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info` 默认值和第二最详细的选项'
- en: '`warn` Third most verbose option'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warn` 第三个最详细的选项'
- en: '`error` Fourth most verbose option'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`error` 第四个最详细的选项'
- en: '`fatal` Least verbose option'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fatal` 最不详细的选项'
- en: The following snippet from a `daemon.json` enables debugging and sets the level
    to `debug`. It will work on all Docker platforms.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`daemon.json`的以下片段启用了调试并将级别设置为`debug`。它适用于所有Docker平台。'
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Be sure to restart Docker after making changes to the file.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 更改文件后，请务必重新启动Docker。
- en: That was the daemon logs. What about container logs?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是守护程序日志。那容器日志呢？
- en: Logs from standalone containers can be viewed with the `docker container logs`
    command, and Swarm Service logs can be viewed with the `docker service logs` command.
    However, Docker supports lots of logging drivers, and they don’t all work with
    the `docker logs` command.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 独立容器的日志可以使用`docker container logs`命令查看，Swarm服务的日志可以使用`docker service logs`命令查看。但是，Docker支持许多日志记录驱动程序，并且它们并不都与`docker
    logs`命令兼容。
- en: 'As well as a driver and configuration for engine logs, every Docker host has
    a default logging driver and configuration for containers. Some of the drivers
    include:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了引擎日志的驱动程序和配置外，每个Docker主机都有默认的容器日志驱动程序和配置。一些驱动程序包括：
- en: '`json-file` (default)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`json-file`（默认）'
- en: '`journald` (only works on Linux hosts running `systemd`)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`journald`（仅在运行`systemd`的Linux主机上有效）'
- en: '`syslog`'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`syslog`'
- en: '`splunk`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`splunk`'
- en: '`gelf`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gelf`'
- en: '`json-file` and `journald` are probably the easiest to configure, and they
    both work with the `docker logs` and `docker service logs` commands. The format
    of the commands is `docker logs <container-name>` and `docker service logs <service-name>`.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`json-file`和`journald`可能是最容易配置的，它们都可以与`docker logs`和`docker service logs`命令一起使用。命令的格式是`docker
    logs <container-name>`和`docker service logs <service-name>`。'
- en: If you’re using other logging drivers you can view logs using the 3-rd party
    platform’s native tools.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用其他日志记录驱动程序，可以使用第三方平台的本机工具查看日志。
- en: The following snippet from a `daemon.json` shows a Docker host configured to
    use `syslog`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下来自`daemon.json`的片段显示了配置为使用`syslog`的Docker主机。
- en: '[PRE17]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`You can configure an individual container, or service, to start with a particular
    logging driver with the `--log-driver` and `--log-opts` flags. These will override
    anything set in `daemon.json`.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`您可以使用`--log-driver`和`--log-opts`标志配置单个容器或服务以使用特定的日志驱动程序。这将覆盖`daemon.json`中设置的任何内容。'
- en: Container logs work on the premise that your application is running as PID 1
    in its container, and sending logs to `STDOUT`, and errors to `STDERR`. The logging
    driver then forwards these “logs” to the locations configured via the logging
    driver.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 容器日志的工作原理是您的应用程序作为其容器中的PID 1运行，并将日志发送到`STDOUT`，将错误发送到`STDERR`。然后，日志驱动程序将这些“日志”转发到通过日志驱动程序配置的位置。
- en: If your application logs to a file, it’s possible to use a symlink to redirect
    log-file writes to STDOUT and STDERR.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的应用程序记录到文件，可以使用符号链接将日志文件写入重定向到STDOUT和STDERR。
- en: The following is an example of running the `docker logs` command against a container
    called “vantage-db” configured to use the `json-file` logging driver.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是针对名为“vantage-db”的容器运行`docker logs`命令的示例，该容器配置为使用`json-file`日志驱动程序。
- en: '[PRE18]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`There’s a good chance you’ll find network connectivity errors reported in
    the daemon logs or container logs.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: “您很有可能会在守护程序日志或容器日志中发现网络连接错误报告。
- en: Service discovery
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务发现
- en: As well as core networking, `libnetwork` also provides some important network
    services.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心网络，`libnetwork`还提供了一些重要的网络服务。
- en: '*Service discovery* allows all containers and Swarm services to locate each
    other by name. The only requirement is that they be on the same network.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务发现*允许所有容器和Swarm服务通过名称定位彼此。唯一的要求是它们在同一个网络上。'
- en: Under the hood, this leverages Docker’s embedded DNS server, as well as a DNS
    resolver in each container. Figure 11.19 shows container “c1” pinging container
    “c2” by name. The same principle applies to Swarm Services.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，这利用了Docker的嵌入式DNS服务器，以及每个容器中的DNS解析器。图11.19显示了容器“c1”通过名称ping容器“c2”。相同的原理也适用于Swarm服务。
- en: '![Figure 11.19](images/figure11-19.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图11.19](images/figure11-19.png)'
- en: Figure 11.19
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.19
- en: Let’s step through the process.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解这个过程。
- en: '**step 1:** The `ping c2` command invokes the local DNS resolver to resolve
    the name “c2” to an IP address. All Docker containers have a local DNS resolver.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤1：** `ping c2`命令调用本地DNS解析器将名称“c2”解析为IP地址。所有Docker容器都有一个本地DNS解析器。'
- en: '**Step 2:** If the local resolver does not have an IP address for “c2” in its
    local cache, it initiates a recursive query to the Docker DNS server. The local
    resolver is pre-configured to know the details of the embedded Docker DNS server.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤2：** 如果本地解析器在其本地缓存中没有“c2”的IP地址，它将发起对Docker DNS服务器的递归查询。本地解析器预先配置为知道嵌入式Docker
    DNS服务器的详细信息。'
- en: '**Step 3:** The Docker DNS server holds name-to-IP mappings for all containers
    created with the `--name` or `--net-alias` flags. This means it knows the IP address
    of container “c2”.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤3：** Docker DNS服务器保存了使用`--name`或`--net-alias`标志创建的所有容器的名称到IP映射。这意味着它知道容器“c2”的IP地址。'
- en: '**Step 4:** The DNS server returns the IP address of “c2” to the local resolver
    in “c1”. It does this because the two containers are on the same network — if
    they were on different networks this would not work.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤4：** DNS服务器将“c2”的IP地址返回给“c1”中的本地解析器。它之所以这样做是因为这两个容器在同一个网络上 - 如果它们在不同的网络上，这将无法工作。'
- en: '**Step 5:** The `ping` command is sent to the IP address of “c2”.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤5：** `ping`命令被发送到“c2”的IP地址。'
- en: Every Swarm Service and standalone container started with the `--name` flag
    will register its name and IP with the Docker DNS service. This means all containers
    and service replicas can use the Docker DNS service to find each other.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 每个使用`--name`标志启动的Swarm服务和独立容器都将其名称和IP注册到Docker DNS服务。这意味着所有容器和服务副本都可以使用Docker
    DNS服务找到彼此。
- en: However, service discovery is *network-scoped*. This means that name resolution
    only works for containers and Services on the same network. If two containers
    are on different networks, they will not be able to resolve each other.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，服务发现是*网络范围的*。这意味着名称解析仅适用于相同网络上的容器和服务。如果两个容器在不同的网络上，它们将无法解析彼此。
- en: One last point on service discovery and name resolution…
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 关于服务发现和名称解析的最后一点...
- en: It’s possible to configure Swarm Services and standalone containers with customized
    DNS options. For example, the `--dns` flag lets you specify a list of custom DNS
    servers to use in case the embedded Docker DNS server cannot resolve a query.
    You can also use the `--dns-search` flag to add custom search domains for queries
    against unqualified names (i.e. when the query is not a fully qualified domain
    name).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 可以配置Swarm服务和独立容器的自定义DNS选项。例如，`--dns`标志允许您指定要在嵌入式Docker DNS服务器无法解析查询时使用的自定义DNS服务器列表。您还可以使用`--dns-search`标志为针对未经验证名称的查询添加自定义搜索域（即当查询不是完全合格的域名时）。
- en: On Linux, these all work by adding entries to the `/etc/resolv.conf` file inside
    the container.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，所有这些都是通过向容器内的`/etc/resolv.conf`文件添加条目来实现的。
- en: The following example will start a new standalone container and add the infamous
    `8.8.8.8` Google DNS server, as well as `dockercerts.com` as search domain to
    append to unqualified queries.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将启动一个新的独立容器，并将臭名昭著的`8.8.8.8` Google DNS服务器添加到未经验证的查询中附加的搜索域`dockercerts.com`。
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`#### Ingress load balancing'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`#### 入口负载平衡'
- en: 'Swarm supports two publishing modes that make Services accessible from outside
    of the cluster:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm支持两种发布模式，使服务可以从集群外部访问：
- en: Ingress mode (default)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口模式（默认）
- en: Host mode
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机模式
- en: Services published via *ingress mode* can be accessed from any node in the Swarm
    — even nodes **not** running a service replica. Services published via *host mode*
    can only be accessed via nodes running service replicas. Figure 11.20 shows the
    difference between the two modes.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*入口模式*发布的服务可以从Swarm中的任何节点访问 - 即使节点**没有**运行服务副本。通过*主机模式*发布的服务只能通过运行服务副本的节点访问。图11.20显示了两种模式之间的区别。
- en: '![Figure 11.20](images/figure11-20.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图11.20](images/figure11-20.png)'
- en: Figure 11.20
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20
- en: Ingress mode is the default. This means that any time you publish a service
    with `-p` or `--publish` it will default to *ingress mode*. To publish a service
    in *host mode* you need to use the long format of the `--publish` flag **and**
    add `mode=host`. Let’s see an example using *host mode*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 入口模式是默认模式。这意味着每当您使用`-p`或`--publish`发布服务时，它将默认为*入口模式*。要在*主机模式*下发布服务，您需要使用`--publish`标志的长格式**并且**添加`mode=host`。让我们看一个使用*主机模式*的例子。
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`A few notes about the command. `docker service create` lets you publish a
    service using either a *long form syntax* or *short form syntax*. The short form
    looks like this: `-p 5000:80` and we’ve seen it a few times already. However,
    you cannot publish a service in *host mode* using short form.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`关于命令的一些说明。`docker service create`允许您使用*长格式语法*或*短格式语法*发布服务。短格式如下：`-p 5000:80`，我们已经看过几次了。但是，您不能使用短格式发布*主机模式*的服务。'
- en: 'The long form looks like this: `--publish published=5000,target=80,mode=host`.
    It’s a comma-separate list with no whitespace after each comma. The options work
    as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 长格式如下：`--publish published=5000,target=80,mode=host`。这是一个逗号分隔的列表，每个逗号后面没有空格。选项的工作如下：
- en: '`published=5000` makes the service available externally via port 5000'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`published=5000`使服务通过端口5000在外部可用'
- en: '`target=80` makes sure that external requests to the `published` port get mapped
    back to port 80 on the service replicas'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target=80`确保对`published`端口的外部请求被映射回服务副本上的端口80'
- en: '`mode=host` makes sure that external requests will only reach the service if
    they come in via nodes running a service replica.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode=host`确保外部请求只会在通过运行服务副本的节点进入时到达服务。'
- en: Ingress mode is what you’ll normally use.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 入口模式是您通常会使用的模式。
- en: Behind the scenes, *ingress mode* uses a layer 4 routing mesh called the **Service
    Mesh** or the **Swarm Mode Service Mesh**. Figure 11.21 shows the basic traffic
    flow of an external request to a service exposed in ingress mode.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，*入口模式*使用了一个称为**服务网格**或**Swarm模式服务网格**的第4层路由网格。图11.21显示了外部请求到入口模式下暴露的服务的基本流量流向。
- en: '![Figure 11.21](images/figure11-21.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图11.21](images/figure11-21.png)'
- en: Figure 11.21
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21
- en: Let’s quickly walk through the diagram.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下图表。
- en: The command at the top is deploying a new Swarm service called “svc1”. It’s
    attaching the service to the `overnet` network and publishing it on port 5000.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 顶部的命令正在部署一个名为“svc1”的新Swarm服务。它将服务附加到`overnet`网络并在5000端口上发布它。
- en: Publishing a Swarm service like this (`--publish published=5000,target=80`)
    will publish it on port 5000 on the ingress network. As all nodes in a Swarm are
    attached to the ingress network, this means the port is published *swarm-wide*.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像这样发布Swarm服务（`--publish published=5000,target=80`）将在入口网络的5000端口上发布它。由于Swarm中的所有节点都连接到入口网络，这意味着端口是*在整个Swarm中*发布的。
- en: Logic is implemented on the cluster ensuring that any traffic hitting the ingress
    network, via **any node**, on port 5000 will be routed to the “svc1” service on
    port 80.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群上实现了逻辑，确保任何命中入口网络的流量，通过**任何节点**，在5000端口上都将被路由到端口80上的“svc1”服务。
- en: At this point, a single replica for the “svc1” service is deployed, and the
    cluster has a mapping rule that says “*all traffic hitting the ingress network
    on port 5000 needs routing to a node running a replica for the “svc1” service*”.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，“svc1”服务部署了一个单个副本，并且集群有一个映射规则，规定“*所有命中入口网络5000端口的流量都需要路由到运行“svc1”服务副本的节点*”。
- en: The red line shows traffic hitting `node1` on port 5000 and being routed to
    the service replica running on node2 via the ingress network.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 红线显示流量命中`node1`的5000端口，并通过入口网络路由到运行在node2上的服务副本。
- en: It’s vital to know that the incoming traffic could have hit any of the four
    Swarm nodes on port 5000 and we would get the same result. This is because the
    service is published *swarm-wide* via the ingress network.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道，传入的流量可能会命中任何一个端口为5000的四个Swarm节点，我们会得到相同的结果。这是因为服务是通过入口网络*在整个Swarm中*发布的。
- en: It’s also vital to know that if there were multiple replicas running, as shown
    in Figure 11.22, the traffic would be balanced across all replicas.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要知道，如果有多个运行的副本，如图11.22所示，流量将在所有副本之间平衡。
- en: '![Figure 11.22](images/figure11-22.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图11.22](images/figure11-22.png)'
- en: Figure 11.22
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.22
- en: Docker Networking - The Commands
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Docker网络-命令
- en: 'Docker networking has its own `docker network` sub-command. The main commands
    include:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Docker网络有自己的`docker network`子命令。主要命令包括：
- en: '`docker network ls` Lists all networks on the local Docker host.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network ls`列出本地Docker主机上的所有网络。'
- en: '`docker network create` Creates new Docker networks. By default, it creates
    them with the `nat` driver on Windows, and the `bridge` driver on Linux. You can
    specify the driver (type of network) with the `-d` flag. `docker network create
    -d overlay overnet` will create a new overlay network called overnet with the
    native Docker `overlay` driver.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network create` 创建新的Docker网络。默认情况下，在Windows上使用`nat`驱动程序创建网络，在Linux上使用`bridge`驱动程序创建网络。您可以使用`-d`标志指定驱动程序（网络类型）。`docker
    network create -d overlay overnet`将使用原生Docker`overlay`驱动程序创建一个名为overnet的新覆盖网络。'
- en: '`docker network inspect` Provides detailed configuration information about
    a Docker network.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network inspect`提供有关Docker网络的详细配置信息。'
- en: '`docker network prune` Deletes all unused networks on a Docker host.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network prune` 删除 Docker 主机上所有未使用的网络。'
- en: '`docker network rm` Deletes specific networks on a Docker host.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker network rm` 删除 Docker 主机上特定的网络。'
- en: Chapter Summary
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 章节总结
- en: The Container Network Model (CNM) is the master design document for Docker networking
    and defines the three major constructs that are used to build Docker networks
    — *sandboxes*, *endpoints*, and *networks*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 容器网络模型（CNM）是 Docker 网络的主设计文档，定义了用于构建 Docker 网络的三个主要构造——*沙盒*、*端点*和*网络*。
- en: '`libnetwork` is the open-source library, written in Go, that implements the
    CNM. It’s used by Docker and is where all of the core Docker networking code lives.
    It also provides Docker’s network control plane and management plane.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`libnetwork` 是用 Go 语言编写的开源库，实现了 CNM。它被 Docker 使用，并且是所有核心 Docker 网络代码的所在地。它还提供了
    Docker 的网络控制平面和管理平面。'
- en: Drivers extend the Docker network stack (`libnetwork`) by adding code to implement
    specific network types, such as bridge networks and overlay networks. Docker ships
    with several built-in drivers, but you can also use 3rd-party drivers.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序通过添加代码来实现特定的网络类型（如桥接网络和覆盖网络）来扩展 Docker 网络堆栈（`libnetwork`）。Docker 预装了几个内置驱动程序，但您也可以使用第三方驱动程序。
- en: Single-host bridge networks are the most basic type of Docker network and are
    suitable for local development and very small applications. They do not scale,
    and they require port mappings if you want to publish your services outside of
    the network. Docker on Linux implements bridge networks using the built-in `bridge`
    driver, whereas Docker on Windows implements them using the built-in `nat` driver.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 单主机桥接网络是最基本的 Docker 网络类型，适用于本地开发和非常小的应用程序。它们不具备可扩展性，如果要将服务发布到网络外部，则需要端口映射。Linux
    上的 Docker 使用内置的 `bridge` 驱动程序实现桥接网络，而 Windows 上的 Docker 使用内置的 `nat` 驱动程序实现它们。
- en: Overlay networks are all the rage and are excellent container-only multi-host
    networks. We’ll talk about them in-depth in the next chapter.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络非常流行，是非常适合容器的多主机网络。我们将在下一章中深入讨论它们。
- en: The `macvlan` driver (`transparent` on Windows) allows you to connect containers
    to existing physical networks and VLANs. They make containers first-class citizens
    by giving them their own MAC and IP addresses. Unfortunately, they require promiscuous
    on the host NIC, meaning they won’t work in the public cloud.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`macvlan` 驱动程序（Windows 上的 `transparent`）允许您将容器连接到现有的物理网络和虚拟局域网。它们通过为容器分配自己的
    MAC 和 IP 地址使容器成为一流公民。不幸的是，它们需要在主机 NIC 上启用混杂模式，这意味着它们在公共云中无法工作。'
- en: Docker also uses `libnetwork` to implement basic service discovery, as well
    as a service mesh for container-based load balancing of ingress traffic.[PRE21]
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 还使用 `libnetwork` 来实现基本的服务发现，以及用于容器负载均衡入口流量的服务网格。
