["```\n$ git clone https://github.com/dockersamples/atsea-sample-shop-app.git\nCloning into `'atsea-sample-shop-app'`...\nremote: Counting objects: `636`, `done`.\nremote: Total `636` `(`delta `0``)`, reused `0` `(`delta `0``)`, pack-reused `636`\nReceiving objects: `100`% `(``636`/636`)`, `7`.23 MiB `|` `28`.25 MiB/s, `done`.\nResolving deltas: `100`% `(``197`/197`)`, `done`. \n```", "```\n`version``:`\n`services``:`\n`networks``:`\n`secrets``:` \n```", "```\nversion: \"3.2\"\nservices:\n    reverse_proxy:\n    database:\n    appserver:\n    visualizer:\n    payment_gateway:\nnetworks:\n    front-tier:\n    back-tier:\n    payment:\nsecrets:\n    postgres_password:\n    staging_token:\n    revprox_key:\n    revprox_cert: \n```", "```\n`networks``:`\n  `front``-``tier``:`\n  `back``-``tier``:`\n  `payment``:`\n    `driver``:` `overlay`\n    `driver_opts``:`\n      `encrypted``:` `'yes'` \n```", "```\n`secrets``:`\n  `postgres_password``:`\n    `external``:` `true`\n  `staging_token``:`\n    `external``:` `true`\n  `revprox_key``:`\n    `external``:` `true`\n  `revprox_cert``:`\n    `external``:` `true` \n```", "```\n`reverse_proxy``:`\n  `image``:` `dockersamples``/``atseasampleshopapp_reverse_proxy`\n  `ports``:`\n    `-` `\"80:80\"`\n    `-` `\"443:443\"`\n  `secrets``:`\n    `-` `source``:` `revprox_cert`\n      `target``:` `revprox_cert`\n    `-` `source``:` `revprox_key`\n      `target``:` `revprox_key`\n  `networks``:`\n    `-` `front``-``tier` \n```", "```\nports:\n  - target: 80\n    published: 80\n    mode: host \n```", "```\nsecrets:\n  - source: revprox_cert\n    target: uber_secret \n```", "```\n`database``:`\n  `image``:` `dockersamples``/``atsea_db`\n  `environment``:`\n    `POSTGRES_USER``:` `gordonuser`\n    `POSTGRES_DB_PASSWORD_FILE``:` `/run/secrets/``postgres_password`\n    `POSTGRES_DB``:` `atsea`\n  `networks``:`\n    `-` `back``-``tier`\n  `secrets``:`\n    `-` `postgres_password`\n  `deploy``:`\n    `placement``:`\n      `constraints``:`\n        `-` `'node.role == worker'` \n```", "```\n`environment``:`\n  `POSTGRES_USER``:` `gordonuser`\n  `POSTGRES_DB_PASSWORD_FILE``:` `/run/secrets/``postgres_password`\n  `POSTGRES_DB``:` `atsea` \n```", "```\n`deploy``:`\n  `placement``:`\n    `constraints``:`\n      `-` `'node.role == worker'` \n```", "```\n`appserver``:`\n  `image``:` `dockersamples``/``atsea_app`\n  `networks``:`\n    `-` `front``-``tier`\n    `-` `back``-``tier`\n    `-` `payment`\n  `deploy``:`\n    `replicas``:` `2`\n    `update_config``:`\n      `parallelism``:` `2`\n      `failure_action``:` `rollback`\n    `placement``:`\n      `constraints``:`\n        `-` `'node.role == worker'`\n    `restart_policy``:`\n      `condition``:` `on``-``failure`\n      `delay``:` `5``s`\n      `max_attempts``:` `3`\n      `window``:` `120``s`\n  `secrets``:`\n    `-` `postgres_password` \n```", "```\n`update_config``:`\n  `parallelism``:` `2`\n  `failure_action``:` `rollback` \n```", "```\n`restart_policy``:`\n  `condition``:` `on``-``failure`\n  `delay``:` `5``s`\n  `max_attempts``:` `3`\n  `window``:` `120``s` \n```", "```\n`visualizer``:`\n  `image``:` `dockersamples``/``visualizer``:``stable`\n  `ports``:`\n    `-` `\"8001:8080\"`\n  `stop_grace_period``:` `1``m30s`\n  `volumes``:`\n    `-` `\"/var/run/docker.sock:/var/run/docker.sock\"`\n  `deploy``:`\n    `update_config``:`\n      `failure_action``:` `rollback`\n    `placement``:`\n      `constraints``:`\n        `-` `'node.role == manager'` \n```", "```\n`payment_gateway``:`\n  `image``:` `dockersamples``/``atseasampleshopapp_payment_gateway`\n  `secrets``:`\n    `-` `source``:` `staging_token`\n      `target``:` `payment_token`\n  `networks``:`\n    `-` `payment`\n  `deploy``:`\n    `update_config``:`\n      `failure_action``:` `rollback`\n    `placement``:`\n      `constraints``:`\n        `-` `'node.role == worker'`\n        `-` `'node.labels.pcidss == yes'` \n```", "```\n     $ docker swarm init\n     Swarm initialized: current node (lhma...w4nn) is now a manager.\n     <Snip> \n    ```", "```\n     //Worker 1 (wrk-1)\n     wrk-1$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377\n     This node joined a swarm as a worker.\n\n     //Worker 2 (wrk-2)\n     wrk-2$ docker swarm join --token SWMTKN-1-2hl6...-...3lqg 172.31.40.192:2377\n     This node joined a swarm as a worker. \n    ```", "```\n     $ docker node ls\n     ID            HOSTNAME   STATUS     AVAILABILITY    MANAGER STATUS\n     lhm...4nn *   mgr-1      Ready      Active          Leader\n     b74...gz3     wrk-1      Ready      Active\n     o9x...um8     wrk-2      Ready      Active \n    ```", "```The Swarm is now ready.\n\nThe `payment_gateway` service has set of placement constraints forcing it to only run on **worker nodes** with the `pcidss=yes` node label. In this step we\u2019ll add that node label to `wrk-1`.\n\nIn the real world you would harden at least one of your Docker nodes to PCI standards before labelling it. However, this is just a lab, so we\u2019ll skip the hardening step and just add the label to `wrk-1`.\n\nRun the following commands from the Swarm manager.\n\n1.  Add the node label to `wrk-1`.\n\n    ```", "```\n\n     `Node labels only apply within the Swarm.` \n`*   Verify the node label.\n\n    ```", "```` \n\n ``The `wrk-1` worker node is now configured so that it can run replicas for the `payment_gateway` service.\n\nThe application defines four secrets, all of which need creating before the app can be deployed:\n\n*   `postgress_password`\n*   `staging_token`\n*   `revprox_cert`\n*   `revprox_key`\n\nRun the following commands from the manager node to create them.\n\n1.  Create a new key pair.\n\n    Three of the secrets will be populated with cryptographic keys. We\u2019ll create the keys in this step and then place them inside of Docker secrets in the next steps.\n\n    ```\n     $ openssl req -newkey rsa:4096 -nodes -sha256 \\\n       -keyout domain.key -x509 -days 365 -out domain.crt \n    ```\n\n     `You\u2019ll have two new files in your current directory. We\u2019ll use them in the next step.` \n`*   Create the `revprox_cert`, `revprox_key`, and `postgress_password` secrets.\n\n    ```\n     $ docker secret create revprox_cert domain.crt\n     cqblzfpyv5cxb5wbvtrbpvrrj\n\n     $ docker secret create revprox_key domain.key\n     jqd1ramk2x7g0s2e9ynhdyl4p\n\n     $ docker secret create postgres_password domain.key\n     njpdklhjcg8noy64aileyod6l \n    ```\n\n    `*   Create the `staging_token` secret.\n\n    ```\n     $ echo staging | docker secret create staging_token -\n     sqy21qep9w17h04k3600o6qsj \n    ```\n\n    `*   List the secrets.\n\n    ```\n     $ docker secret ls\n     ID          NAME                CREATED              UPDATED\n     njp...d6l   postgres_password   47 seconds ago       47 seconds ago\n     cqb...rrj   revprox_cert        About a minute ago   About a minute ago\n     jqd...l4p   revprox_key         About a minute ago   About a minute ago\n     sqy...qsj   staging_token       23 seconds ago       23 seconds ago \n    ````", "```That\u2019s all of the pre-requisites taken care of. Time to deploy the app!\n\n##### Deploying the sample app\n\nIf you haven\u2019t already done so, clone the app\u2019s GitHub repo to your Swarm manager.\n\n```", "```\n\n `Now that you have the code, you are ready to deploy the app.\n\nStacks are deployed using the `docker stack deploy` command. In its basic form, it accepts two arguments:\n\n*   name of the stack file\n*   name of the stack\n\nThe application\u2019s GitHub repository contains a stack file called `docker-stack.yml`, so we\u2019ll use this as stack file. We\u2019ll call the stack `seastack`, though you can choose a different name if you don\u2019t like that.\n\nRun the following commands from within the `atsea-sample-shop-app` directory on the Swarm manager.\n\nDeploy the stack (app).\n\n```", "```\n\n `You can run `docker network ls` and `docker service ls` commands to see the networks and services that were deployed as part of the app.\n\nA few things to note from the output of the command.\n\nThe networks were created before the services. This is because the services attach to the networks, so need the networks to be created before they can start.\n\nDocker prepends the name of the stack to every resource it creates. In our example, the stack is called `seastack`, so all resources are named `seastack_<resource>`. For example, the `payment` network is called `seastack_payment`. Resources that were created prior to the deployment, such as secrets, do not get renamed.\n\nAnother thing to note is the presence of a network called `seastack_default`. This isn\u2019t defined in the stack file, so why was it created? Every service needs to attach to a network, but the `visualizer` service didn\u2019t specify one. Therefore, Docker created one called `seastack_default` and attached it to that.\n\nYou can verify the status of a stack with a couple of commands. `docker stack ls` lists all stacks on the system, including how many services they have. `docker stack ps <stack-name>` gives more detailed information about a particular stack, such as *desired state* and *current state*. Let\u2019s see them both.\n\n```", "```\n\n `The `docker stack ps` command is a good place to start when troubleshooting services that fail to start. It gives an overview of every service in the stack, including which node each replica is scheduled on, current state, desired state, and error message. The following output shows two failed attempts to start a replica for the `reverse_proxy` service on the `wrk-2` node.\n\n```", "```\n\n `For more detailed logs of a particular service you can use the `docker service logs` command. You pass it either the service name/ID, or replica ID. If you pass it the service name or ID, you\u2019ll get the logs for all service replicas. If you pass it a particular replica ID, you\u2019ll only get the logs for that replica.\n\nThe following `docker service logs` command shows the logs for all replicas in the `seastack_reverse_proxy` service that had the two failed replicas in the previous output.\n\n```", "```\n\n `The output is trimmed to fit the page, but you can see that logs from all three service replicas are shown (the two that failed and the one that\u2019s running). Each line starts with the name of the replica, which includes the service name, replica number, replica ID, and name of host that it\u2019s scheduled on. Following that is the log output.\n\n> **Note:** You might have noticed that all of the replicas in the previous output showed as replica number 1\\. This is because Docker created one at a time and only started a new one when the previous one had failed.\n\nIt\u2019s hard to tell because the output is trimmed to fit the book, but it looks like the first two replicas failed because they were relying on something in another service that was still starting (a sort of race condition when dependent services are starting).\n\nYou can follow the logs (`--follow`), tail them (`--tail`), and get extra details (`--details`).\n\nNow that the stack is up and running, let\u2019s see how to manage it.\n\n#### Managing the app\n\nWe know that a *stack* is set of related services and infrastructure that gets deployed and managed as a unit. And while that\u2019s a fancy sentence full of buzzwords, it reminds us that the stack is built from normal Docker resources \u2014 networks, volumes, secrets, services etc. This means we can inspect and reconfigure these with their normal docker commands: `docker network`, `docker volume`, `docker secret`, `docker service`\u2026\n\nWith this in mind, it\u2019s possible to use the `docker service` command to manage services that are part of the stack. A simple example would be using the `docker service scale` command to increase the number of replicas in the `appserver` service. However, **this is not the recommended method!**\n\nThe recommended method is the declarative method, which uses the stack file as the ultimate source of truth. As such, all changes to the stack should be made to the stack file, and the updated stack file used to redeploy the app.\n\nHere\u2019s a quick example of why the imperative method (making changes via the CLI) is bad:\n\n> *Imagine that we have a stack deployed from the `docker-stack.yml` file that we cloned from GitHub earlier in the chapter. This means we have two replicas of the `appserver` service. If we use the `docker service scale` command to change that to 4 replicas, the current state of the cluster will be 4 replicas, but the stack file will still define 2\\. Admittedly, that doesn\u2019t sound like the end of the world. However, imagine we then make a different change to the stack, this time via the stack file, and we roll it out with the `docker stack deploy` command. As part of this rollout, the number of `appserver` replicas in the cluster will be rolled back to 2, because this is what the stack file defines. For this kind of reason, it is recommended to make all changes to the application via the stack file, and to manage the file in a proper version control system.*\n\nLet\u2019s walk through the process of making a couple of declarative changes to the stack.\n\nWe\u2019ll make the following changes:\n\n*   Increase the number of `appserver` replicas from 2 to 10\n*   Increase the stop grace period for the visualizer service to 2 minutes\n\nEdit the `docker-stack.yml` file and update the following two values:\n\n*   `.services.appserver.deploy.replicas=10`\n*   `.services.visualizer.stop_grace_period=2m`\n\nThe relevant sections of the stack file will now look like this:\n\n```", "```\n\n `Save the file and redeploy the app.\n\n```", "```\n\n `Re-deploying the app like this will only update the changed components.\n\nRun a `docker stack ps` to see the number of `appserver` replicas increasing.\n\n```", "```\n\n `The output has been trimmed so that it fits on the page, and so that only the affected services are shown.\n\nNotice that there are two lines for the `visualizer` service. One line shows a replica that was shutdown 3 seconds ago, and the other line shows a replica that has been running for 1 second. This is because we pushed a change to the `visualizer` service, so Swarm terminated the existing replica and started a new one with the new `stop_grace_period` value.\n\nAlso note that we now have 10 replicas for the `appserver` service, and that they are in various states in the \u201cCURRENT STATE\u201d column \u2014 some are *running* whereas others are still *starting*.\n\nAfter enough time, the cluster will converge so that *desired state* and *current state* match. At that point, what is deployed and observed on the cluster will exactly match what is defined in the stack file. This is a happy place to be :-D\n\nThis update pattern should be used for all updates to the app/stack. I.e. **all changes should be made declaratively via the stack file, and rolled out using `docker stack deploy`**.\n\nThe correct way to delete a stack is with the `docker stack rm` command. Be warned though! It deletes the stack without asking for confirmation.\n\n```", "```\n\n `Notice that the networks and services were deleted, but the secrets were not. This is because the secrets were pre-created and existed before the stack was deployed. If your stack defines volumes at the top-level, these will not be deleted by `docker stack rm` either. This is because volumes are intended as long-term persistent data stores and exist independent of the lifecycle of containers, services, and stacks.\n\nCongratulations! You know how to deploy and manage a multi-service app using Docker Stacks.\n\n### Deploying apps with Docker Stacks - The Commands\n\n*   `docker stack deploy` is the command we use to deploy **and** update stacks of services defined in a stack file (usually `docker-stack.yml`).\n*   `docker stack ls` will list all stacks on the Swarm, including how many services they have.\n*   `docker stack ps` gives detailed information about a deployed stack. It accepts the name of the stack as its main argument, lists which node each replica is running on, and shows *desired state* and *current state*.\n*   `docker stack rm` is the command to delete a stack from the Swarm. It does not ask for confirmation before deleting the stack.\n\n### Chapter Summary\n\nStacks are the native Docker solution for deploying and managing multi-service applications. They\u2019re baked into the Docker engine, and offer a simple declarative interface for deploying and managing the entire lifecycle of an application.\n\nWe start with application code and a set of infrastructure requirements \u2014 things like networks, ports, volumes and secrets. We containerize the application and group together all of the app services and infrastructure requirements into a single declarative stack file. We set the number of replicas, as well as rolling update and restart policies. Then we take the file and deploy the application from it using the `docker stack deploy` command.\n\nFuture updates to the deployed app should be done declaratively by checking the stack file out of source control, updating it, re-deploying the app, and checking the stack file back in to source control.\n\nBecause the stack file defines things like number of service replicas, you should maintain separate stack files for each of your environments, such as dev, test and prod.```", "`````````````````````````````"]