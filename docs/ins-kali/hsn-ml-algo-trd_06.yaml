- en: The Machine Learning Process
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习过程
- en: 'In this chapter, we will start to illustrate how you can use a broad range
    of supervised and unsupervised **machine learning** (**ML**) models for algorithmic
    trading. We will explain each model''s assumptions and use cases before we demonstrate
    relevant applications using various Python libraries. The categories of models
    will include:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始说明如何使用广泛的监督和无监督**机器学习**（**ML**）模型进行算法交易。我们将在演示相关应用程序之前解释每个模型的假设和用例，使用各种Python库。模型的类别将包括：
- en: Linear models for the regression and classification of cross-section, time series,
    and panel data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于截面、时间序列和面板数据的线性模型进行回归和分类
- en: Generalized additive models, including non-linear tree-based models, such as
    **decision trees**
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义加性模型，包括非线性基于树的模型，如**决策树**
- en: Ensemble models, including random forest and gradient-boosting machines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成模型，包括随机森林和梯度提升机
- en: Unsupervised linear and nonlinear methods for dimensionality reduction and clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督线性和非线性降维和聚类方法
- en: Neural network models, including recurrent and convolutional architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络模型，包括循环和卷积结构
- en: Reinforcement learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习模型
- en: We will apply these models to the market, fundamental, and alternative data
    sources introduced in the first part of this book. We will further build on the
    material covered so far by showing you how to embed these models in an algorithmic
    trading strategy to generate or combine alpha factors or to optimize the portfolio-management
    process and evaluate their performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这些模型应用到本书第一部分介绍的市场、基本和替代数据来源。我们还将进一步建立在迄今为止涵盖的材料基础上，向您展示如何将这些模型嵌入到算法交易策略中，以生成或结合阿尔法因子，或优化投资组合管理流程并评估其性能。
- en: There are several aspects that many of these models and their uses have in common.
    This chapter covers these common aspects so that we can focus on model-specific
    usage in the following chapters. They include the overarching goal of learning
    a functional relationship from data by optimizing an objective or loss function.
    They also include the closely related methods of measuring model performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型及其用途有许多共同点。本章涵盖了这些共同点，以便我们可以在接下来的章节中专注于特定模型的使用。它们包括通过优化目标或损失函数来学习数据的功能关系的总体目标。它们还包括密切相关的测量模型性能的方法。
- en: We distinguish between unsupervised and supervised learning and supervised regression
    and classification problems, and outline use cases for algorithmic trading. We
    contrast the use of supervised learning for statistical inference of relationships
    between input and output data with the use for the prediction of future outputs
    from future inputs. We also illustrate how prediction errors are due to the model's
    bias or variance, or because of a high noise-to-signal ratio in the data. Most
    importantly, we present methods to diagnose sources of errors and improve your
    model's performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们区分了无监督学习和监督学习以及监督回归和分类问题，并概述了算法交易的用例。我们对比了使用监督学习进行统计推断输入和输出数据之间的关系和使用它来预测未来输入的输出。我们还说明了预测误差是由模型的偏差或方差引起的，或者是由数据中高噪音信号比引起的。最重要的是，我们提出了诊断错误来源并改进模型性能的方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: How supervised and unsupervised learning using data works
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用数据进行监督和无监督学习
- en: How to apply the ML workflow
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何应用机器学习工作流程
- en: How to formulate loss functions for regression and classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何为回归和分类制定损失函数
- en: How to train and evaluate supervised learning models
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练和评估监督学习模型
- en: How the bias-variance trade-off impacts prediction errors
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡如何影响预测误差
- en: How to diagnose and address prediction errors
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何诊断和解决预测误差
- en: How to train a model using cross-validation to manage the bias-variance trade-off
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用交叉验证训练模型来管理偏差-方差权衡
- en: How to implement cross-validation using scikit-learn
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用scikit-learn实现交叉验证
- en: Why the nature of financial data requires different approaches to out-of-sample
    testing
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么金融数据的性质需要不同的方法来进行样本外测试
- en: If you are already quite familiar with ML, feel free to skip ahead and dive
    right into learning how to use linear models to produce and combine alpha factors
    for an algorithmic trading strategy.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对机器学习已经非常熟悉，可以随意跳过并直接学习如何使用线性模型为算法交易策略生成和结合阿尔法因子。
- en: Learning from data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中学习
- en: 'There have been many definitions of ML, which all revolve around the automated
    detection of meaningful patterns in data. Two prominent examples include:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多关于机器学习的定义，它们都围绕着自动检测数据中有意义的模式。两个著名的例子包括：
- en: AI pioneer **Arthur Samuelson** defined ML in 1959 as a subfield of computer
    science that gives computers the ability to learn without being explicitly programmed.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI先驱**阿瑟·塞缪尔森**在1959年定义了机器学习，将其定义为计算机科学的一个子领域，使计算机能够在没有明确编程的情况下学习。
- en: '**Toni Mitchell**, one of the current leaders in the field, pinned down a well-posed
    learning problem more specifically in 1998: a computer program learns from experience
    with respect to a task and a performance measure whether the performance of the
    task improves with experience.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托尼·米切尔**，这个领域的现任领导者之一，更具体地在1998年确定了一个明确定义的学习问题：计算机程序根据任务和绩效度量从经验中学习，看任务的绩效是否随经验而提高。'
- en: Experience is presented to an algorithm in the form of training data. The principal
    difference to previous attempts at building machines that solve problems is that
    the rules that an algorithm uses to make decisions are learned from the data as
    opposed to being programmed or hard-coded—this was the case for expert systems
    prominent in the 1980s.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 经验以训练数据的形式呈现给算法。与以前尝试构建解决问题的机器不同的主要区别在于，算法用于做出决策的规则是从数据中学习而不是被编程或硬编码的——这是20世纪80年代突出的专家系统的情况。
- en: The key challenge of automated learning is to identify patterns in the training
    data that are meaningful when generalizing the model's learning to new data. There
    are a large number of potential patterns that a model could identify, while the
    training data only constitute a sample of the larger set of phenomena that the
    algorithm needs to perform the task in the future. The infinite number of functions
    that could generate the given outputs from the given input make the search process
    impossible to solve without restrictions on the eligible set of functions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动学习的关键挑战是在训练数据中识别有意义的模式，以便将模型的学习泛化到新数据。模型可能识别出大量潜在模式，而训练数据只是未来算法需要执行任务的更大现象集合的样本。可能生成给定输出的无限函数数量使得搜索过程无法在不限制合格函数集的情况下解决。
- en: The types of patterns that an algorithm is capable of learning are limited by
    the size of its hypothesis spaceon the one hand and the amount of information
    contained in the sample data on the other. The size of the hypothesis space varies
    significantly between algorithms. On the one hand, this limitation enables a successful
    search and on the other hand, it implies an inductive bias as the algorithm generalizes
    from the training sample to new data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 算法能够学习的模式类型一方面受其假设空间的大小限制，另一方面受样本数据中包含的信息量的限制。假设空间的大小在不同算法之间变化很大。一方面，这种限制使得成功搜索成为可能，另一方面，它意味着归纳偏差，因为算法从训练样本泛化到新数据。
- en: Hence, the key challenge becomes a matter of how to choose a model with a hypothesis
    space large enough to contain a solution to the learning problem, yet small enough
    to ensure reliable generalization given the size of the training data. With more
    and more informative data, a model with a larger hypothesis space will be successful.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关键挑战变成了如何选择一个假设空间足够大以包含学习问题的解决方案，但又足够小以确保在训练数据规模的情况下可靠泛化。随着越来越多的信息数据，具有更大假设空间的模型将会成功。
- en: The **no-free-lunch theorem** states that there is no universal learning algorithm.
    Instead, a learner's hypothesis space has to be tailored to a specific task using
    prior knowledge about the task domain in order for the search of meaningful patterns
    to succeed. We will pay close attention to the assumptions that a model makes
    about data relationships for a specific task throughout this chapter, and emphasize
    the importance of matching these assumptions with empirical evidence gleaned from
    data exploration. The process required to master the task can be differentiated
    into supervised, unsupervised, and reinforcement learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: “没有免费午餐定理”表明不存在通用学习算法。相反，学习者的假设空间必须根据任务领域的先验知识进行定制，以便成功搜索有意义的模式。在本章中，我们将密切关注模型对特定任务数据关系所做的假设，并强调将这些假设与从数据探索中获得的经验证据相匹配的重要性。掌握任务所需的过程可以区分为监督、无监督和强化学习。
- en: Supervised learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: Supervised learning is the most commonly used type of ML. We will dedicate most
    of the chapters in this book to learning about the various applications of models
    in this category. The term *supervised* implies the presence of an outcome variable
    that guides the learning process—that is, it teaches the algorithm the correct
    solution to the task that is being learned. Supervised learning aims at generalizing
    a functional relationship between input and output data that is learned from individual
    samples and applying it to new data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是最常用的ML类型。本书的大部分章节将致力于学习该类别中模型的各种应用。术语“监督”意味着存在一个指导学习过程的结果变量，即它教给算法正在学习的任务的正确解决方案。监督学习旨在泛化从个体样本学习到新数据的输入和输出数据之间的功能关系。
- en: The output variable is also, depending on the field, interchangeably called
    the label, target, outcome, endogenous, or left-hand-side variable. We will use
    *y[i]* for observations *i = 1, ..., N*, or *y* in vector notation. Some tasks
    are represented by several outcomes, also called **multilabel problems**. The input
    data for a supervised learning problem is also known as features, exogenous, and
    right-hand-side variables, denoted by an *x*[*i* ]for a vector of features for observations
    *i = 1, ..., N*, or *X* in matrix notation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 输出变量在不同领域中也被称为标签、目标、结果、内生或左侧变量。我们将使用*y[i]*表示观测*i = 1, ..., N*，或者在向量表示中使用*y*。一些任务由多个结果表示，也称为多标签问题。监督学习问题的输入数据也被称为特征、外生和右侧变量，用*x*[*i*]表示*i
    = 1, ..., N*的特征向量，或者在矩阵表示中使用*X*。
- en: The solution to a supervised learning problem is a function (![](img/3bd38acb-3634-4212-894b-50e191c3f15c.png)) that
    represents what the model learned about the input-output relationship from the
    sample and approximates the true relationship, represented with ![](img/26f61bd2-e8e2-4a3d-a5ae-196b9ae3e691.png).
    This function can be used to infer statistical associations or potentially even
    causal relationships among variables of interest beyond the sample, or it can
    be used to predict outputs for new input data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题的解决方案是一个表示模型从样本中学到的输入-输出关系的函数，近似真实关系。这个函数可以用于推断样本之外感兴趣变量之间的统计关联或潜在因果关系，也可以用于预测新输入数据的输出。
- en: 'Both goals face an important trade-off: more complex models have more *moving
    parts* that are capable of representing more nuanced relationships, but they may
    also be more difficult to inspect. They are also likely to overfit and learn random
    noise particular to the training sample, as opposed to a systematic signal that
    represents a general pattern of the input-output relationship. Overly simple models,
    on the other hand, will miss signals and deliver biased results. This trade-off
    is known as the **bias-variance trade-off** in supervised learning, but conceptually
    this also applies to the other forms of ML where overly complex models may perform
    poorly beyond the training data.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个目标都面临一个重要的权衡：更复杂的模型有更多的“活动部分”，能够表示更微妙的关系，但也可能更难以检查。它们也可能过度拟合并学习训练样本特有的随机噪音，而不是代表输入-输出关系的一般模式的系统信号。另一方面，过于简单的模型会错过信号并产生偏见的结果。这种权衡在监督学习中被称为**偏差-方差权衡**，但在概念上也适用于其他形式的机器学习，过于复杂的模型可能在训练数据之外表现不佳。
- en: Unsupervised learning
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: When solving an unsupervised learning problem, we only observe the features
    and have no measurements of the outcome. Instead of the prediction of future outcomes
    or the inference of relationships among variables, the task is to find structure
    in the data without any outcome information to guide the search process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决无监督学习问题时，我们只观察特征，没有测量结果。任务不是预测未来结果或推断变量之间的关系，而是在没有任何结果信息的情况下在数据中找到结构。
- en: Often, unsupervised algorithms aim to learn a new representation of the input
    data that is useful for some other tasks. This includes coming up with labels
    that identify commonalities among observations, or a summarized description that
    captures relevant information while requiring data points or features. Unsupervised
    learning algorithms also differ from supervised learning algorithms in the assumptions
    they make about the nature of the structure they are aiming to discover.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，无监督算法旨在学习输入数据的新表示形式，这对其他任务有用。这包括提出标签，以识别观察之间的共同点，或者概括描述，捕捉相关信息，同时需要数据点或特征。无监督学习算法在对它们试图发现的结构的性质上与监督学习算法有所不同。
- en: Applications
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'There are several helpful uses of unsupervised learning that can be applied
    to algorithmic trading, including the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习有几个有用的应用，可以应用于算法交易，包括以下内容：
- en: Grouping together securities with similar risk and return characteristics (see
    hierarchical risk parity in this chapter (which looks at portfolio optimization))
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将具有相似风险和回报特征的证券分组在一起（请参阅本章中的层次风险平价（查看投资组合优化））
- en: Finding a small number of risk factors driving the performance of a much larger
    number of securities
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到驱动大量证券表现的少数风险因素
- en: Identifying trading and price patterns that differ systematically and may pose
    higher risks
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别交易和价格模式的差异，可能带来更高的风险
- en: Identifying latent topics in a body of documents (for example, earnings call
    transcripts) that comprise the most important aspects of those documents
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别文档集合（例如，收益电话转录）中的潜在主题，这些主题包括这些文档的最重要方面
- en: At a high level, these applications rely on methods to identify clusters and
    methods to reduce the dimensionality of the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，这些应用依赖于识别聚类的方法和减少数据维度的方法。
- en: Cluster algorithms
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Cluster algorithms use a measure of similarity to identify observations or data
    attributes that contain similar information. They summarize a dataset by assigning
    a large number of data points to a smaller number of clusters so that the cluster
    members are more closely related to each other than to members of other clusters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法使用相似度度量来识别包含相似信息的观察或数据属性。它们通过将大量数据点分配给较少数量的簇来总结数据，以便簇成员之间的关系比其他簇的成员更密切。
- en: 'Cluster algorithms primarily differ with respect to the type of clusters that
    they will produce, which implies different assumptions about the data generation
    process, listed as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法主要在于它们产生的聚类类型，这意味着它们对数据生成过程的不同假设，列举如下：
- en: '**K-means clustering**: Data points belong to one of the *k* clusters of equal
    size that take an elliptical form'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K均值聚类**：数据点属于* k *个等大小的椭圆形簇之一'
- en: '**Gaussian mixture models**: Data points have been generated by any of the
    various multivariate normal distributions'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合模型**：数据点是由各种多元正态分布生成的'
- en: '**Density-based clusters**: Clusters are of an arbitrary shape and are defined
    only by the existence of a minimum number of nearby data points'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的聚类**：聚类的形状是任意的，仅由附近数据点的最小数量的存在定义'
- en: '**Hierarchical clusters**: Data points belong to various supersets of groups
    that are formed by successively merging smaller clusters'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**：数据点属于通过逐步合并较小簇形成的各种超集的组'
- en: Dimensionality reduction
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Dimensionality reduction produces new data that captures the most important
    information contained in the source data. Rather than grouping existing data into
    clusters, these algorithms transform existing data into a new dataset that uses
    significantly fewer features or observations to represent the original information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 降维产生新数据，捕获源数据中包含的最重要信息。这些算法不是将现有数据分组成簇，而是将现有数据转换为一个使用更少特征或观察来表示原始信息的新数据集。
- en: 'Algorithms differ with respect to the nature of the new dataset they will produce,
    as shown in the following list:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在于它们将产生的新数据集的性质，如下列表所示：
- en: '**Principal component analysis (PCA)**: Finds the linear transformation that
    captures most of the variance in the existing dataset'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**：找到捕获现有数据集中大部分方差的线性变换'
- en: '**Manifold learning**: Identifies a nonlinear transformation that produces
    a lower-dimensional representation of the data'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流形学习**：识别产生数据低维表示的非线性转换'
- en: '**Autoencoders**: Uses neural networks to compress data non-linearly with minimal
    loss of information'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器**：使用神经网络以最小的信息损失非线性压缩数据'
- en: We will dive deeper into linear, non-linear, and neural-network-based unsupervised
    learning models in several of the following chapters, including important applications of
    **natural language processing** (**NLP**) in the form of topic modeling and Word2vec
    feature extraction.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几章中更深入地讨论线性、非线性和基于神经网络的无监督学习模型，包括**自然语言处理**（**NLP**）在主题建模和Word2vec特征提取方面的重要应用。
- en: Reinforcement learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: '**Reinforcement learning** is the third type of ML. It aims to choose the action
    that yields the highest reward, given a set of input data that describes a context
    or environment. It is both dynamic and interactive: the stream of positive and
    negative rewards impacts the algorithm''s learning, and actions taken now may
    influence both the environment and future rewards.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**是第三种ML类型。它旨在选择产生最高奖励的行动，给定描述环境或背景的一组输入数据。它既是动态的又是交互式的：正面和负面奖励的流影响算法的学习，当前采取的行动可能影响环境和未来的奖励。'
- en: The trade-off between the exploitation of a course of action that has been learned
    to yield a certain reward and the exploration of new actions that may increase
    the reward in the future gives rise to a trial-and-error approach. Reinforcement
    learning optimizes the agent's learning using dynamical systems theory and, in
    particular, the optimal control of Markov decision processes with incomplete information.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在利用已学到的行动产生一定奖励和探索可能在未来增加奖励的新行动之间的权衡产生了一种试错方法。强化学习使用动态系统理论和特别是具有不完全信息的马尔可夫决策过程的最优控制来优化代理的学习。
- en: Reinforcement learning differs from supervised learning, where the available
    training data lays out both the context and the correct decision for the algorithm.
    It is tailored to interactive settings where the outcomes only become available
    over time and learning has to proceed in an *online* or continuous fashion as
    the agent acquires new experience. However, some of the most notable progress
    in **Artificial Intelligence** (**AI**) involves reinforcement that uses deep
    learning to approximate functional relationships between actions, environments,
    and future rewards. It also differs from unsupervised learning because feedback
    of the consequences will be available, albeit with a delay.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不同于监督学习，监督学习中可用的训练数据既提供了背景又提供了算法的正确决策。它专门针对交互式设置，其中结果只会随着时间的推移才会变得可用，学习必须以*在线*或连续的方式进行，因为代理获取新经验。然而，人工智能中一些最显著的进展涉及使用深度学习来近似行动、环境和未来奖励之间的功能关系。它也不同于无监督学习，因为尽管有延迟，但后果的反馈是可用的。
- en: Reinforcement learning is particularly suitable for algorithmic trading because
    the concept of a return-maximizing agent in an uncertain, dynamic environment
    has much in common with an investor or a trading strategy that interacts with
    financial markets. This approach has been successfully applied to game-playing
    agents, most prominently to the game of Go, but also to complex video games. It
    is also used in robotics—for example, self-driving cars—or to personalize services
    such as website offerings based on user interaction. We will introduce reinforcement
    learning approaches to building an algorithmic trading strategy in Chapter 21,
    *Reinforcement Learning*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习特别适用于算法交易，因为在不确定的动态环境中，最大化回报的代理概念与与金融市场互动的投资者或交易策略有很多共同之处。这种方法已成功应用于游戏代理，尤其是围棋游戏，但也应用于复杂的视频游戏。它还用于机器人技术
    - 例如，自动驾驶汽车 - 或者根据用户互动个性化服务，例如网站提供。我们将在第21章*强化学习*中介绍强化学习方法来构建算法交易策略。
- en: The machine learning workflow
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: Developing a ML solution for an algorithmic trading strategy requires a systematic
    approach to maximize the chances of success while economizing on resources. It
    is also very important to make the process transparent and replicable in order
    to facilitate collaboration, maintenance, and later refinements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为算法交易策略开发ML解决方案需要系统化的方法，以最大化成功的机会并节约资源。同时，使过程透明和可复制也非常重要，以促进合作、维护和后续的改进。
- en: 'The following chart outlines the key steps from problem definition to the deployment
    of a predictive solution:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表概述了从问题定义到部署预测解决方案的关键步骤：
- en: '![](img/7e355379-9a0d-4652-94f1-55e430698c19.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e355379-9a0d-4652-94f1-55e430698c19.png)'
- en: 'The process is iterative throughout the sequence, and the effort required at
    different stages will vary according to the project, but this process should generally
    include the following steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程在整个序列中是迭代的，不同阶段需要的工作量会根据项目而变化，但这个过程通常应包括以下步骤：
- en: Frame the problem, identify a target metric, and define success
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建问题框架，确定目标指标，并定义成功
- en: Source, clean, and validate the data
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集、清洗和验证数据
- en: Understand your data and generate informative features
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解数据并生成信息性特征
- en: Pick one or more machine learning algorithms suitable for your data
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个或多个适合您数据的机器学习算法
- en: Train, test, and tune your models
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练、测试和调整模型
- en: Use your model to solve the original problem
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的模型解决原始问题
- en: We will walk through these steps in the following sections using a simple example
    to illustrate some of the key points.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中通过一个简单的例子来走过这些步骤，以阐明一些关键点。
- en: Basic walkthrough – k-nearest neighbors
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本步骤 - k最近邻
- en: The `machine_learning_workflow.ipynb`notebook in this chapter's folder of the
    book's GitHub repository contains several examples that illustrate the machine
    learning workflow using a dataset of house prices.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的GitHub存储库中的`machine_learning_workflow.ipynb`笔记本包含了使用房价数据集演示机器学习工作流程的几个示例。
- en: We will use the fairly straightforward **k-nearest neighbors** (**KNN**) algorithm
    that allows us to tackle both regression and classification problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相当直观的**k最近邻**（**KNN**）算法，它允许我们解决回归和分类问题。
- en: In its default `sklearn` implementation, it identifies the `k` nearest data
    points (based on the Euclidean distance) to make a prediction. It predicts the
    most frequent class among the neighbors or the average outcome in the classification
    or regression case, respectively.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在其默认的`sklearn`实现中，它识别`k`个最近的数据点（基于欧几里得距离）来进行预测。在分类或回归情况下，它分别预测邻居中最频繁的类别或平均结果。
- en: Frame the problem – goals and metrics
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定问题-目标和指标
- en: The starting point for any machine learning exercise is the ultimate use case
    it aims to address. Sometimes, this goal will be statistical inference in order
    to identify an association between variables or even a causal relationship. Most
    frequently, however, the goal will be the direct prediction of an outcome to yield
    a trading signal.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习练习的起点都是其旨在解决的最终用例。有时，这个目标将是统计推断，以便识别变量之间的关联或甚至因果关系。然而，最常见的目标将是直接预测结果以产生交易信号。
- en: 'Both inference and prediction use metrics to evaluate how well a model achieves
    its objective. We will focus on common objective functions and the corresponding
    error metrics for predictive models that can be distinguished by the variable
    type of the output: continuous output variables imply a regression problem, categorical
    variables imply classification, and the special case of ordered categorical variables
    implies ranking problems.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 推断和预测都使用指标来评估模型实现其目标的程度。我们将重点关注常见的目标函数以及用于区分输出变量类型的预测模型的相应误差指标：连续输出变量意味着回归问题，分类变量意味着分类，有序分类变量的特殊情况意味着排名问题。
- en: The problem may be the efficient combination of several alpha factors and could
    be framed as a regression problem that aims to predict returns, a binary classification
    problem that aims to predict the direction of future price movements, or a multiclass
    problem that aims to assign stocks to various performance classes. In the following
    section, we will introduce these objectives and look at how to measure and interpret
    related error metrics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 问题可能是有效地组合几个alpha因子，并且可以被构建为旨在预测回报的回归问题，旨在预测未来价格走势方向的二元分类问题，或旨在将股票分配到各种绩效类别的多类问题。在接下来的部分中，我们将介绍这些目标，并探讨如何测量和解释相关的误差指标。
- en: Prediction versus inference
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测与推断
- en: The functional relationship produced by a supervised learning algorithm can
    be used for inference—that is, to gain insights into how the outcomes are generated—or
    for prediction—that is, to generate accurate output estimates (represented by ![](img/ff88b978-e72b-4c0c-b9a9-c837a8f06f32.png)) for unknown
    or future inputs (represented by *X*).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法产生的功能关系可以用于推断，即获得有关结果生成方式的见解，或用于预测，即为未知或未来的输入生成准确的输出估计（由![](img/ff88b978-e72b-4c0c-b9a9-c837a8f06f32.png)表示）。
- en: For algorithmic trading, inference can be used to estimate the causal or statistical
    dependence of the returns of an asset on a risk factor, whereas prediction can
    be used to forecast the risk factor. Combining the two can yield a prediction
    of the asset price, which in turn can be translated into a trading signal.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算法交易，推断可以用于估计资产回报与风险因素之间的因果或统计依赖关系，而预测可以用于预测风险因素。结合这两者可以产生资产价格的预测，进而可以转化为交易信号。
- en: Statistical inference is about drawing conclusions from sample data about the
    parameters of the underlying probability distribution or the population. Potential
    conclusions include hypothesis tests about the characteristics of the distribution
    of an individual variable, or the existence or strength of numerical relationships
    among variables. They also include point or interval estimates of statistical
    metrics.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 统计推断是关于从样本数据中对概率分布的参数或总体进行推断的。潜在的结论包括关于单个变量分布特征的假设检验，或者变量之间的数值关系的存在或强度的假设检验。它们还包括统计指标的点估计或区间估计。
- en: Inference depends on the assumptions about the process that generates the data
    in the first place. We will review these assumptions and the tools that are used
    for inference with linear models where they are well established. More complex
    models make fewer assumptions about the structural relationship between input
    and output, and instead approach the task of function approximation more openly
    while treating the data-generating process as a black box. These models, including
    decision trees, ensemble models, and neural networks, are focused on and often
    outperform when used for prediction tasks. However, random forests have recently
    gained a framework for inference that we will introduce later.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 推断取决于首先生成数据的过程的假设。我们将回顾这些假设以及用于线性模型推断的工具，其中它们得到了很好的建立。更复杂的模型对输入和输出之间的结构关系做出更少的假设，并且更加开放地处理函数逼近的任务，同时将生成数据的过程视为黑匣子。这些模型，包括决策树、集成模型和神经网络，在用于预测任务时通常专注并且经常表现出色。然而，随机森林最近已经获得了推断的框架，我们将在后面介绍。
- en: Causal inference
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 因果推断
- en: Causal inference aims to identify relationships so that certain input values
    imply certain outputs—for example, a certain constellation of macro variables
    causing the price of a given asset to move in a certain way, assuming all other
    variables remain constant.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断旨在识别关系，以便某些输入值暗示某些输出，例如，假设所有其他变量保持不变，某些宏观变量的特定组合导致给定资产价格以某种方式变动。
- en: Statistical inference about relationships among two or more variables produces
    measures of correlation that can only be interpreted as a causal relationship
    when several other conditions are met—for example, when alternative explanations
    or reverse causality has been ruled out. Meeting these conditions requires an
    experimental setting where all relevant variables of interest can be fully controlled
    to isolate causal relationships. Alternatively, quasi-experimental settings expose
    units of observations to changes in inputs in a randomized way to rule out that
    other observable or unobservable features are responsible for the observed effects
    of the change in the environment.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 关于两个或多个变量之间关系的统计推断产生的相关性度量只有在满足其他几个条件时才能被解释为因果关系，例如，当排除了替代解释或逆向因果关系时。满足这些条件需要一个实验设置，其中所有感兴趣的相关变量都可以完全控制，以隔离因果关系。另外，准实验设置以随机方式暴露观察单位对输入的变化，以排除其他可观察或不可观察的特征对环境变化的观察效果负责的可能性。
- en: These conditions are rarely met so inferential conclusions need to be treated
    with care. The same applies to the performance of predictive models that also
    rely on the statistical association between features and outputs, which may change
    with other factors that are not part of the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些条件很少得到满足，因此推论性结论需要谨慎对待。同样适用于依赖于特征和输出之间的统计关联的预测模型的性能，这种关联可能会随着不属于模型的其他因素的改变而改变。
- en: The non-parametric nature of the KNN model does not lend itself well to inference,
    so we'll postpone this step in the workflow until we encounter linear models in
    the next chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: KNN模型的非参数特性不利于推断，因此我们将推迟工作流程中的这一步，直到我们在下一章遇到线性模型为止。
- en: Regression problems
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归问题
- en: Regression problems aim to predict a continuous variable. The **root**-**mean-square error**
    (**RMSE**) is the most popular loss function and error metric, not least because
    it is differentiable. The loss is symmetric, but larger errors weigh more in the
    calculation. Using the square root has the advantage of measuring the error in
    the units of the target variable. The same metric in combination with the **RMSE
    log of the error** (**RMSLE**) is appropriate when the target is subject to exponential
    growth because of its asymmetric penalty that weights negative errors less than
    positive errors. You can also log-transform the target first and then use the
    RMSE, as we do in the example later in this section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题旨在预测连续变量。**均方根误差**（**RMSE**）是最受欢迎的损失函数和误差度量，因为它是可微分的。损失是对称的，但较大的误差在计算中权重更大。使用平方根的优势在于以目标变量的单位度量误差。与**RMSE误差的对数**（**RMSLE**）结合使用相同的度量是合适的，当目标受到指数增长的影响时，因为它的不对称惩罚使负误差的权重小于正误差。您还可以首先对目标进行对数变换，然后使用RMSE，就像我们在本节后面的示例中所做的那样。
- en: The **mean absolute errors** (**MAE**) and **median absolute errors** (**MedAE**)
    are symmetric but do not weigh larger errors more. The MedAE is robust to outliers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）和**中位数绝对误差**（**MedAE**）是对称的，但不会使较大的误差更重要。MedAE对异常值具有鲁棒性。'
- en: The explained variance score computes the proportion of the target variance
    that the model accounts for and varies between 0 and 1\. The R² score or coefficient
    of determination yields the same outcome the mean of the residuals is 0, but can
    differ otherwise. In particular, it can be negative when calculated on out-of-sample
    data (or for a linear regression without intercept).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 解释的方差得分计算模型解释的目标方差的比例，介于0和1之间。R²分数或确定系数产生相同的结果，即残差的均值为0，但在其他情况下可能会有所不同。特别是，在对样本外数据进行计算时（或者对没有截距的线性回归进行计算时），它可能是负数。
- en: 'The following table defines the formulas used for calculation and the corresponding
    `sklearn` function that can be imported from the `metrics` module. The `scoring `parameter
    is used in combination with automated train-test functions (such as `cross_val_score`
    and `GridSearchCV`) that we will introduce later in this section, and which are
    illustrated in the accompanying notebook:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格定义了用于计算的公式和相应的`sklearn`函数，可以从`metrics`模块中导入。`scoring`参数与自动化的训练-测试函数（如`cross_val_score`和`GridSearchCV`）结合使用，我们将在本节后面介绍，并在附带的笔记本中进行演示：
- en: '| **Name ** | **Formula** | **sklearn** | **Scoring parameter** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **公式** | **sklearn** | **评分参数** |'
- en: '| Mean squared error | ![](img/44aab2d2-d348-4d35-84d6-4e123f612378.png) |
    `mean_squared_error` | `neg_mean_squared_error` |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 均方误差 | ![](img/44aab2d2-d348-4d35-84d6-4e123f612378.png) | `mean_squared_error`
    | `neg_mean_squared_error` |'
- en: '| Mean squared log error | ![](img/e16997d7-d66c-4974-bcf0-6162216cbb8c.png)
    | `mean_squared_log_error` | `neg_mean_squared_log_error` |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 均方对数误差 | ![](img/e16997d7-d66c-4974-bcf0-6162216cbb8c.png) | `mean_squared_log_error`
    | `neg_mean_squared_log_error` |'
- en: '| Mean absolute error | ![](img/32b35cf0-ffd8-45c5-a8a3-fc90a334258b.png) |
    `mean_absolute_error` | `neg_mean_absolute_error` |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 平均绝对误差 | ![](img/32b35cf0-ffd8-45c5-a8a3-fc90a334258b.png) | `mean_absolute_error`
    | `neg_mean_absolute_error` |'
- en: '| Median absolute error | ![](img/023b4786-ab8a-4929-9bf8-6e8eb0a458dc.png)
    | `median_absolute_error` | `neg_median_absolute_error` |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 中位数绝对误差 | ![](img/023b4786-ab8a-4929-9bf8-6e8eb0a458dc.png) | `median_absolute_error`
    | `neg_median_absolute_error` |'
- en: '| Explained variance | ![](img/f19a2388-1866-4e7c-b552-e92f72083642.png) |
    `explained_variance_score` | `explained_variance` |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 解释方差 | ![](img/f19a2388-1866-4e7c-b552-e92f72083642.png) | `explained_variance_score`
    | `explained_variance` |'
- en: '| R2 score | ![](img/2561413f-fcfd-4bd9-9b85-8fd55f4c9012.png) | `r2_score`
    | `r2` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| R2分数 | ![](img/2561413f-fcfd-4bd9-9b85-8fd55f4c9012.png) | `r2_score` | `r2`
    |'
- en: 'The following screenshot shows the various error metrics for the house price
    regression demonstrated in the notebook:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了笔记本中展示的房价回归的各种误差度量：
- en: '![](img/2e196a95-046a-486e-a190-8476e6604cf3.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e196a95-046a-486e-a190-8476e6604cf3.png)'
- en: The `sklearn` function also supports multilabel evaluation—that is, assigning
    multiple outcome values to a single observation; see the documentation referenced
    on GitHub for more details ([https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`函数还支持多标签评估，即为单个观察结果分配多个结果值；有关更多详细信息，请参阅GitHub上引用的文档（[https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading/tree/master/Chapter06)）。'
- en: Classification problems
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类问题
- en: Classification problems have categorical outcome variables. Most predictors
    will output a score to indicate whether an observation belongs to a certain class.
    In the second step, these scores are then translated into actual predictions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题具有分类结果变量。大多数预测器会输出一个分数，以指示观察是否属于某个类别。在第二步，这些分数被转化为实际预测。
- en: In the binary case, where we will label the classes positive and negative, the
    score typically varies between zero or is normalized accordingly. Once the scores are
    converted into 0-1 predictions, there can be four outcomes, because each of the
    two existing classes can be either correctly or incorrectly predicted. With more
    than two classes, there can be more cases if you differentiate between the several potential
    mistakes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元情况下，我们将类别标记为正和负，分数通常在零和一之间变化或相应地进行归一化。一旦分数转换为0-1预测，就会有四种结果，因为两个现有类别中的每一个都可以被正确或错误地预测。如果区分多于两个类别，那么如果区分多于两个类别，就会有更多的情况。
- en: 'All error metrics are computed from the breakdown of predictions across the
    four fields of the 2 x 2 confusion matrix that associates actual and predicted
    classes. The metrics listed in the table shown in the following diagram, such
    as accuracy, evaluate a model for a given threshold:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所有错误指标都是从2x2混淆矩阵的四个领域的预测中计算出来的，该矩阵将实际和预测类别相关联。表中列出的指标，如准确度，评估了给定阈值的模型：
- en: '![](img/a5196319-bcc0-4ff8-a447-aa59982f60fc.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5196319-bcc0-4ff8-a447-aa59982f60fc.png)'
- en: A classifier does not necessarily need to output calibrated probabilities, but
    should rather produce scores that are relative to each other in distinguishing
    positive from negative cases. Hence, the threshold is a decision variable that
    can and should be optimized, taking into account the costs and benefits of correct
    and incorrect predictions. A lower threshold implies more positive predictions,
    with a potentially rising false positive rate, and for a higher threshold, the
    opposite is likely to be true.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器不一定需要输出校准的概率，而应该产生相对于彼此的分数，以区分正负案例。因此，阈值是一个决策变量，可以并且应该进行优化，考虑到正确和错误预测的成本和收益。较低的阈值意味着更多的正预测，可能会导致假阳性率上升，而较高的阈值则相反。
- en: Receiver operating characteristics and the area under the curve
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收者操作特征和曲线下面积
- en: The **receiver operating characteristics** (**ROC**) curve allows us to visualize,
    organize, and select classifiers based on their performance. It computes all the
    combinations of **true positive rates** (**TPR**) and **false positive rates**
    (**FPR**) that result from producing predictions using any of the predicted scores
    as a threshold. It then plots these pairs on a square, the side of which has a
    measurement of one in length.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）曲线允许我们根据它们的性能来可视化、组织和选择分类器。它计算了使用任何预测分数作为阈值进行预测所产生的所有**真正率**（**TPR**）和**假正率**（**FPR**）的组合。然后将这些对绘制在一个边长为1的正方形上。'
- en: A classifier that makes random predictions (taking into account class imbalance)
    will on average yield TPR and FPR that are equal so that the combinations will
    lie on the diagonal, which becomes the benchmark case. Since an underperforming
    classifier would benefit from relabeling the predictions, this benchmark also
    becomes the minimum.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个进行随机预测的分类器（考虑到类别不平衡）平均会产生相等的TPR和FPR，使得组合位于对角线上，这成为基准情况。由于表现不佳的分类器会受益于重新标记预测，因此这个基准也成为最小值。
- en: The **area under the curve** (**AUC**) is defined as the area under the ROC
    plot that varies between 0.5 and the maximum of 1\. It is a summary measure of
    how well the classifier's scores are able to rank data points with respect to
    their class membership. More specifically, the AUC of a classifier has the important
    statistical property of representing the probability that the classifier will
    rank a randomly chosen positive instance higher than a randomly chosen negative
    instance, which is equivalent to the Wilcoxon ranking test. In addition, the AUC
    has the benefit of not being sensitive to class imbalances.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**曲线下面积**（**AUC**）被定义为ROC图中的面积，其值在0.5和最大值1之间变化。它是分类器分数能够根据其类成员资格对数据点进行排名的摘要度量。更具体地说，分类器的AUC具有重要的统计特性，表示分类器将随机选择的正实例排在随机选择的负实例之前的概率，这等同于Wilcoxon排名检验。此外，AUC具有不对类别不平衡敏感的好处。'
- en: Precision-recall curves
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线
- en: 'When predictions for one of the classes are of particular interest, precision
    and recall curves visualize the trade-off between these error metrics for different
    thresholds. Both measures evaluate the quality of predictions for a particular
    class. The following list shows how they are applied to the positive class:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当对其中一个类别的预测特别感兴趣时，精确度和召回率曲线可视化了这些错误指标在不同阈值下的权衡。这两个指标评估了特定类别的预测质量。以下列表显示了它们如何应用于正类：
- en: '**Recall** measures the share of actual positive class members that a classifier
    predicts as positive for a given threshold. It originates in information retrieval
    and measures the share of relevant documents successfully identified by a search
    algorithm.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**衡量了分类器对于给定阈值预测为正的实际正类成员的比例。它起源于信息检索，衡量了搜索算法成功识别相关文档的比例。'
- en: '**Precision**, in contrast, measures the share of positive predictions that
    are correct.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度**，相反，衡量了正确预测的占比。'
- en: Recall typically increases with a lower threshold, but precision may decrease.
    Precision-recall curves visualize the attainable combinations and allow for the
    optimization of the threshold given the costs and benefits of missing a lot of
    relevant cases or producing lower-quality predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率通常随着较低的阈值而增加，但精度可能会降低。精确率-召回率曲线可视化了可达到的组合，并允许根据错过大量相关案例或产生较低质量预测的成本和收益来优化阈值。
- en: The F1 scoreis a harmonic mean of precision and recall for a given threshold
    and can be used to numerically optimize the threshold while taking into account
    the relative weights that these two metrics should assume.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数是给定阈值的精确率和召回率的调和平均数，并且可以用于在考虑这两个指标应该承担的相对权重的情况下进行数值优化阈值。
- en: 'The following chart illustrates the ROC curve and corresponding AUC alongside
    the precision-recall curve and the F1 score that, using equal weights for precision
    and recall, yields an optimal threshold of 0.37\. The chart is taken from the
    accompanying notebook where you can find the code for the KNN classifier that
    operates on binarized housing prices:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了ROC曲线和相应的AUC，以及精确率-召回率曲线和F1分数，使用精确率和召回率的相等权重，得出了0.37的最佳阈值。该图取自伴随的笔记本，您可以在其中找到操作二值化房价的KNN分类器的代码：
- en: '![](img/c4da6c85-3930-49a8-a440-a2aa3172c2c1.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4da6c85-3930-49a8-a440-a2aa3172c2c1.png)'
- en: Collecting and preparing the data
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和准备数据
- en: We already addressed important aspects of the sourcing of market, fundamental,
    and alternative data, and will continue to work with various examples of these
    sources as we illustrate the application of the various models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了市场、基本和替代数据的采集重要方面，并将继续使用各种示例来说明各种模型的应用。
- en: In addition to market and fundamental data that we will access through the Quantopian
    platform, we will also acquire and transform text data as we explore natural language
    processing and image data when we look at image processing and recognition. Besides
    obtaining, cleaning, and validating the data to relate it to trading data typically
    available in a time-series format, it is important to store it in a format that
    allows for fast access to enable quick exploration and iteration. We have recommended
    the HDF and parquet formats. For larger data volumes, Apache Spark represents
    the best solution.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们将通过Quantopian平台访问的市场和基本数据，我们还将在探索自然语言处理和图像处理和识别时获取和转换文本数据和图像数据。除了获取、清理和验证数据以将其与通常以时间序列格式可用的交易数据相关联之外，还重要的是将其存储在可以快速访问以实现快速探索和迭代的格式中。我们推荐使用HDF和parquet格式。对于更大的数据量，Apache
    Spark是最佳解决方案。
- en: Explore, extract, and engineer features
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索、提取和设计特征
- en: 'Understanding the distribution of individual variables and the relationships
    among outcomes and features is the basis for picking a suitable algorithm. This
    typically starts with visualizations such as scatter plots, as illustrated in
    the companion notebook (and shown in the following image), but also includes numerical
    evaluations ranging from linear metrics, such as the correlation, to nonlinear
    statistics, such as the Spearman rank correlation coefficient that we encountered
    when we introduced the information coefficient. It also includes information-theoretic
    measures, such as mutual information, as illustrated in the next subsection:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 了解单个变量的分布以及结果和特征之间的关系是选择合适算法的基础。这通常从可视化开始，例如散点图，如伴随笔记本中所示（并在下图中显示），但也包括从线性度量，如相关性，到非线性统计，如我们在介绍信息系数时遇到的Spearman秩相关系数的数值评估。它还包括信息理论度量，如互信息，如下一小节所示：
- en: '![](img/56162613-8d69-409f-affe-4be75c407875.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56162613-8d69-409f-affe-4be75c407875.png)'
- en: Scatter plots
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图
- en: 'A systematic exploratory analysis is also the basis of what is often the single
    most important ingredient of a successful predictive model: the engineering of features
    that extract information contained in the data, but which are not necessarily
    accessible to the algorithm in their raw form. Feature engineering benefits from
    domain expertise, the application of statistics and information theory, and creativity.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的探索性分析也是成功预测模型的通常最重要的组成部分：提取数据中包含的信息，但原始形式下算法可能无法访问的特征的工程化。特征工程受益于领域专业知识、统计和信息理论的应用，以及创造力。
- en: It relies on an ingenious choice of data transformations that effectively tease
    out the systematic relationship between input and output data. There are many
    choices that include outlier detection and treatment, functional transformations,
    and the combination of several variables, including unsupervised learning. We
    will illustrate examples throughout but will emphasize that this feature is best
    learned through experience. Kaggle is a great place to learn from other data scientists
    who share their experiences with the Kaggle community.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它依赖于巧妙选择的数据转换，有效地揭示输入和输出数据之间的系统关系。有许多选择，包括异常值检测和处理、功能转换以及包括无监督学习在内的多个变量的组合。我们将在整个过程中举例说明，但将强调这一特征最好通过经验学习。Kaggle是一个很好的地方，可以从其他数据科学家那里学习，他们与Kaggle社区分享他们的经验。
- en: Using information theory to evaluate features
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用信息理论来评估特征
- en: The **mutual information** (**MI**) between a feature and the outcome is a measure
    of the mutual dependence between the two variables. It extends the notion of correlation
    to nonlinear relationships. More specifically, it quantifies the information obtained
    about one random variable through the other random variable.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 特征和结果之间的**互信息**（**MI**）是两个变量之间相互依赖的度量。它扩展了对非线性关系的相关性概念。更具体地，它量化了通过另一个随机变量获得的信息。
- en: 'The concept of MI is closely related to the fundamental notion of entropy of
    a random variable. Entropy quantifies the amount of information contained in a
    random variable. Formally, the mutual information—*I*(*X*, *Y*)—of two random
    variables, *X* and *Y*, is defined as the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: MI的概念与随机变量的熵的基本概念密切相关。熵量化了随机变量中包含的信息量。形式上，两个随机变量*X*和*Y*的互信息*I*(*X*, *Y*)定义如下：
- en: '![](img/cd6521f9-4de8-4c81-b5f0-22a165d97f0d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd6521f9-4de8-4c81-b5f0-22a165d97f0d.png)'
- en: The `sklearn` function implements `feature_selection.mutual_info_regression`
    that computes the mutual information between all features and a continuous outcome
    to select the features that are most likely to contain predictive information.
    There is also a classification version (see the documentation for more details).
    The notebook `mutual_information.ipynb` notebook contains an application to the
    financial data we created in [Chapter 4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml),
     *Alpha Factor Research*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`函数实现了`feature_selection.mutual_info_regression`，计算所有特征与连续结果之间的互信息，以选择最有可能包含预测信息的特征。还有一个分类版本（详细信息请参阅文档）。笔记本`mutual_information.ipynb`包含了我们在[第4章](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml)中创建的金融数据的应用，*Alpha
    Factor Research*。'
- en: Selecting an ML algorithm
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择ML算法
- en: The remainder of this book will introduce several model families, ranging from
    linear models, which make fairly strong assumptions about the nature of the functional
    relationship between input and output variables, to deep neural networks, which
    make very few assumptions. As mentioned in the introductory section, fewer assumptions
    will require more data with significant information about the relationship so
    that the learning process can be successful.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的其余部分将介绍几个模型家族，从对输入和输出变量之间的功能关系性质做出相当强的假设的线性模型，到对功能关系性质做出很少假设的深度神经网络。正如在介绍部分中提到的，更少的假设将需要更多包含关于关系的重要信息的数据，以便学习过程能够成功。
- en: We will outline the key assumptions and how to test them where applicable as
    we introduce these models.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在介绍这些模型时概述关键假设以及如何在适用的情况下进行测试。
- en: Design and tune the model
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计和调整模型
- en: The ML process includes steps to diagnose and manage model complexity based
    on estimates of the model's generalization error. An unbiased estimate requires
    a statistically sound and efficient procedure, as well as error metrics that align
    with the output variable type, which also determines whether we are dealing with
    a regression, classification, or ranking problem.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ML过程包括诊断和管理模型复杂性的步骤，基于模型泛化误差的估计。无偏估计需要一个统计上合理且高效的过程，以及与输出变量类型一致的误差度量，这也决定了我们处理的是回归、分类还是排名问题。
- en: The bias-variance trade-off
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: 'The errors that an ML model makes when predicting outcomes for new input data
    can be broken down into reducible and irreducible parts. The irreducible part
    is due to random variation (noise) in the data that is not measured, such as relevant
    but missing variables or natural variation. The reducible part of the generalization
    error, in turn, can be broken down into bias and variance. Both are due to differences
    between the true functional relationship and the assumptions made by the machine
    learning algorithm, as detailed in the following list:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型在预测新输入数据的结果时所产生的错误可以分解为可减少和不可减少的部分。不可减少部分是由数据中的随机变化（噪音）引起的，这些变化没有被测量，比如相关但缺失的变量或自然变化。泛化误差的可减少部分又可以分解为偏差和方差。这两者都是由于机器学习算法对真实功能关系与所做假设之间的差异引起的，详细信息如下列表所示：
- en: '**Error due to bias**: The hypothesis is too simple to capture the complexity
    of the true functional relationship. As a result, whenever the model attempts
    to learn the true function, it makes systematic mistakes and, on average, the
    predictions will be similarly biased. This is also called **underfitting**.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差引起的误差**：假设过于简单，无法捕捉真实功能关系的复杂性。因此，每当模型试图学习真实函数时，它会产生系统性错误，平均而言，预测将会有类似的偏差。这也被称为**欠拟合**。'
- en: '**Error due to variance**: The algorithm is overly complex in view of the true
    relationship. Instead of capturing the true relationship, it overfits to the data
    and extracts patterns from the noise. As a result, it learns different functional
    relationships from each sample, and out-of-sample predictions will vary widely.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差引起的误差**：算法过于复杂，无法捕捉真实关系。它不是捕捉真实关系，而是过度拟合数据并从噪音中提取模式。因此，它从每个样本中学习到不同的功能关系，样本外预测将会有很大的变化。'
- en: Underfitting versus overfitting
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合
- en: The following diagram illustrates overfitting by approximating a cosine function
    using increasingly complex polynomials and measuring the in-sample error. More
    specifically, we draw 10 random samples with some added noise (*n *= 30) to learn
    a polynomial of varying complexity (see the code in the accompanying notebook).
    Each time, the model predicts new data points and we capture the mean-squared
    error for these predictions, as well as the standard deviation of these errors.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下图通过使用越来越复杂的多项式逼近余弦函数并测量样本内误差来说明过拟合。更具体地说，我们抽取了10个随机样本并添加了一些噪音(*n*=30)来学习不同复杂度的多项式（请参见附带笔记本中的代码）。每次，模型预测新数据点，我们捕获这些预测的均方误差，以及这些误差的标准差。
- en: 'The left-hand panel in the following diagram shows a polynomial of degree 1;
    a straight line clearly underfits the true function. However, the estimated line
    will not differ dramatically from one sample drawn from the true function to the
    next. The middle panel shows that a degree 5 polynomial approximates the true
    relationship reasonably well on the [0, 1] interval. On the other hand, a polynomial
    of degree 15 fits the small sample almost perfectly, but provides a poor estimate
    of the true relationship: it overfits to the random variation in the sample data
    points, and the learned function will vary strongly with each sample drawn:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下图左侧面板显示了一次多项式；直线明显拟合不足真实函数。然而，估计的直线在从真实函数中抽取的样本之间不会有明显差异。中间面板显示了五次多项式在[0, 1]区间内合理逼近真实关系。另一方面，十五次多项式几乎完美拟合小样本，但提供了对真实关系的糟糕估计：它对样本数据点的随机变化过拟合，学习函数将随每次抽样而大幅变化：
- en: '![](img/2e20ae8c-f606-40c4-8a14-c223376e5cc7.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e20ae8c-f606-40c4-8a14-c223376e5cc7.png)'
- en: Managing the trade-off
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权衡处理
- en: Let's further illustrate the impact of overfitting versus underfitting by trying
    to learn a Taylor series approximation of the cosine function of ninth degree
    with some added noise. In the following diagram, we draw random samples of the
    true function and fit polynomials that underfit, overfit, and provide an approximately
    correct degree of flexibility. We then predict out-of-sample and measure the RMSE.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过尝试学习带有一些添加噪音的九次余弦函数的泰勒级数逼近来进一步说明过拟合与拟合不足的影响。在下图中，我们从真实函数中绘制随机样本，并拟合拟合不足、过拟合和提供大致正确灵活性程度的多项式。然后我们进行样本外预测并测量RMSE。
- en: The high bias but low variance of a polynomial of degree 3 compares to the low
    bias but exceedingly high variance of the various prediction errors visible in
    the first panel. The left-hand panel shows the distribution of the errors that
    result from subtracting the true function values. The underfit case of a straight
    line produces a poor in-sample fit and is significantly off target out of sample.
    The overfit model shows the best fit in-sample with the smallest dispersion of
    errors, but the price is a large variance out-of-sample. The appropriate model
    that matches the functional form of the true model performs the best by far out-of-sample.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一个面板中可见的各种预测误差相比，三次多项式的高偏差但低方差与真实函数值相减所产生的误差分布显示在左侧面板中。拟合不足的直线情况在样本内拟合效果差，样本外偏离严重。过拟合模型在样本内显示出最佳拟合，误差分散最小，但样本外方差较大。与真实模型匹配的适当模型在样本外表现最佳。
- en: 'The right-hand panel of the following screenshot shows the actual predictions
    rather than the errors to demonstrate what the different types of fit look like
    in practice:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图的右侧面板显示了实际预测而不是误差，以展示不同类型拟合的实际效果：
- en: '![](img/6d8b7763-5776-41db-81a6-b1f6dfc415ff.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d8b7763-5776-41db-81a6-b1f6dfc415ff.png)'
- en: Learning curves
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: A learning curve plots the evolution of train and test errors against the size
    of the dataset used to learn the functional relationship. It is a useful tool
    to diagnose the bias-variance trade-off for a given model because the errors will
    behave differently. A model with a high bias will have a high but similar training
    error, both in-sample and out-of-sample, whereas an overfit model will have a
    very low training error.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线绘制了训练和测试误差随用于学习函数关系的数据集大小的演变。它是诊断给定模型的偏差-方差权衡的有用工具，因为误差会表现出不同的行为。高偏差的模型将在样本内和样本外都有高但相似的训练误差，而过拟合模型将有非常低的训练误差。
- en: 'The declining out-of-sample error illustrates that overfit models may benefit
    from additional data or tools to limit the model''s complexity, such as regularization,
    whereas underfit models need to use either more features or otherwise increase
    the complexity of the model, as shown in the following screenshot:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 下降的样本外误差说明过拟合模型可能受益于额外数据或限制模型复杂性的工具，如正则化，而拟合不足模型需要使用更多特征或以其他方式增加模型的复杂性，如下图所示：
- en: '![](img/61a03d74-7a22-4c44-8672-2b303bdc9c4e.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61a03d74-7a22-4c44-8672-2b303bdc9c4e.png)'
- en: How to use cross-validation for model selection
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用交叉验证进行模型选择
- en: When several candidate models (that is, algorithms) are available for your use
    case, the act of choosing one of them is called the **model selection** problem. Model
    selection aims to identify the model that will produce the lowest prediction error
    given new data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的用例中有多个候选模型（即算法）可用时，选择其中一个的行为称为**模型选择**问题。模型选择旨在确定在给定新数据的情况下产生最低预测误差的模型。
- en: 'An unbiased estimate of this generalization error requires a test on data that
    was not part of model training. Hence, we only use part of the available data
    to train the model and set aside another part of the data to test the model. In
    order to obtain an unbiased estimate of the prediction error, absolutely no information
    about the test set may leak into the training set, as shown in the following diagram:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对这种泛化误差的无偏估计需要对未参与模型训练的数据进行测试。因此，我们只使用可用数据的一部分来训练模型，并将另一部分数据保留用于测试模型。为了获得对预测误差的无偏估计，测试集的任何信息都不能泄漏到训练集中，如下图所示：
- en: '![](img/7e07b113-5977-4a29-a34a-0fdb59a25de0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e07b113-5977-4a29-a34a-0fdb59a25de0.png)'
- en: There are several methods that can be used to split the available data, which
    differ in terms of the amount of data used for training, the variance of the error
    estimates, the computational intensity, and whether structural aspects of the
    data are taken into account when splitting the data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以用于拆分可用数据，这些方法在用于训练的数据量、误差估计的方差、计算强度以及在拆分数据时是否考虑数据的结构方面有所不同。
- en: How to implement cross-validation in Python
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在Python中实现交叉验证
- en: 'We will illustrate various options for splitting data into training and test
    sets by showing how the indices of a mock dataset with ten observations are assigned
    to the train and test set (see `cross_validation.py` for details), as shown in
    following code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过展示一个模拟数据集的索引如何分配给训练集和测试集的方式，来说明将数据分割成训练集和测试集的各种选项（有关详细信息，请参见`cross_validation.py`），如下代码所示：
- en: '[PRE0]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Basic train-test split
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的训练-测试分割
- en: 'For a single split of your data into a training and a test set, use `sklearn.model_selection.train_test_split`,
    where the `shuffle` parameter, by default ensures the randomized selection of
    observations, which in turn can be replicated by setting `random_state`. There
    is also a `stratify` parameter that, for a classification problem, ensures that
    the train and test sets will contain approximately the same shares of each class,
    as shown in the following code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将数据一次性分割为训练集和测试集，使用`sklearn.model_selection.train_test_split`，其中`shuffle`参数默认确保随机选择观测值，这可以通过设置`random_state`来复制。还有一个`stratify`参数，对于分类问题，确保训练集和测试集包含大致相同比例的每个类别，如下代码所示：
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, we train a model using all data except row numbers `6` and `9`,
    which will be used to generate predictions and measure the errors given on the
    `know` labels. This method is useful for quick evaluation but is sensitive to
    the split, and the standard error of the test error estimate will be higher.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用除行号`6`和`9`之外的所有数据来训练模型，这些行号将用于生成预测并根据`know`标签给出错误。这种方法对于快速评估很有用，但对分割敏感，测试误差估计的标准误差会更高。
- en: Cross-validation
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: '**Cross-validation** (**CV**) is a popular strategy for model selection. The
    main idea behind CV is to split the data one or several times so that each split
    is used once as a validation set and the remainder as a training set: part of
    the data (the training sample) is used to train the algorithm, and the remaining
    part (the validation sample) is used for estimating the risk of the algorithm.
    Then, CV selects the algorithm with the smallest estimated risk.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**（**CV**）是一种流行的模型选择策略。CV的主要思想是将数据分割一次或多次，以便每个拆分都被用作一次验证集，其余部分被用作训练集：数据的一部分（训练样本）用于训练算法，剩下的部分（验证样本）用于估计算法的风险。然后，CV选择估计风险最小的算法。'
- en: While the data-splitting heuristic is very general, a key assumption of CV is
    that the data is **independently and identically distributed** (**IID**). In the
    following sections, we will see that, for time series data, this is often not
    the case and requires a different approach.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据分割的启发式方法非常普遍，但交叉验证的一个关键假设是数据是**独立同分布**（**IID**）。在接下来的章节中，我们将看到，对于时间序列数据，这通常不是这种情况，需要采用不同的方法。
- en: Using a hold-out test set
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用保留测试集
- en: 'When selecting hyperparameters based on their validation score, be aware that
    this validation score is biased because of multiple tests, and is no longer a
    good estimate of the generalization error. For an unbiased estimate of the error
    rate, we have to estimate the score from a fresh dataset, as shown in the following
    diagram:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于验证分数选择超参数时，要注意验证分数存在偏差，因为进行了多次测试，不再是对泛化误差的良好估计。为了得到一个无偏的误差率估计，我们必须从新的数据集中估计分数，如下图所示：
- en: '![](img/06e62cd0-8b6d-485a-9724-f59bfa81c3fc.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06e62cd0-8b6d-485a-9724-f59bfa81c3fc.png)'
- en: 'For this reason, we use a three-way split of the data, as illustrated in the
    preceding diagram: one part is used in cross-validation and is repeatedly split
    into a training and validation set. The remainder is set aside as a hold-out set
    that is only used once cross-validation is complete to generate an unbiased test
    error estimate. We will illustrate this method as we start building ML models
    in the next chapter.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用数据的三向分割，如前面的图所示：一部分用于交叉验证，并反复分成训练集和验证集。其余部分作为保留集，只有在交叉验证完成后才使用，以生成一个无偏的测试误差估计。我们将在下一章开始构建机器学习模型时说明这种方法。
- en: KFold iterator
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KFold迭代器
- en: 'The `sklearn.model_selection.KFold` iterator produces several disjunct splits
    and assigns each of these splits once to the validation set, as shown in the following
    code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.model_selection.KFold`迭代器产生几个不相交的拆分，并将每个这些拆分一次分配给验证集，如下代码所示：'
- en: '[PRE2]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In addition to the number of splits, most CV objects take a `shuffle` argument
    that ensures randomization. To render results reproducible, set the `random_state`,
    as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拆分的数量，大多数CV对象还接受一个`shuffle`参数，以确保随机化。为了使结果可重现，设置`random_state`，如下所示：
- en: '[PRE3]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Leave-one-out CV
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留一法交叉验证
- en: 'The original CV implementation used a leave-one-out method that used each observation
    once as the validation set, as shown in the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的CV实现使用了留一法，每个观测值都被用作一次验证集，如下代码所示：
- en: '[PRE4]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This maximizes the number of models that are trained, which increases computational
    costs. While the validation sets do not overlap, the overlap of training sets
    is maximized, driving up the correlation of models and their prediction errors.
    As a result, the variance of the prediction error is higher for a model with a
    larger number of folds.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这最大化了训练的模型数量，增加了计算成本。虽然验证集不重叠，但训练集的重叠被最大化，增加了模型和其预测误差的相关性。因此，对于具有更多折叠的模型，预测误差的方差更高。
- en: Leave-P-Out CV
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留P法交叉验证
- en: 'A similar version to leave-one-out CV is leave-P-out CV, which generates all
    possible combinations of `p` data rows, as shown in the following code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类似的版本是留一法交叉验证，它生成所有可能的`p`数据行的组合，如下代码所示：
- en: '[PRE5]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ShuffleSplit
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ShuffleSplit
- en: 'The `sklearn.model_selection.ShuffleSplit` object creates independent splits
    with potentially overlapping validation sets, as shown in the following code:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.model_selection.ShuffleSplit`对象创建独立的拆分，可能存在重叠的验证集，如下代码所示：'
- en: '[PRE6]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameter tuning with scikit-learn
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行参数调整
- en: Model selection typically involves repeated cross-validation of the out-of-sample
    performance of models using different algorithms (such as linear regression and
    random forest) or different configurations. Different configurations may involve
    changes to hyperparameters or the inclusion or exclusion of different variables.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择通常涉及使用不同算法（如线性回归和随机森林）或不同配置重复交叉验证模型的样本外性能。不同的配置可能涉及对超参数的更改或包含或排除不同变量。
- en: The `yellowbricks` library extends the `sklearn` API to generate diagnostic
    visualization tools to facilitate the model-selection process. These tools can
    be used to investigate relationships among features, analyze classification or
    regression errors, monitor cluster algorithm performance, inspect the characteristics
    of text data, and help with model selection. We will demonstrate validation and
    learning curves that provide valuable information during the parameter-tuning
    phase—see the `machine_learning_workflow.ipynb` notebook for implementation details.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`yellowbricks`库扩展了`sklearn` API，生成诊断可视化工具，以便于模型选择过程。这些工具可用于调查特征之间的关系，分析分类或回归错误，监视集群算法性能，检查文本数据的特征，并帮助模型选择。我们将演示提供有价值信息的验证和学习曲线，在参数调整阶段—请参阅`machine_learning_workflow.ipynb`笔记本以获取实现细节。'
- en: Validation curves with yellowbricks
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用yellowbricks进行验证曲线
- en: Validation curves (see the left-hand panel in the following graph) visualize
    the impact of a single hyperparameter on a model's cross-validation performance.
    This is useful to determine whether the model underfits or overfits the given
    dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 验证曲线（请参见下图左侧面板）可视化单个超参数对模型交叉验证性能的影响。这有助于确定模型是否对给定数据集欠拟合或过拟合。
- en: 'In our example of the **KNeighborsRegressor** that only has a single hyperparameter,
    we can clearly see that the model underfits for values of *k* above 20, where
    the validation error drops as we reduce the number of neighbors, thereby making
    our model more complex, because it makes predictions for more distinct groups
    of neighbors or areas in the feature space. For values below 20, the model begins
    to overfit as training and validation errors diverge and average out-of-sample
    performance quickly deteriorates, as shown in the following graph:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的**KNeighborsRegressor**示例中，只有一个超参数，我们可以清楚地看到当*k*大于20时，模型欠拟合，验证错误随着减少邻居数量而下降，从而使我们的模型更复杂，因为它对更多不同的邻居或特征空间中的区域进行预测。对于小于20的值，模型开始过拟合，训练和验证错误分歧，平均样本外性能迅速恶化，如下图所示：
- en: '![](img/f8e64722-fd9d-4ae9-bb41-e84f239625d1.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8e64722-fd9d-4ae9-bb41-e84f239625d1.png)'
- en: Learning curves
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: The learning curve (see the right-hand panel of the preceding chart for our
    house price regression example) helps determine whether a model's cross-validation
    performance would benefit from additional data and whether prediction errors are
    more driven by bias or by variance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线（请参见我们房价回归示例的上图右侧面板）有助于确定模型的交叉验证性能是否会受益于额外数据，以及预测错误是更多地受到偏差还是方差的驱动。
- en: If training and cross-validation performance converges, then more data is unlikely
    to improve the performance. At this point, it is important to evaluate whether
    the model performance meets expectations, determined by a human benchmark. If
    this is not the case, then you should modify the model's hyperparameter settings
    to better capture the relationship between the features and the outcome, or choose
    a different algorithm with a higher capacity to capture complexity.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练和交叉验证性能趋于一致，那么更多的数据不太可能改善性能。在这一点上，评估模型性能是否符合人类基准的期望非常重要。如果不符合，那么应该修改模型的超参数设置，以更好地捕捉特征和结果之间的关系，或者选择具有更高复杂性捕捉能力的不同算法。
- en: In addition, the variation of train and test errors shown by the shaded confidence
    intervals provide clues about the bias and variance sources of the prediction
    error. Variability around the cross-validation error is evidence of variance,
    whereas variability for the training set suggests bias, depending on the size
    of the training error.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由阴影置信区间显示的训练和测试错误的变化提供了关于预测错误的偏差和方差来源的线索。交叉验证错误的变化表明方差，而训练集的变化则表明偏差，取决于训练错误的大小。
- en: In our example, the cross-validation performance has continued to drop, but
    the incremental improvements have shrunk and the errors have plateaued, so there
    are unlikely to be many benefits from a larger training set. On the other hand,
    the data is showing substantial variance given the range of validation errors
    compared to that shown for the training errors.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，交叉验证性能持续下降，但改进幅度已经缩小，错误已经趋于稳定，因此增加更多的训练集可能不会有太多好处。另一方面，数据显示出相当大的方差，与训练错误相比，验证错误的范围更大。
- en: Parameter tuning using GridSearchCV and pipeline
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GridSearchCV和pipeline进行参数调整
- en: Since hyperparameter tuning is a key ingredient of the machine learning workflow,
    there are tools to automate this process. The `sklearn` library includes a `GridSearchCV`
    interface that cross-validates all combinations of parameters in parallel, captures
    the result, and automatically trains the model using the parameter setting that
    performed best during cross-validation on the full dataset.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于超参数调整是机器学习工作流的关键组成部分，因此有工具可以自动化这个过程。`sklearn`库包括一个`GridSearchCV`接口，可以并行交叉验证所有参数组合，捕获结果，并在完整数据集上自动训练使用在交叉验证中表现最佳的参数设置的模型。
- en: In practice, the training and validation sets often require some processing
    prior to cross-validation. Scikit-learn offers the `Pipeline` to also automate
    any requisite feature-processing steps in the automated hyperparameter tuning
    facilitated by `GridSearchCV`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，训练和验证集通常需要在交叉验证之前进行一些处理。Scikit-learn提供了`Pipeline`，也可以自动化任何必要的特征处理步骤，在`GridSearchCV`所提供的自动化超参数调整中。
- en: You can look at the implementation examples in the included `machine_learning_workflow.ipynb`
    notebook to see these tools in action.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看包含的`machine_learning_workflow.ipynb`笔记本中的实现示例，以查看这些工具的实际应用。
- en: Challenges with cross-validation in finance
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金融交叉验证的挑战
- en: A key assumption for the cross-validation methods discussed so far is the **independent
    and identical** (**iid**) distribution of the samples available for training.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的交叉验证方法的一个关键假设是训练样本的**独立和相同**（**iid**）分布。
- en: For financial data, this is often not the case. On the contrary, financial data
    is neither independently nor identically distributed because of serial correlation
    and time-varying standard deviation, also known as **heteroskedasticity** (see
    the next two chapters for more details). The `TimeSeriesSplit` in the `sklearn.model_selection`
    module aims to address the linear order of time-series data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于金融数据，情况通常并非如此。相反，金融数据既不是独立分布的，也不是相同分布的，因为存在串行相关性和时变标准差，也称为**异方差性**（有关更多细节，请参见接下来的两章）。`sklearn.model_selection`模块中的`TimeSeriesSplit`旨在解决时间序列数据的线性顺序。
- en: Time series cross-validation with sklearn
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用sklearn进行时间序列交叉验证
- en: The time series nature of the data implies that cross-validation produces a
    situation where data from the future will be used to predict data from the past.
    This is unrealistic at best and data snooping at worst, to the extent that future
    data reflects past events.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的时间序列性意味着交叉验证会产生这样一种情况：未来的数据将被用来预测过去的数据。这充其量是不现实的，最坏的情况是数据窥探，因为未来的数据反映了过去的事件。
- en: 'To address time dependency, the `sklearn.model_selection.TimeSeriesSplit` object
    implements a walk-forward test with an expanding training set, where subsequent
    training sets are supersets of past training sets, as shown in the following code:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决时间依赖性，`sklearn.model_selection.TimeSeriesSplit`对象实现了一个具有扩展训练集的前向测试，其中后续训练集是过去训练集的超集，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can use the `max_train_size` parameter to implement walk-forward cross-validation,
    where the size of the training set remains constant over time, similar to how
    `zipline` tests a trading algorithm. Scikit-learn facilitates the design of custom
    cross-validation methods using subclassing, which we will implement in the following
    chapters.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`max_train_size`参数来实现前向交叉验证，其中训练集的大小随时间保持恒定，类似于`zipline`测试交易算法的方式。Scikit-learn通过子类化来方便地设计自定义交叉验证方法，我们将在接下来的章节中实现。
- en: Purging, embargoing, and combinatorial CV
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清除、禁止和组合CV
- en: For financial data, labels are often derived from overlapping data points as
    returns are computed from prices in multiple periods. In the context of trading
    strategies, the results of a model's prediction, which may imply taking a position
    in an asset, may only be known later, when this decision is evaluated—for example,
    when a position is closed out.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融数据中，标签通常是从重叠的数据点中派生出来的，因为收益是从多个时期的价格计算出来的。在交易策略的背景下，模型预测的结果可能意味着在资产中建立头寸，但只有在稍后评估这个决定时才能知道，例如当头寸被平仓时。
- en: 'The resulting risks include the leaking of information from the test into the
    training set, likely leading to an artificially inflated performance that needs
    to be addressed by ensuring that all data is point-in-time—that is, truly available
    and known at the time it is used as the input for a model. Several methods have
    been proposed by Marcos Lopez de Prado in *Advances in Financial Machine Learning* to
    address these challenges of financial data for cross-validation, as shown in the
    following list:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由此产生的风险包括测试信息泄漏到训练集中，可能导致人为膨胀的性能，需要通过确保所有数据都是点对点的，即在用作模型输入时真正可用和已知的数据来解决。Marcos
    Lopez de Prado在《金融机器学习进展》中提出了几种方法来解决金融数据在交叉验证中的挑战，如下列表所示：
- en: '**Purging**: Eliminate training data points where the evaluation occurs after
    the prediction of a point-in-time data point in the validation set to avoid look-ahead
    bias.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清除**：消除训练数据点，其中评估发生在验证集中的点对时间数据点的预测之后，以避免前瞻性偏差。'
- en: '**Embargoing**: Further eliminate training samples that follow a test period.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁止**：进一步消除跟随测试期的训练样本。'
- en: '**Combinatorial cross-validation**: Walk-forward CV severely limits the historical
    paths that can be tested. Instead, given T observations, compute all possible
    train/test splits for *N*<*T* groups that each maintain their order, and purge
    and embargo potentially overlapping groups. Then, train the model on all combinations
    of *N*-*k* groups while testing the model on the remaining *k* groups. The result
    is a much larger number of possible historical paths.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合交叉验证**：前向CV严重限制了可以测试的历史路径。相反，给定T个观察值，计算*N*<*T*组的所有可能的训练/测试拆分，每个组都保持其顺序，并清除和禁止潜在重叠的组。然后，在所有*N*-*k*组的组合上训练模型，同时在剩余的*k*组上测试模型。结果是可能的历史路径数量大大增加。'
- en: Prado's *Advances in Financial Machine Learning* contains sample code to implement
    these approaches; the code is also available via the new library, `timeseriescv`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Prado的《金融机器学习进展》包含了实现这些方法的示例代码；该代码也可以通过新库`timeseriescv`获得。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the challenge of learning from data and looked
    at supervised, unsupervised, and reinforcement models as the principal forms of
    learning that we will study in this book to build algorithmic trading strategies.
    We discussed the need for supervised learning algorithms to make assumptions about
    the functional relationships that they attempt to learn in order to limit the
    search space while incurring an inductive bias that may lead to excessive generalization
    errors.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了从数据中学习的挑战，并讨论了监督、无监督和强化模型作为我们将在本书中研究的主要学习形式，以构建算法交易策略。我们讨论了监督学习算法需要对它们试图学习的功能关系做出假设，以限制搜索空间，同时产生归纳偏差，可能导致过度泛化错误。
- en: We presented key aspects of the ML workflow, introduced the most common error
    metrics for regression and classification models, explained the bias-variance
    trade-off, and illustrated the various tools for managing the model selection
    process using cross-validation.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了ML工作流程的关键方面，介绍了回归和分类模型最常见的错误度量标准，解释了偏差-方差权衡，并举例说明了使用交叉验证管理模型选择过程的各种工具。
- en: In the following chapter, we will dive into linear models for regression and
    classification to develop our first algorithmic trading strategies that use ML.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究用于回归和分类的线性模型，以开发我们的第一个使用ML的算法交易策略。
