- en: Clustering with Docker Swarm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Swarm进行集群化
- en: We have already covered all the fundamental aspects of the Continuous Delivery
    pipeline. In this chapter, we will see how to change the Docker environment from
    a single Docker host into a cluster of machines and how to use it all together
    with Jenkins.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了持续交付流水线的所有基本方面。在本章中，我们将看到如何将Docker环境从单个Docker主机更改为一组机器，并如何与Jenkins一起使用它。
- en: 'This chapter covers the following points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: Explaining the concept of server clustering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释服务器集群的概念
- en: Introducing Docker Swarm and its most important features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Docker Swarm及其最重要的功能
- en: Presenting how to build a swarm cluster from multiple Docker hosts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍如何从多个Docker主机构建群集
- en: Running and scaling Docker images on a cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群上运行和扩展Docker镜像
- en: 'Exploring advanced swarm features: rolling updates, draining nodes, multiple
    manager nodes, and tuning the scheduling strategy'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索高级群集功能：滚动更新、排水节点、多个管理节点和调整调度策略
- en: Deploying the Docker Compose configuration on a cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群上部署Docker Compose配置
- en: Introducing Kubernetes and Apache Mesos as alternatives to Docker Swarm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Kubernetes和Apache Mesos作为Docker Swarm的替代方案
- en: Dynamically scaling Jenkins agents on a cluster
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群上动态扩展Jenkins代理
- en: Server clustering
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器集群
- en: So far, we have have interacted with each of the machines individually. Even
    when we used Ansible to repeat the same operations on multiple servers, we had
    to explicitly specify on which host the given service should be deployed. In most
    cases, however, if servers share the same physical location, we are not interested
    on which particular machine the service is deployed. All we need is to have it
    accessible and replicated in many instances. How can we configure a set of machines
    to work together so that adding a new one would require no additional setup? This
    is the role of clustering.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经分别与每台机器进行了交互。即使我们使用Ansible在多台服务器上重复相同的操作，我们也必须明确指定应在哪台主机上部署给定服务。然而，在大多数情况下，如果服务器共享相同的物理位置，我们并不关心服务部署在哪台特定的机器上。我们所需要的只是让它可访问并在许多实例中复制。我们如何配置一组机器以便它们共同工作，以至于添加新的机器不需要额外的设置？这就是集群的作用。
- en: In this section, you will be introduced to the concept of server clustering
    and the Docker Swarm toolkit.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将介绍服务器集群的概念和Docker Swarm工具包。
- en: Introducing server clustering
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍服务器集群
- en: 'A server cluster is a set of connected computers that work together in a way
    that they can be used similarly to a single system. Servers are usually connected
    through the local network with a connection fast enough to ensure a small influence
    of the fact that services are distributed. A simple server cluster is presented
    in the following image:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器集群是一组连接的计算机，它们以一种可以类似于单个系统的方式一起工作。服务器通常通过本地网络连接，连接速度足够快，以确保服务分布的影响很小。下图展示了一个简单的服务器集群：
- en: '![](assets/5932276e-c8df-4777-80c1-69ac1bbccded.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/5932276e-c8df-4777-80c1-69ac1bbccded.png)'
- en: A user accesses the cluster via a host called the manager, whose interface should
    be similar to a usual Docker host. Inside the cluster, there are multiple worker
    nodes that receive tasks, execute them, and notify the manager of their current
    state. The manager is responsible for the orchestration process, including task
    dispatching, service discovery, load balancing, and worker failure detection.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 用户通过称为管理器的主机访问集群，其界面应类似于常规Docker主机。在集群内，有多个工作节点接收任务，执行它们，并通知管理器它们的当前状态。管理器负责编排过程，包括任务分派、服务发现、负载平衡和工作节点故障检测。
- en: The manager can also execute tasks, which is the default configuration in Docker
    Swarm. However, for large clusters, the manager should be configured for management
    purposes only.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 管理者也可以执行任务，这是Docker Swarm的默认配置。然而，对于大型集群，管理者应该配置为仅用于管理目的。
- en: Introducing Docker Swarm
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Docker Swarm
- en: Docker Swarm is a native clustering system for Docker that turns a set of Docker
    hosts into one consistent cluster, called a swarm. Each host connected to the
    swarm plays the role of a manager or a worker (there must be at least one manager
    in a cluster). Technically, the physical location of the machines does not matter;
    however, it's reasonable to have all Docker hosts inside one local network, otherwise,
    managing operations (or reaching consensus between multiple managers) can take
    a significant amount of time.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm是Docker的本地集群系统，将一组Docker主机转换为一个一致的集群，称为swarm。连接到swarm的每个主机都扮演管理者或工作节点的角色（集群中必须至少有一个管理者）。从技术上讲，机器的物理位置并不重要；然而，将所有Docker主机放在一个本地网络中是合理的，否则，管理操作（或在多个管理者之间达成共识）可能需要大量时间。
- en: Since Docker 1.12, Docker Swarm is natively integrated into Docker Engine as
    swarm mode. In older versions, it was necessary to run the swarm container on
    each of the hosts to provide the clustering functionality.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自Docker 1.12以来，Docker Swarm已经作为swarm模式被原生集成到Docker Engine中。在旧版本中，需要在每个主机上运行swarm容器以提供集群功能。
- en: Regarding the terminology, in swarm mode, a running image is called a **service,**
    as opposed to a **container**, which is run on a single Docker host. One service
    runs a specified number of **tasks.** A task is an atomic scheduling unit of the
    swarm that holds the information about the container and the command that should
    be run inside the container. A **replica** is each container that is run on the
    node. The number of replicas is the expected number of all containers for the
    given service.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于术语，在swarm模式下，运行的镜像称为**服务**，而不是在单个Docker主机上运行的**容器**。一个服务运行指定数量的**任务**。任务是swarm的原子调度单元，保存有关容器和应在容器内运行的命令的信息。**副本**是在节点上运行的每个容器。副本的数量是给定服务的所有容器的预期数量。
- en: 'Let''s look at an image presenting the terminology and the Docker Swarm clustering
    process:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下展示术语和Docker Swarm集群过程的图像：
- en: '![](assets/503d41a5-5167-45a9-ac26-547095f5f638.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/503d41a5-5167-45a9-ac26-547095f5f638.png)'
- en: We start by specifying a service, the Docker image and the number of replicas.
    The manager automatically assigns tasks to worker nodes. Obviously, each replicated
    container is run from the same Docker image. In the context of the presented flow,
    Docker Swarm can be viewed as a layer on top of the Docker Engine mechanism that
    is responsible for container orchestration.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先指定一个服务，Docker镜像和副本的数量。管理者会自动将任务分配给工作节点。显然，每个复制的容器都是从相同的Docker镜像运行的。在所呈现的流程的上下文中，Docker
    Swarm可以被视为Docker Engine机制的一层，负责容器编排。
- en: In the preceding sample image, we have three tasks, and each of them is run
    on a separate Docker host. Nevertheless, it may also happen that all containers
    would be started on the same Docker host. Everything depends on the manager node
    that allocates tasks to worker nodes using the scheduling strategy. We will show
    how to configure that strategy later, in a separate section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例图像中，我们有三个任务，每个任务都在单独的Docker主机上运行。然而，也可能所有容器都在同一个Docker主机上启动。一切取决于分配任务给工作节点的管理节点使用的调度策略。我们将在后面的单独章节中展示如何配置该策略。
- en: Docker Swarm features overview
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm功能概述
- en: 'Docker Swarm provides a number interesting features. Let''s walk through the
    most important ones:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm提供了许多有趣的功能。让我们来看看最重要的几个：
- en: '**Load balancing**: Docker Swarm takes care of the load balancing and assigning
    unique DNS names so that the application deployed on the cluster can be used in
    the same way as deployed on a single Docker host. In other words, a swarm can
    publish ports in a similar manner as the Docker container, and then the swarm
    manager distributes requests among the services in the cluster.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡：Docker Swarm负责负载均衡和分配唯一的DNS名称，使得部署在集群上的应用可以与部署在单个Docker主机上的应用一样使用。换句话说，一个集群可以以与Docker容器类似的方式发布端口，然后集群管理器在集群中的服务之间分发请求。
- en: '**Dynamic role management**: Docker hosts can be added to the swarm at runtime,
    so there is no need for a cluster restart. What''s more, the role of the node
    (manager or worker) can also be dynamically changed.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态角色管理：Docker主机可以在运行时添加到集群中，因此无需重新启动集群。而且，节点的角色（管理器或工作节点）也可以动态更改。
- en: '**Dynamic service scaling**: Each service can be dynamically scaled up or down
    with the Docker client. The manager node takes care of adding or removing containers
    from the nodes.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态服务扩展：每个服务都可以通过Docker客户端动态地扩展或缩减。管理节点负责从节点中添加或删除容器。
- en: '**Failure recovery**: Nodes are constantly monitored by the manager and, if
    any of them fails, new tasks are started on different machines so that the declared
    number of replicas would be constant. It''s also possible to create multiple manager
    nodes in order to prevent a breakdown in case one of them fails.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障恢复：管理器不断监视节点，如果其中任何一个失败，新任务将在不同的机器上启动，以便声明的副本数量保持不变。还可以创建多个管理节点，以防止其中一个失败时发生故障。
- en: '**Rolling updates**: An update to services can be applied incrementally; for
    example, if we have 10 replicas and we would like to make a change, we can define
    a delay between the deployment to each replica. In such a case, when anything
    goes wrong, we never end up with a scenario where no replica is working correctly.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新：对服务的更新可以逐步应用；例如，如果我们有10个副本并且想要进行更改，我们可以定义每个副本部署之间的延迟。在这种情况下，当出现问题时，我们永远不会出现没有副本正常工作的情况。
- en: '**Two service modes:** There are two modes in which can be run:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种服务模式：可以运行在两种模式下：
- en: '**Replicated services**: The specified number of replicated containers are
    distributed among the nodes based on the scheduling strategy algorithm'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制服务：指定数量的复制容器根据调度策略算法分布在节点之间。
- en: '**Global services**: One container is run on every available node in the cluster'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全球服务：集群中的每个可用节点上都运行一个容器
- en: '**Security:** As everything is in Docker, Docker Swarm enforces the TLS authentication
    and the communication encryption. It''s also possible to use CA (or self-signed)
    certificates.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性：由于一切都在Docker中，Docker Swarm强制执行TLS身份验证和通信加密。还可以使用CA（或自签名）证书。
- en: Let's see how all of this looks in practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在实践中是什么样子。
- en: Docker Swarm in practice
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际中的Docker Swarm
- en: Docker Engine includes the Swarm mode by default, so there is no additional
    installation process required. Since Docker Swarm is a native Docker clustering
    system, managing cluster nodes is done by the `docker` command and is therefore
    very simple and intuitive. Let's start by creating a manager node with two worker
    nodes. Then, we will run and scale a service from a Docker image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Engine默认包含了Swarm模式，因此不需要额外的安装过程。由于Docker Swarm是一个本地的Docker集群系统，管理集群节点是通过`docker`命令完成的，因此非常简单和直观。让我们首先创建一个管理节点和两个工作节点。然后，我们将从Docker镜像运行和扩展一个服务。
- en: Setting up a Swarm
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立一个Swarm
- en: 'In order to set up a Swarm, we need to initialize the manager node. We can
    do this using the following command on a machine that is supposed to become the
    manager:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置一个Swarm，我们需要初始化管理节点。我们可以在一个即将成为管理节点的机器上使用以下命令来做到这一点：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A very common practice is to use the `--advertise-addr <manager_ip>` parameter,
    because if the manager machine has more than one potential network interfaces,
    then `docker swarm init` can fail.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常常见的做法是使用`--advertise-addr <manager_ip>`参数，因为如果管理机器有多个潜在的网络接口，那么`docker swarm
    init`可能会失败。
- en: In our case, the manager machine has the IP address `192.168.0.143` and, obviously,
    it has to be reachable from the worker nodes (and vice versa). Note that the command
    to execute on worker machines was printed to the console. Also note that a special
    token has been generated. From now on, it will be used to connect a machine to
    the cluster and should be kept secret.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，管理机器的IP地址是`192.168.0.143`，显然，它必须能够从工作节点（反之亦然）访问。请注意，在控制台上打印了要在工作机器上执行的命令。还要注意，已生成了一个特殊的令牌。从现在开始，它将被用来连接机器到集群，并且必须保密。
- en: 'We can check that the Swarm has been created using the `docker node` command:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker node`命令来检查Swarm是否已创建：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When the manager is up and running, we are ready to add worker nodes to the
    Swarm.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当管理器正常运行时，我们准备将工作节点添加到Swarm中。
- en: Adding worker nodes
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加工作节点
- en: 'In order to add a machine to the Swarm, we have to log in to the given machine
    and execute the following command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将一台机器添加到Swarm中，我们必须登录到给定的机器并执行以下命令：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can check that the node has been added to the Swarm with the `docker node
    ls` command. Assuming that we''ve added two node machines, the output should look
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker node ls`命令来检查节点是否已添加到Swarm中。假设我们已经添加了两个节点机器，输出应该如下所示：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At this point, we have a cluster that consists of three Docker hosts, `ubuntu-manager`,
    `ubuntu-worker1`, and `ubuntu-worker2`. Let's see how we can run a service on
    this cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有一个由三个Docker主机组成的集群，`ubuntu-manager`，`ubuntu-worker1`和`ubuntu-worker2`。让我们看看如何在这个集群上运行一个服务。
- en: Deploying a service
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署一个服务
- en: 'In order to run an image on a cluster, we don''t use `docker run` but the Swarm-dedicated
    `docker service` command (which is executed on the manager node). Let''s start
    a single `tomcat` application and give it the name `tomcat`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在集群上运行一个镜像，我们不使用`docker run`，而是使用专门为Swarm设计的`docker service`命令（在管理节点上执行）。让我们启动一个单独的`tomcat`应用并给它命名为`tomcat`：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The command created the service and therefore sent a task to start a container
    on one of the nodes. Let''s list the running services:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令创建了服务，因此发送了一个任务来在一个节点上启动一个容器。让我们列出正在运行的服务：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The log confirms that the `tomcat` service is running, and it has one replica
    (one Docker container is running). We can examine the service even more closely:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 日志确认了`tomcat`服务正在运行，并且有一个副本（一个Docker容器正在运行）。我们甚至可以更仔细地检查服务：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you are interested in the detailed information about a service, you can use
    the `docker service inspect <service_name>` command.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对服务的详细信息感兴趣，可以使用`docker service inspect <service_name>`命令。
- en: 'From the console output, we can see that the container is running on the manager
    node (`ubuntu-manager`). It could have been started on any other node as well;
    the manager automatically chooses the worker node using the scheduling strategy
    algorithm. We can confirm that the container is running using the well-known `docker
    ps` command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台输出中，我们可以看到容器正在管理节点（`ubuntu-manager`）上运行。它也可以在任何其他节点上启动；管理器会自动使用调度策略算法选择工作节点。我们可以使用众所周知的`docker
    ps`命令来确认容器正在运行：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If we don't want a task to be executed on the manager node, we can constrain
    the service with the `--constraint node.role==worker` option. The other possibility
    is to disable the manager completely from executing tasks with `docker node update
    --availability drain <manager_name>`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不希望任务在管理节点上执行，可以使用`--constraint node.role==worker`选项来限制服务。另一种可能性是完全禁用管理节点执行任务，使用`docker
    node update --availability drain <manager_name>`。
- en: Scaling service
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展服务
- en: 'When the service is running, we can scale it up or down so that it will be
    running in many replicas:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务运行时，我们可以扩展或缩小它，以便它在许多副本中运行：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can check that the service has been scaled:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查服务是否已扩展：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that this time, two containers are running on the `manager` node, one on
    the `ubuntu-worker1` node, and the other on the `ubuntu-worker2` node. We can
    check that they are really running by executing `docker ps` on each of the machines.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这次有两个容器在`manager`节点上运行，一个在`ubuntu-worker1`节点上，另一个在`ubuntu-worker2`节点上。我们可以通过在每台机器上执行`docker
    ps`来检查它们是否真的在运行。
- en: 'If we want to remove the services, it''s enough to execute the following command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要删除服务，只需执行以下命令即可：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can check with the `docker service ls` command that the service has been
    removed, and therefore all related `tomcat` containers were stopped and removed
    from all the nodes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`docker service ls`命令检查服务是否已被删除，因此所有相关的`tomcat`容器都已停止并从所有节点中删除。
- en: Publishing ports
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发布端口
- en: 'Docker services, similar to the containers, have a port forwarding mechanism.
    We use it by adding the `-p <host_port>:<container:port>` parameter. Here''s what
    starting a service could look like:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Docker服务，类似于容器，具有端口转发机制。我们可以通过添加`-p <host_port>:<container:port>`参数来使用它。启动服务可能如下所示：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, we can open a browser and see the Tomcat's main page under the address
    `http://192.168.0.143:8080/`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以打开浏览器，在地址`http://192.168.0.143:8080/`下查看Tomcat的主页。
- en: The application is available on the manager host that acts as a load balancer
    and distributes requests to worker nodes. What may sound a little less intuitive
    is the fact that we can access Tomcat using the IP address of any worker, for
    example, if worker nodes are available under `192.168.0.166` and `192.168.0.115`,
    we can access the same running container with `http://192.168.0.166:8080/` and
    `http://192.168.0.115:8080/`. This is possible because Docker Swarm creates a
    routing mesh, in which each node has the information how to forward the published
    port.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序可在充当负载均衡器并将请求分发到工作节点的管理主机上使用。可能听起来有点不太直观的是，我们可以使用任何工作节点的IP地址访问Tomcat，例如，如果工作节点在`192.168.0.166`和`192.168.0.115`下可用，我们可以使用`http://192.168.0.166:8080/`和`http://192.168.0.115:8080/`访问相同的运行容器。这是可能的，因为Docker
    Swarm创建了一个路由网格，其中每个节点都有如何转发已发布端口的信息。
- en: You can read more about how the load balancing and routing are done by Docker
    Swarm at [https://docs.docker.com/engine/swarm/ingress/](https://docs.docker.com/engine/swarm/ingress/).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以阅读有关Docker Swarm如何进行负载平衡和路由的更多信息[https://docs.docker.com/engine/swarm/ingress/](https://docs.docker.com/engine/swarm/ingress/)。
- en: By default, the internal Docker Swarm load balancing is used. Therefore, it's
    enough to address all requests to the manager's machine, and it will take care
    of its distribution between nodes. The other option is to configure an external
    load balancer (for example, HAProxy or Traefik).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，使用内部Docker Swarm负载平衡。因此，只需将所有请求发送到管理机器，它将负责在节点之间进行分发。另一种选择是配置外部负载均衡器（例如HAProxy或Traefik）。
- en: We have discussed the basic usage of Docker Swarm. Let's now dive into more
    challenging features.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了Docker Swarm的基本用法。现在让我们深入了解更具挑战性的功能。
- en: Advanced Docker Swarm
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级Docker Swarm
- en: Docker Swarm offers a lot of interesting features that are useful in the Continuous
    Delivery process. In this section, we will walk through the most important ones.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm提供了许多在持续交付过程中有用的有趣功能。在本节中，我们将介绍最重要的功能。
- en: Rolling updates
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滚动更新
- en: Imagine you deploy a new version of your application. You need to update all
    replicas in the cluster. One option would be to stop the whole Docker Swarm service
    and to run a new one from the updated Docker image. Such approach, however, causes
    downtime between the moment when the service is stopped and the moment when the
    new one is started. In the Continuous Delivery process, downtime is not acceptable,
    since the deployment can take place after every source code change, which is simply
    often. Then, how can we provide zero-downtime deployment in a cluster? This is
    the role of rolling updates.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您部署了应用程序的新版本。您需要更新集群中的所有副本。一种选择是停止整个Docker Swarm服务，并从更新后的Docker镜像运行一个新的服务。然而，这种方法会导致服务停止和新服务启动之间的停机时间。在持续交付过程中，停机时间是不可接受的，因为部署可以在每次源代码更改后进行，这通常是经常发生的。那么，在集群中如何实现零停机部署呢？这就是滚动更新的作用。
- en: 'A rolling update is an automatic method for replacing a service, replica by
    a replica, in a way that some of the replicas are working all the time. Docker
    Swarm uses rolling updates by default, and they can be steered with two parameters:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新是一种自动替换服务副本的方法，一次替换一个副本，以确保一些副本始终在工作。Docker Swarm默认使用滚动更新，并且可以通过两个参数进行控制：
- en: '`update-delay`: Delay between starting one replica and stopping the next one
    (0 seconds by default)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update-delay`：启动一个副本和停止下一个副本之间的延迟（默认为0秒）'
- en: '`update-parallelism`: Maximum number of replicas updated at the same time (one
    by default)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update-parallelism`：同时更新的最大副本数量（默认为1）'
- en: 'The Docker Swarm rolling update process looks as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm滚动更新过程如下：
- en: Stop the `<update-parallelism>` number of tasks (replicas).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止`<update-parallelism>`数量的任务（副本）。
- en: In their place, run the same number of updated tasks.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在它们的位置上，运行相同数量的更新任务。
- en: If a task returns the **RUNNING** state, then wait for the `<update-delay>`
    period.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个任务返回**RUNNING**状态，那么等待`<update-delay>`时间。
- en: If, at any time, any task returns the **FAILED** state, then pause the update.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果任何时候任何任务返回**FAILED**状态，则暂停更新。
- en: The value of the `update-parallelism` parameter should be adapted to the number
    of replicas we run. If the number is small and booting the service is fast, it's
    reasonable to keep the default value of 1\. The `update-delay` parameter should
    be set to the period longer than the expected boot time of our application so
    that we will notice the failure, and therefore pause the update.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`update-parallelism`参数的值应该根据我们运行的副本数量进行调整。如果数量较小，服务启动速度很快，保持默认值1是合理的。`update-delay`参数应设置为比我们应用程序预期的启动时间更长的时间，这样我们就会注意到失败，因此暂停更新。'
- en: 'Let''s look at an example and change the Tomcat application from version 8
    to version 9\. Suppose we have the `tomcat:8` service with five replicas:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子，将Tomcat应用程序从版本8更改为版本9。假设我们有`tomcat:8`服务，有五个副本：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can check that all replicas are running with the `docker service ps tomcat` command.
    Another useful command that helps examine the service is the `docker service inspect` command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker service ps tomcat`命令检查所有副本是否正在运行。另一个有用的命令是`docker service inspect`命令，可以帮助检查服务：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can see that the service has five replicas created out of the image `tomcat:8`.
    The command output also includes the information about the parallelism and the
    delay time between updates (as set by the options in the `docker service create` command).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到服务已经创建了五个副本，来自于`tomcat:8`镜像。命令输出还包括有关并行性和更新之间的延迟时间的信息（由`docker service
    create`命令中的选项设置）。
- en: 'Now, we can update the service into the `tomcat:9` image:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将服务更新为`tomcat:9`镜像：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s check what happens:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看发生了什么：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that the first replica of `tomcat:8` has been shut down and the first
    `tomcat:9` is already running. If we kept on checking the output of the `docker
    service ps tomcat` command, we would notice that after every 10 seconds, another
    replica is in the shutdown state and a new one is started. If we also monitored
    the `docker inspect` command, we would see that the value **UpdateStatus: State**
    will change to **updating** and then, when the update is done, to **completed**.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，`tomcat:8`的第一个副本已关闭，第一个`tomcat:9`已经在运行。如果我们继续检查`docker service ps tomcat`命令的输出，我们会注意到每隔10秒，另一个副本处于关闭状态，新的副本启动。如果我们还监视`docker
    inspect`命令，我们会看到值**UpdateStatus: State**将更改为**updating**，然后在更新完成后更改为**completed**。'
- en: A rolling update is a very powerful feature that allows zero downtime deployment
    and it should always be used in the Continuous Delivery process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新是一个非常强大的功能，允许零停机部署，并且应该始终在持续交付过程中使用。
- en: Draining nodes
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排水节点
- en: When we need to stop a worker node for maintenance, or we would just like to
    remove it from the cluster, we can use the Swarm draining node feature. Draining
    the node means asking the manager to move all tasks out of a given node and exclude
    it from receiving new tasks. As a result, all replicas are running only on the
    active nodes and the drained nodes are idle.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要停止工作节点进行维护，或者我们只是想将其从集群中移除时，我们可以使用Swarm排水节点功能。排水节点意味着要求管理器将所有任务移出给定节点，并排除它不接收新任务。结果，所有副本只在活动节点上运行，排水节点处于空闲状态。
- en: 'Let''s look how this works in practice. Suppose we have three cluster nodes
    and a Tomcat service with five replicas:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在实践中是如何工作的。假设我们有三个集群节点和一个具有五个副本的Tomcat服务：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s check on which nodes the replicas are running:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下副本正在哪些节点上运行：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'There are two replicas running on the `ubuntu-worker2` node. Let''s drain that
    node:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个副本正在`ubuntu-worker2`节点上运行。让我们排水该节点：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The node is put into the **drain** availability, so all replicas should be
    moved out of it:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 节点被设置为**drain**可用性，因此所有副本应该移出该节点：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can see that new tasks were started on the `ubuntu-worker1` node and the
    old replicas were shut down. We can check the status of the nodes:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到新任务在`ubuntu-worker1`节点上启动，并且旧副本已关闭。我们可以检查节点的状态：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As expected, the `ubuntu-worker2` node is available (status `Ready`), but its
    availability is set to drain, which means it doesn''t host any tasks. If we would
    like to get the node back, we can check its availability to `active`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`ubuntu-worker2`节点可用（状态为`Ready`），但其可用性设置为排水，这意味着它不托管任何任务。如果我们想要将节点恢复，可以将其可用性检查为`active`：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: A very common practice is to drain the manager node and, as a result, it will
    not receive any tasks, but do management work only.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常常见的做法是排水管理节点，结果是它不会接收任何任务，只做管理工作。
- en: 'An alternative method to draining the node would be to execute the `docker
    swarm leave` command from the worker. This approach, however, has two drawbacks:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 排水节点的另一种方法是从工作节点执行`docker swarm leave`命令。然而，这种方法有两个缺点：
- en: For a moment, there are fewer replicas than expected (after leaving the swarm
    and before the master starts new tasks on other nodes)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一段时间，副本比预期少（离开Swarm之后，在主节点开始在其他节点上启动新任务之前）
- en: The master does not control if the node is still in the cluster
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点不控制节点是否仍然在集群中
- en: For these reasons, if we plan to stop the worker for some time and then get
    it back, it's recommended to use the draining node feature.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，如果我们计划暂停工作节点一段时间然后再启动它，建议使用排空节点功能。
- en: Multiple manager nodes
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多个管理节点
- en: Having a single manager node is risky because when the manager machine is down,
    the whole cluster is down. This situation is, obviously, not acceptable in the
    case of business-critical systems. In this section, we present how to manage multiple
    master nodes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有单个管理节点是有风险的，因为当管理节点宕机时，整个集群也会宕机。在业务关键系统的情况下，这种情况显然是不可接受的。在本节中，我们将介绍如何管理多个主节点。
- en: 'In order to add a new manager node to the system, we need to first execute
    the following command on the (currently single) manager node:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将新的管理节点添加到系统中，我们需要首先在（当前单一的）管理节点上执行以下命令：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output shows the token and the entire command that needs to be executed
    on the machine that is supposed to become the manager. After executing it, we
    should see that a new manager was added.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了令牌和需要在即将成为管理节点的机器上执行的整个命令。执行完毕后，我们应该看到添加了一个新的管理节点。
- en: Another option to add a manager is to promote it from the worker role using
    the `docker node promote <node>` command. In order to get it back to the worker
    role, we can use the `docker node demote <node>` command.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种添加管理节点的选项是使用`docker node promote <node>`命令将其从工作节点角色提升为管理节点。为了将其重新转换为工作节点角色，我们可以使用`docker
    node demote <node>`命令。
- en: 'Suppose we have added two additional managers; we should see the following
    output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经添加了两个额外的管理节点；我们应该看到以下输出：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the new managers have the manager status set to reachable (or left
    empty), while the old manager is the leader. The reason for this is the fact that
    there is always one primary node responsible for all Swarm management and orchestration
    decisions. The leader is elected from the managers using the Raft consensus algorithm,
    and when it is down, a new leader is elected.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，新的管理节点的管理状态设置为可达（或留空），而旧的管理节点是领导者。其原因是始终有一个主节点负责所有Swarm管理和编排决策。领导者是使用Raft共识算法从管理节点中选举出来的，当它宕机时，会选举出一个新的领导者。
- en: Raft is a consensus algorithm that is used to make decisions in distributed
    systems. You can read more about how it works (and see a visualization) at [https://raft.github.io/](https://raft.github.io/).
    A very popular alternative algorithm for the same goal is called Paxos.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Raft是一种共识算法，用于在分布式系统中做出决策。您可以在[https://raft.github.io/](https://raft.github.io/)上阅读有关其工作原理的更多信息（并查看可视化）。用于相同目的的非常流行的替代算法称为Paxos。
- en: 'Suppose we shut down the `ubuntu-manager` machine; let''s have a look at how
    the new leader was elected:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们关闭了`ubuntu-manager`机器；让我们看看新领导者是如何选举的：
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that even when one of the managers is down, the swarm can work correctly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使其中一个管理节点宕机，Swarm也可以正常工作。
- en: There is no limit for the number of managers, so it may sound that the more
    managers the better fault tolerance. It's true, however, having a lot of managers
    has an impact on the performance because all Swarm-state-related decisions (for
    example, adding a new node or leader election) have to be agreed between all managers
    using the Raft algorithm. As a result, the number of managers is always a tradeoff
    between the fault tolerance and the performance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 管理节点的数量没有限制，因此听起来管理节点越多，容错能力就越好。这是真的，然而，拥有大量管理节点会影响性能，因为所有与Swarm状态相关的决策（例如，添加新节点或领导者选举）都必须使用Raft算法在所有管理节点之间达成一致意见。因此，管理节点的数量始终是容错能力和性能之间的权衡。
- en: The Raft algorithm itself has a constraint on the number of managers. Distributed
    decisions have to be approved by the majority of nodes, called a quorum. This
    fact implies that an odd number of managers is recommended.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Raft算法本身对管理者的数量有限制。分布式决策必须得到大多数节点的批准，称为法定人数。这一事实意味着建议使用奇数个管理者。
- en: To understand why, let's see what would happen if we had two managers. In that
    case, the majority is two, so if any of the managers is down, then it's not possible
    to reach the quorum and therefore elect the leader. As a result, losing one machine
    makes the whole cluster out of order. We added a manager, but the whole cluster
    became less fault tolerant. The situation would be different in the case of three
    managers. Then, the majority is still two, so losing one manager does not stop
    the whole cluster. This is the fact that even though it's not technically forbidden,
    only odd number of managers makes sense.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么，让我们看看如果我们有两个管理者会发生什么。在这种情况下，法定人数是两个，因此如果任何一个管理者宕机，那么就不可能达到法定人数，因此也无法选举领导者。结果，失去一台机器会使整个集群失效。我们增加了一个管理者，但整个集群变得不太容错。在三个管理者的情况下情况会有所不同。然后，法定人数仍然是两个，因此失去一个管理者不会停止整个集群。这是一个事实，即使从技术上讲并不是被禁止的，但只有奇数个管理者是有意义的。
- en: The more the managers in the cluster, the more the Raft-related operations are
    involved. Then, the `manager` nodes should be put into the drain availability
    in order to save their resources.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的管理者越多，就涉及到越多与Raft相关的操作。然后，“管理者”节点应该被放入排水可用性，以节省它们的资源。
- en: Scheduling strategy
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度策略
- en: So far, we have learned that the manager automatically assigns a worker node
    to a task. In this section, we dive deeper into what automatically means. We present
    the Docker Swarm scheduling strategy and a way to configure it according to our
    needs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到管理者会自动将工作节点分配给任务。在本节中，我们将深入探讨自动分配的含义。我们介绍Docker Swarm调度策略以及根据我们的需求进行配置的方法。
- en: 'Docker Swarm uses two criteria for choosing the right worker node:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm使用两个标准来选择合适的工作节点：
- en: '**Resource availability**: Scheduler is aware of the resources available on
    nodes. It uses the so-called **spread strategy** that attempts to schedule the
    task on the least loaded node, provided it meets the criteria specified by labels
    and constraints.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源可用性**：调度器知道节点上可用的资源。它使用所谓的**扩展策略**，试图将任务安排在负载最轻的节点上，前提是它符合标签和约束指定的条件。'
- en: '**Labels and constraints**:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签和约束**：'
- en: Label is an attribute of a node. Some labels are assigned automatically, for
    example, `node.id` or `node.hostname`; others can be defined by the cluster admin,
    for example, `node.labels.segment`
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签是节点的属性。有些标签是自动分配的，例如“node.id”或“node.hostname”；其他可以由集群管理员定义，例如“node.labels.segment”。
- en: Constraint is a restriction applied by the creator of the service, for example,
    choosing only nodes with the given label
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约束是服务创建者应用的限制，例如，仅选择具有特定标签的节点
- en: Labels are divided into two categories, `node.labels` and `engine.labels`. The
    first one is added by the operational team; the second one is collected by Docker
    Engine, for example, operating system or hardware specifics.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 标签分为两类，“node.labels”和“engine.labels”。第一类是由运营团队添加的；第二类是由Docker Engine收集的，例如操作系统或硬件特定信息。
- en: 'As an example, if we would like to run the Tomcat service on the concrete node,
    `ubuntu-worker1`, then we need to use the following command:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想在具体节点“ubuntu-worker1”上运行Tomcat服务，那么我们需要使用以下命令：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can also add a custom label to the node:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以向节点添加自定义标签：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding command added a label, `node.labels.segment`, with the value
    `AA`. Then, we can use it while running the service:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令添加了一个标签“node.labels.segment”，其值为“AA”。然后，在运行服务时我们可以使用它：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This command runs the `tomcat` replicas only on the nodes that are labeled with
    the given segment, `AA`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令只在标记有给定段`AA`的节点上运行`tomcat`副本。
- en: Labels and constraints give us the flexibility to configure the nodes on which
    service replicas would be run. This approach, even though valid in many cases,
    should not be overused, since it's best to keep the replicas distributed on multiple
    nodes and let Docker Swarm take care of the right scheduling process.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 标签和约束使我们能够配置服务副本将在哪些节点上运行。尽管这种方法在许多情况下是有效的，但不应该过度使用，因为最好让副本分布在多个节点上，并让Docker
    Swarm负责正确的调度过程。
- en: Docker Compose with Docker Swarm
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Compose与Docker Swarm
- en: We have described how to use Docker Swarm in order to deploy a service, which
    in turn runs multiple containers from the given Docker image. On the other hand,
    there is Docker Compose, which provides a method to define the dependencies between
    containers and enables scaling containers, but everything is done within one Docker
    host. How do we merge both technologies so that we can specify the `docker-compose.yml`
    file and automatically distribute the containers on a cluster? Luckily, there
    is Docker Stack.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经描述了如何使用Docker Swarm来部署一个服务，该服务又从给定的Docker镜像中运行多个容器。另一方面，还有Docker Compose，它提供了一种定义容器之间依赖关系并实现容器扩展的方法，但所有操作都在一个Docker主机内完成。我们如何将这两种技术合并起来，以便我们可以指定`docker-compose.yml`文件，并自动将容器分布在集群上？幸运的是，有Docker
    Stack。
- en: Introducing Docker Stack
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Docker Stack
- en: 'Docker Stack is a method to run multiple-linked containers on a Swarm cluster.
    To understand better how it links Docker Compose with Docker Swarm, let''s take
    a look at the following figure:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Stack是在Swarm集群上运行多个关联容器的方法。为了更好地理解它如何将Docker Compose与Docker Swarm连接起来，让我们看一下下面的图：
- en: '![](assets/86ad1636-d244-4a44-9c67-c64b8080eba1.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/86ad1636-d244-4a44-9c67-c64b8080eba1.png)'
- en: Docker Swarm orchestrates which container is run on which physical machine.
    The containers, however, don't have any dependencies between themselves, so in
    order for them to communicate, we would need to link them manually. Docker Compose,
    in contrast, provides linking between the containers. In the example from the
    preceding figure, one Docker image (deployed in three replicated containers) depends
    on another Docker image (deployed as one container). All containers, however,
    run on the same Docker host, so the horizontal scaling is limited to the resources
    of one machine. Docker Stack connects both technologies and allows using the `docker-compose.yml`
    file to run the complete environment of linked containers deployed on a cluster
    of Docker hosts.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm编排哪个容器在哪台物理机上运行。然而，容器之间没有任何依赖关系，因此为了它们进行通信，我们需要手动链接它们。相反，Docker
    Compose提供了容器之间的链接。在前面图中的例子中，一个Docker镜像（部署为三个复制的容器）依赖于另一个Docker镜像（部署为一个容器）。然而，所有容器都运行在同一个Docker主机上，因此水平扩展受限于一台机器的资源。Docker
    Stack连接了这两种技术，并允许使用`docker-compose.yml`文件在一组Docker主机上运行链接容器的完整环境。
- en: Using Docker Stack
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Stack
- en: 'As an example, let''s use the `calculator` image that depends on the `redis`
    image. Let''s split the process into four steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们使用依赖于`redis`镜像的`calculator`镜像。让我们将这个过程分为四个步骤：
- en: Specifying `docker-compose.yml`.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定`docker-compose.yml`。
- en: Running the Docker Stack command.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行Docker Stack命令。
- en: Verifying the services and containers.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证服务和容器。
- en: Removing the stack.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除堆栈。
- en: Specifying docker-compose.yml
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定docker-compose.yml
- en: 'We already defined the `docker-compose.yml` file in the previous chapters and
    it looked similar to the following one:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在前面的章节中定义了`docker-compose.yml`文件，它看起来类似于以下内容：
- en: '[PRE28]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that all images must be pushed to the registry before running the `docker
    stack` command so that they would be accessible from all nodes. It is therefore
    not possible to build images inside `docker-compose.yml`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有镜像在运行`docker stack`命令之前必须推送到注册表，以便它们可以从所有节点访问。因此，不可能在`docker-compose.yml`中构建镜像。
- en: With the presented docker-compose.yml configuration, we will run three `calculator`
    containers and one `redis` container. The endpoint of the calculator service will
    be exposed on port `8881`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所提供的docker-compose.yml配置，我们将运行三个`calculator`容器和一个`redis`容器。计算器服务的端点将在端口`8881`上公开。
- en: Running the docker stack command
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行docker stack命令
- en: 'Let''s use the `docker stack` command to run the services, which in turn will
    start containers on the cluster:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`docker stack`命令来运行服务，这将在集群上启动容器：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Docker plans to simplify the syntax so that the `stack` word would not be needed,
    for example, `docker deploy --compose-file docker-compose.yml app`. At the time
    of writing, it's only available in the experimental version.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Docker计划简化语法，以便不需要`stack`这个词，例如，`docker deploy --compose-file docker-compose.yml
    app`。在撰写本文时，这仅在实验版本中可用。
- en: Verifying the services and containers
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证服务和容器
- en: 'The services have started. We can check that they are running with the `docker
    service ls` command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 服务已经启动。我们可以使用`docker service ls`命令来检查它们是否正在运行：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can look even closer at the services and check on which Docker hosts they
    have been deployed:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以更仔细地查看服务，并检查它们部署在哪些Docker主机上：
- en: '[PRE31]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can see that one of the `calculator` containers and the `redis` containers
    are started on the `ubuntu-manager` machine. Two other `calculator` containers
    run on the `ubuntu-worker1` and `ubuntu-worker2` machines.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`ubuntu-manager`机器上启动了一个`calculator`容器和一个`redis`容器。另外两个`calculator`容器分别在`ubuntu-worker1`和`ubuntu-worker2`机器上运行。
- en: Note that we explicitly specified the port number under which the `calculator`
    web service should be published. Therefore, we are able to access the endpoint
    via the manager's IP address `http://192.168.0.143:8881/sum?a=1&b=2`. The operation
    returns `3` as a result and caches it in the Redis container.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们明确指定了`calculator` web服务应该发布的端口号。因此，我们可以通过管理者的IP地址`http://192.168.0.143:8881/sum?a=1&b=2`来访问端点。操作返回`3`作为结果，并将其缓存在Redis容器中。
- en: Removing the stack
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除stack
- en: 'When we''re done with the stack, we can remove everything using the convenient
    `docker stack rm` command:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成了stack，我们可以使用方便的`docker stack rm`命令来删除所有内容：
- en: '[PRE32]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Using Docker Stack allows running the Docker Compose specification on the Docker
    Swarm cluster. Note that we used the exact `docker-compose.yml` format, which
    is a great benefit, because there is no need to specify anything extra for the
    Swarm.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker Stack允许在Docker Swarm集群上运行Docker Compose规范。请注意，我们使用了确切的`docker-compose.yml`格式，这是一个很大的好处，因为对于Swarm，不需要指定任何额外的内容。
- en: The merger of both technologies gives us the real power of deploying applications
    on Docker because we don't need to think about individual machines. All we need
    to specify is how our (micro) services are dependent on each other, express it
    in the docker-compose.yml format, and let Docker take care of everything else.
    The physical machines can be then treated simply as a set of resources.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术的合并使我们能够在Docker上部署应用程序的真正力量，因为我们不需要考虑单独的机器。我们只需要指定我们的（微）服务如何相互依赖，用docker-compose.yml格式表达出来，然后让Docker来处理其他一切。物理机器可以简单地被视为一组资源。
- en: Alternative cluster management systems
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代集群管理系统
- en: Docker Swarm is not the only system for clustering Docker containers. Even though
    it's the one available out of the box, there may be some valid reasons to install
    a third-party cluster manager. Let's walk through the most popular alternatives.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 不是唯一用于集群 Docker 容器的系统。尽管它是开箱即用的系统，但可能有一些有效的理由安装第三方集群管理器。让我们来看一下最受欢迎的替代方案。
- en: Kubernetes
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is an open source cluster management system originally designed by
    Google. Even though it's not Docker-native, the integration is smooth, and there
    are many additional tools that help with this process; for example, **kompose**
    can translate the `docker-compose.yml` files into Kubernetes configuration files.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个由谷歌最初设计的开源集群管理系统。尽管它不是 Docker 原生的，但集成非常顺畅，而且有许多额外的工具可以帮助这个过程；例如，**kompose**
    可以将 `docker-compose.yml` 文件转换成 Kubernetes 配置文件。
- en: 'Let''s take a look at the simplified architecture of Kubernetes:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 Kubernetes 的简化架构：
- en: '![](assets/6ae7e3ed-c6d5-4034-bbb2-e34bae100556.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/6ae7e3ed-c6d5-4034-bbb2-e34bae100556.png)'
- en: Kubernetes is similar to Docker Swarm in a way that it also has master and worker
    nodes. Additionally, it introduces the concept of a **pod** that represents a
    group of containers deployed and scheduled together. Most pods have a few containers
    that make up a service. Pods are dynamically built and removed depending on the
    changing requirements.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 和 Docker Swarm 类似，它也有主节点和工作节点。此外，它引入了 **pod** 的概念，表示一组一起部署和调度的容器。大多数
    pod 都有几个容器组成一个服务。Pod 根据不断变化的需求动态构建和移除。
- en: Kubernetes is relatively young. Its development started in 2014; however, it's
    based on Google's experience, and this is one of the reasons why it's one of the
    most popular cluster management systems available on the market. There are more
    and more organizations that migrated to Kubernetes, such as eBay, Wikipedia, and
    Pearson.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 相对较年轻。它的开发始于 2014 年；然而，它基于谷歌的经验，这是它成为市场上最受欢迎的集群管理系统之一的原因之一。越来越多的组织迁移到
    Kubernetes，如 eBay、Wikipedia 和 Pearson。
- en: Apache Mesos
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mesos
- en: Apache Mesos is an open source scheduling and clustering system started at the
    University of California, Berkeley, in 2009, long before Docker emerged. It provides
    an abstraction layer over CPU, disk space, and RAM. One of the great advantages
    of Mesos is that it supports any Linux application, not necessarily (Docker) containers.
    This is why it's possible to create a cluster out of thousands of machines and
    use it for both Docker containers and other programs, for example, Hadoop-based
    calculations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mesos 是一个在 2009 年由加州大学伯克利分校发起的开源调度和集群系统，早在 Docker 出现之前就开始了。它提供了一个在 CPU、磁盘空间和内存上的抽象层。Mesos
    的一个巨大优势是它支持任何 Linux 应用程序，不一定是（Docker）容器。这就是为什么可以创建一个由数千台机器组成的集群，并且用于 Docker 容器和其他程序，例如基于
    Hadoop 的计算。
- en: 'Let''s look at the figure presenting the Mesos architecture:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下展示 Mesos 架构的图：
- en: '![](assets/8debdc02-d7c6-4f97-948f-e6f51db3e6ef.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8debdc02-d7c6-4f97-948f-e6f51db3e6ef.png)'
- en: Apache Mesos, similar to other clustering systems, has the master-slave architecture.
    It uses node agents installed on every node for communication, and it provides
    two types of schedulers, Chronos - for cron-style repeating tasks and Marathon
    - providing a REST API to orchestrate services and containers.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mesos，类似于其他集群系统，具有主从架构。它使用安装在每个节点上的节点代理进行通信，并提供两种类型的调度器，Chronos - 用于
    cron 风格的重复任务和 Marathon - 提供 REST API 来编排服务和容器。
- en: Apache Mesos is very mature compared to other clustering systems, and it has
    been adopted in a large number of organizations, such as Twitter, Uber, and CERN.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他集群系统相比，Apache Mesos 非常成熟，并且已经被许多组织采用，如 Twitter、Uber 和 CERN。
- en: Comparing features
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较功能
- en: 'Kubernetes, Docker Swarm, and Mesos are all good choices for the cluster management
    system. All of them are free and open source, and all of them provide important
    cluster management features such as load balancing, service discovery, distributed
    storage, failure recovery, monitoring, secret management, and rolling updates.
    All of them can also be used in the Continuous Delivery process without huge differences.
    This is because, in the Dockerized infrastructure, they all address the same issue,
    the clustering of Docker containers. Nevertheless, obviously, the systems are
    not exactly the same. Let''s have a look at a table presenting the differences:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes、Docker Swarm和Mesos都是集群管理系统的不错选择。它们都是免费且开源的，并且它们都提供重要的集群管理功能，如负载均衡、服务发现、分布式存储、故障恢复、监控、秘钥管理和滚动更新。它们在持续交付过程中也可以使用，没有太大的区别。这是因为，在Docker化的基础设施中，它们都解决了同样的问题，即Docker容器的集群化。然而，显然，这些系统并不完全相同。让我们看一下表格，展示它们之间的区别：
- en: '|  | **Docker Swarm** | **Kubernetes** | **Apache Mesos** |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | **Docker Swarm** | **Kubernetes** | **Apache Mesos** |'
- en: '| **Docker support** | Native | Supports Docker as one of the container types
    in the pod | Mesos agents (slaves) can be configured to host Docker containers
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **Docker支持** | 本机支持 | 支持Docker作为Pod中的容器类型之一 | Mesos代理（从属）可以配置为托管Docker容器'
- en: '| **Application types** | Docker images | Containerized applications (Docker,
    rkt, and hyper) | Any application that can be run on Linux (also containers) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| **应用程序类型** | Docker镜像 | 容器化应用程序（Docker、rkt和hyper） | 任何可以在Linux上运行的应用程序（也包括容器）
    |'
- en: '| **Application definition** | Docker Compose configuration | Pods configuration,
    replica sets, replication controllers, services, and deployments | Application
    groups formed in the tree structure |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| **应用程序定义** | Docker Compose配置 | Pod配置，副本集，复制控制器，服务和部署 | 以树形结构形成的应用程序组'
- en: '| **Setup process** | Very simple | Depending on the infrastructure, it may
    require running one command or many complex operations | Fairly involved, it requires
    configuring Mesos, Marathon, Chronos, Zookeeper, and Docker support |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: 设置过程 | 非常简单 | 根据基础设施的不同，可能需要运行一个命令或者进行许多复杂的操作 | 相当复杂，需要配置Mesos、Marathon、Chronos、Zookeeper和Docker支持
- en: '| **API** | Docker REST API | REST API | Chronos/Marathon REST API |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **API** | Docker REST API | REST API | Chronos/Marathon REST API'
- en: '| **User Interface** | Docker console client, third-party web applications,
    such as Shipyard | Console tools, native Web UI (Kubernetes Dashboard) | Official
    web interfaces for Mesos, Marathon, and Chronos |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **用户界面** | Docker控制台客户端，Shipyard等第三方Web应用 | 控制台工具，本机Web UI（Kubernetes仪表板）
    | Mesos、Marathon和Chronos的官方Web界面'
- en: '| **Cloud integration** | Manual installation required | Cloud-native support
    from most providers (Azure, AWS, Google Cloud, and others) | Support from most
    cloud providers |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **云集成** | 需要手动安装 | 大多数提供商（Azure、AWS、Google Cloud等）提供云原生支持 | 大多数云提供商提供支持'
- en: '| **Maximum cluster size** | 1,000 nodes | 1,000 nodes | 50,000 nodes |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **最大集群大小** | 1,000个节点 | 1,000个节点 | 50,000个节点 |'
- en: '| **Autoscaling** | Not available | Provides horizontal pod autoscaling based
    on the observed CPU usage | Marathon provides autoscaling based on resource (CPU/Memory)
    consumption, number of requests per second, and queue length |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **自动扩展** | 不可用 | 根据观察到的CPU使用情况提供水平Pod自动扩展 | Marathon根据资源（CPU/内存）消耗、每秒请求的数量和队列长度提供自动扩展'
- en: Obviously, apart from Docker Swarm, Kubernetes, and Apache Mesos, there are
    other clustering systems available in the market. They are, however, not that
    popular and their usage decreases over time.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，除了Docker Swarm、Kubernetes和Apache Mesos之外，市场上还有其他可用的集群系统。然而，它们并不那么受欢迎，它们的使用量随着时间的推移而减少。
- en: No matter which system you choose, you can use it not only for the staging/production
    environments but also to scale Jenkins agents. Let's have a look at how to do
    it.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪个系统，您都可以将其用于暂存/生产环境，也可以用于扩展Jenkins代理。让我们看看如何做到这一点。
- en: Scaling Jenkins
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展Jenkins
- en: The obvious use cases for server clustering are the staging and production environments.
    When used, it's enough to attach a physical machine in order to increase the capacity
    of the environment. Nevertheless, in the context of Continuous Delivery, we may
    also want to improve the Jenkins infrastructure by running Jenkins agent (slave)
    nodes on a cluster. In this section, we take a look at two different methods to
    achieve this goal.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器集群的明显用例是暂存和生产环境。在使用时，只需连接物理机即可增加环境的容量。然而，在持续交付的背景下，我们可能还希望通过在集群上运行Jenkins代理（从属）节点来改进Jenkins基础设施。在本节中，我们将看两种不同的方法来实现这个目标。
- en: Dynamic slave provisioning
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态从属配置
- en: We saw dynamic slave provisioning in [Chapter 3](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml),
    *Configuring Jenkins*. With Docker Swarm, the idea stays exactly the same. When
    the build is started, the Jenkins master runs a container from the Jenkins slave
    Docker image, and the Jenkinsfile script is executed inside the container. Docker
    Swarm, however, makes the solution more powerful since we are not limited to a
    single Docker host machine but can provide real horizontal scaling. Adding a new
    Docker host to the cluster effectively scales up the capacity of the Jenkins infrastructure.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在《配置Jenkins》的[第3章](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml)中看到了动态从属配置。使用Docker
    Swarm，这个想法保持完全一样。当构建开始时，Jenkins主服务器会从Jenkins从属Docker镜像中运行一个容器，并在容器内执行Jenkinsfile脚本。然而，Docker
    Swarm使解决方案更加强大，因为我们不再局限于单个Docker主机，而是可以提供真正的水平扩展。向集群添加新的Docker主机有效地扩展了Jenkins基础设施的容量。
- en: At the time of writing, the Jenkins Docker plugin does not support Docker Swarm.
    One of the solutions is to use Kubernetes or Mesos as the cluster management system.
    Each of them has a dedicated Jenkins plugin: Kubernetes Plugin ([https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin](https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin))
    and Mesos Plugin ([https://wiki.jenkins.io/display/JENKINS/Mesos+Plugin](https://wiki.jenkins.io/display/JENKINS/Mesos+Plugin)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Jenkins Docker插件不支持Docker Swarm。其中一个解决方案是使用Kubernetes或Mesos作为集群管理系统。它们每个都有一个专用的Jenkins插件：Kubernetes插件（[https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin](https://wiki.jenkins.io/display/JENKINS/Kubernetes+Plugin)）和Mesos插件（[https://wiki.jenkins.io/display/JENKINS/Mesos+Plugin](https://wiki.jenkins.io/display/JENKINS/Mesos+Plugin)）。
- en: No matter how the slaves are provisioned, we always configure them by installing
    the appropriate plugin and adding the entry to the Cloud section in Manage Jenkins
    | Configure System.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 无论从属是如何配置的，我们总是通过安装适当的插件并在Manage Jenkins | Configure System的Cloud部分中添加条目来配置它们。
- en: Jenkins Swarm
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jenkins Swarm
- en: If we don't want to use the dynamic slave provisioning, then another solution
    for clustering Jenkins slaves is to use Jenkins Swarm. We described how to use
    it in [Chapter 3](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml), *Configuring Jenkins*.
    Here, we add the description for Docker Swarm.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想使用动态从属配置，那么集群化Jenkins从属的另一个解决方案是使用Jenkins Swarm。我们在《配置Jenkins》的[第3章](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml)中描述了如何使用它。在这里，我们为Docker
    Swarm添加描述。
- en: 'First, let''s have a look at how to run the Jenkins Swarm slave using the Docker
    image built from the swarm-client.jar tool. There are a few of them available
    on Docker Hub; we can use the csanchez/jenkins-swarm-slave image:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何使用从swarm-client.jar工具构建的Docker镜像来运行Jenkins Swarm从属。Docker Hub上有一些可用的镜像；我们可以使用csanchez/jenkins-swarm-slave镜像：
- en: '[PRE33]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This command execution should have exactly the same effect as the on presented in [Chapter
    3](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml), *Configuring Jenkins*; it dynamically
    adds a slave to the Jenkins master.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令执行应该与[第3章](f46b0c80-7f67-476c-9a17-8f1238f0b359.xhtml)中介绍的具有完全相同的效果，*配置Jenkins*；它动态地向Jenkins主节点添加一个从节点。
- en: 'Then, to get the most of Jenkins Swarm, we can run the slave containers on
    the Docker Swarm cluster:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了充分利用Jenkins Swarm，我们可以在Docker Swarm集群上运行从节点容器：
- en: '[PRE34]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding command starts five slaves on the cluster and attaches them to
    the Jenkins master. Please note that it is very simple to scale Jenkins horizontally
    by executing the docker service scale command.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令在集群上启动了五个从节点，并将它们附加到了Jenkins主节点。请注意，通过执行docker service scale命令，可以非常简单地通过水平扩展Jenkins。
- en: Comparison of dynamic slave provisioning and Jenkins Swarm
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态从节点配置和Jenkins Swarm的比较
- en: 'Dynamic slave provisioning and Jenkins Swarm can be both run on a cluster that
    results in the architecture presented in the following diagram:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 动态从节点配置和Jenkins Swarm都可以在集群上运行，从而产生以下图表中呈现的架构：
- en: '![](assets/165c3a7a-c681-4d65-bb26-93517bf1e7e2.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/165c3a7a-c681-4d65-bb26-93517bf1e7e2.png)'
- en: Jenkins slaves are run on the cluster and therefore are very easily scaled up
    and down. If we need more Jenkins resources, we scale up Jenkins slaves. If we
    need more cluster resources, we add more physical machines to the cluster.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins从节点在集群上运行，因此非常容易进行水平扩展和缩减。如果我们需要更多的Jenkins资源，我们就扩展Jenkins从节点。如果我们需要更多的集群资源，我们就向集群添加更多的物理机器。
- en: 'The difference between the two solutions is that the dynamic slave provisioning
    automatically adds a Jenkins slave to the cluster before each build. The benefit
    of such approach is that we don''t even have to think about how many Jenkins slaves
    should be running at the moment since the number automatically adapts to the number
    of pipeline builds. This is why, in most cases, the dynamic slave provisioning
    is the first choice. Nevertheless, Jenkins Swarm also carries a few significant
    benefits:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种解决方案之间的区别在于，动态从节点配置会在每次构建之前自动向集群添加一个Jenkins从节点。这种方法的好处是，我们甚至不需要考虑此刻应该运行多少Jenkins从节点，因为数量会自动适应流水线构建的数量。这就是为什么在大多数情况下，动态从节点配置是首选。然而，Jenkins
    Swarm也具有一些显著的优点：
- en: '**Control of the number of slaves**: Using Jenkins Swarm, we explicitly decide
    how many Jenkins slaves should be running at the moment.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制从节点数量**：使用Jenkins Swarm，我们明确决定此刻应该运行多少Jenkins从节点。'
- en: '**Stateful slaves**: Many builds share the same Jenkins slave, which may sound
    like a drawback; however, it becomes an advantage when a build requires downloading
    a lot of dependent libraries from the internet. In the case of dynamic slave provisioning,
    to cache the dependencies, we would need to set up a shared volume.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态的从节点**：许多构建共享相同的Jenkins从节点，这可能听起来像一个缺点；然而，当一个构建需要从互联网下载大量依赖库时，这就成为了一个优势。在动态从节点配置的情况下，为了缓存这些依赖，我们需要设置一个共享卷。'
- en: '**Control of where the slaves are running**: Using Jenkins Swarm, we can decide
    not to run slaves on the cluster but to choose the host machine dynamically; for
    example, for many startups, when the cluster infrastructure is costly, slaves
    can be dynamically run on the laptop of a developer who is starting the build.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制从节点运行的位置**：使用Jenkins Swarm，我们可以决定不在集群上运行从节点，而是动态选择主机；例如，对于许多初创公司来说，当集群基础设施成本高昂时，从节点可以动态地在开始构建的开发人员的笔记本电脑上运行。'
- en: Clustering Jenkins slaves bring a lot of benefits and it is what the modern
    Jenkins architecture should look like. This way, we can provide the dynamic horizontal
    scaling of the infrastructure for the Continuous Delivery process.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 集群化Jenkins从属节点带来了许多好处，这就是现代Jenkins架构应该看起来的样子。这样，我们可以为持续交付过程提供动态的水平扩展基础设施。
- en: Exercises
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'In this chapter, we have covered Docker Swarm and the clustering process in
    detail. In order to enhance this knowledge, we recommend the following exercises:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细介绍了Docker Swarm和集群化过程。为了增强这方面的知识，我们建议进行以下练习：
- en: 'Set up a swarm cluster that consists of three nodes:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个由三个节点组成的Swarm集群：
- en: Use one machine as the manager node and two machines as worker nodes
  id: totrans-246
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一台机器作为管理节点，另外两台机器作为工作节点
- en: You can use physical machines connected to one network, machines from the cloud
    provider, or VirtualBox machines with the shared network
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用连接到一个网络的物理机器，来自云提供商的机器，或者具有共享网络的VirtualBox机器
- en: Check that the cluster is configured properly using the `docker node` command
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `docker node` 命令检查集群是否正确配置
- en: 'Run/scale a hello world service on the cluster:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群上运行/扩展一个hello world服务：
- en: The service can look exactly the same as described in the exercises for [Chapter
    2](aa58c16d-41c0-4364-9eae-26b60a05c510.xhtml), *Introducing Docker*
  id: totrans-250
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务可以与[第2章](aa58c16d-41c0-4364-9eae-26b60a05c510.xhtml)中描述的完全相同
- en: Publish the port so that it will be accessible from outside of the cluster
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布端口，以便可以从集群外部访问
- en: Scale the service to five replicas
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将服务扩展到五个副本
- en: Make a request to the "hello world" service and check which of the containers
    is serving the request
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向“hello world”服务发出请求，并检查哪个容器正在提供请求
- en: 'Scale Jenkins using slaves deployed on the Swarm cluster:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在Swarm集群上部署的从属节点来扩展Jenkins：
- en: Use Jenkins Swarm or dynamic slave provisioning
  id: totrans-255
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Jenkins Swarm或动态从属节点供应
- en: Run a pipeline build and check that it is executed on one of the clustered slaves
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行管道构建并检查它是否在其中一个集群化的从属节点上执行
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we took a look at the clustering methods for Docker environments
    that enable setting up the complete staging/production/Jenkins environment. Here
    are the key takeaways from the chapter:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看了一下Docker环境的集群化方法，这些方法可以实现完整的分段/生产/Jenkins环境的设置。以下是本章的要点：
- en: Clustering is a method of configuring a set of machines in a way that, in many
    respects, can be viewed as a single system
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类是一种配置一组机器的方法，从许多方面来看，它可以被视为一个单一的系统
- en: Docker Swarm is the native clustering system for Docker
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm是Docker的本地集群系统
- en: Docker Swarm clusters can be dynamically configured using built-in Docker commands
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用内置的Docker命令动态配置Docker Swarm集群
- en: Docker images can be run and scaled on the cluster using the docker service command
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用docker service命令在集群上运行和扩展Docker镜像
- en: Docker Stack is a method to run the Docker Compose configuration on a Swarm
    cluster
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Stack是在Swarm集群上运行Docker Compose配置的方法
- en: The most popular clustering systems that support Docker are Docker Swarm, Kubernetes,
    and Apache Mesos
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持Docker的最流行的集群系统是Docker Swarm、Kubernetes和Apache Mesos
- en: Jenkins agents can be run on a cluster using the dynamic slave provisioning
    or the Jenkins Swarm plugin
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jenkins代理可以使用动态从属节点供应或Jenkins Swarm插件在集群上运行
- en: In the next chapter, we will describe the more advanced aspects of the Continuous
    Delivery process and present the best practices for building pipelines
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将描述持续交付过程的更高级方面，并介绍构建流水线的最佳实践
