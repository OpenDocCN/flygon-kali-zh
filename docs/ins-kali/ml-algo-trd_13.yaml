- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无监督学习识别数据驱动的风险因素和资产配置
- en: '*Chapter 6*, *The Machine Learning Process*, introduced how unsupervised learning
    adds value by uncovering structures in data without the need for an outcome variable
    to guide the search process. This contrasts with supervised learning, which was
    the focus of the last several chapters: instead of predicting future outcomes,
    unsupervised learning aims to learn an informative representation of the data
    that helps explore new data, discover useful insights, or solve some other task
    more effectively.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*第6章*，*机器学习过程*介绍了无监督学习如何通过发现数据中的结构来增加价值，而无需结果变量来指导搜索过程。这与前几章的重点是监督学习形成对比：无监督学习的目标不是预测未来结果，而是学习数据的信息表示，以帮助更有效地探索新数据，发现有用的见解，或解决其他一些任务。'
- en: 'Dimensionality reduction and clustering are the main tasks for unsupervised
    learning:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 降维和聚类是无监督学习的主要任务：
- en: '**Dimensionality reduction** transforms the existing features into a new, smaller
    set while minimizing the loss of information. Algorithms differ by how they measure
    the loss of information, whether they apply linear or nonlinear transformations
    or which constraints they impose on the new feature set.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**将现有特征转换为一个新的、更小的集合，同时最小化信息损失。算法在衡量信息损失的方式、是否应用线性或非线性转换或对新特征集施加哪些约束方面有所不同。'
- en: '**Clustering algorithms** identify and group similar observations or features
    instead of identifying new features. Algorithms differ in how they define the
    similarity of observations and their assumptions about the resulting groups.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法**识别和分组相似的观察或特征，而不是识别新特征。算法在定义观察相似性和对结果组的假设方面有所不同。'
- en: These unsupervised algorithms are useful when a **dataset does not contain an
    outcome**. For instance, we may want to extract tradeable information from a large
    body of financial reports or news articles. In *Chapter 14*, *Text Data for Trading
    – Sentiment Analysis*, we'll use topic modeling to discover hidden themes that
    allow us to explore and summarize content more effectively, and identify meaningful
    relationships that can help us to derive signals.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当**数据集不包含结果**时，这些无监督算法非常有用。例如，我们可能希望从大量财务报告或新闻文章中提取可交易的信息。在*第14章*，*用于交易的文本数据
    - 情感分析*中，我们将使用主题建模来发现隐藏的主题，从而更有效地探索和总结内容，并识别有助于推导信号的有意义的关系。
- en: The algorithms are also useful when we want to **extract information independently
    from an outcome**. For example, rather than using third-party industry classifications,
    clustering allows us to identify synthetic groupings based on the attributes of
    assets useful for our purposes, such as returns over a certain time horizon, exposure
    to risk factors, or similar fundamentals. In this chapter, we will learn how to
    use clustering to manage portfolio risks by identifying hierarchical relationships
    among asset returns.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望**独立地从结果中提取信息**时，这些算法也非常有用。例如，与使用第三方行业分类不同，聚类允许我们根据资产的属性识别出对我们有用的合成分组，例如在特定时间范围内的回报，风险因素的暴露，或类似的基本面。在本章中，我们将学习如何使用聚类来通过识别资产回报之间的分层关系来管理投资组合风险。
- en: 'More specifically, after reading this chapter, you will understand:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，在阅读本章后，您将了解：
- en: How **principal component analysis** (**PCA**) and **independent component analysis**
    (**ICA**) perform linear dimensionality reduction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）和**独立成分分析**（**ICA**）如何执行线性降维'
- en: Identifying data-driven risk factors and eigenportfolios from asset returns
    using PCA
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA从资产回报中识别数据驱动的风险因素和特征组合
- en: Effectively visualizing nonlinear, high-dimensional data using manifold learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何有效地使用流形学习来可视化非线性，高维数据
- en: Using T-SNE and UMAP to explore high-dimensional image data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用T-SNE和UMAP来探索高维图像数据
- en: How k-means, hierarchical, and density-based clustering algorithms work
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值，层次和基于密度的聚类算法的工作原理
- en: Using agglomerative clustering to build robust portfolios with hierarchical
    risk parity
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用凝聚聚类构建具有分层风险平衡的强大投资组合
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和额外资源的链接。笔记本包括图像的彩色版本。
- en: Dimensionality reduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: In linear algebra terms, the features of a dataset create a **vector space**
    whose dimensionality corresponds to the number of linearly independent rows or
    columns, whichever is larger. Two columns are linearly dependent when they are
    perfectly correlated so that one can be computed from the other using the linear
    operations of addition and multiplication.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数术语中，数据集的特征创建一个**向量空间**，其维度对应于线性独立行或列的数量，以较大者为准。当两列完全相关时，它们是线性相关的，因此可以使用线性运算的加法和乘法从一个列计算另一个列。
- en: In other words, they are parallel vectors that represent the same direction
    rather than different ones in the data and thus only constitute a single dimension.
    Similarly, if one variable is a linear combination of several others, then it
    is an element of the vector space created by those columns and does not add a
    new dimension of its own.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它们是代表数据中相同方向而不是不同方向的平行向量，因此只构成一个维度。同样，如果一个变量是其他几个变量的线性组合，则它是由这些列创建的向量空间的元素，并且不会增加自己的新维度。
- en: 'The number of dimensions of a dataset matters because each new dimension can
    add a signal concerning an outcome. However, there is also a downside known as
    the **curse of dimensionality**: as the number of independent features grows while
    the number of observations remains constant, the average distance between data
    points also grows, and the density of the feature space drops exponentially, with
    dramatic implications for **machine learning** (**ML**). **Prediction becomes
    much harder** when observations are more distant, that is, different from each
    other. Alternative data sources, like text and images, typically are of high dimensionality,
    but they generally affect models that rely on a large number of features. The
    next section addresses the resulting challenges.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的维度数量很重要，因为每个新维度都可能添加有关结果的信号。然而，也存在一个被称为“维度诅咒”的缺点：随着独立特征的数量增加，观察数量保持恒定，数据点之间的平均距离也增加，特征空间的密度呈指数级下降，对机器学习（ML）有着重大影响。当观察值相距较远时，即彼此不同，预测变得更加困难。替代数据源，如文本和图像，通常具有较高的维度，但它们通常会影响依赖大量特征的模型。下一节将讨论由此产生的挑战。
- en: Dimensionality reduction seeks to **represent the data more efficiently** by
    using fewer features. To this end, algorithms project the data to a lower-dimensional
    space while discarding any variation that is not informative, or by identifying
    a lower-dimensional subspace or manifold on or near to where the data lives.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 降维旨在通过使用更少的特征更有效地表示数据。为此，算法将数据投影到一个低维空间，同时丢弃任何不具信息量的变化，或者通过识别数据所在的低维子空间或流形来实现。
- en: A **manifold** is a space that locally resembles Euclidean space. One-dimensional
    manifolds include a line or a circle, but not the visual representation of the
    number eight due to the crossing point.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 流形是在局部上类似于欧几里得空间的空间。一维流形包括一条线或一个圆，但不包括由于交叉点而无法在视觉上表示数字八。
- en: The manifold hypothesis maintains that high-dimensional data often resides in
    a lower-dimensional space, which, if identified, permits a faithful representation
    of the data in this subspace. Refer to Fefferman, Mitter, and Narayanan (2016)
    for background information and the description of an algorithm that tests this
    hypothesis.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 流形假设认为高维数据通常存在于低维空间中，如果确定了这一点，就可以在这个子空间中忠实地表示数据。有关背景信息和测试这一假设的算法描述，请参阅Fefferman、Mitter和Narayanan（2016）。
- en: Dimensionality reduction, therefore, compresses the data by finding a different,
    smaller set of variables that capture what matters most in the original features
    to minimize the loss of information. Compression helps counter the curse of dimensionality,
    economizes on memory, and permits the visualization of salient aspects of higher-dimensional
    data that is otherwise very difficult to explore.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 降维，因此，通过找到一个不同的、更小的变量集来捕捉原始特征中最重要的内容，从而压缩数据，以最小化信息损失。压缩有助于对抗维度诅咒，节省内存，并允许可视化高维数据的显著方面，否则很难探索。
- en: 'Dimensionality reduction algorithms differ by the constraints they impose on
    the new variables and how they aim to minimize the loss of information (see Burges
    2010 for an excellent overview):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降维算法通过对新变量施加的约束以及它们如何旨在最小化信息损失而有所不同（参见Burges 2010，了解更多信息）：
- en: '**Linear algorithms** like PCA and ICA constrain the new variables to be linear
    combinations of the original features; for example, hyperplanes in a lower-dimensional
    space. Whereas PCA requires the new features to be uncorrelated, ICA goes further
    and imposes statistical independence, implying the absence of both linear and
    nonlinear relationships.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像PCA和ICA这样的线性算法限制新变量是原始特征的线性组合；例如，是低维空间中的超平面。而PCA要求新特征是不相关的，ICA则进一步要求统计独立性，意味着不存在线性和非线性关系。
- en: '**Nonlinear algorithms** are not restricted to hyperplanes and can capture
    a more complex structure in the data. However, given the infinite number of options,
    the algorithms still need to make assumptions in order to arrive at a solution.
    Later in this section, we will explain how **t-distributed Stochastic Neighbor
    Embedding** (**t-SNE**) and **Uniform Manifold Approximation and Projection**
    (**UMAP**) are very useful to visualize higher-dimensional data. *Figure 13.1*
    illustrates how manifold learning identifies a two-dimensional subspace in the
    three-dimensional feature space. (The notebook `manifold_learning` illustrates
    the use of additional algorithms, including local linear embedding.)![](img/B15439_13_01.png)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性算法不受超平面的限制，可以捕捉数据中更复杂的结构。然而，由于选项的数量是无限的，算法仍然需要做出假设才能得出解决方案。本节后面将解释t-分布随机邻域嵌入（t-SNE）和均匀流形逼近和投影（UMAP）如何非常有用地可视化高维数据。图13.1说明了流形学习如何在三维特征空间中识别出一个二维子空间。（笔记本`manifold_learning`说明了使用其他算法，包括局部线性嵌入。）![](img/B15439_13_01.png)
- en: 'Figure 13.1: Nonlinear dimensionality reduction'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：非线性降维
- en: The curse of dimensionality
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: An increase in the number of dimensions of a dataset means that there are more
    entries in the vector of features that represents each observation in the corresponding
    Euclidean space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集维度的增加意味着在表示相应欧几里得空间中的每个观察的特征向量中有更多的条目。
- en: We measure the distance in a vector space using the Euclidean distance, also
    known as the L² norm, which we applied to the vector of linear regression coefficients
    to train a regularized ridge regression.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用欧几里得距离来衡量向量空间中的距离，也称为L²范数，我们将其应用于线性回归系数的向量，以训练正则化的岭回归。
- en: 'The Euclidean distance between two *n*-dimensional vectors with Cartesian coordinates
    *p* = (*p*[1], *p*[2], ..., *p*[n]) and *q* = (*q*[1], *q*[2], ..., *q*[n]) is
    computed using the familiar formula developed by Pythagoras:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 两个具有笛卡尔坐标*p* = (*p*[1]，*p*[2]，...，*p*[n])和*q* = (*q*[1]，*q*[2]，...，*q*[n])的*n*维向量之间的欧几里得距离是使用毕达哥拉斯开发的熟悉公式计算的：
- en: '![](img/B15439_13_001.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_001.png)'
- en: Therefore, each new dimension adds a non-negative term to the sum so that the
    distance increases with the number of dimensions for distinct vectors. In other
    words, as the number of features grows for a given number of observations, the
    feature space becomes increasingly sparse, that is, less dense or emptier. On
    the flip side, the lower data density requires more observations to keep the average
    distance between the data points the same.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个新维度都会为总和增加一个非负项，使得距离随着不同向量的维度数量增加而增加。换句话说，随着给定观察数量的特征数量增加，特征空间变得越来越稀疏，即更少或更空。另一方面，较低的数据密度需要更多的观察来保持数据点之间的平均距离相同。
- en: '*Figure 13.2* illustrates the exponential growth in the number of data points
    needed to maintain the average distance among observations as the number of dimensions
    increases. 10 points uniformly distributed on a line correspond to 10² points
    in two dimensions and 10³ points in three dimensions in order to keep the density
    constant.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.2*说明了随着维度数量增加，为保持观察之间的平均距离所需的数据点数量呈指数增长。在线上均匀分布的10个点对应于二维中的10²个点，在三维中对应于10³个点，以保持密度恒定。'
- en: '![](img/B15439_13_02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_02.png)'
- en: 'Figure 13.2: The number of features required to keep the average distance constant
    grows exponentially with the number of dimensions'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2：保持平均距离恒定所需的特征数量随着维度数量的指数增长
- en: The notebook `the_curse_of_dimensionality` in the GitHub repository folder for
    this section simulates how the average and minimum distances between data points
    increase as the number of dimensions grows (see *Figure 13.3*).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub存储库文件夹中的笔记本`the_curse_of_dimensionality`模拟了随着维度数量增加，数据点之间的平均距离和最小距离如何增加（见*图13.3*）。
- en: '![](img/B15439_13_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_03.png)'
- en: 'Figure 13.3: Average distance of 1,000 data points in a unit hypercube'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：单位超立方体中1,000个数据点的平均距离
- en: The **simulation** randomly samples up to 2,500 features in the range [0, 1]
    from an uncorrelated uniform or a correlated normal distribution. The average
    distance between data points increases to over 11 times the unitary feature range
    for the normal distribution, and to over 20 times in the (extreme) case of an
    uncorrelated uniform distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该**模拟**从不相关均匀分布或相关正态分布中随机抽取高达2500个特征，范围为[0,1]。数据点之间的平均距离增加到正态分布的单位特征范围的11倍以上，以及在不相关均匀分布的情况下超过20倍。
- en: When the **distance between observations** grows, supervised ML becomes more
    difficult because predictions for new samples are less likely to be based on learning
    from similar training features. Put simply, the number of possible unique rows
    grows exponentially as the number of features increases, making it much harder
    to efficiently sample the space. Similarly, the complexity of the functions learned
    by flexible algorithms that make fewer assumptions about the actual relationship
    grows exponentially with the number of dimensions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当**观察之间的距离**增加时，监督机器学习变得更加困难，因为对新样本的预测不太可能是基于类似的训练特征学习的。简而言之，随着特征数量的增加，可能的唯一行数呈指数增长，使得高效地对空间进行采样变得更加困难。同样，对于对实际关系做出较少假设的灵活算法学习的函数的复杂性也随着维度数量的增加而呈指数增长。
- en: Flexible algorithms include the tree-based models we saw in *Chapter 11*, *Random
    Forests – A Long-Short Strategy for Japanese Stocks*, and *Chapter 12*, *Boosting
    Your Trading Strategy*. They also include the deep neural networks that we will
    cover later in the book, starting with *Chapter 16*, *Word Embeddings for Earnings
    Calls and SEC Filings*. The variance of these algorithms increases as **more dimensions
    add opportunities to overfit** to noise, resulting in poor generalization performance.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 灵活的算法包括我们在第11章中看到的基于树的模型，《随机森林-日本股票的多空策略》，以及第12章，《提升您的交易策略》。它们还包括我们将在本书后面介绍的深度神经网络，从第16章开始，《用于收益电话和SEC文件的词嵌入》。随着更多维度增加，这些算法的方差增加，会导致过度拟合噪音，从而导致泛化性能不佳。
- en: Dimensionality reduction leverages the fact that, in practice, features are
    often correlated or exhibit little variation. If so, it can compress data without
    losing much of the signal and complements the use of regularization to manage
    prediction error due to variance and model complexity.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 降维利用了实际上特征经常相关或变化很小的事实。如果是这样，它可以在不损失信号的情况下压缩数据，并且可以辅助使用正则化来管理由于方差和模型复杂性而产生的预测误差。
- en: 'The critical question that we take on in the following section then becomes:
    what are the best ways to find a lower-dimensional representation of the data?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要探讨的关键问题是：找到数据的低维表示的最佳方法是什么？
- en: Linear dimensionality reduction
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性降维
- en: Linear dimensionality reduction algorithms compute linear combinations that
    **translate**, **rotate**, and **rescale the original features** to capture significant
    variations in the data, subject to constraints on the characteristics of the new
    features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 线性降维算法计算线性组合，**转换**，**旋转**和**重新缩放原始特征**，以捕捉数据中的显着变化，同时受到新特征特性的约束。
- en: PCA, invented in 1901 by Karl Pearson, finds new features that reflect directions
    of maximal variance in the data while being mutually uncorrelated. ICA, in contrast,
    originated in signal processing in the 1980s with the goal of separating different
    signals while imposing the stronger constraint of statistical independence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是由Karl Pearson于1901年发明的，它找到反映数据中最大方差方向的新特征，同时相互不相关。相比之下，ICA起源于20世纪80年代的信号处理，其目标是在施加更强的统计独立性约束的同时分离不同的信号。
- en: This section introduces these two algorithms and then illustrates how to apply
    PCA to asset returns in order to learn risk factors from the data, and build so-called
    eigenportfolios for systematic trading strategies.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了这两种算法，然后说明了如何将PCA应用于资产收益，以从数据中学习风险因素，并构建所谓的特征组合用于系统交易策略。
- en: Principal component analysis
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主成分分析
- en: PCA finds linear combinations of the existing features and uses these principal
    components to represent the original data. The number of components is a hyperparameter
    that determines the target dimensionality and can be, at most, equal to the lesser
    of the number of rows or columns.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PCA找到现有特征的线性组合，并使用这些主成分来表示原始数据。成分的数量是一个超参数，它决定了目标维度，最多可以等于行数或列数中较小的那个。
- en: PCA aims to capture most of the variance in the data to make it easy to recover
    the original features and ensures that each component adds information. It reduces
    dimensionality by projecting the original data into the principal component space.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PCA旨在捕捉数据中的大部分方差，以便轻松恢复原始特征，并确保每个成分都添加信息。它通过将原始数据投影到主成分空间来降低维度。
- en: The PCA algorithm works by identifying a sequence of components, each of which
    aligns with the direction of maximum variance in the data after accounting for
    variation captured by previously computed components. The sequential optimization
    ensures that new components are not correlated with existing components and produces
    an orthogonal basis for a vector space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法通过识别一系列成分来工作，每个成分都与数据中方差的最大方向对齐，同时考虑先前计算的成分捕获的变化。顺序优化确保新成分与现有成分不相关，并为向量空间产生一个正交基。
- en: This new basis is a rotation of the original basis, such that the new axes point
    in the direction of successively decreasing variance. The decline in the amount
    of variance of the original data explained by each principal component reflects
    the extent of correlation among the original features. In other words, the share
    of components that captures, for example, 95 percent of the original variation
    provides insight into the linearly independent information in the original data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新基是原始基的旋转，使得新轴指向逐渐减小的方差方向。每个主成分解释原始数据方差量的下降反映了原始特征之间相关性的程度。换句话说，捕获例如95%原始变化的成分份额提供了关于原始数据中线性独立信息的见解。
- en: Visualizing PCA in 2D
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在2D中可视化PCA
- en: '*Figure 13.4* illustrates several aspects of PCA for a two-dimensional random
    dataset (refer to the notebook `pca_key_ideas`):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.4*展示了PCA在二维随机数据集上的几个方面（参考笔记本`pca_key_ideas`）：'
- en: The left panel shows how the first and second principal components align with
    the **directions of maximum variance** while being orthogonal.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧面板显示了第一和第二主成分如何与**最大方差的方向**对齐，同时正交。
- en: The central panel shows how the first principal component minimizes the **reconstruction
    error**, measured as the sum of the distances between the data points and the
    new axis.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中央面板显示了第一主成分如何最小化**重构误差**，重构误差被定义为数据点与新轴之间的距离的总和。
- en: The right panel illustrates **supervised OLS** (refer to *Chapter 7*, *Linear
    Models – From Risk Factors to Return Forecasts* ), which approximates the outcome
    (*x*[2]) by a line computed from the single feature *x*[1]. The vertical lines
    highlight how OLS minimizes the distance along the outcome axis, whereas PCA minimizes
    the distances that are orthogonal to the hyperplane.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧面板展示了**监督OLS**（参考*第7章*，*线性模型-从风险因素到收益预测*），它通过从单个特征*x*[1]计算的一条线来近似结果（*x*[2]）。垂直线突出显示了OLS如何最小化沿结果轴的距离，而PCA最小化与超平面正交的距离。
- en: '![](img/B15439_13_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_04.png)'
- en: 'Figure 13.4: PCA in 2D from various perspectives'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：来自不同角度的2D PCA
- en: Key assumptions made by PCA
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCA的关键假设
- en: 'PCA makes several assumptions that are important to keep in mind. These include:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCA有几个重要的假设需要牢记。这些包括：
- en: High variance implies a high signal-to-noise ratio.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高方差意味着高信噪比。
- en: The data is standardized so that the variance is comparable across features.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被标准化，以便各个特征的方差是可比较的。
- en: Linear transformations capture the relevant aspects of the data.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换捕捉数据的相关方面。
- en: Higher-order statistics beyond the first and second moments do not matter, which
    implies that the data has a normal distribution.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过一阶和二阶矩的高阶统计不重要，这意味着数据呈正态分布。
- en: The emphasis on the first and second moments aligns with standard risk/return
    metrics, but the normality assumption may conflict with the characteristics of
    market data. Market data often exhibits skew or kurtosis (fat tails) that differ
    from those of the normal distribution and will not be taken into account by PCA.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对第一和第二矩的强调与标准的风险/收益指标一致，但正态性假设可能与市场数据的特征相冲突。市场数据通常表现出与正态分布不同的偏斜或峰度（厚尾），这些特征不会被PCA考虑进去。
- en: How the PCA algorithm works
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCA算法的工作原理
- en: The algorithm finds vectors to create a hyperplane of target dimensionality
    that minimizes the reconstruction error, measured as the sum of the squared distances
    of the data points to the plane. As illustrated previously, this goal corresponds
    to finding a sequence of vectors that align with directions of maximum retained
    variance given the other components, while ensuring all principal components are
    mutually orthogonal.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法找到向量来创建一个目标维度的超平面，以最小化重构误差，重构误差被定义为数据点到平面的距离的平方和。正如之前所示，这个目标对应于找到一系列向量，这些向量与其他成分给定的最大保留方差的方向对齐，同时确保所有主成分相互正交。
- en: In practice, the algorithm solves the problem either by computing the eigenvectors
    of the covariance matrix or by using the **singular value decomposition** (**SVD**).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，该算法通过计算协方差矩阵的特征向量或使用**奇异值分解**（**SVD**）来解决问题。
- en: We illustrate the computation using a randomly generated three-dimensional ellipse
    with 100 data points, as shown in the left panel of *Figure 13.5*, including the
    two-dimensional hyperplane defined by the first two principal components. (Refer
    to the notebook `the_math_behind_pca` for the code samples in the following three
    sections.)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用随机生成的三维椭圆来说明计算，其中包括100个数据点，如*图13.5*的左侧面板所示，包括由前两个主成分定义的二维超平面。（有关以下三个部分的代码示例，请参阅笔记本`the_math_behind_pca`。）
- en: '![](img/B15439_13_05.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_05.png)'
- en: 'Figure 13.5: Visual representation of dimensionality reduction from 3D to 2D'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：从3D到2D的降维的可视化表示
- en: PCA based on the covariance matrix
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于协方差矩阵的PCA
- en: 'We first compute the principal components using the square covariance matrix
    with the pairwise sample covariances for the features *x*[i], *x*[j], *i*, *j*
    = 1, ..., *n* as entries in row *i* and column *j*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用方阵协方差矩阵计算主成分，其中特征*x*[i]，*x*[j]，*i*，*j*=1，...，*n*的成对样本协方差作为行*i*和列*j*的条目：
- en: '![](img/B15439_13_002.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_002.png)'
- en: 'For a square matrix *M* of *n* dimension, we define the eigenvectors ![](img/B15439_13_003.png)
    and eigenvalues ![](img/B15439_13_004.png)[i], *i*=1, ..., *n* as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*n*维度的方阵*M*，我们定义特征向量![](img/B15439_13_003.png)和特征值![](img/B15439_13_004.png)[i]，*i*=1，...，*n*如下：
- en: '![](img/B15439_13_005.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_005.png)'
- en: 'Therefore, we can represent the matrix *M* using eigenvectors and eigenvalues,
    where *W* is a matrix that contains the eigenvectors as column vectors, and *L*
    is a matrix that contains ![](img/B15439_13_006.png)[i] as diagonal entries (and
    0s otherwise). We define the **eigendecomposition** as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用特征向量和特征值来表示矩阵*M*，其中*W*是一个包含特征向量作为列向量的矩阵，*L*是一个包含![](img/B15439_13_006.png)[i]作为对角线条目（其他情况下为0）的矩阵。我们将**特征分解**定义为：
- en: '![](img/B15439_13_007.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_007.png)'
- en: 'Using NumPy, we implement this as follows, where the pandas DataFrame data
    contains the 100 data points of the ellipse:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumPy，我们实现如下，其中pandas DataFrame数据包含椭圆的100个数据点：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we calculate the eigenvectors and eigenvalues of the covariance matrix.
    The eigenvectors contain the principal components (where the sign is arbitrary):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算协方差矩阵的特征向量和特征值。特征向量包含主成分（符号是任意的）：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can compare the result with the result obtained from sklearn and find that
    they match in absolute terms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将结果与从sklearn获得的结果进行比较，并发现它们在绝对值上匹配：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also **verify the eigendecomposition**, starting with the diagonal matrix
    *L* that contains the eigenvalues:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以**验证特征分解**，从包含特征值的对角矩阵*L*开始：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We find that the result does indeed hold:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现结果确实成立：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PCA using the singular value decomposition
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用奇异值分解的PCA
- en: Next, we'll take a look at the alternative computation using the SVD. This algorithm
    is slower when the number of observations is greater than the number of features
    (which is the typical case) but yields better **numerical stability**, especially
    when some of the features are strongly correlated (which is often the reason to
    use PCA in the first place).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下使用SVD的替代计算。当观测数量大于特征数量（这是典型情况）时，此算法速度较慢，但在一些特征强相关（这通常是使用PCA的原因）的情况下，它提供更好的**数值稳定性**。
- en: SVD generalizes the eigendecomposition that we just applied to the square and
    symmetric covariance matrix to the more general case of *m* x *n* rectangular
    matrices. It has the form shown at the center of the following figure. The diagonal
    values of ![](img/B15439_13_008.png) are the singular values, and the transpose
    of *V** contains the principal components as column vectors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: SVD将我们刚刚应用于方阵和对称协方差矩阵的特征分解推广到更一般的*m* x *n*矩形矩阵的情况。它的形式如下图中所示。![](img/B15439_13_008.png)的对角线值是奇异值，*V*的转置包含主成分作为列向量。
- en: '![](img/B15439_13_06.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_06.png)'
- en: 'Figure 13.6: The SVD decomposed'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：SVD分解
- en: 'In this case, we need to make sure our data is centered with mean zero (the
    computation of the covariance earlier took care of this):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们需要确保我们的数据以零均值居中（先前的协方差计算已经处理了这个问题）：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using the centered data, we compute the SVD:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用居中的数据，我们计算奇异值分解：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can convert the vector `s`, which contains only singular values, into an
    *n* x *m* matrix and show that the decomposition works:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将仅包含奇异值的向量`s`转换为一个*n* x *m*矩阵，并展示分解的工作原理：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We find that the decomposition does indeed reproduce the standardized data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现分解确实可以再现标准化数据：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Lastly, we confirm that the columns of the transpose of *V** contain the principal
    components:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们确认转置的*V*的列包含主成分：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the next section, we will demonstrate how sklearn implements PCA.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将演示sklearn如何实现PCA。
- en: PCA with sklearn
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用sklearn进行PCA
- en: The `sklearn.decomposition.PCA` implementation follows the standard API based
    on the `fit()` and `transform()` methods that compute the desired number of principal
    components and project the data into the component space, respectively. The convenience
    method `fit_transform()` accomplishes this in a single step.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.decomposition.PCA`实现遵循基于`fit()`和`transform()`方法的标准API，这些方法计算所需数量的主成分并将数据投影到组件空间。方便的方法`fit_transform()`可以在一步中完成这个过程。'
- en: 'PCA offers three different algorithms that can be specified using the `svd_solver`
    parameter:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PCA提供了三种不同的算法，可以使用`svd_solver`参数进行指定：
- en: '**full** computes the exact SVD using the LAPACK solver provided by scipy.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**full**使用由scipy提供的LAPACK求解器计算精确的SVD。'
- en: '**arpack** runs a truncated version suitable for computing less than the full
    number of components.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**arpack**运行适用于计算少于完整成分数量的截断版本。'
- en: '**randomized** uses a sampling-based algorithm that is more efficient when
    the dataset has more than 500 observations and features, and the goal is to compute
    less than 80 percent of the components.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**randomized**使用基于抽样的算法，当数据集具有超过500个观测值和特征时更有效，并且目标是计算少于80％的成分时更有效。'
- en: '**auto** also randomizes where it is most efficient; otherwise, it uses the
    full SVD.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**auto**也会随机选择最有效的位置；否则，它会使用完整的SVD。'
- en: Please view the references on GitHub for algorithmic implementation details.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有关算法实现细节，请在GitHub上查看参考资料。
- en: 'Other key configuration parameters of the PCA object are:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对象的其他关键配置参数包括：
- en: '**n_components**: Compute all principal components by passing `None` (the default),
    or limit the number to `int`. For `svd_solver=full`, there are two additional
    options: a `float` in the interval [0, 1] computes the number of components required
    to retain the corresponding share of the variance in the data, and the option
    `mle` estimates the number of dimensions using the maximum likelihood.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_components**：通过传递`None`（默认值）来计算所有主成分，或者限制数量为`int`。对于`svd_solver=full`，还有两个额外选项：在区间[0,1]的`float`计算保留数据中相应方差份额所需的成分数量，选项`mle`使用最大似然估计来估计维度的数量。'
- en: '**whiten**: If `True`, it standardizes the component vectors to unit variance,
    which, in some cases, can be useful in a predictive model (the default is `False`).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**whiten**：如果为`True`，则将组件向量标准化为单位方差，在某些情况下可能对预测模型有用（默认值为`False`）。'
- en: 'To compute the first two principal components of the three-dimensional ellipsis
    and project the data into the new space, use `fit_transform()`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算三维椭球的前两个主成分并将数据投影到新空间中，请使用`fit_transform()`：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The explained variance of the first two components is very close to 100 percent:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个成分的解释方差非常接近100％：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Figure 13.5* shows the projection of the data into the new two-dimensional
    space.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.5*显示了数据投影到新的二维空间中。'
- en: Independent component analysis
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: ICA is another linear algorithm that identifies a new basis to represent the
    original data but pursues a different objective than PCA. Refer to Hyvärinen and
    Oja (2000) for a detailed introduction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ICA是另一个线性算法，它确定一个新的基来表示原始数据，但追求的目标与PCA不同。有关详细介绍，请参阅Hyvärinen和Oja（2000）。
- en: ICA emerged in signal processing, and the problem it aims to solve is called
    **blind source separation**. It is typically framed as the cocktail party problem,
    where a given number of guests are speaking at the same time so that a single
    microphone records overlapping signals. ICA assumes there are as many different
    microphones as there are speakers, each placed at different locations so that
    they record a different mix of signals. ICA then aims to recover the individual
    signals from these different recordings.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ICA起源于信号处理，它旨在解决的问题称为**盲源分离**。通常被描述为鸡尾酒会问题，即一定数量的客人同时讲话，以至于单个麦克风记录重叠的信号。ICA假设有与说话者数量相同的不同麦克风，每个麦克风放置在不同的位置，以便记录不同的信号混合。然后ICA旨在从这些不同的录音中恢复单独的信号。
- en: In other words, there are *n* original signals and an unknown square mixing
    matrix *A* that produces an *n*-dimensional set of *m* observations so that
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，有*n*个原始信号和一个未知的方阵混合矩阵*A*，产生一个*n*维的*m*个观测值集合，以便
- en: '![](img/B15439_13_009.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_009.png)'
- en: The goal is to find the matrix *W* = *A*^(-1) that untangles the mixed signals
    to recover the sources.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到矩阵*W* = *A*^(-1)，解开混合信号以恢复源信号。
- en: The ability to uniquely determine the matrix *W* hinges on the non-Gaussian
    distribution of the data. Otherwise, *W* could be rotated arbitrarily given the
    multivariate normal distribution's symmetry under rotation. Furthermore, ICA assumes
    the mixed signal is the sum of its components and is, therefore, unable to identify
    Gaussian components because their sum is also normally distributed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一确定矩阵*W*的能力取决于数据的非高斯分布。否则，由于多变量正态分布在旋转下的对称性，*W*可以任意旋转。此外，ICA假设混合信号是其组成部分的总和，因此无法识别高斯成分，因为它们的总和也是正态分布的。
- en: ICA assumptions
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ICA假设
- en: 'ICA makes the following critical assumptions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ICA做出以下关键假设：
- en: The sources of the signals are statistically independent
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号的来源在统计上是独立的
- en: Linear transformations are sufficient to capture the relevant information
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性变换足以捕获相关信息
- en: The independent components do not have a normal distribution
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立成分不具有正态分布
- en: The mixing matrix *A* can be inverted
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合矩阵*A*可以被反转
- en: ICA also requires the data to be centered and whitened, that is, to be mutually
    uncorrelated with unit variance. Preprocessing the data using PCA, as outlined
    earlier, achieves the required transformations.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ICA还要求数据经过中心化和白化处理，即相互不相关且单位方差。使用PCA对数据进行预处理，如前所述，可以实现所需的转换。
- en: The ICA algorithm
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ICA算法
- en: '`FastICA`, used by sklearn, is a fixed-point algorithm that uses higher-order
    statistics to recover the independent sources. In particular, it maximizes the
    distance to a normal distribution for each component as a proxy for independence.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`FastICA`是sklearn使用的固定点算法，它使用高阶统计量来恢复独立源。特别是，它最大化每个成分与正态分布的距离，作为独立性的代理。'
- en: An alternative algorithm called `InfoMax` minimizes the mutual information between
    components as a measure of statistical independence.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为`InfoMax`的替代算法最小化成分之间的互信息作为统计独立性的度量。
- en: ICA with sklearn
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用sklearn进行ICA
- en: The ICA implementation by sklearn uses the same interface as PCA, so there is
    little to add. Note that there is no measure of explained variance because ICA
    does not compute components successively. Instead, each component aims to capture
    the independent aspects of the data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn的ICA实现使用与PCA相同的接口，因此几乎没有什么可补充的。请注意，没有解释方差的度量，因为ICA不会逐步计算成分。相反，每个成分旨在捕获数据的独立方面。
- en: Manifold learning – nonlinear dimensionality reduction
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流形学习-非线性降维
- en: Linear dimensionality reduction projects the original data onto a lower-dimensional
    hyperplane that aligns with informative directions in the data. The focus on linear
    transformations simplifies the computation and echoes common financial metrics,
    such as PCA's goal to capture the maximum variance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 线性降维将原始数据投影到与数据中的信息方向对齐的低维超平面上。专注于线性变换简化了计算，并呼应了常见的金融指标，比如PCA的目标是捕捉最大的方差。
- en: However, linear approaches will naturally ignore signals reflected in nonlinear
    relationships in the data. Such relationships are very important in alternative
    datasets containing, for example, image or text data. Detecting such relationships
    during exploratory analysis can provide important clues about the data's potential
    signal content.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，线性方法自然会忽略数据中非线性关系反映的信号。在包含图像或文本数据的替代数据集中，这样的关系非常重要。在探索性分析中检测这样的关系可以提供关于数据潜在信号内容的重要线索。
- en: In contrast, the **manifold hypothesis** emphasizes that high-dimensional data
    often lies on or near a lower-dimensional nonlinear manifold that is embedded
    in the higher-dimensional space. The two-dimensional Swiss roll displayed in *Figure
    13.1* (at the beginning of this chapter) illustrates such a topological structure.
    Manifold learning aims to find the manifold of intrinsic dimensionality and then
    represent the data in this subspace. A simplified example uses a road as a one-dimensional
    manifold in a three-dimensional space and identifies data points using house numbers
    as local coordinates.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**流形假设**强调高维数据通常位于或接近嵌入在高维空间中的低维非线性流形。*图13.1*中显示的二维瑞士卷（在本章开头）说明了这样的拓扑结构。流形学习旨在找到固有维度的流形，然后在这个子空间中表示数据。一个简化的例子使用一条道路作为三维空间中的一维流形，并使用房屋编号作为局部坐标来识别数据点。
- en: Several techniques approximate a lower-dimensional manifold. One example is
    **locally linear embedding** (**LLE**), which was invented by Lawrence Saul and
    Sam Roweis (2000) and used to "unroll" the Swiss roll shown in *Figure 13.1* (view
    the examples in the `manifold_learning_lle` notebook).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种技术可以近似一个低维流形。一个例子是**局部线性嵌入**（**LLE**），Lawrence Saul和Sam Roweis（2000年）发明了这种方法，并用于“展开”*图13.1*中显示的瑞士卷（查看`manifold_learning_lle`笔记本中的示例）。
- en: For each data point, LLE identifies a given number of nearest neighbors and
    computes weights that represent each point as a linear combination of its neighbors.
    It finds a lower-dimensional embedding by linearly projecting each neighborhood
    on global internal coordinates on the lower-dimensional manifold and can be thought
    of as a sequence of PCA applications.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据点，LLE识别给定数量的最近邻居，并计算代表每个点的权重，使其表示为其邻居的线性组合。它通过在全局内部坐标上线性投影每个邻域到较低维度流形上来找到一个较低维度的嵌入，并且可以被看作是一系列PCA应用。
- en: 'Visualization requires that the reduction is at least three dimensions, possibly
    below the intrinsic dimensionality, and poses the **challenge of faithfully representing
    both the local and global structure**. This challenge relates to the curse of
    dimensionality; that is, while the volume of a sphere expands exponentially with
    the number of dimensions, the lower-dimensional space available to represent high-dimensional
    data is much more limited. For instance, in 12 dimensions, there can be 13 equidistant
    points; however, in two dimensions, there can only be 3 that form a triangle with
    sides of equal length. Therefore, accurately reflecting the distance of one point
    to its high-dimensional neighbors in lower dimensions risks distorting the relationships
    among all other points. The result is the **crowding problem**: to maintain global
    distances, local points may need to be placed too closely together.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化要求降维至少为三维，可能低于固有维度，并提出了**忠实地表示局部和全局结构的挑战**。这个挑战与维度灾难有关；也就是说，虽然球体的体积随着维数的增加呈指数级增长，但用于表示高维数据的低维空间要少得多。例如，在12维空间中，可能有13个等距点；然而，在二维空间中，可能只有3个点组成等边三角形。因此，在低维空间准确地反映一个点与其高维邻居的距离可能会扭曲所有其他点之间的关系。结果就是**拥挤问题**：为了保持全局距离，局部点可能需要被放置得太靠近。
- en: The next two sections cover techniques that have allowed us to make progress
    in addressing the crowding problem for the visualization of complex datasets.
    We will use the fashion MNIST dataset, which is a more sophisticated alternative
    to the classic handwritten digit MNIST benchmark data used for computer vision.
    It contains 60,000 training and 10,000 test images of fashion objects in 10 classes
    (take a look at the sample images in the notebook `manifold_learning_intro`).
    The goal of a manifold learning algorithm for this data is to detect whether the classes
    lie on distinct manifolds to facilitate their recognition and differentiation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两节将介绍一些技术，这些技术使我们在解决复杂数据可视化中的拥挤问题方面取得了进展。我们将使用时尚MNIST数据集，这是经典手写数字MNIST基准数据的更复杂的替代品，用于计算机视觉。它包含10个类别的时尚物品的60,000个训练图像和10,000个测试图像（在笔记本`manifold_learning_intro`中查看样本图像）。对于这些数据，流形学习算法的目标是检测类别是否位于不同的流形上，以便促进它们的识别和区分。
- en: t-distributed Stochastic Neighbor Embedding
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: t-分布随机邻居嵌入
- en: t-SNE is an award-winning algorithm, developed by Laurens van der Maaten and
    Geoff Hinton in 2008, to detect patterns in high-dimensional data. It takes a probabilistic,
    nonlinear approach to locate data on several different but related low-dimensional
    manifolds. The algorithm emphasizes keeping similar points together in low dimensions
    as opposed to maintaining the distance between points that are apart in high dimensions,
    which results from algorithms like PCA that minimize squared distances.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是一种屡获殊荣的算法，由Laurens van der Maaten和Geoff Hinton于2008年开发，用于检测高维数据中的模式。它采用概率非线性方法来定位数据在几个不同但相关的低维流形上。该算法强调将低维中相似的点放在一起，而不是保持高维中相距较远的点之间的距离，这是由PCA等算法最小化平方距离所导致的。
- en: The algorithm proceeds by **converting high-dimensional distances into (conditional)
    probabilities**, where high probabilities imply low distance and reflect the likelihood
    of sampling two points based on similarity. It accomplishes this by, first, positioning
    a normal distribution over each point and computing the density for a point and
    each neighbor, where the `perplexity` parameter controls the effective number
    of neighbors. In the second step, it arranges points in low dimensions and uses
    similarly computed low-dimensional probabilities to match the high-dimensional
    distribution. It measures the difference between the distributions using the Kullback-Leibler
    divergence, which puts a high penalty on misplacing similar points in low dimensions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过将高维距离转换为（条件）概率来进行，其中高概率意味着低距离，并反映了基于相似性对两个点进行采样的可能性。它通过首先在每个点上定位一个正态分布并计算点和每个邻居的密度来实现这一点，其中`perplexity`参数控制有效邻居的数量。在第二步中，它将点排列在低维中，并使用类似计算的低维概率来匹配高维分布。它使用Kullback-Leibler散度来衡量分布之间的差异，这对于在低维中放置相似点有很高的惩罚。
- en: The low-dimensional probabilities use a Student's t-distribution with one degree
    of freedom because it has fatter tails that reduce the penalty of misplacing points
    that are more distant in high dimensions to manage the crowding problem.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 低维概率使用具有一个自由度的学生t分布，因为它具有更厚的尾部，可以减少在高维中放置更远点的惩罚，以解决拥挤问题。
- en: The upper panels in *Figure 13.7* show how t-SNE is able to differentiate between
    the FashionMNIST image classes. A higher perplexity value increases the number
    of neighbors used to compute the local structure and gradually results in more
    emphasis on global relationships. (Refer to the repository for a high-resolution
    color version of this figure.)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.7*中的上部面板显示了t-SNE能够区分FashionMNIST图像类别。更高的perplexity值增加了用于计算局部结构的邻居数量，并逐渐更加强调全局关系。（有关此图的高分辨率彩色版本，请参阅存储库。）'
- en: '![](img/B15439_13_07.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_07.png)'
- en: 'Figure 13.7: t-SNE and UMAP visualization of Fashion MNIST image data for different
    hyperparameters'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：Fashion MNIST图像数据的t-SNE和UMAP可视化，用于不同的超参数
- en: t-SNE is the current state of the art in high-dimensional data visualization.
    Weaknesses include the computational complexity that scales quadratically in the
    number *n* of points because it evaluates all pairwise distances, but a subsequent
    tree-based implementation has reduced the cost to *n* log *n*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE是当前高维数据可视化的最新技术。其缺点包括计算复杂度随着点数*n*的平方级增长，因为它评估所有成对距离，但随后基于树的实现已将成本降低到*n*
    log *n*。
- en: Unfortunately, t-SNE does not facilitate the projection of new data points into
    the low-dimensional space. The compressed output is not a very useful input for
    distance- or density-based cluster algorithms because t-SNE treats small and large
    distances differently.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，t-SNE不能便利地将新数据点投影到低维空间中。压缩输出对于基于距离或密度的聚类算法并不是一个非常有用的输入，因为t-SNE对待小距离和大距离是不同的。
- en: Uniform Manifold Approximation and Projection
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统一流形逼近和投影
- en: UMAP is a more recent algorithm for visualization and general dimensionality
    reduction. It assumes the data is uniformly distributed on a locally connected
    manifold and looks for the closest low-dimensional equivalent using fuzzy topology. It
    uses a `neighbors` parameter, which impacts the result in a similar way to `perplexity`
    in the preceding section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP是一种更近期的用于可视化和一般降维的算法。它假设数据在局部连接的流形上均匀分布，并寻找最接近的低维等价物，使用模糊拓扑。它使用`neighbors`参数，对结果产生类似于前一节中`perplexity`的影响。
- en: It is faster and hence scales better to large datasets than t-SNE and sometimes
    preserves the global structure better than t-SNE. It can also work with different
    distance functions, including cosine similarity, which is used to measure the
    distance between word count vectors.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它比t-SNE更快，因此在大型数据集上更具规模，并且有时比t-SNE更好地保留全局结构。它还可以使用不同的距离函数，包括余弦相似度，用于衡量单词计数向量之间的距离。
- en: The preceding figure illustrates how UMAP does indeed move the different clusters
    further apart, whereas t-SNE provides more granular insight into the local structure.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上图说明了UMAP确实将不同的聚类分开，而t-SNE提供了更详细的对局部结构的洞察。
- en: The notebook also contains interactive Plotly visualizations for each of the
    algorithms that permit the exploration of the labels and identify which objects
    are placed close to each other.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本还包含每个算法的交互式Plotly可视化，允许探索标签并识别哪些对象彼此靠近。
- en: PCA for trading
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于交易的PCA
- en: 'PCA is useful for algorithmic trading in several respects, including:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: PCA在算法交易中有几个方面是有用的：
- en: The data-driven derivation of risk factors by applying PCA to asset returns
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将PCA应用于资产回报来进行数据驱动的风险因素推导
- en: The construction of uncorrelated portfolios based on the principal components
    of the correlation matrix of asset returns
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于资产回报的相关矩阵的主成分构建不相关投资组合
- en: We will illustrate both of these applications in this section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中说明这两种应用。
- en: Data-driven risk factors
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据驱动的风险因素
- en: In *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*, we
    explored **risk factor models** used in quantitative finance to capture the main
    drivers of returns. These models explain differences in returns on assets based
    on their exposure to systematic risk factors and the rewards associated with these
    factors. In particular, we explored the **Fama-French approach**, which specifies
    factors based on prior knowledge about the empirical behavior of average returns,
    treats these factors as observable, and then estimates risk model coefficients
    using linear regression.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*“线性模型-从风险因素到收益预测”中，我们探讨了量化金融中使用的**风险因素模型**，以捕捉收益的主要驱动因素。这些模型根据资产对系统性风险因素的暴露以及与这些因素相关的回报来解释资产收益的差异。特别是，我们探讨了**法玛-法rench方法**，该方法根据有关平均收益的经验行为的先验知识指定因素，将这些因素视为可观察的，然后使用线性回归估计风险模型系数。
- en: An alternative approach treats risk factors as **latent variables** and uses
    factor analytic techniques like PCA to simultaneously learn the factors from data
    and estimate how they drive returns. In this section, we will demonstrate how
    this method derives factors in a purely statistical or data-driven way with the
    advantage of not requiring ex ante knowledge of the behavior of asset returns
    (see the notebook `pca_and_risk_factor_models` for more details).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法将风险因素视为**潜在变量**，并使用因子分析技术如PCA来同时从数据中学习因素并估计它们如何驱动收益。在本节中，我们将演示这种方法如何以纯粹的统计或数据驱动方式推导因素，其优势在于不需要对资产收益行为的先验知识（有关更多详细信息，请参阅笔记本`pca_and_risk_factor_models`）。
- en: Preparing the data – top 350 US stocks
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据-前350支美国股票
- en: 'We will use the Quandl stock price data and select the daily adjusted close
    prices of the 500 stocks with the largest market capitalization and data for the
    2010-2018 period. We will then compute the daily returns as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Quandl股价数据，并选择市值最大的500支股票的日调整收盘价数据，时间跨度为2010年至2018年。然后我们将按以下方式计算每日收益：
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We obtain 351 stocks and returns for over 2,000 trading days:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了351支股票和超过2000个交易日的收益：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'PCA is sensitive to outliers, so we winsorize the data at the 2.5 percent and
    97.5 percent quantiles, respectively:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对异常值敏感，因此我们分别在2.5％和97.5％的分位数处对数据进行截尾处理：
- en: 'PCA does not permit missing data, so we will remove any stocks that do not
    have data for at least 95 percent of the time period. Then, in a second step,
    we will remove trading days that do not have observations on at least 95 percent
    of the remaining stocks:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: PCA不允许缺失数据，因此我们将删除至少95％时间段内没有数据的任何股票。然后，在第二步中，我们将删除至少95％的剩余股票观察日：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We are left with 315 equity return series covering a similar period:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们剩下315个股票收益系列，涵盖了类似的时间段：
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We impute any remaining missing values using the average return for any given
    trading day:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用任何给定交易日的平均收益来填补任何剩余的缺失值：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Running PCA to identify the key return drivers
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行PCA以识别主要的收益驱动因素
- en: 'Now we are ready to fit the principal components model to the asset returns
    using default parameters to compute all of the components using the full SVD algorithm:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用默认参数将主成分模型拟合到资产收益，以使用完整的SVD算法计算所有成分：
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We find that the most important factor explains around 55 percent of the daily
    return variation. The dominant factor is usually interpreted as "the market,"
    whereas the remaining factors can be interpreted as industry or style factors
    in line with our discussions in *Chapter 5*, *Portfolio Optimization and Performance
    Evaluation*, and *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*,
    depending on the results of a closer inspection (please refer to the next example).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现最重要的因素解释了大约55％的每日收益变化。主导因素通常被解释为“市场”，而其余因素可以根据更详细的检查结果（请参考下一个示例）解释为行业或风格因素，与我们在*第5章*“投资组合优化和绩效评估”和*第7章*“线性模型-从风险因素到收益预测”中的讨论一致。
- en: The plot on the right of *Figure 13.8* shows the cumulative explained variance
    and indicates that around 10 factors explain 60 percent of the returns of this
    cross-section of stocks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.8*右侧的图表显示了累积解释方差，并指出大约10个因素解释了这个股票横截面的60％的收益。'
- en: '![](img/B15439_13_08.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_08.png)'
- en: 'Figure 13.8: (Cumulative) explained return variance by PCA-based risk factors'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：（累积）基于PCA的风险因素解释的收益方差
- en: 'The notebook contains a **simulation** for a broader cross-section of stocks
    and the longer 2000-2018 time period. It finds that, on average, the first three
    components explained 40 percent, 10 percent, and 5 percent of 500 randomly selected
    stocks, as shown in *Figure 13.9*:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本包含了对更广泛的股票横截面和更长的2000年至2018年时间段的**模拟**。结果发现，平均而言，前三个成分解释了500支随机选定股票的40％、10％和5％，如*图13.9*所示：
- en: '![](img/B15439_13_09.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_09.png)'
- en: 'Figure 13.9: Explained variance of the top 10 principal components—100 trials'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：前10个主成分的解释方差-100次试验
- en: The cumulative plot shows a typical "elbow" pattern that can help to identify
    a suitable target dimensionality as the number of components beyond which additional
    components add less incremental value.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 累积图显示了典型的“拐点”模式，可以帮助确定适当的目标维度，即超过该维度的附加组件将增加较少的增量价值。
- en: 'We can select the top two principal components to verify that they are indeed
    uncorrelated:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择前两个主成分来验证它们是否确实不相关：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Moreover, we can plot the time series to highlight how each factor captures
    different volatility patterns, as shown in the following figure:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以绘制时间序列以突出每个因素捕捉不同的波动模式，如下图所示：
- en: '![](img/B15439_13_10.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_10.png)'
- en: 'Figure 13.10: Return volatility patterns captured by the first two principal
    components'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：第一和第二主成分捕捉的收益波动模式
- en: A risk factor model would employ a subset of the principal components as features
    to predict future returns, similar to our approach in *Chapter 7*, *Linear Models
    – From Risk Factors to Return Forecasts*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 风险因素模型将使用主成分的子集作为特征来预测未来的回报，类似于我们在*第7章*中的方法，*线性模型-从风险因素到回报预测*。
- en: Eigenportfolios
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征组合
- en: Another application of PCA involves the covariance matrix of the normalized
    returns. The principal components of the correlation matrix capture most of the
    covariation among assets in descending order and are mutually uncorrelated. Moreover,
    we can use standardized principal components as portfolio weights. You can find
    the code example for this section in the notebook `pca_and_eigen_portfolios`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的另一个应用涉及标准化回报的协方差矩阵。相关矩阵的主成分按降序捕获了资产之间的大部分协变化，并且彼此不相关。此外，我们可以使用标准化的主成分作为投资组合权重。您可以在笔记本`pca_and_eigen_portfolios`中找到本节的代码示例。
- en: 'Let''s use the 30 largest stocks with data for the 2010-2018 period to facilitate
    the exposition:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用2010-2018年期间有数据的30个最大股票来便于阐述：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We again winsorize and also normalize the returns:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次对回报进行winsorize和标准化：
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After dropping assets and trading days like in the previous example, we are
    left with 23 assets and over 2,000 trading days. We compute the return covariance
    and estimate all of the principal components to find that the two largest explain
    55.9 percent and 15.5 percent of the covariation, respectively:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在像前面的例子中舍弃资产和交易日后，我们剩下23个资产和2000多个交易日。我们计算回报协方差，并估计所有主成分，发现前两个解释了55.9%和15.5%的协变化，分别：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we select and normalize the four largest components so that they sum
    to 1, and we can use them as weights for portfolios that we can compare to an
    EW portfolio formed from all of the stocks:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择并标准化四个最大的组件，使它们总和为1，然后我们可以将它们用作投资组合的权重，以便与由所有股票组成的EW投资组合进行比较：
- en: '[PRE22]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The weights show distinct emphasis, as you can see in *Figure 13.11*. For example,
    Portfolio 3 puts large weights on Mastercard and Visa, the two payment processors
    in the sample, whereas Portfolio 2 has more exposure to technology companies:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 权重显示出明显的重点，正如您在*图13.11*中所看到的。例如，投资组合3在Mastercard和Visa上投入了大量权重，这两家支付处理公司在样本中，而投资组合2更多地暴露于科技公司：
- en: '![](img/B15439_13_11.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_11.png)'
- en: 'Figure 13.11: Eigenportfolio weights'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11：特征组合权重
- en: When comparing the performance of each portfolio over the sample period to "the
    market" consisting of our small sample, we find that Portfolio 1 performs very
    similarly, whereas the other portfolios capture different return patterns (see
    *Figure 13.12*).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当将每个投资组合在样本期间的表现与由我们的小样本组成的“市场”进行比较时，我们发现投资组合1的表现非常相似，而其他投资组合捕捉到了不同的回报模式（见*图13.12*）。
- en: '![](img/B15439_13_12.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_12.png)'
- en: 'Figure 13.12: Cumulative eigenportfolio returns'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.12：累积特征组合回报
- en: Clustering
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Both clustering and dimensionality reduction summarize the data. As we have
    just discussed, dimensionality reduction compresses the data by representing it
    using new, fewer features that capture the most relevant information. Clustering
    algorithms, in contrast, assign existing observations to subgroups that consist
    of similar data points.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维都对数据进行了总结。正如我们刚刚讨论的那样，降维通过使用捕获最相关信息的新的、更少的特征来压缩数据。相比之下，聚类算法将现有的观察结果分配给由相似数据点组成的子组。
- en: Clustering can serve to better understand the data through the lens of categories
    learned from continuous variables. It also permits you to automatically categorize
    new objects according to the learned criteria. Examples of related applications
    include hierarchical taxonomies, medical diagnostics, and customer segmentation.
    Alternatively, clusters can be used to represent groups as prototypes, using,
    for example, the midpoint of a cluster as the best representatives of learned
    grouping. An example application includes image compression.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以帮助更好地理解通过连续变量学习到的类别的数据。它还允许您根据学习到的标准自动对新对象进行分类。相关应用的示例包括分层分类法、医学诊断和客户细分。或者，可以使用集群来表示组，例如，将集群的中点作为学习到的分组的最佳代表。一个示例应用包括图像压缩。
- en: 'Clustering algorithms differ with respect to their strategy of identifying
    groupings:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在识别分组策略方面有所不同：
- en: '**Combinatorial** algorithms select the most coherent of different groupings
    of observations.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合**算法选择不同观察结果的最一致的分组。'
- en: '**Probabilistic** modeling estimates distributions that most likely generated
    the clusters.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率**建模估计最有可能生成集群的分布。'
- en: '**Hierarchical clustering** finds a sequence of nested clusters that optimizes
    coherence at any given stage.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**找到了一系列嵌套的集群，以在任何给定阶段优化一致性。'
- en: 'Algorithms also differ by the notion of what constitutes a useful collection
    of objects that needs to match the data characteristics, domain, and goal of the
    applications. Types of groupings include:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 算法还因其对匹配数据特征、领域和应用目标的有用对象集合的概念而有所不同。分组类型包括：
- en: Clearly separated groups of various shapes
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种形状的明显分离的组
- en: Prototype- or center-based, compact clusters
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原型或基于中心的紧凑集群
- en: Density-based clusters of arbitrary shape
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意形状的基于密度的集群
- en: Connectivity- or graph-based clusters
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于连接或图的集群
- en: 'Important additional aspects of a clustering algorithm include whether it:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的重要附加方面包括：
- en: Requires exclusive cluster membership
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要独占集群成员资格
- en: Makes hard, that is, binary, or soft, probabilistic assignments
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使硬性的，即二进制的，或软性的，概率性的分配
- en: Is complete and assigns all data points to clusters
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是完整的，并将所有数据点分配给集群
- en: The following sections introduce key algorithms, including **k-means**, **hierarchical**,
    and **density-based clustering**, as well as **Gaussian mixture models** (**GMMs**).
    The notebook `clustering_algos` compares the performance of these algorithms on
    different, labeled datasets to highlight strengths and weaknesses. It uses mutual
    information (refer to *Chapter 6*, *The Machine Learning Process*) to measure
    the congruence of cluster assignments and labels.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节介绍了关键算法，包括**k均值**、**层次**和**基于密度**的聚类，以及**高斯混合模型**（**GMMs**）。笔记本`clustering_algos`比较了这些算法在不同的标记数据集上的性能，以突出它们的优势和劣势。它使用互信息（参见*第6章*，*机器学习过程*）来衡量聚类分配和标签的一致性。
- en: k-means clustering
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k均值聚类
- en: k-means is the most well-known clustering algorithm, and it was first proposed
    by Stuart Lloyd at Bell Labs in 1957\. It finds *k* centroids and assigns each
    data point to exactly one cluster with the goal of minimizing the within-cluster
    variance (called *inertia*). It typically uses the Euclidean distance, but other
    metrics can also be used. k-means assumes that clusters are spherical and of equal
    size and ignores the covariance among features.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: k均值是最著名的聚类算法，最早由贝尔实验室的斯图尔特·劳埃德（Stuart Lloyd）于1957年提出。它找到*k*个质心，并将每个数据点分配到恰好一个簇，目标是最小化簇内方差（称为*惯性*）。它通常使用欧氏距离，但也可以使用其他度量。k均值假设簇是球形且大小相等，并忽略特征之间的协方差。
- en: Assigning observations to clusters
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将观测分配给簇
- en: 'The problem is computationally difficult (NP-hard) because there are *k*^N
    ways to partition the *N* observations into *k* clusters. The standard iterative
    algorithm delivers a local optimum for a given *k* and proceeds as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在计算上很困难（NP-hard），因为将*N*个观测值分成*k*个簇有*k*^N种方法。标准的迭代算法为给定的*k*提供了局部最优解，并按以下方式进行：
- en: Randomly define *k* cluster centers and assign points to the nearest centroid
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机定义*k*个簇中心并将点分配给最近的质心
- en: 'Repeat:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复：
- en: For each cluster, compute the centroid as the average of the features
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个簇，计算特征的平均值作为质心
- en: Assign each observation to the closest centroid
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个观测分配给最近的质心
- en: 'Convergence: assignments (or within-cluster variation) don''t change'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛：分配（或簇内变化）不变
- en: The notebook `kmeans_implementation` shows you how to code the algorithm using
    Python. It visualizes the algorithm's iterative optimization and demonstrates
    how the resulting centroids partition the feature space into areas called Voronoi
    that delineate the clusters. The result is optimal for the given initialization,
    but alternative starting positions will produce different results. Therefore,
    we compute multiple clusterings from different initial values and select the solution
    that minimizes within-cluster variance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`kmeans_implementation`向您展示了如何使用Python编写该算法。它可视化了算法的迭代优化，并演示了生成的质心如何将特征空间分区为称为Voronoi的区域，这些区域勾画出了聚类。结果对于给定的初始化是最佳的，但是不同的起始位置将产生不同的结果。因此，我们从不同的初始值计算多个聚类，并选择最小化簇内方差的解决方案。
- en: k-means requires continuous or one-hot encoded categorical variables. Distance
    metrics are typically sensitive to scale, making it necessary to standardize features
    to ensure they have equal weight.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: k均值需要连续或独热编码的分类变量。距离度量通常对比例敏感，因此需要标准化特征以确保它们具有相等的权重。
- en: The **strengths** of k-means include its wide range of applicability, fast convergence,
    and linear scalability to large data while producing clusters of even size. The
    **weaknesses** include the need to tune the hyperparameter *k*, no guarantee of
    finding a global optimum, the restrictive assumption that clusters are spheres,
    and features not being correlated. It is also sensitive to outliers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: k均值的**优势**包括其广泛的适用性，快速收敛，以及对大数据的线性可扩展性，同时产生均匀大小的簇。**劣势**包括需要调整超参数*k*，无法保证找到全局最优解，限制性假设簇是球体，以及特征不相关。它也对离群值敏感。
- en: Evaluating cluster quality
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估簇质量
- en: Cluster quality metrics help select from among alternative clustering results.
    The notebook `kmeans_evaluation` illustrates the following options.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 簇质量度量有助于从不同的聚类结果中进行选择。笔记本`kmeans_evaluation`说明了以下选项。
- en: The **k-means objective** function suggests we compare the evolution of the
    inertia or within-cluster variance. Initially, additional centroids decrease the
    inertia sharply because new clusters improve the overall fit. Once an appropriate
    number of clusters has been found (assuming it exists), new centroids reduce the
    **within-cluster variance** by much less, as they tend to split natural groupings.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**k均值目标**函数建议我们比较惯性或簇内方差的演变。最初，额外的质心会急剧减少惯性，因为新的簇会改善整体拟合。一旦找到适当数量的簇（假设存在），新的质心减少**簇内方差**的程度要少得多，因为它们倾向于分割自然的分组。'
- en: Therefore, when k-means finds a good cluster representation of the data, the
    **inertia** tends to follow an elbow-shaped path similar to the explained variance
    ratio for PCA (take a look at the notebook for implementation details).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当k均值找到数据的良好簇表示时，**惯性**往往会遵循类似于PCA的解释方差比例的拐点路径（查看实现细节的笔记本）。
- en: 'The **silhouette coefficient** provides a more detailed picture of cluster
    quality. It answers the question: how far are the points in the nearest cluster
    relative to the points in the assigned cluster? To this end, it compares the mean
    intra-cluster distance *a* to the mean distance of the nearest cluster *b* and
    computes the following score *s*:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**轮廓系数**提供了更详细的簇质量图景。它回答了一个问题：最近簇中的点与分配簇中的点有多远？为此，它将簇内平均距离*a*与最近簇的平均距离*b*进行比较，并计算以下分数*s*：'
- en: '![](img/B15439_13_010.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_010.png)'
- en: The score can vary between -1 and 1, but negative values are unlikely in practice
    because they imply that the majority of points are assigned to the wrong cluster.
    A useful visualization of the silhouette score compares the values for each data
    point to the global average because it highlights the coherence of each cluster
    relative to the global configuration. The rule of thumb is to avoid clusters with
    mean scores below the average for all samples.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 得分可以在-1和1之间变化，但在实践中不太可能出现负值，因为这意味着大多数点被分配到错误的簇。轮廓分数的有用可视化将每个数据点的值与全局平均值进行比较，因为它突出了每个簇相对于全局配置的一致性。经验法则是避免平均分数低于所有样本的平均值的簇。
- en: '*Figure 13.13* shows an excerpt from the silhouette plot for three and four
    clusters, where the former highlights the poor fit of cluster 1 by subpar contributions
    to the global silhouette score, whereas all of the four clusters have some values
    that exhibit above-average scores.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.13*显示了三个和四个簇的轮廓图摘录，前者突出了簇1对全局轮廓分数的贡献不足，而所有四个簇都有一些值表现出高于平均分数的情况。'
- en: '![](img/B15439_13_13.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_13.png)'
- en: 'Figure 13.13: Silhouette plots for three and four clusters'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.13：三个和四个簇的轮廓图
- en: In sum, given the usually unsupervised nature, it is necessary to vary the hyperparameters
    of the cluster algorithms and evaluate the different results. It is also important
    to calibrate the scale of the features, particularly when some should be given
    a higher weight and thus be measured on a larger scale. Finally, to validate the
    robustness of the results, use subsets of data to identify whether particular
    cluster patterns emerge consistently.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，鉴于通常是无监督的性质，有必要改变聚类算法的超参数并评估不同的结果。调整特征的比例也很重要，特别是当一些特征应该被赋予更高的权重，因此应该在更大的比例上进行测量。最后，为了验证结果的稳健性，使用数据子集来确定是否会一致出现特定的聚类模式。
- en: Hierarchical clustering
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Hierarchical clustering avoids the need to specify a target number of clusters
    because it assumes that data can successively be merged into increasingly dissimilar
    clusters. It does not pursue a global objective but decides incrementally how
    to produce a sequence of nested clusters that range from a single cluster to clusters
    consisting of the individual data points.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类避免了需要指定目标簇数量的需要，因为它假设数据可以逐渐合并成越来越不同的簇。它不追求全局目标，而是逐步决定如何产生一系列从单个簇到由个体数据点组成的簇的嵌套簇。
- en: Different strategies and dissimilarity measures
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同的策略和不相似度度量
- en: 'There are two approaches to hierarchical clustering:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类有两种方法：
- en: '**Agglomerative clustering** proceeds bottom-up, sequentially merging two of
    the remaining groups based on similarity.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**凝聚聚类**是自下而上进行，根据相似性依次合并剩余的两个群体。'
- en: '**Divisive clustering** works top-down and sequentially splits the remaining
    clusters to produce the most distinct subgroups.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分裂聚类**自上而下工作，依次分裂剩余的簇以产生最不同的子群。'
- en: Both groups produce *N*-1 hierarchical levels and facilitate the selection of
    clustering at the level that best partitions data into homogenous groups. We will
    focus on the more common agglomerative clustering approach.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 两个群体产生*N*-1层次水平，并有助于选择最佳将数据分成同质群体的聚类水平。我们将重点关注更常见的凝聚聚类方法。
- en: The agglomerative clustering algorithm departs from the individual data points
    and computes a similarity matrix containing all mutual distances. It then takes
    *N*-1 steps until there are no more distinct clusters and, each time, updates
    the similarity matrix to substitute elements that have been merged by the new
    cluster so that the matrix progressively shrinks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 凝聚聚类算法从单个数据点出发，计算包含所有相互距离的相似性矩阵。然后，它经过*N*-1步，直到没有更多不同的簇，每次更新相似性矩阵以替换已被新簇合并的元素，使矩阵逐渐缩小。
- en: 'While hierarchical clustering does not have hyperparameters like k-means, the
    **measure of dissimilarity** between clusters (as opposed to individual data points)
    has an important impact on the clustering result. The options differ as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然层次聚类不像k均值那样具有超参数，但簇之间的**不相似度度量**对聚类结果有重要影响。选项有以下不同：
- en: '**Single-link**: Distance between the nearest neighbors of two clusters'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接**：两个簇的最近邻之间的距离'
- en: '**Complete link**: Maximum distance between the respective cluster members'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接**：各自簇成员之间的最大距离'
- en: '**Ward''s method**: Minimize within-cluster variance'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ward方法**：最小化簇内方差'
- en: '**Group average**: Uses the cluster midpoint as a reference distance'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组平均**：使用簇中点作为参考距离'
- en: Visualization – dendrograms
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化 - 系统树图
- en: Hierarchical clustering provides insight into degrees of similarity among observations
    as it continues to merge data. A significant change in the similarity metric from
    one merge to the next suggests that a natural clustering existed prior to this
    point.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类提供了对观察之间相似度程度的洞察，因为它继续合并数据。从一次合并到下一次合并的相似度度量的显著变化表明在此之前存在自然的聚类。
- en: The **dendrogram** visualizes the successive merges as a binary tree, displaying
    the individual data points as leaves and the final merge as the root of the tree.
    It also shows how the similarity monotonically decreases from the bottom to the
    top. Therefore, it is natural to select a clustering by cutting the dendrogram.
    Refer to the notebook `hierarchical_clustering` for implementation details.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统树图**将连续的合并可视化为二叉树，将个体数据点显示为叶子，最终合并显示为树的根。它还显示了相似性如何从底部到顶部单调减少。因此，自然地选择通过切割系统树图来进行聚类。有关实施细节，请参阅笔记本`hierarchical_clustering`。'
- en: '*Figure 13.14* illustrates the dendrogram for the classic Iris dataset with
    four classes and three features using the four different distance metrics introduced
    in the preceding section. It evaluates the fit of the hierarchical clustering
    using the **cophenetic correlation** coefficient that compares the pairwise distances
    among points and the cluster similarity metric at which a pairwise merge occurred.
    A coefficient of 1 implies that closer points always merge earlier.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.14*说明了经典的鸢尾花数据集的树状图，该数据集有四个类别和三个特征，使用了前一节介绍的四种不同的距离度量。它评估了使用**共辐系数**比较点之间的成对距离和成对合并发生的簇相似度度量的层次聚类的拟合度。系数为1意味着更接近的点总是更早地合并。'
- en: '![](img/B15439_13_14.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_14.png)'
- en: 'Figure 13.14: Dendrograms and cophenetic correlation for different dissimilarity
    measures'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.14：不同不相似度度量的树状图和共辐系数
- en: Different linkage methods produce different dendrogram "looks" so that we cannot
    use this visualization to compare results across methods. In addition, the Ward
    method, which minimizes the within-cluster variance, may not properly reflect
    the change in variance from one level to the next. Instead, the dendrogram can
    reflect the total within-cluster variance at different levels, which may be misleading.
    Alternative quality metrics are more appropriate, such as the **cophenetic correlation** or
    measures like **inertia** if aligned with the overall goal.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的链接方法会产生不同的树状图“外观”，因此我们不能使用此可视化来比较不同方法的结果。此外，Ward方法，它最小化簇内方差，可能无法正确反映从一个级别到下一个级别的方差变化。相反，树状图可以反映不同级别的总体簇内方差，这可能是误导性的。更合适的替代质量度量是**共辐系数**或者如果与总体目标一致的**惯性**等度量。
- en: 'The **strengths** of hierarchical clustering include:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的**优势**包括：
- en: The algorithm does not need the specific number of clusters but, instead, provides
    insight about potential clustering by means of an intuitive visualization.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法不需要特定数量的簇，而是通过直观的可视化提供有关潜在聚类的见解。
- en: It produces a hierarchy of clusters that can serve as a taxonomy.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它产生可以作为分类法的簇的层次结构。
- en: It can be combined with k-means to reduce the number of items at the start of
    the agglomerative process.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以与k均值结合，在凝聚过程开始时减少项目数量。
- en: 'On the other hand, its **weaknesses** include:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，它的**劣势**包括：
- en: The high cost in terms of computation and memory due to the numerous similarity
    matrix updates.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于众多相似矩阵更新而产生的计算和内存成本高昂。
- en: All merges are final so that it does not achieve the global optimum.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有合并都是最终的，因此它无法达到全局最优。
- en: The curse of dimensionality leads to difficulties with noisy, high-dimensional
    data.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度诅咒导致嘈杂的高维数据存在困难。
- en: Density-based clustering
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于密度的聚类
- en: Density-based clustering algorithms assign cluster membership based on proximity
    to other cluster members. They pursue the goal of identifying dense regions of
    arbitrary shapes and sizes. They do not require the specification of a certain
    number of clusters but instead rely on parameters that define the size of a neighborhood
    and a density threshold.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的聚类算法根据与其他簇成员的接近程度分配簇成员资格。它们追求识别任意形状和大小的密集区域的目标。它们不需要指定一定数量的簇，而是依赖于定义邻域大小和密度阈值的参数。
- en: 'We''ll outline the two popular algorithms: DBSCAN and its newer hierarchical
    refinement. Refer to the notebook `density_based_clustering` for the relevant
    code samples and the link in this chapter''s `README` on GitHub to a Quantopian
    example by Jonathan Larking that uses DBSCAN for a pairs trading strategy.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将概述两种流行的算法：DBSCAN及其更新的层次细化。请参考笔记本`density_based_clustering`中的相关代码示例，以及本章的GitHub上的`README`链接，其中包含Jonathan
    Larking使用DBSCAN进行配对交易策略的Quantopian示例。
- en: DBSCAN
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN
- en: '**Density-based spatial clustering of applications with noise** (**DBSCAN**)
    was developed in 1996 and awarded the KDD Test of Time award at the 2014 KDD conference
    because of the attention it has received in theory and practice.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**具有噪声的基于密度的空间聚类应用**（**DBSCAN**）于1996年开发，并在2014年KDD会议上获得了KDD时间测试奖，因为它在理论和实践中受到了关注。'
- en: It aims to identify core and non-core samples, where the former extend a cluster
    and the latter are part of a cluster but do not have sufficient nearby neighbors
    to further grow the cluster. Other samples are outliers and are not assigned to
    any cluster.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在识别核心和非核心样本，前者扩展了一个簇，后者是簇的一部分，但没有足够的附近邻居来进一步扩展簇。其他样本是离群值，不属于任何簇。
- en: It uses a parameter `eps` for the radius of the neighborhood and `min_samples`
    for the number of members required for core samples. It is deterministic and exclusive
    and has difficulties with clusters of different density and high-dimensional data.
    It can be challenging to tune the parameters to the requisite density, especially
    as it is often not constant.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用参数`eps`表示邻域的半径，`min_samples`表示核心样本所需的成员数量。它是确定性的和排他的，并且在不同密度和高维数据的簇上存在困难。调整参数以满足必要密度可能具有挑战性，特别是因为它通常不是恒定的。
- en: Hierarchical DBSCAN
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次DBSCAN
- en: '**Hierarchical DBSCAN** (**HDBSCAN**) is a more recent development that assumes
    clusters are islands of potentially differing density to overcome the DBSCAN challenges
    just mentioned. It also aims to identify the core and non-core samples. It uses
    the parameters `min_cluster_size` and `min_samples` to select a neighborhood and
    extend a cluster. The algorithm iterates over multiple `eps` values and chooses
    the most stable clustering. In addition to identifying clusters of varying density,
    it provides insight into the density and hierarchical structure of the data.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次DBSCAN**（**HDBSCAN**）是一个更近期的发展，它假设簇是潜在不同密度的岛屿，以克服刚才提到的DBSCAN的挑战。它还旨在识别核心和非核心样本。它使用参数`min_cluster_size`和`min_samples`来选择邻域并扩展簇。该算法在多个`eps`值上进行迭代，并选择最稳定的聚类。除了识别不同密度的簇外，它还提供了有关数据密度和层次结构的见解。'
- en: '*Figure 13.15* shows how DBSCAN and HDBSCAN, respectively, are able to identify
    clusters that differ in shape significantly from those discovered by k-means,
    for example. The selection of the clustering algorithm is a function of the structure
    of your data; refer to the pairs trading strategy that was referenced earlier
    in this section for a practical example.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.15*显示了DBSCAN和HDBSCAN分别能够识别与k-means发现的集群形状明显不同的集群。聚类算法的选择取决于数据的结构；请参考本节前面提到的配对交易策略，以获得一个实际示例。'
- en: '![](img/B15439_13_15.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_15.png)'
- en: 'Figure 13.15: Comparing the DBSCAN and HDBSCAN clustering algorithms'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.15：比较DBSCAN和HDBSCAN聚类算法
- en: Gaussian mixture models
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: GMMs are generative models that assume the data has been generated by a mix
    of various multivariate normal distributions. The algorithm aims to estimate the
    mean and covariance matrices of these distributions.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: GMM是生成模型，假设数据是由各种多元正态分布的混合生成的。该算法旨在估计这些分布的均值和协方差矩阵。
- en: 'A GMM generalizes the k-means algorithm: it adds covariance among features
    so that clusters can be ellipsoids rather than spheres, while the centroids are
    represented by the means of each distribution. The GMM algorithm performs soft
    assignments because each point has a probability of being a member of any cluster.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: GMM泛化了k-means算法：它在特征之间添加了协方差，使得集群可以是椭圆形而不是球形，而中心点由每个分布的均值表示。GMM算法执行软分配，因为每个点都有成为任何集群成员的概率。
- en: The notebook `gaussian_mixture_models` demonstrates the implementation and visualizes
    the resulting cluster. You are likely to prefer GMM over other clustering algorithms
    when the k-means assumption of spherical clusters is too constraining; GMM often
    needs fewer clusters to produce a good fit given its greater flexibility. The
    GMM algorithm is also preferable when you need a generative model; because GMM
    estimates the probability distributions that generated the samples, it is easy
    to generate new samples based on the result.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`gaussian_mixture_models`演示了实现并可视化生成的集群。当k-means假设球形集群过于约束时，您可能更喜欢GMM而不是其他聚类算法；鉴于其更大的灵活性，GMM通常需要更少的集群来产生良好的拟合。当您需要一个生成模型时，GMM算法也更可取；因为GMM估计生成样本的概率分布，所以基于结果生成新样本很容易。
- en: Hierarchical clustering for optimal portfolios
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于最优投资组合的层次聚类
- en: In *Chapter 5*, *Portfolio Optimization and Performance Evaluation*, we discussed
    several methods that aim to choose portfolio weights for a given set of assets
    to optimize the risk and return profile of the resulting portfolio. These included
    the mean-variance optimization of Markowitz's modern portfolio theory, the Kelly
    criterion, and risk parity. In this section, we cover **hierarchical risk parity**
    (**HRP**), a more recent innovation (Prado 2016) that leverages hierarchical clustering
    to assign position sizes to assets based on the risk characteristics of subgroups.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章*，*投资组合优化和绩效评估*中，我们讨论了几种旨在选择给定资产组合的投资组合权重以优化结果投资组合的风险和回报特征的方法。这些方法包括马科维茨现代投资组合理论的均值-方差优化、凯利准则和风险平价。在本节中，我们介绍了**层次风险平价**（**HRP**），这是一种更近期的创新（Prado
    2016），它利用层次聚类根据子组的风险特征分配资产的头寸大小。
- en: We will first present how HRP works and then compare its performance against
    alternatives using a long-only strategy driven by the gradient boosting models
    we developed in the last chapter.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍HRP的工作原理，然后使用上一章中开发的梯度提升模型来比较其性能与替代方案的性能。
- en: How hierarchical risk parity works
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次风险平价的工作原理
- en: 'The key ideas of hierarchical risk parity are to do the following:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 层次风险平价的关键思想是做以下几点：
- en: Use hierarchical clustering of the covariance matrix to group assets with a
    similar correlation structure together
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用协方差矩阵的层次聚类将具有相似相关结构的资产分组在一起
- en: Reduce the number of degrees of freedom by only considering similar assets as
    substitutes when constructing the portfolio
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过仅在构建投资组合时考虑相似资产作为替代品来减少自由度的数量
- en: Refer to the notebook and Python files in the subfolder `hierarchical_risk_parity`
    for implementation details.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 有关实施细节，请参考子文件夹`hierarchical_risk_parity`中的笔记本和Python文件。
- en: The first step is to compute a distance matrix that represents proximity for
    correlated assets and meets distance metric requirements. The resulting matrix
    becomes an input to the SciPy hierarchical clustering function that computes the
    successive clusters using one of several available methods, as discussed previously
    in this chapter.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是计算一个距离矩阵，代表相关资产的接近程度并满足距离度量的要求。得到的矩阵成为输入到SciPy层次聚类函数的一个输入，该函数使用本章前面讨论过的几种可用方法之一计算连续的集群。
- en: '[PRE23]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `linkage_matrix` can be used as input to the `sns.clustermap` function to
    visualize the resulting hierarchical clustering. The dendrogram displayed by seaborn
    shows how individual assets and clusters of assets merged based on their relative
    distances (see the left panel of *Figure 13.16*).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`linkage_matrix`可以作为输入传递给`sns.clustermap`函数，以可视化生成的层次聚类。seaborn显示的树状图展示了基于它们的相对距离，个别资产和资产集群是如何合并的（参见*图13.16*的左侧面板）。'
- en: '[PRE24]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Compared to a `seaborn.heatmap` of the original correlation matrix, there is
    now significantly more structure in the sorted data (the right panel) compared
    to the original correlation matrix displayed in the central panel.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始相关矩阵的`seaborn.heatmap`相比，现在排序数据中有更多的结构（右侧面板）与中央面板中显示的原始相关矩阵相比。
- en: '![](img/B15439_13_16.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_16.png)'
- en: 'Figure 13.16: Original and clustered correlation matrix'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.16：原始和聚类相关矩阵
- en: Using the tickers sorted according to the hierarchy induced by the clustering
    algorithm, HRP now proceeds to compute a top-down inverse-variance allocation
    that successively adjusts weights depending on the variance of the subclusters
    further down the tree.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用根据聚类算法引起的层次排序的股票代码，HRP现在继续计算自上而下的反方差分配，根据树下进一步子集的方差调整权重。
- en: '[PRE25]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To this end, the algorithm uses a bisectional search to allocate the variance
    of a cluster to its elements based on their relative riskiness.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，该算法使用二分搜索来根据相对风险分配集群的方差给其元素。
- en: '[PRE26]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The resulting portfolio allocation produces weights that sum to 1 and reflect
    the structure present in the correlation matrix (refer to the notebook for details).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的投资组合分配产生的权重总和为1，并反映了相关矩阵中存在的结构（有关详细信息，请参阅笔记本）。
- en: Backtesting HRP using an ML trading strategy
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ML交易策略回测HRP
- en: Now that we know how HRP works, we would like to test how it performs in practice
    compared to some alternatives, namely a simple equal-weighted portfolio and a
    mean-variance optimized portfolio. You can find the code samples for this section
    and additional details and analyses in the notebook `pf_optimization_with_hrp_zipline_benchmark`.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了HRP的工作原理，我们想测试它在实践中与一些替代方案相比的表现，即简单的等权重组合和均值方差优化组合。您可以在笔记本`pf_optimization_with_hrp_zipline_benchmark`中找到本节的代码示例和额外的细节和分析。
- en: To this end, we'll build on the gradient boosting models developed in the last
    chapter. We will backtest a strategy for 2015-2017 with a universe of the 1,000
    most liquid US stocks. The strategy relies on the model predictions to enter long
    positions in the 25 stocks with the highest positive return prediction for the
    next day. On a daily basis, we rebalance our holdings so that the weights for
    our target positions match the values suggested by HRP.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将在上一章中开发的梯度提升模型的基础上进行。我们将对2015-2017年的策略进行回测，使用美国最流动的1,000支股票作为投资范围。该策略依赖于模型预测，以便在下一个交易日中选择预测收益最高的25支股票进行多头头寸。每天，我们重新平衡我们的持仓，以使我们的目标头寸的权重与HRP建议的值相匹配。
- en: Ensembling the gradient boosting model predictions
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成梯度提升模型预测
- en: 'We begin by averaging the predictions of the 10 models that performed best
    during the 2015-16 cross-validation period (refer to *Chapter 12*, *Boosting Your
    Trading Strategy*, for details), as shown in the following code excerpt:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对在2015-16交叉验证期间表现最佳的10个模型的预测进行平均（有关详细信息，请参阅*第12章*“提升您的交易策略”），如下面的代码摘录所示：
- en: '[PRE27]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'On a daily basis, we obtain the model predictions and select the top 25 tickers.
    If there are at least 20 tickers with positive forecasts, we enter the long positions
    and close all of the other holdings:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 每天，我们获得模型预测并选择前25个股票代码。如果至少有20个股票代码有正面预测，我们会进入多头头寸并关闭所有其他持仓：
- en: '[PRE28]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Using PyPortfolioOpt to compute HRP weights
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyPortfolioOpt计算HRP权重
- en: 'PyPortfolioOpt, which we used in *Chapter 5*, *Portfolio Optimization and Performance
    Evaluation*, to compute mean-variance optimized weights, also implements HRP.
    We''ll run it as part of the scheduled rebalancing that takes place every morning.
    It needs the return history for the target assets and returns a dictionary of
    ticker-weight pairs that we use to place orders:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: PyPortfolioOpt在*第5章*“组合优化和绩效评估”中使用，用于计算均值方差优化权重，也实现了HRP。我们将在每天早上进行的计划重新平衡的一部分中运行它。它需要目标资产的回报历史，并返回一个我们用来下订单的股票-权重对的字典：
- en: '[PRE29]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Markowitz rebalancing follows a similar process, as outlined in *Chapter 5*,
    *Portfolio Optimization and Performance Evaluation*, and is included in the notebook.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 马科维茨再平衡遵循类似的过程，如*第5章*“组合优化和绩效评估”中所述，并包含在笔记本中。
- en: Performance comparison with pyfolio
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与pyfolio的绩效比较
- en: The following charts show the cumulative returns for the in- and out-of-sample
    (with respect to the ML model selection process) of the **equal-weighted** (**EW**),
    the HRP, and the **mean-variance** (**MV**) optimized portfolios.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了**等权重**（**EW**）、HRP和**均值方差**（**MV**）优化组合的样本内和样本外的累积收益。
- en: '![](img/B15439_13_17.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_13_17.png)'
- en: 'Figure 13.17: Cumulative returns for the different portfolios'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.17：不同投资组合的累积收益
- en: The cumulative returns are 207.3 percent for MV, 133 percent for EW, and 75.1
    percent for HRP. The Sharpe ratios are 1.16, 1.01, and 0.83, respectively. Alpha
    returns are 0.28 for MV, 0.16 for EW, and 0.16 for HRP, with betas of 1.77, 1.87,
    and 1.67, respectively.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 累积收益为MV为207.3％，EW为133％，HRP为75.1％。夏普比率分别为1.16，1.01和0.83。Alpha收益为MV为0.28，EW为0.16，HRP为0.16，贝塔分别为1.77，1.87和1.67。
- en: Therefore, it turns out that, in this particular context, the often-criticized
    MV approach does best, while HRP comes up last. However, be aware that the results
    are quite sensitive to the number of stocks traded, the time period, and other
    factors.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种特定情况下，通常受到批评的MV方法表现最佳，而HRP表现最差。但是，请注意，结果对交易的股票数量、时间段和其他因素非常敏感。
- en: Try it out for yourself, and learn which technique performs best under the circumstances
    most relevant for you!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 自己尝试一下，并了解在您最关心的情况下哪种技术表现最佳！
- en: Summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored unsupervised learning methods that allow us to
    extract valuable signals from our data without relying on the help of outcome
    information provided by labels.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了无监督学习方法，这些方法允许我们从数据中提取有价值的信号，而无需依赖标签提供的结果信息。
- en: We learned how to use linear dimensionality reduction methods like PCA and ICA
    to extract uncorrelated or independent components from data that can serve as
    risk factors or portfolio weights. We also covered advanced nonlinear manifold
    learning techniques that produce state-of-the-art visualizations of complex, alternative
    datasets. In the second part of the chapter, we covered several clustering methods
    that produce data-driven groupings under various assumptions. These groupings
    can be useful, for example, to construct portfolios that apply risk-parity principles
    to assets that have been clustered hierarchically.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何使用线性降维方法，比如PCA和ICA，从数据中提取不相关或独立的成分，这些成分可以作为风险因素或投资组合权重。我们还涵盖了先进的非线性流形学习技术，可以生成复杂的替代数据的最新可视化效果。在本章的第二部分，我们涵盖了几种聚类方法，根据不同的假设产生数据驱动的分组。例如，这些分组可以用于构建按照风险平价原则对层次聚类的资产进行投资组合构建。
- en: In the next three chapters, we will learn about various machine learning techniques
    for a key source of alternative data, namely natural language processing for text
    documents.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三章中，我们将学习关于各种机器学习技术，用于替代数据的一个关键来源，即自然语言处理文本文档。
