- en: Chapter 8. Exploring Additional Features of Swarm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。探索Swarm的其他功能
- en: 'In this chapter, we''re going to discuss and deepen our knowledge on two very
    important topics related to Docker and orchestration systems: networking and consensus.
    In particular, we''ll see how to:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论并加深我们对Docker和编排系统的两个非常重要的主题的了解：网络和共识。特别是，我们将看到如何：
- en: Foundations of Libnetwork
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libnetwork的基础。
- en: Basic security of Libnetwork
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libnetwork的基本安全性
- en: Routing mesh
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由网格
- en: Overlay networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: The Network Control Plane
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络控制平面
- en: Libkv
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libkv
- en: Libnetwork
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Libnetwork
- en: Libnetwork is the networking stack designed from the ground-up to work with
    Docker regardless of platforms, environments, operating systems, or infrastructures.
    Libnetwork is not only an interface for the network driver. It's not only a library
    to manage VLAN or VXLAN networks but it does more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Libnetwork是从头开始设计的网络堆栈，可以与Docker一起使用，无论平台、环境、操作系统还是基础设施如何。Libnetwork不仅是网络驱动程序的接口。它不仅是一个管理VLAN或VXLAN网络的库，它做得更多。
- en: 'Libnetwork is a full networking stack and consists of three planes, the **Management
    Plane**, the **Control Plane**, and the **Data Plane** as shown in the following
    diagram:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Libnetwork是一个完整的网络堆栈，由三个平面组成，即**管理平面**、**控制平面**和**数据平面**，如下图所示：
- en: '![Libnetwork](images/image_08_001.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Libnetwork](images/image_08_001.jpg)'
- en: The **Management Plane** allows users, operators, or tools to manage the network
    infrastructure. These operations include network monitoring. The Management Plane
    represents the Docker network user experiences, provides the APIs. It's also extensible
    via management plugins, such as IPAM plugins, which, for example, allows us to
    control how we assign IP addresses to each container.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理平面**允许用户、运营商或工具管理网络基础设施。这些操作包括网络监控。管理平面代表Docker网络用户体验，提供API。它还可以通过管理插件进行扩展，例如IPAM插件，例如，允许我们控制如何为每个容器分配IP地址。'
- en: The **Control Plane** is implemented in the -scoped gossip protocol, service-discovery,
    encryption key distribution is added directly.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面**是在-scoped gossip协议中实现的，直接添加了服务发现、加密密钥分发。'
- en: In brief, the **Data Plane** is responsible for moving network packets between
    two Endpoints. Network plugins work for each Data Plane. By default, there are
    a few built-in drivers. For example, the overlay driver we met in the previous
    chapters directly uses the features inside Linux and Windows kernels, so there
    is no driver code for this kind of network. This is also applied for bridge, IPVLAN,
    and MacVLAN drivers. In contrast, other third-party networks need their own implementation
    in the form of plugins.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简而言之，**数据平面**负责在两个端点之间传输网络数据包。网络插件适用于每个数据平面。默认情况下，有一些内置驱动程序。例如，我们在前几章中遇到的覆盖驱动程序直接使用Linux和Windows内核中的功能，因此对于这种类型的网络，没有驱动程序代码。这也适用于桥接、IPVLAN和MacVLAN驱动程序。相比之下，其他第三方网络需要以插件的形式进行自己的实现。
- en: Following the usual Docker UX, that states the components should just work on
    any environment, also the networking stack must be portable. And to make Docker's
    networking stack portable, its design and implementation must be solid. For example,
    the Management Plane cannot be controlled by any other component. Also, the Control
    Plane cannot be replaced by other components. If we allowed that, the networking
    stack would break when we change our application environment from one to another.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循通常的Docker UX，即组件应该在任何环境下都能正常工作，网络堆栈也必须是可移植的。为了使Docker的网络堆栈可移植，其设计和实现必须是牢固的。例如，管理平面不能被任何其他组件控制。同样，控制平面也不能被其他组件替换。如果允许这样做，当我们将应用程序环境从一个环境更改为另一个环境时，网络堆栈将会崩溃。
- en: Networking plugins
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络插件
- en: The Data Plane is designed to be pluggable. In fact, it can be only managed
    by built-in or external plugins. For example, MacVLAN was implemented as a plugin
    into Docker 1.12 without affecting other parts of the system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平面设计为可插拔的。事实上，它只能由内置或外部插件管理。例如，MacVLAN被实现为Docker 1.12中的一个插件，而不会影响系统的其他部分。
- en: The most remarkable thing is that we can have several Drivers and plugins on
    the same networking stack they can work without interfering with one another.
    So typically, in Swarm, we can have an overlay network, a bridge network as well
    as a host driver running on the same cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最值得注意的是，我们可以在同一网络堆栈上拥有多个驱动程序和插件，它们可以在不相互干扰的情况下工作。因此，在Swarm中，我们通常可以在同一集群上运行覆盖网络、桥接网络以及主机驱动程序。
- en: Container Networking Model
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器网络模型
- en: 'Libnetwork is designed and implemented to serve the Docker Swarm requirements
    to run Docker''s distributed applications. That is, Libnetwork is actually the
    Docker Networking Fabric. The foundation of Libnetwork is a model called **Container
    Networking Model** (**CNM**). It is a well-defined basic model that describes
    how containers connect to the given networks. The CNM consists of three components:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Libnetwork的设计和实现是为了满足Docker Swarm运行Docker分布式应用程序的要求。也就是说，Libnetwork实际上是Docker网络基础架构。Libnetwork的基础是一个称为**容器网络模型**（**CNM**）的模型。这是一个明确定义的基本模型，描述了容器如何连接到给定的网络。CNM由三个组件组成：
- en: '**Sandbox**: This is an isolation containing the configuration of the network
    stack of the container.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**沙盒**：这是一个包含容器网络堆栈配置的隔离。'
- en: '**Endpoint**: This is a connection point that only belongs to a network and
    a sandbox.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端点**：这是一个仅属于网络和沙盒的连接点。'
- en: '**Network**: This is a group of endpoints which allowed to community freely
    among them. A network consists of one or more endpoints.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：这是一组允许彼此自由通信的端点。一个网络由一个或多个端点组成。'
- en: The Drivers represent the Data Plane. Every Driver, being overlay, bridge, or
    MacVLAN are in the form of Plugins. Each plugin works in a Data Plane specific
    to it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序代表数据平面。每个驱动程序，无论是覆盖、桥接还是MacVLAN，都以插件的形式存在。每个插件都在特定于其的数据平面上工作。
- en: In the system, there is a built-in IPAM by default. This is an important issue
    because each container must have an IP address attached. So it's necessary to
    have an IPAM system built-in, which allows each container to be able to connect
    to each otheras we did in the traditional way and we need an IP address for others
    to talk to the container. We also require to define subnets as well as ranges
    of IP addresses. Also, the system is designed for IPAM to be pluggable. This means
    that it allows us to have our own DHCP drivers or allow plumbing the system to
    an existing DHCP server.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统中，默认情况下有一个内置的IPAM。这是一个重要问题，因为每个容器必须有一个附加的IP地址。因此，有必要内置一个IPAM系统，允许每个容器能够像我们以传统方式那样连接到彼此，并且我们需要一个IP地址让其他人与容器通信。我们还需要定义子网以及IP地址范围。此外，该系统设计为IPAM可插拔。这意味着它允许我们拥有自己的DHCP驱动程序或允许将系统连接到现有的DHCP服务器。
- en: As previously mentioned, Libnetwork supports multihost networking out-of-the-box.
    Components worth to discuss for the multihost networking are its Data and Control
    Planes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Libnetwork支持开箱即用的多主机网络。值得讨论的多主机网络的组件是其数据平面和控制平面。
- en: The Control Plane currently included in Docker 1.12 uses the gossip mechanism
    as the general discovery system for nodes. This gossip protocol-based network
    works on another layer in parallel of the Raft consensus system. Basically, we
    have twodifferent membership mechanisms work at the same time. Libnetwork allows
    the driver from other plugins to commonly use the control plane.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 1.12中的控制平面目前使用八卦机制作为节点的一般发现系统。这种基于八卦协议的网络在Raft一致性系统的另一层上同时工作。基本上，我们有两种不同的成员机制同时工作。Libnetwork允许其他插件的驱动程序共同使用控制平面。
- en: 'These are the features of Libnetwork''s Control plane:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是Libnetwork控制平面的特点：
- en: It's secure and encrypted out-of-the-box
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是安全和加密的
- en: Every single data plane can use it
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据平面都可以使用它
- en: It provides native service discovery and load balancing out-of-the-box
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了原生的服务发现和负载均衡功能
- en: Docker 1.12 implements VIP-based service discovery in Swarm. This service works
    by mapping a Virtual IP address of the container to the DNS records. Then all
    DNS records are shared via gossip. In Docker 1.12, with the introduction of the
    concept of service, this notion fits directly to the concept of discovery.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 1.12在Swarm中实现了基于VIP的服务发现。这项服务通过将容器的虚拟IP地址映射到DNS记录来工作。然后所有的DNS记录都通过八卦进行共享。在Docker
    1.12中，随着服务概念的引入，这个概念直接适用于发现的概念。
- en: In Docker 1.11 and previous versions, it was necessary instead to use container
    names and aliases to "simulate" service discovery and do DNS roundrobin to perform
    some kind of primitive load balancing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker 1.11和之前的版本中，需要使用容器名称和别名来“模拟”服务发现，并进行DNS轮询来执行某种原始的负载均衡。
- en: Libnetwork carries on the principle of battery included but removable, which
    is implemented as the plugin system. In the future, Libnetwork will gradually
    expand the plugin system to cover other networking parts, for example, load balancing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Libnetwork延续了“电池内置但可拆卸”的原则，这是作为插件系统实现的。在未来，Libnetwork将逐渐扩展插件系统，以涵盖其他网络部分，例如负载均衡。
- en: Encryption and routing mesh
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加密和路由网格
- en: The model at the heart of Libnetwork is CNM, as previously mentioned. In Swarm
    mode, libnetwork is built in a cluster-aware mode and supports multi-host networking
    without external key value stores. The overlay network fits naturally in this
    model. And both Data plane and Control plane encryption has been introduced. With
    encrypted Control Plane, routing information on VXLAN, for example, for which
    container has which MAC address and which IP address, is automatically secured.
    Also, with Routing Mesh, CNM provides a decentralized mechanism allowing you to
    access services from any IP of the cluster. When a request comes from the outsideand
    hits any node of the cluster, the traffic will be routed to a working container.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，Libnetwork的核心模型是CNM。在Swarm模式下，libnetwork以集群感知模式构建，并支持多主机网络而无需外部键值存储。覆盖网络自然适应于这种模型。同时引入了数据平面和控制平面加密。通过加密的控制平面，例如VXLAN上的路由信息，容器具有哪个MAC地址和哪个IP地址，将自动得到保护。此外，通过路由网格，CNM提供了一种分散的机制，允许您从集群的任何IP访问服务。当请求来自外部并命中集群的任何节点时，流量将被路由到一个工作容器。
- en: MacVLAN
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MacVLAN
- en: The new Driver in 1.12 is MacVLAN. MacVLAN is a performant driver designed to
    allow the Docker network to plumb to the existing VLAN, for example, a corporate
    one, letting everything to continue to work. There is a scenario where we will
    gradually migrate workloads from the original VLAN to Docker and MacVLAN will
    help plumb the Docker cluster to the original VLAN. This will make the Docker
    networks integrated with the underlay network and the containers will be able
    to work in the same VLAN.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 1.12版本中的新驱动程序是MacVLAN。MacVLAN是一个高性能的驱动程序，旨在允许Docker网络连接到现有的VLAN，例如公司的一个VLAN，让一切继续工作。有一种情况是我们将逐渐将工作负载从原始VLAN迁移到Docker，MacVLAN将帮助将Docker集群连接到原始VLAN。这将使Docker网络与底层网络集成，容器将能够在相同的VLAN中工作。
- en: We could just create a network with the MacVLAN driver and specify the real
    subnet to the network. We can also specify a range of IP addresses only for the
    containers. Also, we can exclude some IP addresses, for example, the gateway,
    from assigning to containers with `--aux-address`. The parent interface of the
    MacVLAN driver is the interface we would like to connect this network to. As previously
    mentioned, MacVLAN yields the best performance of all drivers. Its Linux implementation
    isextremely lightweight. They just enforce the separation between networks and
    connection to the physical parent network, rather than implemented as traditional
    Linux bridge for network isolation. The use of MacVLAN driver requires Linux Kernel
    3.9 - 3.19 or 4.x.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用MacVLAN驱动程序创建一个网络，并将真实子网指定给该网络。我们还可以为容器指定一系列IP地址。此外，我们可以使用`--aux-address`排除一些IP地址，例如网关，不分配给容器。MacVLAN驱动程序的父接口是我们希望将该网络连接到的接口。如前所述，MacVLAN驱动程序的性能最佳。其Linux实现非常轻量级。它们只是强制执行网络之间的分离和连接到物理父网络，而不是作为传统的Linux桥接实现网络隔离。MacVLAN驱动程序的使用需要Linux内核3.9-3.19或4.x。
- en: Overlay networks
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: Because Swarm cluster is now a native feature built into the Docker Engine,
    this allows the creation of overlay networks very easy without using external
    key-value stores.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Swarm集群现在是内置在Docker Engine中的本机功能，这使得创建覆盖网络非常容易，而无需使用外部键值存储。
- en: Manager nodes are responsible for managing the state of the networks. All the
    networking states are kept inside the Raft log. The main difference between Raft
    implementation in the Swarm mode and the external key-value store is that the
    embedded Raft has far higher performance than the external ones. Our own experiments
    confirmed that the external key-value store will stick around 100-250 nodes, while
    the embedded Raft helped us scale the system to 4,700 nodes in the Swarm3k event.
    This is because the external Raft store basically has high network latency. When
    we need to agree on some states, we will be incurred from the network round-trips,
    while the embedded Raft store is just there in memory.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 管理节点负责管理网络的状态。所有网络状态都保存在Raft日志中。Swarm模式中Raft实现与外部键值存储的主要区别在于，嵌入式Raft的性能远高于外部存储。我们自己的实验证实，外部键值存储将保持在100-250个节点左右，而嵌入式Raft帮助我们将系统扩展到了Swarm3k事件中的4,700个节点。这是因为外部Raft存储基本上具有很高的网络延迟。当我们需要就某些状态达成一致时，我们将从网络往返中产生开销，而嵌入式Raft存储只是存在于内存中。
- en: In the past, when we wanted to do any network-related action, assigning IP address
    to the containers, for example, significant network latency happened as we always
    talk to the external store. For the embedded Raft, when we would like to have
    a consensus on values, we can do it right away with the in-memory store.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，当我们想要执行任何与网络相关的操作，例如为容器分配IP地址时，由于我们总是与外部存储进行通信，会发生显着的网络延迟。对于嵌入式Raft，当我们希望就某些值达成共识时，我们可以立即在内存存储中进行。
- en: '![Overlay networks](images/image_08_002.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![覆盖网络](images/image_08_002.jpg)'
- en: 'When we create a network with the overlay driver, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用覆盖驱动程序创建网络时，如下所示：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The command will talk to the allocator. Then there will be a subnet reservation,
    in this case `10.9.0.0/24`, and agree related values right away in the manager
    host in its memory once it''s allocated. We would like to create a service after
    that. Then we will later connect that service to the network. When we create a
    service, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 命令将与分配器交谈。然后将进行子网保留，例如`10.9.0.0/24`，并在分配后立即在管理主机的内存中同意相关值。之后我们想要创建一个服务。然后我们稍后将该服务连接到网络。当我们创建一个服务时，如下所示：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The orchestrator creates a number of tasks (containers) for that service. Then
    each created task will be assigned an IP address. The allocation will be working
    again during this assignment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器为该服务创建了一些任务（容器）。然后为每个创建的任务分配一个IP地址。在此分配过程中，分配将再次起作用。
- en: 'After the task creation is done:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任务创建完成后：
- en: The task gets an IP address
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务获得一个IP地址
- en: Its network-related information will be committed into the Raft log store
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其与网络相关的信息将被提交到Raft日志存储中
- en: After the commit is done by the allocation, the scheduler will be moving the
    task to another state
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分配完成后，调度程序将任务移动到另一个状态
- en: The Dispatcher dispatches each task to one of the worker nodes
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度程序将每个任务分派给工作节点之一
- en: Finally, the container associated to that task will be running on the Docker
    Engine
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，与该任务关联的容器将在Docker Engine上运行
- en: If a task is not able to allocate its network resource, it will be stuck there
    at the allocated state and will not be scheduled. This is the important difference
    from the previous versions of Docker that in the network system of Swarm mode,
    the concept of allocation state is obvious. With this, it improves the overall
    allocation cycle of the system a lot. When we talk about the allocation, we refer
    not only to the allocation of IP addresses, but also to related driver artifacts.
    For an overlay network, it needs to reserve a VXLAN identifier, which is a set
    of global identifiers for each VXLAN. This identifier reservation is done by the
    Network Allocator.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务无法分配其网络资源，它将停留在分配状态，并且不会被调度。这是与以前版本的Docker不同的重要差异，在Swarm模式的网络系统中，分配状态的概念是明显的。通过这种方式，它大大改善了系统的整体分配周期。当我们谈论分配时，我们不仅指IP地址的分配，还包括相关的驱动程序工件。对于覆盖网络，需要保留一个VXLAN标识符，这是每个VXLAN的一组全局标识符。这个标识符的保留是由网络分配器完成的。
- en: In the future, for a plugin to do the same allocation mechanism, it will be
    enough to implement only some interfaces and make the state being automatically
    managed by Libnetwork and stored into the Raft log. With this, the resource allocation
    is in the centralized way, so we can achieve consistency and consensus. With consensus,
    we need a highly efficient consensus protocol.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将来，要实现相同的分配机制，插件只需实现一些接口，并使状态自动由Libnetwork管理并存储到Raft日志中。通过这种方式，资源分配是以集中方式进行的，因此我们可以实现一致性和共识。有了共识，我们需要一个高效的共识协议。
- en: Network Control Plane
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络控制平面
- en: Network Control Plane is a subsystem of Libnetwork to manage routing information
    and we need a protocol that converge quickly to do that job. For example, Libnetwork
    does not use BGP as the protocol (despite that BGP is great at scalability to
    support very large number of endpoints), because point BGP won't converge quick
    enough to use in the highly dynamic environment such as the software container
    environment.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 网络控制平面是Libnetwork的一个子系统，用于管理路由信息，我们需要一个能够快速收敛的协议来完成这项工作。例如，Libnetwork不使用BGP作为协议（尽管BGP在支持非常大数量的端点方面非常出色），因为点对点的BGP收敛速度不够快，无法在诸如软件容器环境这样高度动态的环境中使用。
- en: In a container-centric world, the networking system is expected to change very
    quickly, especially for the new Docker service model, which requires a massive
    and fast IP assignation. We want the routing information to converge very rapidly
    as well, especially at a big scale, for example, for more than 10,000 containers.
    In Swarm2k and Swarm3k experiments, we really did start 10,000 containers at a
    time. Especially, in Swarm3k, we started 4,000 NGINX containers on the Ingress
    load-balancing network. Without a fine implementation, this number of scale won't
    work correctly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在以容器为中心的世界中，网络系统预计会发生非常快速的变化，特别是对于新的Docker服务模型，它需要大规模且快速的IP分配。我们也希望路由信息在大规模情况下能够非常快速地收敛，例如对于超过10,000个容器。在Swarm2k和Swarm3k的实验中，我们确实一次启动了10,000个容器。特别是在Swarm3k中，我们在入口负载均衡网络上启动了4,000个NGINX容器。如果没有良好的实现，这种规模的数量将无法正常工作。
- en: 'To solve this problem, the Libnetwork team chose to include the gossip protocol
    in the Network Control Plane. The internal algorithm of the protocol works like
    this: It choses 3 neighbors and then propagates the same information; in the case
    of Libnetwork, the routing and other network related information. The Gossip protocol
    will do this process repeatedly, until every node shares the same information.
    With this technique, the whole cluster will receive the information very quickly,
    in a matter of seconds.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Libnetwork团队选择在网络控制平面中包含八卦协议。协议的内部算法工作方式如下：它选择3个邻居，然后传播相同的信息；在Libnetwork的情况下，是路由和其他与网络相关的信息。八卦协议将重复执行此过程，直到每个节点共享相同的信息。通过这种技术，整个集群将在几秒钟内非常快速地接收到信息。
- en: '![Network Control Plane](images/image_08_003.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![网络控制平面](images/image_08_003.jpg)'
- en: Anyway, the whole cluster does not need the same information all the time. Every
    node on the cluster does not need to know information of all the networks. Only
    nodes in a particular network need to know its own networking information. To
    optimize this for Libnetwork, the team implemented two scopes, *Cluster Scoped
    Gossip Communication* and *Network Scoped Gossip Communication*. What we have
    explained so far is the Cluster Scope Gossip Communication, while Network Scoped
    Gossip Communication limits the network information within a particular network.
    When a network expands to cover addition nodes, its gossip scoped broadcast will
    also cover them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，整个集群并不需要始终具有相同的信息。集群中的每个节点并不需要知道所有网络的信息。只有特定网络中的节点需要知道自己的网络信息。为了优化Libnetwork，团队实现了两个范围，*集群范围的八卦通信*和*网络范围的八卦通信*。到目前为止，我们所解释的是集群范围的八卦通信，而网络范围的八卦通信将限制特定网络内的网络信息。当网络扩展以覆盖额外节点时，其八卦范围广播也将覆盖它们。
- en: This activity is built on top Docker's CNM and therefore relieson the network
    abstraction. From the Figure, we have node **w1**, **w2**, and **w3** in the Left
    network and also **w3**, **w4**, **w5** in the right network. The left network
    performs gossip and only **w1**, **w2**, **w3** would know its routing information.
    You may observe that w3 is in both the networks. Therefore, it will receive routing
    information of all left and right networks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这项活动是建立在Docker的CNM之上的，因此依赖于网络抽象。从图中可以看出，左侧网络中有节点**w1**、**w2**和**w3**，右侧网络中也有**w3**、**w4**和**w5**。左侧网络执行八卦，只有**w1**、**w2**、**w3**才会知道其路由信息。您可能会注意到w3同时存在于两个网络中。因此，它将接收所有左侧和右侧网络的路由信息。
- en: Libkv
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Libkv
- en: '`libkv` is a unified library to interact with different key-value store backends.
    `libkv` was originally part of Docker Swarm v1 in the very first versions of the
    development. Later, all code related to key-value store discovery services was
    refactored and moved to [www.github.com/docker/libkv](https://github.com/docker/libkv).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`libkv`是一个统一的库，用于与不同的键值存储后端进行交互。`libkv`最初是Docker Swarm v1的一部分，在最初的开发版本中。后来，所有与键值存储发现服务相关的代码都经过了重构，并移至[www.github.com/docker/libkv](https://github.com/docker/libkv)。'
- en: '`libkv` allows you to execute CRUD operations and also to watch key-value entries
    from different backends, so we can use the same code to work with all HA distributed
    key-value stores, which are **Consul**, **Etcd**, and **ZooKeeper** as shown in
    the following figure. At the time of writing, libkv also supports a local store
    implemented using **BoltDB**.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`libkv`允许您执行CRUD操作，还可以从不同的后端观察键值条目，因此我们可以使用相同的代码与所有HA分布式键值存储一起工作，这些存储包括**Consul**，**Etcd**和**ZooKeeper**，如下图所示。在撰写本文时，libkv还支持使用**BoltDB**实现的本地存储。'
- en: '![Libkv](images/image_08_004.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Libkv](images/image_08_004.jpg)'
- en: How to use libkv
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用libkv
- en: 'To start with `libkv,` we need to understand how to call its APIs first. Here''s
    the `libkv Store` interface in Go, for every store implementation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用`libkv`，我们首先需要了解如何调用其API。以下是Go中的`libkv Store`接口，适用于每个存储实现：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We need to know how to `Put`, `Get`, `Delete`, and `Watch` to basically interact
    with a store.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要知道如何`Put`，`Get`，`Delete`和`Watch`来基本地与存储进行交互。
- en: 'Make sure you also have Go and Git installed on your machine and the Git executable
    is on your PATH. Then, we need to do a number of go get to install dependencies
    for our program:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的计算机上还安装了Go和Git，并且Git可执行文件位于您的PATH上。然后，我们需要执行一些go get来安装我们程序的依赖项：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here we provide with a skeleton. You need to start a single-node `Consul` before
    you try to run the following program:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们提供了一个框架。在尝试运行以下程序之前，您需要启动一个单节点的`Consul`：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can also test getting your value with curl. The value you've put should
    be there. We should continue playing with the libkv APIs,which are `Get` and `Delete`.
    It's left for the readers as an exercise.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用curl测试获取您的值。您放置的值应该在那里。我们应该继续使用`libkv`的API，即`Get`和`Delete`。这留给读者作为练习。
- en: Summary
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covers Libnetwork, one of the most important parts of Docker Swarm.
    We have discussed its Management Plane, Control Plane, and Data Plane. This chapter
    also includes some techniques on how to use `libkv`, a key-value abstraction to
    implement your own service discovery system. In the next chapter, we'll focus
    on security. In the next chapter, we will learn how to secure a swarm cluster.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了Libnetwork，这是Docker Swarm中最重要的部分之一。我们已经讨论了其管理平面，控制平面和数据平面。本章还包括一些关于如何使用`libkv`的技术，这是一个键值抽象，用于实现您自己的服务发现系统。在下一章中，我们将专注于安全性。在下一章中，我们将学习如何保护一个集群。
