- en: '*Chapter 13*: Extending Kubernetes with CRDs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：使用CRD扩展Kubernetes'
- en: This chapter explains the many possibilities for extending the functionality
    of Kubernetes. It begins with a discussion of the **Custom Resource Definition**
    (**CRD**), a Kubernetes-native way to specify custom resources that can be acted
    on by the Kubernetes API using familiar `kubectl` commands such as `get`, `create`,
    `describe`, and `apply`. It is followed by a discussion of the Operator pattern,
    an extension of the CRD. It then details some of the hooks that cloud providers
    attach to their Kubernetes implementations, and ends with a brief introduction
    to the greater cloud-native ecosystem. Using the concepts learned in this chapter,
    you will be able to architect and develop extensions to your Kubernetes cluster,
    unlocking advanced usage patterns.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了扩展Kubernetes功能的许多可能性。它从讨论**自定义资源定义**（**CRD**）开始，这是一种Kubernetes本地的方式，用于指定可以通过熟悉的`kubectl`命令（如`get`、`create`、`describe`和`apply`）对其进行操作的自定义资源。接下来是对运算符模式的讨论，这是CRD的扩展。然后详细介绍了云提供商附加到其Kubernetes实现的一些钩子，并以对更大的云原生生态系统的简要介绍结束。使用本章学到的概念，您将能够设计和开发对Kubernetes集群的扩展，解锁高级使用模式。
- en: The case study in this chapter will include creating two simple CRDs to support
    an example application. We'll begin with CRDs, which will give you a good base
    understanding of how extensions can build on the Kubernetes API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的案例研究将包括创建两个简单的CRD来支持一个示例应用程序。我们将从CRD开始，这将让您对扩展如何构建在Kubernetes API上有一个良好的基础理解。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: How to extend Kubernetes with **Custom Resource Definitions** (**CRDs**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用**自定义资源定义**（**CRD**）扩展Kubernetes
- en: Self-managing functionality with Kubernetes operators
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes运算符进行自管理功能
- en: Using cloud-specific Kubernetes extensions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定于云的Kubernetes扩展
- en: Integrating with the ecosystem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与生态系统集成
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，您需要一台支持`kubectl`命令行工具的计算机，以及一个正常运行的Kubernetes集群。请参阅[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*，了解快速启动和运行Kubernetes的几种方法，以及如何安装`kubectl`工具。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在书籍的GitHub存储库中找到，网址为[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter13)。
- en: How to extend Kubernetes with custom resource definitions
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用自定义资源定义扩展Kubernetes
- en: Let's start with the basics. What is a CRD? We know that Kubernetes has an API
    model where we can perform operations against resources. Some examples of Kubernetes
    resources (which you should be well acquainted with by now) are Pods, PersistentVolumes,
    Secrets, and others.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础知识开始。什么是CRD？我们知道Kubernetes有一个API模型，我们可以对资源执行操作。一些Kubernetes资源的例子（现在你应该对它们非常熟悉）是Pods、PersistentVolumes、Secrets等。
- en: Now, what if we want to implement some custom functionality in our cluster,
    write our own controllers, and store the state of our controllers somewhere? We
    could, of course, store the state of our custom functionality in a SQL or NoSQL
    database running on Kubernetes or elsewhere (which is actually one of the strategies
    for extending Kubernetes) – but what if our custom functionality acts more as
    an extension of Kubernetes functionality, instead of a completely separate application?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想在集群中实现一些自定义功能，编写我们自己的控制器，并将控制器的状态存储在某个地方，我们可以，当然，将我们自定义功能的状态存储在Kubernetes或其他地方运行的SQL或NoSQL数据库中（这实际上是扩展Kubernetes的策略之一）-但是如果我们的自定义功能更像是Kubernetes功能的扩展，而不是完全独立的应用程序呢？
- en: 'In cases like this, we have two options:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有两个选择：
- en: Custom resource definitions
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义资源定义
- en: API aggregation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API聚合
- en: API aggregation allows advanced users to build their own resource APIs outside
    of the Kubernetes API server and use their own storage – and then aggregate those
    resources at the API layer so they can be queried using the Kubernetes API. This
    is obviously highly extensible and is essentially just using the Kubernetes API
    as a proxy to your own custom functionality, which may or may not actually integrate
    with Kubernetes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: API聚合允许高级用户在Kubernetes API服务器之外构建自己的资源API，并使用自己的存储，然后在API层聚合这些资源，以便可以使用Kubernetes
    API进行查询。这显然是非常可扩展的，实质上只是使用Kubernetes API作为代理来使用您自己的自定义功能，这可能实际上与Kubernetes集成，也可能不会。
- en: The other option is CRDs, where we can use the Kubernetes API and underlying
    data store (`etcd`) instead of building our own. We can use the `kubectl` and
    `kube api` methods that we know to interact with our own custom functionality.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是CRDs，我们可以使用Kubernetes API和底层数据存储（`etcd`）而不是构建我们自己的。我们可以使用我们知道的`kubectl`和`kube
    api`方法与我们自己的自定义功能进行交互。
- en: In this book, we will not discuss API aggregation. While definitely more flexible
    than CRDs, this is an advanced topic that deserves a thorough understanding of
    the Kubernetes API and a thorough perusal of the Kubernetes documentation to do
    it right. You can learn more about API aggregation in the Kubernetes documentation
    at [https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们不会讨论API聚合。虽然比CRDs更灵活，但这是一个高级主题，需要对Kubernetes API有深入的了解，并仔细阅读Kubernetes文档以正确实施。您可以在Kubernetes文档中了解更多关于API聚合的信息[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)。
- en: So, now that we know that we are using the Kubernetes control plane as our own
    stateful store for our new custom functionality, we need a schema. Similar to
    how the Pod resource spec in Kubernetes expects certain fields and configurations,
    we can tell Kubernetes what we expect for our new custom resources. Let's go through
    the spec for a CRD now.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们知道我们正在使用Kubernetes控制平面作为我们自己的有状态存储来存储我们的新自定义功能，我们需要一个模式。类似于Kubernetes中Pod资源规范期望特定字段和配置，我们可以告诉Kubernetes我们对新的自定义资源期望什么。现在让我们来看一下CRD的规范。
- en: Writing a custom resource definition
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写自定义资源定义
- en: For CRDs, Kubernetes uses the OpenAPI V3 specification. For more information
    on OpenAPI V3, you can check the official documentation at [https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md),
    but we'll soon see how exactly this translates into Kubernetes CRD definitions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CRDs，Kubernetes使用OpenAPI V3规范。有关OpenAPI V3的更多信息，您可以查看官方文档[https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md)，但我们很快将看到这如何转化为Kubernetes
    CRD定义。
- en: Let's take a look at an example CRD spec. Now let's be clear, this is not how
    YAMLs of any specific record of this CRD would look. Instead, this is simply where
    we define the requirements for the CRD inside of Kubernetes. Once created, Kubernetes
    will accept resources matching the spec and we can start making our own records
    of this type.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个CRD规范的示例。现在让我们明确一点，这不是任何特定记录的YAML的样子。相反，这只是我们在Kubernetes内部定义CRD的要求的地方。一旦创建，Kubernetes将接受与规范匹配的资源，我们就可以开始制作我们自己的这种类型的记录。
- en: 'Here''s an example YAML for a CRD spec, which we are calling `delayedjob`.
    This highly simplistic CRD is intended to start a container image job on a delay,
    which prevents users from having to script in a delayed start for their container.
    This CRD is quite brittle, and we don''t recommend anyone actually use it, but
    it does well to highlight the process of building a CRD. Let''s start with a full
    CRD spec YAML, then break it down:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个CRD规范的示例YAML，我们称之为`delayedjob`。这个非常简单的CRD旨在延迟启动容器镜像作业，这样用户就不必为他们的容器编写延迟启动的脚本。这个CRD非常脆弱，我们不建议任何人真正使用它，但它确实很好地突出了构建CRD的过程。让我们从一个完整的CRD规范YAML开始，然后分解它：
- en: Custom-resource-definition-1.yaml
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源定义-1.yaml
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's review the parts of this file. At first glance, it looks like your typical
    Kubernetes YAML spec – and that's because it is! In the `apiVersion` field, we
    have `apiextensions.k8s.io/v1`, which is the standard since Kubernetes `1.16`
    (before then it was `apiextensions.k8s.io/v1beta1`). Our `kind` will always be
    `CustomResourceDefinition`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来审视一下这个文件的部分。乍一看，它看起来像是您典型的Kubernetes YAML规范 - 因为它就是！在`apiVersion`字段中，我们有`apiextensions.k8s.io/v1`，这是自Kubernetes
    `1.16`以来的标准（在那之前是`apiextensions.k8s.io/v1beta1`）。我们的`kind`将始终是`CustomResourceDefinition`。
- en: The `metadata` field is when things start to get specific to our resource. We
    need to structure the `name` metadata field as the `plural` form of our resource,
    then a period, then its group. Let's take a quick diversion from our YAML file
    to discuss how groups work in the Kubernetes API.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`metadata`字段是当事情开始变得特定于我们的资源时。我们需要将`name`元数据字段结构化为我们资源的`复数`形式，然后是一个句号，然后是它的组。让我们从我们的YAML文件中快速偏离一下，讨论一下Kubernetes
    API中组的工作原理。'
- en: Understanding Kubernetes API groups
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解Kubernetes API组
- en: Groups are a way that Kubernetes segments resources in its API. Each group corresponds
    to a different subpath of the Kubernetes API server.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 组是Kubernetes在其API中分割资源的一种方式。每个组对应于Kubernetes API服务器的不同子路径。
- en: 'By default, there is a legacy group called the core group – which corresponds
    to resources accessed on the `/api/v1` endpoint in the Kubernetes REST API. By
    extension, these legacy group resources have `apiVersion: v1` in their YAML specs.
    An example of one of the resources in the core group is the Pod.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '默认情况下，有一个名为核心组的遗留组 - 它对应于在Kubernetes REST API的`/api/v1`端点上访问的资源。因此，这些遗留组资源在其YAML规范中具有`apiVersion:
    v1`。核心组中资源的一个例子是Pod。'
- en: Next, there is the set of named groups – which correspond to resources that
    can be accessed on `REST` URLs formed as `/apis/<GROUP NAME>/<VERSION>`. These
    named groups form the bulk of Kubernetes resources. However, the oldest and most
    basic resources, such as the Pod, Service, Secret, and Volume, are in the core
    group. An example of a resource that is in a named group is the `StorageClass`
    resource, which is in the `storage.k8s.io` group.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，有一组命名的组 - 这些组对应于可以在`REST` URL上访问的资源，形式为`/apis/<GROUP NAME>/<VERSION>`。这些命名的组构成了Kubernetes资源的大部分。然而，最古老和最基本的资源，如Pod、Service、Secret和Volume，都在核心组中。一个在命名组中的资源的例子是`StorageClass`资源，它在`storage.k8s.io`组中。
- en: Important note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: To see which resource is in which group, you can check the official Kubernetes
    API docs for whatever version of Kubernetes you are using. For example, the version
    `1.18` docs would be at [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看哪个资源属于哪个组，您可以查看您正在使用的Kubernetes版本的官方Kubernetes API文档。例如，版本`1.18`的文档将位于[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18)。
- en: CRDs can specify their own named group, which means that the specific CRD will
    be available on a `REST` endpoint that the Kubernetes API server can listen on.
    With that in mind, let's get back to our YAML file, so we can talk about the main
    portion of the CRD – the versions spec.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CRD可以指定自己的命名组，这意味着特定的CRD将在Kubernetes API服务器可以监听的`REST`端点上可用。考虑到这一点，让我们回到我们的YAML文件，这样我们就可以讨论CRD的主要部分-版本规范。
- en: Understanding custom resource definition versions
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自定义资源定义版本
- en: As you can see, we have chosen the group `delayedresources.mydomain.com`. This
    group would theoretically hold any other CRDs of the delayed kind – for instance,
    `DelayedDaemonSet` or `DelayedDeployment`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们选择了组`delayedresources.mydomain.com`。该组理论上将包含任何其他延迟类型的CRD-例如，`DelayedDaemonSet`或`DelayedDeployment`。
- en: Next, we have the main portion of our CRD. Under `versions`, we can define one
    or more CRD versions (in the `name` field), along with the API specification for
    that version of the CRD. Then, when you create an instance of your CRD, you can
    define which version you will be using for the version parameter in the `apiVersion`
    key of your YAML – for instance, `apps/v1`, or in this case, `delayedresources.mydomain.com/v1`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有我们的CRD的主要部分。在`versions`下，我们可以定义一个或多个CRD版本（在`name`字段中），以及该CRD版本的API规范。然后，当您创建CRD的实例时，您可以在YAML文件的`apiVersion`键的版本参数中定义您将使用的版本-例如，`apps/v1`，或在这种情况下，`delayedresources.mydomain.com/v1`。
- en: Each version item also has a `served` attribute, which is essentially a way
    to define whether the given version is enabled or disabled. If `served` is `false`,
    the version will not be created by the Kubernetes API, and the API requests (or
    `kubectl` commands) for that version will fail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个版本项还有一个`served`属性，这实质上是一种定义给定版本是否启用或禁用的方式。如果`served`为`false`，则该版本将不会被Kubernetes
    API创建，并且该版本的API请求（或`kubectl`命令）将失败。
- en: 'In addition, it is possible to define a `deprecated` key on a specific version,
    which will cause Kubernetes to return a warning message when requests are made
    to the API using the deprecated version. This is how a CRD. `yaml` file with a
    deprecated version looks – we have removed some of the spec to keep the YAML short:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以在特定版本上定义一个`deprecated`键，这将导致Kubernetes在使用弃用版本进行API请求时返回警告消息。这就是带有弃用版本的CRD的`yaml`文件的样子-我们已删除了一些规范，以使YAML文件简短：
- en: Custom-resource-definition-2.yaml
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源定义-2.yaml
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, we have marked `v1` as deprecated, and also include a deprecation
    warning for Kubernetes to send as a response. If we do not include a deprecation
    warning, a default message will be used.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们已将`v1`标记为已弃用，并且还包括一个弃用警告，以便Kubernetes作为响应发送。如果我们不包括弃用警告，将使用默认消息。
- en: Moving further down, we have the `storage` key, which interacts with the `served`
    key. The reason this is necessary is that while Kubernetes supports multiple active
    (aka `served`) versions of a resource at the same time, only one of those versions
    can be stored in the control plane. However, the `served` attribute means that
    multiple versions of a resource can be served by the API. So how does that even
    work?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步向下移动，我们有`storage`键，它与`served`键交互。这是必要的原因是，虽然Kubernetes支持同时拥有多个活动（也就是`served`）版本的资源，但是只能有一个版本存储在控制平面中。然而，`served`属性意味着API可以提供多个版本的资源。那么这是如何工作的呢？
- en: The answer is that Kubernetes will convert the CRD object from whatever the
    stored version is to the version you ask for (or vice versa, when creating a resource).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，Kubernetes将把CRD对象从存储的版本转换为您要求的版本（或者反过来，在创建资源时）。
- en: How is this conversion handled? Let's skip past the rest of the version attributes
    to the `conversion` key to see how.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换是如何处理的？让我们跳过其余的版本属性，看看`conversion`键是如何工作的。
- en: The `conversion` key lets you specify a strategy for how Kubernetes will convert
    CRD objects between whatever your served version is and whatever the stored version
    is. If the two versions are the same – for instance, if you ask for a `v1` resource
    and the stored version is `v1`, then no conversion will happen.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`conversion`键允许您指定Kubernetes将如何在您的服务版本和存储版本之间转换CRD对象的策略。如果两个版本相同-例如，如果您请求一个`v1`资源，而存储的版本是`v1`，那么不会发生转换。'
- en: The default value here as of Kubernetes 1.13 is `none`. With the `none` setting,
    Kubernetes will not do any conversion between fields. It will simply include the
    fields that are supposed to be present on the `served` (or stored, if creating
    a resource) version.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 截至Kubernetes 1.13的默认值是`none`。使用`none`设置，Kubernetes不会在字段之间进行任何转换。它只会包括应该出现在`served`（或存储，如果创建资源）版本上的字段。
- en: 'The other possible conversion strategy is `Webhook`, which allows you to define
    a custom webhook that will take in one version and do the proper conversion to
    your intended version. Here is an example of our CRD with a `Webhook` conversion
    strategy – we''ve cut out some of the version schema for conciseness:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的转换策略是`Webhook`，它允许您定义一个自定义Webhook，该Webhook将接收一个版本并对其进行适当的转换为您想要的版本。这里有一个使用`Webhook`转换策略的CRD示例-为了简洁起见，我们省略了一些版本模式：
- en: Custom-resource-definition-3.yaml
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Custom-resource-definition-3.yaml
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, the `Webhook` strategy lets us define a URL that requests will
    be made to with information about the incoming resource object, its current version,
    and the version it needs to be converted to.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`Webhook`策略让我们定义一个URL，请求将发送到该URL，其中包含有关传入资源对象、其当前版本和需要转换为的版本的信息。
- en: The idea is that our `Webhook` server will then handle the conversion and pass
    back the corrected Kubernetes resource object. The `Webhook` strategy is complex
    and can have many possible configurations, which we will not get into in depth
    in this book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是我们的`Webhook`服务器将处理转换并传回修正后的Kubernetes资源对象。`Webhook`策略是复杂的，可以有许多可能的配置，我们在本书中不会深入讨论。
- en: Important note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To see how conversion Webhooks can be configured, check the official Kubernetes
    documentation at [https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何配置转换Webhooks，请查看官方Kubernetes文档[https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/)。
- en: Now, back to our `version` entry in the YAML! Under the `served` and `storage`
    keys, we see the `schema` object, which contains the actual specification of our
    resource. As previously mentioned, this follows the OpenAPI Spec v3 schema.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到我们在YAML中的`version`条目！在`served`和`storage`键下，我们看到`schema`对象，其中包含我们资源的实际规范。如前所述，这遵循OpenAPI
    Spec v3模式。
- en: 'The `schema` object, which was removed from the preceding code block for space
    reasons, is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`schema`对象，由于空间原因已从前面的代码块中删除，如下所示：'
- en: Custom-resource-definition-3.yaml (continued)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源定义-3.yaml（续）
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, we support a field for `delaySeconds`, which will be an integer,
    and `image`, which is a string that corresponds to our container image. If we
    really wanted to make the `DelayedJob` production-ready, we would want to include
    all sorts of other options to make it closer to the original Kubernetes Job resource
    – but that isn't our intent here.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们支持`delaySeconds`字段，它将是一个整数，以及`image`，它是一个与我们的容器映像相对应的字符串。如果我们真的想要使`DelayedJob`达到生产就绪状态，我们会希望包括各种其他选项，使其更接近原始的Kubernetes
    Job资源 - 但这不是我们的意图。
- en: Moving further back in the original code block, outside the versions list, we
    see some other attributes. First is the `scope` attribute, which can be either
    `Cluster` or `Namespaced`. This tells Kubernetes whether to treat instances of
    the CRD object as namespace-specific resources (such as Pods, Deployments, and
    so on) or instead as cluster-wide resources – like namespaces themselves, since
    getting namespace objects within a namespace doesn't make any sense!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始代码块中进一步向后移动，超出版本列表，我们看到一些其他属性。首先是`scope`属性，可以是`Cluster`或`Namespaced`。这告诉Kubernetes是否将CRD对象的实例视为特定于命名空间的资源（例如Pods，Deployments等），还是作为集群范围的资源
    - 就像命名空间本身一样，因为在命名空间中获取命名空间对象是没有意义的！
- en: Finally, we have the `names` block, which lets you define both a plural and
    singular form of your resource name, to be used in various situations (for instance,
    `kubectl get pods` and `kubectl get pod` both work).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`names`块，它允许您定义资源名称的复数和单数形式，以在各种情况下使用（例如，`kubectl get pods`和`kubectl get
    pod`都可以工作）。
- en: The `names` block also lets you define the camel-cased `kind` value, which will
    be used in the resource YAML, as well as one or more `shortNames`, which can be
    used to refer to the resource in the API or `kubectl` – for instance, `kubectl
    get po`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`names`块还允许您定义驼峰命名的`kind`值，在资源YAML中将使用该值，以及一个或多个`shortNames`，可以用来在API或`kubectl`中引用该资源
    - 例如，`kubectl get po`。'
- en: 'With our CRD specification YAML explained, let''s take a look at an instance
    of our CRD – as defined by the spec we just reviewed, the YAML will look like
    this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 解释了我们的CRD规范YAML后，让我们来看一下我们CRD的一个实例 - 正如我们刚刚审查的规范所定义的，YAML将如下所示：
- en: Delayed-job.yaml
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Delayed-job.yaml
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, this is just like our CRD defined this object. Now, with all
    our pieces in place, let's test out our CRD!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这就像我们的CRD定义了这个对象。现在，所有的部分都就位了，让我们测试一下我们的CRD！
- en: Testing a custom resource definition
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试自定义资源定义
- en: 'Let''s go ahead and test out our CRD concept on Kubernetes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续在Kubernetes上测试我们的CRD概念：
- en: 'First, let''s create the CRD spec in Kubernetes – the same way we would create
    any other object:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们在Kubernetes中创建CRD规范 - 就像我们创建任何其他对象一样：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will result in the following output:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, Kubernetes will accept requests for our `DelayedJob` resource. We can
    test this out by finally creating one using the preceding resource YAML:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，Kubernetes将接受对我们的`DelayedJob`资源的请求。我们可以通过最终使用前面的资源YAML创建一个来测试这一点：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we''ve defined our CRD properly, we will see the following output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正确定义了我们的CRD，我们将看到以下输出：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the Kubernetes API server has successfully created our instance
    of `DelayedJob`!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Kubernetes API服务器已成功创建了我们的`DelayedJob`实例！
- en: Now, you may be asking a very relevant question – now what? This is an excellent
    question, because the truth is that we have accomplished nothing more so far than
    essentially adding a new `table` to the Kubernetes API database.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能会问一个非常相关的问题 - 现在怎么办？这是一个很好的问题，因为事实上，到目前为止，我们实际上什么也没有做，只是向Kubernetes API数据库添加了一个新的`表`。
- en: Just because we gave our `DelayedJob` resource an application image and a `delaySeconds`
    field does not mean that any functionality like what we intend will actually occur.
    By creating our instance of `DelayedJob`, we have just added an entry to that
    `table`. We can fetch it, edit it, or delete it using the Kubernetes API or `kubectl`
    commands, but no application functionality has been implemented.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为我们给我们的`DelayedJob`资源一个应用程序镜像和一个`delaySeconds`字段，并不意味着我们打算的任何功能实际上会发生。通过创建我们的`DelayedJob`实例，我们只是向那个`表`添加了一个条目。我们可以使用Kubernetes
    API或`kubectl`命令获取它，编辑它或删除它，但没有实现任何应用功能。
- en: In order to actually get our `DelayedJob` resource to do something, we need
    a custom controller that will take our instance of `DelayedJob` and do something
    with it. In the end, we still need to implement actual container functionality
    using the official Kubernetes resources – Pods et al.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的`DelayedJob`资源真正做些什么，我们需要一个自定义控制器，它将获取我们的`DelayedJob`实例并对其进行操作。最终，我们仍然需要使用官方Kubernetes资源（如Pods等）来实现实际的容器功能。
- en: This is what we're going to discuss now. There are many ways to build custom
    controllers for Kubernetes, but a popular way is the **Operator pattern**. Let's
    move onto the next section to see how we can give our `DelayedJob` resource a
    life of its own.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们现在要讨论的。有许多构建Kubernetes自定义控制器的方法，但流行的方法是**运算符模式**。让我们继续下一节，看看我们如何让我们的`DelayedJob`资源拥有自己的生命。
- en: Self-managing functionality with Kubernetes operators
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kubernetes运算符自管理功能
- en: No discussion of Kubernetes operators would be possible without first discussing
    the **Operator Framework**. A common misconception is that operators are specifically
    built via the Operator Framework. The Operator Framework is an open source framework
    originally created by Red Hat to make it easy to write Kubernetes operators.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有首先讨论**Operator Framework**之前，不可能讨论Kubernetes运算符。一个常见的误解是，运算符是通过Operator Framework专门构建的。Operator
    Framework是一个开源框架，最初由Red Hat创建，旨在简化编写Kubernetes运算符。
- en: In reality, an operator is simply a custom controller that interfaces with Kubernetes
    and acts on resources. The Operator Framework is one opinionated way to make Kubernetes
    operators, but there are many other open source frameworks you can use – or, you
    can make one from scratch!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，运算符只是一个与Kubernetes接口并对资源进行操作的自定义控制器。Operator Framework是一种使Kubernetes运算符的一种偏见方式，但还有许多其他开源框架可以使用
    - 或者，您可以从头开始制作一个！
- en: When building an operator using frameworks, two of the most popular options
    are the aforementioned **Operator Framework** and **Kubebuilder**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用框架构建运算符时，最流行的两个选项是前面提到的**Operator Framework**和**Kubebuilder**。
- en: Both of these projects have a lot in common. They both make use of `controller-tools`
    and `controller-runtime`, which are two libraries for building Kubernetes controllers
    that are officially supported by the Kubernetes project. If you are building an
    operator from scratch, using these officially supported controller libraries will
    make things much easier.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个项目有很多共同之处。它们都使用`controller-tools`和`controller-runtime`，这是两个由Kubernetes项目官方支持的构建Kubernetes控制器的库。如果您从头开始构建运算符，使用这些官方支持的控制器库将使事情变得更容易。
- en: Unlike the Operator Framework, Kubebuilder is an official part of the Kubernetes
    project, much like the `controller-tools` and `controller-runtime` libraries –
    but both projects have their pros and cons. Importantly, both these options, and
    the Operator pattern in general, have the controller running on the cluster. It
    may seem obvious that this is the best option, but you could run your controller
    outside of the cluster and have it work the same. To get started with the Operator
    Framework, check the official GitHub at [https://github.com/operator-framework](https://github.com/operator-framework).
    For Kubebuilder, you can check [https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与Operator Framework不同，Kubebuilder是Kubernetes项目的官方部分，就像`controller-tools`和`controller-runtime`库一样
    - 但这两个项目都有其优缺点。重要的是，这两个选项以及一般的Operator模式都是在集群上运行控制器。这似乎是最好的选择，但你也可以在集群外运行控制器，并且它可以正常工作。要开始使用Operator
    Framework，请查看官方GitHub网站[https://github.com/operator-framework](https://github.com/operator-framework)。对于Kubebuilder，你可以查看[https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder)。
- en: Most operators, regardless of the framework, follow a control-loop paradigm
    – let's see how this idea works.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数操作员，无论使用哪种框架，都遵循控制循环范式 - 让我们看看这个想法是如何工作的。
- en: Mapping the operator control loop
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射操作员控制循环
- en: A control loop is a control scheme in system design and programming that consists
    of a never-ending loop of logical processes. Typically, a control loop implements
    a measure-analyze-adjust approach, where it measures the current state of the
    system, analyzes what changes are required to bring it in line with the intended
    state, and then adjusts the system components to bring it in line with (or at
    least closer to) the intended state.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 控制循环是系统设计和编程中的控制方案，由一系列逻辑过程组成的永无止境的循环。通常，控制循环实现了一种测量-分析-调整的方法，它测量系统的当前状态，分析需要做出哪些改变使其与预期状态一致，然后调整系统组件使其与预期状态一致（或至少更接近预期状态）。
- en: 'In Kubernetes operators or controllers specifically, this operation usually
    works like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的操作员或控制器中，这个操作通常是这样工作的：
- en: First, a `watch` step – that is, watching the Kubernetes API for changes in
    the intended state, which is stored in `etcd`.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是一个“监视”步骤 - 也就是监视Kubernetes API中预期状态的变化，这些状态存储在`etcd`中。
- en: Then, an `analyze` step – which is the controller deciding what to do to bring
    the cluster state in line with the intended state.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后是一个“分析”步骤 - 控制器决定如何使集群状态与预期状态一致。
- en: And lastly, an `update` step – which is updating the cluster state to fulfill
    the intent of the cluster changes.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后是一个“更新”步骤 - 更新集群状态以实现集群变化的意图。
- en: 'To help understand the control loop, here is a diagram showing how the pieces
    fit together:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解控制循环，这里有一个图表显示了这些部分是如何组合在一起的：
- en: '![Figure 13.1 – Measure Analyze Update Loop](image/B14790_13_01.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 - 测量分析更新循环](image/B14790_13_01.jpg)'
- en: Figure 13.1 – Measure Analyze Update Loop
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 - 测量分析更新循环
- en: 'Let''s use the Kubernetes scheduler – which is itself a control loop process
    – to illustrate this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Kubernetes调度器来说明这一点 - 它本身就是一个控制循环过程：
- en: 'Let''s start with a hypothetical cluster in a steady state: all Pods are scheduled,
    Nodes are healthy, and everything is operating normally.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从一个假设的集群开始，处于稳定状态：所有的Pod都已经调度，节点也健康，一切都在正常运行。
- en: Then, a user creates a new Pod.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，用户创建了一个新的Pod。
- en: 'We''ve discussed before that the kubelet works on a `pull` basis. This means
    that when a kubelet creates a Pod on its Node, that Pod was already assigned to
    that Node via the scheduler. However, when Pods are first created via a `kubectl
    create` or `kubectl apply` command, the Pod isn''t scheduled or assigned anywhere.
    This is where our scheduler control loop starts:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过kubelet是基于`pull`的工作方式。这意味着当kubelet在其节点上创建一个Pod时，该Pod已经通过调度器分配给了该节点。然而，当通过`kubectl
    create`或`kubectl apply`命令首次创建Pod时，该Pod尚未被调度或分配到任何地方。这就是我们的调度器控制循环开始的地方：
- en: The first step is **Measure**, where the scheduler reads the state of the Kubernetes
    API. When listing Pods from the API, it discovers that one of the Pods is not
    assigned to a Node. It now moves to the next step.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是**测量**，调度器从Kubernetes API读取状态。当从API列出Pod时，它发现其中一个Pod未分配给任何节点。现在它转移到下一步。
- en: Next, the scheduler performs an analysis of the cluster state and Pod requirements
    in order to decide which Node the Pod should be assigned to. As we discussed in
    previous chapters, this takes into account Pod resource limits and requests, Node
    statuses, placement controls, and so on, which makes it a fairly complex process.
    Once this processing is complete, the update step can start.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，调度器对集群状态和Pod需求进行分析，以决定将Pod分配给哪个节点。正如我们在前几章中讨论的那样，这涉及到Pod资源限制和请求、节点状态、放置控制等等，这使得它成为一个相当复杂的过程。一旦处理完成，更新步骤就可以开始了。
- en: Finally, **Update** – the scheduler updates the cluster state by assigning the
    Pod to the Node obtained from the *step 2* analysis. At this point, the kubelet
    takes over on its own control loop and creates the relevant container(s) for the
    Pod on its Node.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，**更新** - 调度器通过将Pod分配给从*步骤2*分析中获得的节点来更新集群状态。此时，kubelet接管自己的控制循环，并为其节点上的Pod创建相关的容器。
- en: Next, let's take what we learned from the scheduler control loop and apply it
    to our very own `DelayedJob` resource.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将从调度器控制循环中学到的内容应用到我们自己的`DelayedJob`资源上。
- en: Designing an operator for a custom resource definition
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为自定义资源定义设计运算符
- en: Actually, coding an operator for our `DelayedJob` CRD is outside the scope of
    our book since it requires knowledge of a programming language. If you're choosing
    a programming language to build an operator with, Go offers the most interoperability
    with the Kubernetes SDK, **controller-tools**, and **controller-runtime**, but
    any programming language where you can write HTTP requests will work, since that
    is the basis for all of the SDKs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，为我们的`DelayedJob` CRD编写运算符超出了我们书的范围，因为这需要对编程语言有所了解。如果您选择使用Go构建运算符，它提供了与Kubernetes
    SDK、**controller-tools**和**controller-runtime**最多的互操作性，但任何可以编写HTTP请求的编程语言都可以使用，因为这是所有SDK的基础。
- en: However, we will still walk through the steps of implementing an operator for
    our `DelayedJob` CRD with some pseudocode. Let's take it step by step.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍将逐步实现`DelayedJob` CRD的运算符步骤，使用一些伪代码。让我们一步一步来。
- en: 'Step 1: Measure'
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤1：测量
- en: First comes the **Measure** step, which we will implement in our pseudocode
    as a `while` loop that runs forever. In a production implementation, there would
    be debouncing, error handling, and a bunch of other concerns, but we'll keep it
    simple for this illustrative example.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是**测量**步骤，我们将在我们的伪代码中实现为一个永远运行的`while`循环。在生产实现中，会有去抖动、错误处理和一堆其他问题，但是对于这个说明性的例子，我们会保持简单。
- en: 'Take a look at the pseudo code for this loop, which is essentially the main
    function of our application:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下这个循环的伪代码，这实际上是我们应用程序的主要功能：
- en: Main-function.pseudo
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Main-function.pseudo
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, the loop in our `main` function calls the Kubernetes API to
    find a list of the `delayedjobs` CRDs stored in `etcd`. This is the `measure`
    step. It then calls the analysis step, and with the results of that, calls the
    update step to schedule any `DelayedJobs` that need to be scheduled.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，在我们的`main`函数中的循环调用Kubernetes API来查找存储在`etcd`中的`delayedjobs` CRD列表。这是`measure`步骤。然后调用分析步骤，并根据其结果调用更新步骤来安排需要安排的任何`DelayedJobs`。
- en: Important note
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Keep in mind that the Kubernetes scheduler is still going to do the actual container
    scheduling in this example – but we need to boil down our `DelayedJob` into an
    official Kubernetes resource first.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个例子中，Kubernetes调度程序仍然会执行实际的容器调度 - 但是我们首先需要将我们的`DelayedJob`简化为官方的Kubernetes资源。
- en: After the update step, our loop waits for a full 5 seconds before performing
    the loop again. This sets the cadence of the control loop. Next, let's move on
    to the analysis step.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新步骤之后，我们的循环在执行循环之前等待完整的5秒。这确定了控制循环的节奏。接下来，让我们继续进行分析步骤。
- en: 'Step 2: Analyze'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤2：分析
- en: 'Next, let''s review the **Analysis** step of our operator, which is the `analyzeDelayedJobs`
    function in our controller pseudocode:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来审查我们操作员的**Analysis**步骤，这是我们控制器伪代码中的`analyzeDelayedJobs`函数：
- en: Analysis-function.pseudo
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 分析函数伪代码
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the preceding function loops through the list of `DelayedJob`
    objects from the cluster as passed from the **Measure** loop. It then checks to
    see if the `DelayedJob` has been scheduled yet by checking the value of one of
    the object's annotations. If it hasn't been scheduled yet, it adds an object to
    an array called `listOfJobsToSchedule`, which contains the image specified in
    the `DelayedJob` object, a command to sleep for the number of seconds that was
    specified in the `DelayedJob` object, and the original name of the `DelayedJob`,
    which we will use to mark as scheduled in the **Update** step.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，前面的函数循环遍历了从**Measure**循环传递的集群中的`DelayedJob`对象列表。然后，它检查`DelayedJob`是否已经通过检查对象的注释之一的值来进行了调度。如果尚未安排，它将向名为`listOfJobsToSchedule`的数组添加一个对象，该数组包含`DelayedJob`对象中指定的图像，一个命令以睡眠指定的秒数，以及`DelayedJob`的原始名称，我们将在**Update**步骤中用来标记为已调度。
- en: Finally, in the **Analyze** step the `analyzeDelayedJobs` function returns our
    newly created `listOfJobsToSchedule` array back to the main function. Let's wrap
    up our Operator design with the final update step, which is the `scheduleDelayedJobs`
    function in our main loop.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在**Analyze**步骤中，`analyzeDelayedJobs`函数将我们新创建的`listOfJobsToSchedule`数组返回给主函数。让我们用最终的更新步骤来结束我们的操作员设计，这是我们主循环中的`scheduleDelayedJobs`函数。
- en: 'Step 3: Update'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤3：更新
- en: 'Finally, the **Update** part of our control loop will take the outputs from
    our analysis and update the cluster as necessary to create the intended state.
    Here''s the pseudocode:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的控制循环的**Update**部分将从我们的分析中获取输出，并根据需要更新集群以创建预期的状态。以下是伪代码：
- en: Update-function.pseudo
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 更新函数伪代码
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this case, we are taking our regular Kubernetes object, which was derived
    from our `DelayedJob` object, and creating it in Kubernetes so the `Kube` scheduler
    can pick up on it, create the relevant Pod, and manage it. Once we create the
    regular Job object with the delay, we also update our `DelayedJob` CRD instance
    with an annotation that sets the `is-scheduled` annotation to `true`, preventing
    it from getting rescheduled.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们正在使用从我们的`DelayedJob`对象派生的常规Kubernetes对象，并在Kubernetes中创建它，以便`Kube`调度程序可以找到它，创建相关的Pod并管理它。一旦我们使用延迟创建了常规作业对象，我们还会使用注释更新我们的`DelayedJob`
    CRD实例，将`is-scheduled`注释设置为`true`，以防止它被重新调度。
- en: This completes our control loop – from this point, the `Kube` scheduler takes
    over and our CRD is given life as a Kubernetes Job object, which controls a Pod,
    which is finally assigned to a Node and a container is scheduled to run our code!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的控制循环 - 从这一点开始，`Kube`调度器接管并且我们的CRD被赋予生命作为一个Kubernetes Job对象，它控制一个Pod，最终分配给一个Node，并且一个容器被调度来运行我们的代码！
- en: This example is of course highly simplified, but you would be surprised how
    many Kubernetes operators perform a simple control loop to coordinate CRDs and
    boil them down to basic Kubernetes resources. Operators can get very complicated
    and perform application-specific functions such as backing up databases, emptying
    Persistent Volumes, and others – but this functionality is usually tightly coupled
    to whatever is being controlled.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个例子是高度简化的，但你会惊讶地发现有多少Kubernetes操作员执行一个简单的控制循环来协调CRD并将其简化为基本的Kubernetes资源。操作员可以变得非常复杂，并执行特定于应用程序的功能，例如备份数据库、清空持久卷等，但这种功能通常与被控制的内容紧密耦合。
- en: Now that we've discussed the Operator pattern in a Kubernetes controller, we
    can talk about some of the open source options for cloud-specific Kubernetes controllers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了Kubernetes控制器中的操作员模式，我们可以谈谈一些特定于云的Kubernetes控制器的开源选项。
- en: Using cloud-specific Kubernetes extensions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特定于云的Kubernetes扩展
- en: Usually available by default in managed Kubernetes services such as Amazon EKS,
    Azure AKS, and Google Cloud's GKE, cloud-specific Kubernetes extensions and controllers
    can integrate tightly with the cloud platform in question and make it easy to
    control other cloud resources from Kubernetes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在托管的Kubernetes服务（如Amazon EKS、Azure AKS和Google Cloud的GKE）中默认可用，特定于云的Kubernetes扩展和控制器可以与相关的云平台紧密集成，并且可以轻松地从Kubernetes控制其他云资源。
- en: Even without adding any additional third-party components, a lot of this cloud-specific
    functionality is available in upstream Kubernetes via the **cloud-controller-manager**
    (**CCM**) component, which contains many options for integrating with the major
    cloud providers. This is the functionality that is usually enabled by default
    in the managed Kubernetes services on each public cloud – but they can be integrated
    with any cluster running on that specific cloud platform, managed or not.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 即使不添加任何额外的第三方组件，许多这些特定于云的功能都可以通过**云控制器管理器**（**CCM**）组件在上游Kubernetes中使用，该组件包含许多与主要云提供商集成的选项。这通常是在每个公共云上的托管Kubernetes服务中默认启用的功能，但它们可以与在特定云平台上运行的任何集群集成，无论是托管还是非托管。
- en: In this section, we will review a few of the more common cloud extensions to
    Kubernetes, both in **cloud-controller-manager (CCM)** and functionality that
    requires the installation of other controllers such as **external-dns** and **cluster-autoscaler**.
    Let's start with some of the heavily used CCM functionality.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一些常见的云扩展到Kubernetes中，包括**云控制器管理器（CCM）**和需要安装其他控制器的功能，例如**external-dns**和**cluster-autoscaler**。让我们从一些常用的CCM功能开始。
- en: Understanding the cloud-controller-manager component
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解云控制器管理器组件
- en: As reviewed in [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016),
    *Communicating with Kubernetes*, CCM is an officially supported Kubernetes controller
    that provides hooks into the functionality of several public cloud services. To
    function, the CCM component needs to be started with access permissions to the
    cloud service in question – for instance, an IAM role in AWS.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)中所述，*与Kubernetes通信*，CCM是一个官方支持的Kubernetes控制器，提供了对几个公共云服务功能的钩子。为了正常运行，CCM组件需要以访问特定云服务的权限启动，例如在AWS中的IAM角色。
- en: For officially supported clouds such as AWS, Azure, and Google Cloud, CCM can
    simply be run as a DaemonSet within the cluster. We use a DaemonSet since CCM
    can perform tasks such as creating persistent storage in the cloud provider, and
    it needs to be able to attach storage to specific Nodes. If you're using a cloud
    that isn't officially supported, you can run CCM for that specific cloud, and
    you should follow the specific instructions in that project. These alternate types
    of CCM are usually open source and can be found on GitHub. For the specifics of
    installing CCM, let's move on to the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于官方支持的云，如AWS、Azure和Google Cloud，CCM可以简单地作为集群中的DaemonSet运行。我们使用DaemonSet，因为CCM可以执行诸如在云提供商中创建持久存储等任务，并且需要能够将存储附加到特定的节点。如果您使用的是官方不支持的云，您可以为该特定云运行CCM，并且应该遵循该项目中的具体说明。这些替代类型的CCM通常是开源的，可以在GitHub上找到。关于安装CCM的具体信息，让我们继续下一节。
- en: Installing cloud-controller-manager
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装cloud-controller-manager
- en: Typically, CCM is configured when the cluster is created. As mentioned in the
    previous section, managed services such as EKS, AKS, and GKE will already have
    this component enabled, but even Kops and Kubeadm expose the CCM component as
    a flag in the installation process.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在创建集群时配置CCM。如前一节所述，托管服务，如EKS、AKS和GKE，将已经启用此组件，但即使Kops和Kubeadm也将CCM组件作为安装过程中的一个标志暴露出来。
- en: Assuming you have not installed CCM any other way and plan to use one of the
    officially supported public clouds from the upstream version, you can install
    CCM as a DaemonSet.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您尚未以其他方式安装CCM并计划使用上游版本的官方支持的公共云之一，您可以将CCM安装为DaemonSet。
- en: 'First, you will need a `ServiceAccount`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要一个`ServiceAccount`：
- en: Service-account.yaml
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Service-account.yaml
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This `ServiceAccount` will be used to give the necessary access to the CCM.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`ServiceAccount`将被用来给予CCM必要的访问权限。
- en: 'Next, we''ll need a `ClusterRoleBinding`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个`ClusterRoleBinding`：
- en: Clusterrolebinding.yaml
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Clusterrolebinding.yaml
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, we need to give the `cluster-admin` role access to our CCM service
    account. The CCM will need to be able to edit Nodes, among other things.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们需要给`cluster-admin`角色访问我们的CCM服务账户。CCM将需要能够编辑节点，以及其他一些操作。
- en: Finally, we can deploy the CCM `DaemonSet` itself. You will need to fill in
    this YAML file with the proper settings for your specific cloud provider – check
    your cloud provider's documentation on Kubernetes for this information.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以部署CCM的`DaemonSet`本身。您需要使用适合您特定云提供商的正确设置填写此YAML文件-查看您云提供商关于Kubernetes的文档以获取这些信息。
- en: 'The `DaemonSet` spec is quite long, so we''ll review it in two parts. First,
    we have the template for the `DaemonSet` with the required labels and names:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`DaemonSet`规范非常长，因此我们将分两部分进行审查。首先，我们有`DaemonSet`的模板，其中包含所需的标签和名称：'
- en: Daemonset.yaml
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Daemonset.yaml
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, to match our `ServiceAccount`, we are running the CCM in the
    `kube-system` namespace. We are also labeling the `DaemonSet` with the `k8s-app`
    label to distinguish it as a Kubernetes control plane component.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，为了匹配我们的`ServiceAccount`，我们在`kube-system`命名空间中运行CCM。我们还使用`k8s-app`标签对`DaemonSet`进行标记，以将其区分为Kubernetes控制平面组件。
- en: 'Next, we have the spec of the `DaemonSet`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有`DaemonSet`的规范：
- en: Daemonset.yaml (continued)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Daemonset.yaml（续）
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, there are a couple of places in this spec that you will need
    to review your chosen cloud provider's documentation or cluster networking setup
    to find the proper values. Particularly in the networking flags such as `--cluster-cidr`
    and `--configure-cloud-routes`, where values could change based on how you have
    set up your cluster, even on a single cloud provider.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，此规范中有一些地方需要查看您选择的云提供商的文档或集群网络设置，以找到正确的值。特别是在网络标志中，例如`--cluster-cidr`和`--configure-cloud-routes`，这些值可能会根据您如何设置集群而改变，即使在单个云提供商上也是如此。
- en: Now that we have CCM running on our cluster one way or another, let's dive into
    some of the capabilities it provides.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们在集群中以某种方式运行CCM，让我们深入了解它提供的一些功能。
- en: Understanding the cloud-controller-manager capabilities
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解云控制器管理器的功能
- en: The default CCM provides capabilities in a few key areas. For starters, the
    CCM contains subsidiary controllers for Nodes, routes, and Services. Let's review
    each in turn to see what it affords us, starting with the Node/Node lifecycle
    controller.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的CCM在一些关键领域提供了功能。首先，CCM包含了节点、路由和服务的子控制器。让我们依次审查每个，看看它为我们提供了什么，从节点/节点生命周期控制器开始。
- en: The CCM Node/Node lifecycle controller
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CCM节点/节点生命周期控制器
- en: The CCM Node controller makes sure that the cluster state, as far as which Nodes
    are in the cluster, is equivalent to what is in the cloud provider's systems.
    A simple example of this is autoscaling groups in AWS. When using AWS EKS (or
    just Kubernetes on AWS EC2, though that requires additional configuration), it
    is possible to configure worker node groups in an AWS autoscaling group that will
    scale up or down depending on the CPU or memory usage of the nodes. When these
    nodes are added and initialized by the cloud provider, the CCM nodes controller
    will ensure that the cluster has a node resource for each Node presented by the
    cloud provider.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: CCM节点控制器确保集群状态，就是集群中的节点与云提供商系统中的节点是等价的。一个简单的例子是AWS中的自动扩展组。在使用AWS EKS（或者只是在AWS
    EC2上使用Kubernetes，尽管这需要额外的配置）时，可以配置AWS自动扩展组中的工作节点组，根据节点的CPU或内存使用情况进行扩展或缩减。当这些节点由云提供商添加和初始化时，CCM节点控制器将确保集群对于云提供商呈现的每个节点都有一个节点资源。
- en: Next, let's move on to the routes controller.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们转向路由控制器。
- en: The CCM routes controller
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CCM路由控制器
- en: The CCM routes controller takes care of configuring your cloud provider's networking
    settings in a way that supports a Kubernetes cluster. This can include the allocation
    of IPs and setting routes between Nodes. The services controller also handles
    networking – but the external aspect.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CCM路由控制器负责以支持Kubernetes集群的方式配置云提供商的网络设置。这可能包括分配IP和在节点之间设置路由。服务控制器也处理网络 - 但是外部方面。
- en: The CCM services controller
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CCM服务控制器
- en: The CCM services controller provides a lot of the "magic" of running Kubernetes
    on a public cloud provider. One such aspect that we reviewed in [*Chapter 5*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127),
    *Services and Ingress – Communicating with the Outside World*, is the `LoadBalancer`
    service. For instance, on a cluster configured with AWS CCM, a Service of type
    `LoadBalancer` will automatically configure a matching AWS Load Balancer resource,
    providing an easy way to expose services in your cluster without dealing with
    `NodePort` settings or even Ingress.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: CCM服务控制器提供了在公共云提供商上运行Kubernetes的“魔力”。我们在[*第5章*](B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127)中审查的一个方面是`LoadBalancer`服务，*服务和入口
    - 与外部世界通信*，例如，在配置了AWS CCM的集群上，类型为`LoadBalancer`的服务将自动配置匹配的AWS负载均衡器资源，为您提供了一种在集群中公开服务的简单方法，而无需处理`NodePort`设置甚至Ingress。
- en: Now that we understand what the CCM provides, we can venture further and talk
    about a couple of the other cloud provider extensions that are often used when
    running Kubernetes on the public cloud. First, let's look at `external-dns`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了CCM提供的内容，我们可以进一步探讨一下在公共云上运行Kubernetes时经常使用的一些其他云提供商扩展。首先，让我们看看`external-dns`。
- en: Using external-dns with Kubernetes
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Kubernetes的external-dns
- en: The `external-dns` library is an officially supported Kubernetes add-on that
    allows the cluster to configure external DNS providers to provide DNS resolution
    for services and ingress in an automated fashion. The `external-dns` add-on supports
    a broad range of cloud providers such as AWS and Azure, and also other DNS services
    such as Cloudflare.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`external-dns`库是一个官方支持的Kubernetes插件，允许集群配置外部DNS提供程序以自动化方式为服务和Ingress提供DNS解析。`external-dns`插件支持广泛的云提供商，如AWS和Azure，以及其他DNS服务，如Cloudflare。'
- en: Important note
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: In order to install `external-dns`, you can check the official GitHub repository
    at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`external-dns`，您可以在[https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)上查看官方GitHub存储库。
- en: Once `external-dns` is implemented on your cluster, it's simple to create new
    DNS records in an automated fashion. To test `external-dns` with a service, we
    simply need to create a service in Kubernetes with the proper annotation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在您的集群上实施了`external-dns`，就可以简单地以自动化的方式创建新的DNS记录。要测试`external-dns`与服务的配合，我们只需要在Kubernetes中创建一个带有适当注释的服务。
- en: 'Let''s see what this looks like:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是什么样子：
- en: service.yaml
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: service.yaml
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, we only need to add an annotation for the `external-dns` controller
    to check, with the domain record to be created in DNS. The domain and hosted zone
    must of course be accessible by your `external-dns` controller – for instance,
    on AWS Route 53 or Azure DNS. Check the specific documentation on the `external-dns`
    GitHub repository for specifics.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们只需要为`external-dns`控制器添加一个注释，以便检查要在DNS中创建的域记录。当然，域和托管区必须可以被您的`external-dns`控制器访问
    - 例如，在AWS Route 53或Azure DNS上。请查看`external-dns` GitHub存储库上的具体文档。
- en: Once the Service is up and running, `external-dns` will pick up the annotation
    and create a new DNS record. This pattern is excellent for multi-tenancy or per-version
    deploys since with something like a Helm chart, variables can be used to change
    the domain depending on which version or branch of the application is deployed
    – for instance, `v1.myapp.mydomain.com`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务启动运行，`external-dns`将获取注释并创建一个新的DNS记录。这种模式非常适合多租户或每个版本部署，因为像Helm图表这样的东西可以使用变量来根据应用程序的部署版本或分支来更改域
    - 例如，`v1.myapp.mydomain.com`。
- en: 'For Ingress, this is even easier – you just need to specify a host on your
    Ingress record, like so:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Ingress，这甚至更容易 - 您只需要在Ingress记录中指定一个主机，就像这样：
- en: ingress.yaml
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ingress.yaml
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This host value will automatically create a DNS record pointing to whatever
    method your Ingress is using – for instance, a Load Balancer on AWS.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此主机值将自动创建一个DNS记录，指向Ingress正在使用的任何方法 - 例如，在AWS上的负载均衡器。
- en: Next, let's talk about how the **cluster-autoscaler** library works.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们谈谈**cluster-autoscaler**库的工作原理。
- en: Using the cluster-autoscaler add-on
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用cluster-autoscaler插件
- en: Similar to `external-dns`, `cluster-autoscaler` is an officially supported add-on
    for Kubernetes that supports some major cloud providers with specific functionality.
    The purpose of `cluster-autoscaler` is to trigger the scaling of the number of
    Nodes in a cluster. It performs this process by controlling the cloud provider's
    own scaling resources, such as AWS autoscaling groups.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与“external-dns”类似，“cluster-autoscaler”是Kubernetes的一个官方支持的附加组件，支持一些主要的云提供商具有特定功能。
    “cluster-autoscaler”的目的是触发集群中节点数量的扩展。它通过控制云提供商自己的扩展资源（例如AWS自动缩放组）来执行此过程。
- en: The cluster autoscaler will perform an upward scaling action the moment any
    single Pod fails to schedule due to resource constraints on a Node, but only if
    a Node of the existing Node size (for instance, a `t3.medium` sized Node in AWS)
    would allow the Pod to be scheduled.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器将在任何单个Pod由于节点上的资源限制而无法调度时执行向上缩放操作，但仅当现有节点大小（例如，在AWS中为“t3.medium”大小的节点）可以允许Pod被调度时才会执行。
- en: Similarly, the cluster autoscaler will perform a downward scaling action the
    moment any Node could be emptied of Pods without causing memory or CPU pressure
    on any of the other Nodes.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，集群自动缩放器将在任何节点可以在不会对其他节点造成内存或CPU压力的情况下清空Pod时执行向下缩放操作。
- en: To install `cluster-autoscaler`, simply follow the correct instructions from
    your cloud provider, for the cluster type and intended version of the `cluster-autoscaler`.
    For instance, the AWS installation instructions for `cluster-autoscaler` on EKS
    are found at [https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装“cluster-autoscaler”，只需按照您的云提供商的正确说明，为集群类型和预期的“cluster-autoscaler”版本进行操作。例如，EKS上的AWS“cluster-autoscaler”的安装说明可在[https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/](https://aws.amazon.com/premiumsupport/knowledge-center/eks-cluster-autoscaler-setup/)找到。
- en: Next, let's look at how you can find open and closed source extensions for Kubernetes
    by examining the Kubernetes ecosystem.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何通过检查Kubernetes生态系统来找到开源和闭源的扩展。
- en: Integrating with the ecosystem
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与生态系统集成
- en: The Kubernetes (and more generally, cloud-native) ecosystem is massive, consisting
    of hundreds of popular open source software libraries, and thousands more fledgling
    ones. This can be tough to navigate since every month brings new technologies
    to vet, and acquisitions, rollups, and companies going out of business can turn
    your favorite open source library into an unmaintained mess.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes（以及更一般地说，云原生）生态系统是庞大的，包括数百个流行的开源软件库，以及成千上万个新兴的软件库。这可能很难导航，因为每个月都会有新的技术需要审查，而收购、合并和公司倒闭可能会将您最喜欢的开源库变成一个未维护的混乱。
- en: Thankfully, there is some structure in this ecosystem, and it's worth knowing
    about it in order to help navigate the dearth of options in cloud-native open
    source. The first big structural component of this is the **Cloud Native Computing
    Foundation** or **CNCF**.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个生态系统中有一些结构，了解它是值得的，以帮助导航云原生开源选项的匮乏。这其中的第一个重要结构组件是**云原生计算基金会**或**CNCF**。
- en: Introducing the Cloud Native Computing Foundation
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍云原生计算基金会
- en: The CNCF is a sub-foundation of the Linux Foundation, which is a non-profit
    entity that hosts open source projects and coordinates an ever-changing list of
    companies that contribute to and use open source software.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF是Linux基金会的一个子基金会，它是一个主持开源项目并协调不断变化的公司列表的非营利实体，这些公司为和使用开源软件做出贡献。
- en: The CNCF was founded almost entirely to shepherd the future of the Kubernetes
    project. It was announced alongside the 1.0 release of Kubernetes and has since
    grown to encompass hundreds of projects in the cloud-native space – from Prometheus
    to Envoy to Helm, and many more.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF几乎完全是为了引导Kubernetes项目的未来而成立的。它是在Kubernetes 1.0发布时宣布的，并且此后已经发展到涵盖了云原生空间中的数百个项目
    - 从Prometheus到Envoy到Helm，以及更多。
- en: The best way to see an overview of the CNCF's constituent projects is to check
    out the CNCF Cloud Native Landscape, which can be found at [https://landscape.cncf.io/](https://landscape.cncf.io/).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 了解CNCF组成项目的最佳方法是查看CNCF Cloud Native Landscape，网址为[https://landscape.cncf.io/](https://landscape.cncf.io/)。
- en: The CNCF Landscape is a good place to start if you are interested in possible
    solutions to a problem you are experiencing with Kubernetes or cloud-native. For
    every category (monitoring, logging, serverless, service mesh, and others), there
    are several open source options to vet and choose from.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对你在Kubernetes或云原生中遇到的问题感兴趣，CNCF Landscape是一个很好的起点。对于每个类别（监控、日志记录、无服务器、服务网格等），都有几个开源选项供您选择。
- en: This is both a strength and weakness of the current ecosystem of cloud-native
    technologies. There are a significant number of options available, which makes
    the correct path often unclear, but also means that you will likely be able to
    find a solution that is close to your exact needs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当前云原生技术生态系统的优势和劣势。有大量的选择可用，这使得正确的路径通常不明确，但也意味着你可能会找到一个接近你确切需求的解决方案。
- en: The CNCF also operates an official Kubernetes forum, which can be joined from
    the Kubernetes official website at [kubernetes.io](http://kubernetes.io). The
    URL of the Kubernetes forums is [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF还经营着一个官方的Kubernetes论坛，可以从Kubernetes官方网站[kubernetes.io](http://kubernetes.io)加入。Kubernetes论坛的网址是[https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)。
- en: Finally, it is relevant to mention *KubeCon*/*CloudNativeCon*, a large conference
    that is run by the CNCF and encompasses topics including Kubernetes itself and
    many ecosystem projects. *KubeCon* gets larger every year, with almost 12,000
    attendees for *KubeCon* *North America* in 2019.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得一提的是*KubeCon*/*CloudNativeCon*，这是由CNCF主办的一个大型会议，涵盖了Kubernetes本身和许多生态项目等主题。*KubeCon*每年都在扩大规模，2019年*KubeCon*
    *North America*有近12,000名与会者。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about extending Kubernetes. First, we talked about
    CRDs – what they are, some relevant use cases, and how to implement them in your
    cluster. Next, we reviewed the concept of an operator in Kubernetes and discussed
    how to use an operator, or custom controller, to give life to your CRD.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何扩展Kubernetes。首先，我们讨论了CRDs - 它们是什么，一些相关的用例，以及如何在集群中实现它们。接下来，我们回顾了Kubernetes中操作员的概念，并讨论了如何使用操作员或自定义控制器来赋予CRD生命。
- en: Then, we discussed cloud-provider-specific extensions to Kubernetes including
    `cloud-controller-manager`, `external-dns`, and `cluster-autoscaler`. Finally,
    we wrapped up with an introduction to the cloud-native open source ecosystem at
    large and some great ways to discover projects for your use case.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了针对Kubernetes的特定于云供应商的扩展，包括`cloud-controller-manager`、`external-dns`和`cluster-autoscaler`。最后，我们介绍了大型云原生开源生态系统以及发现适合你使用情况的项目的一些好方法。
- en: The skills you used in this chapter will help you extend your Kubernetes cluster
    to interface with your cloud provider as well as your own custom functionality.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的技能将帮助您扩展Kubernetes集群，以便与您的云提供商以及您自己的自定义功能进行接口。
- en: In the next chapter, we'll talk about two nascent architectural patterns as
    applied to Kubernetes – serverless and service meshes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论作为应用于Kubernetes的两种新兴架构模式 - 无服务器和服务网格。
- en: Questions
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between a served version and a stored version of a CRD?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是CRD的服务版本和存储版本之间的区别？
- en: What are three typical parts of a custom controller or operator control loop?
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义控制器或操作员控制循环的三个典型部分是什么？
- en: How does `cluster-autoscaler` interact with existing cloud provider scaling
    solutions such as AWS autoscaling groups?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cluster-autoscaler`如何与现有的云提供商扩展解决方案（如AWS自动扩展组）交互？'
- en: Further reading
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'CNCF Landscape: [https://landscape.cncf.io/](https://landscape.cncf.io/)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNCF景观：[https://landscape.cncf.io/](https://landscape.cncf.io/)
- en: 'Official Kubernetes Forums: [https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方Kubernetes论坛：[https://discuss.kubernetes.io/](https://discuss.kubernetes.io/)
