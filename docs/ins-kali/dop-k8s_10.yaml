- en: Kubernetes on GCP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCP上的Kubernetes
- en: '**Google Cloud Platform** (**GCP**) is getting popular in the public cloud
    industry that is provided by Google. GCP has similar concepts as AWS such as VPC,
    Compute Engine, Persistent Disk, Load Balancing, and several managed services.
    In this chapter, you will learn about GCP and how to set up Kubernetes on GCP
    through the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Platform（GCP）在公共云行业中越来越受欢迎，由Google提供。GCP与AWS具有类似的概念，如VPC、计算引擎、持久磁盘、负载均衡和一些托管服务。在本章中，您将了解GCP以及如何通过以下主题在GCP上设置Kubernetes：
- en: Understanding GCP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解GCP
- en: Using and understanding GCP components
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用和理解GCP组件
- en: Using **Google Container Engine** (**GKE**), the hosted Kubernetes service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Google Container Engine（GKE），托管的Kubernetes服务
- en: Introduction to GCP
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCP简介
- en: GCP was officially launched in 2011\. But not like AWS; at the beginning, GCP
    provided **PaaS** (**Platform as a Service**) first. So you can deploy your application
    directly, instead of launching VM. After that, keep enhance functionality that
    supports a variety of services.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: GCP于2011年正式推出。但与AWS不同的是，GCP最初提供了PaaS（平台即服务）。因此，您可以直接部署您的应用程序，而不是启动虚拟机。之后，不断增强功能，支持各种服务。
- en: The most important service for Kubernetes users is GKE, which is a hosted Kubernetes
    service. So you can get some relief from Kubernetes installation, upgrade, and
    management. It has a pay–as–you–go style approach to use the Kubernetes cluster.
    GKE is also a very active service that keeps providing new versions of Kubernetes
    in a timely manner, and also keeps coming up with new features and management
    tools for Kubernetes as well.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kubernetes用户来说，最重要的服务是GKE，这是一个托管的Kubernetes服务。因此，您可以从Kubernetes的安装、升级和管理中得到一些缓解。它采用按使用Kubernetes集群的方式付费。GKE也是一个非常活跃的服务，不断及时提供新版本的Kubernetes，并为Kubernetes提供新功能和管理工具。
- en: Let's take a look at what kind of foundation and services are provided by GCP
    and then explore GKE.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看GCP提供了什么样的基础设施和服务，然后探索GKE。
- en: GCP components
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCP组件
- en: GCP provides a web console and **command-line interface** (**CLI**). Both are
    easy and straightforward to control GCP infrastructure, but Google accounts (such
    as Gmail) are required. Once you have a Google account, go to the GCP sign up
    page ([https://cloud.google.com/free/](https://cloud.google.com/free/)) to set
    up your GCP account creation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GCP提供了Web控制台和命令行界面（CLI）。两者都很容易直接地控制GCP基础设施，但需要Google账户（如Gmail）。一旦您拥有了Google账户，就可以转到GCP注册页面（[https://cloud.google.com/free/](https://cloud.google.com/free/)）来创建您的GCP账户。
- en: 'If you want to control via CLI, you need to install Cloud SDK ([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)),
    which is similar to AWS CLI that you can use to list, create, update, and delete
    GCP resources. After installing Cloud SDK, you need to configure it with the following
    command to associate it to a GCP account:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想通过CLI进行控制，您需要安装Cloud SDK（[https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)），这类似于AWS
    CLI，您可以使用它来列出、创建、更新和删除GCP资源。安装Cloud SDK后，您需要使用以下命令将其配置到GCP账户：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: VPC
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VPC
- en: VPC in GCP is quite a different policy compared with AWS. First of all, you
    don't need to set CIDR prefix to VPC, in other words, you cannot set CIDR to VPC.
    Instead, you just add one or some subnets to the VPC. Because subnet is always
    coming with certain CIDR blocks, therefore, GCP VPC is identified as a logical
    group of subnets, and subnets within VPC can communicate with each other.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与AWS相比，GCP中的VPC政策有很大不同。首先，您不需要为VPC设置CIDR前缀，换句话说，您不能为VPC设置CIDR。相反，您只需向VPC添加一个或多个子网。因为子网总是带有特定的CIDR块，因此GCP
    VPC被识别为一组逻辑子网，VPC内的子网可以相互通信。
- en: 'Note that GCP VPC has two modes, either **auto** or **custom**. If you choose
    auto, it will create some subnets on each region with predefined CIDR blocks.
    For example, if you type the following command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，GCP VPC有两种模式，即**自动**或**自定义**。如果您选择自动模式，它将在每个区域创建一些具有预定义CIDR块的子网。例如，如果您输入以下命令：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It will create 11 subnets as shown in the following screenshot (because, as
    of August, 2017, GCP has 11 regions):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它将创建11个子网，如下面的屏幕截图所示（因为截至2017年8月，GCP有11个区域）：
- en: '![](../images/00130.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00130.jpeg)'
- en: Auto mode VPC is probably good to start with. However, in auto mode, you can't
    specify CIDR prefix and 11 subnets from all regions might not fit with your use
    case. For example, if you want to integrate to your on–premise data center via
    VPN, or want to create subnets from a particular region only.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自动模式VPC可能是一个很好的起点。但是，在自动模式下，您无法指定CIDR前缀，而且来自所有区域的11个子网可能无法满足您的用例。例如，如果您想通过VPN集成到您的本地数据中心，或者只想从特定区域创建子网。
- en: 'In this case, choose custom mode VPC, then you can create subnets with desired
    CIDR prefix manually. Type the following command to create custom mode VPC:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，选择自定义模式VPC，然后可以手动创建具有所需CIDR前缀的子网。输入以下命令以创建自定义模式VPC：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Because custom mode VPC won''t create any subnets as shown in the following
    screenshot, let''s add subnets onto this custom mode VPC:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因为自定义模式VPC不会像下面的屏幕截图所示创建任何子网，让我们在这个自定义模式VPC上添加子网：
- en: '![](../images/00131.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00131.jpeg)'
- en: Subnets
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子网
- en: Subnet in GCP, its always across multiple zones (availability zone) within region.
    In other words, you can't create subnets on a single zone like AWS. You always
    need to specify entire regions when creating a subnet.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP中，子网始终跨越区域内的多个区域（可用区）。换句话说，您无法像AWS一样在单个区域创建子网。创建子网时，您总是需要指定整个区域。
- en: In addition, there are no significant concepts of public and private subnets
    such as AWS (combination of route and internet gateway or NAT gateway to determine
    as a public or private subnet). This is because all subnets in GCP have a route
    to internet gateway.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与AWS不同（结合路由和Internet网关或NAT网关确定为公共或私有子网的重要概念），GCP没有明显的公共和私有子网概念。这是因为GCP中的所有子网都有指向Internet网关的路由。
- en: Instead of subnet level access control, GCP uses host (instance) level access
    control using **network tags** to ensure the network security. It will be described
    in more detail in the following section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GCP使用**网络标签**而不是子网级别的访问控制，以确保网络安全。这将在下一节中进行更详细的描述。
- en: It might make network administrators nervous, however, GCP best practice brings
    you much more simplified and scalable VPC administration, because you can add
    subnets anytime to expand entire network blocks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会让网络管理员感到紧张，但是GCP最佳实践为您带来了更简化和可扩展的VPC管理，因为您可以随时添加子网以扩展整个网络块。
- en: Technically, you can launch VM instance to set up as a NAT gateway or HTTP proxy,
    and then create a custom priority route for the private subnet that points to
    the NAT/proxy instance to achieve an AWS–like private subnet.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，您可以启动VM实例设置为NAT网关或HTTP代理，然后为指向NAT/代理实例的私有子网创建自定义优先级路由，以实现类似AWS的私有子网。
- en: 'Please refer to the following online document for details:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅以下在线文档：
- en: '[https://cloud.google.com/compute/docs/vpc/special-configurations](https://cloud.google.com/compute/docs/vpc/special-configurations)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/compute/docs/vpc/special-configurations](https://cloud.google.com/compute/docs/vpc/special-configurations)'
- en: 'One more thing, an interesting and unique concept of GCP VPC is that you can
    add different CIDR prefix network blocks to the single VPC. For example, if you
    have custom mode VPC then add the following three subnets:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事，GCP VPC的一个有趣且独特的概念是，您可以将不同的CIDR前缀网络块添加到单个VPC中。例如，如果您有自定义模式VPC，然后添加以下三个子网：
- en: '`subnet-a` (`10.0.1.0/24`) from `us-west1`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subnet-a` (`10.0.1.0/24`) 来自 `us-west1`'
- en: '`subnet-b` (`172.16.1.0/24`) from `us-east1`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subnet-b` (`172.16.1.0/24`) 来自 `us-east1`'
- en: '`subnet-c` (`192.168.1.0/24`) from `asia-northeast1`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subnet-c` (`192.168.1.0/24`) 来自 `asia-northeast1`'
- en: 'The following commands will create three subnets from three different regions
    with different CIDR prefix:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将从三个不同的区域创建三个具有不同CIDR前缀的子网：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The result will be the following web console. If you are familiar with AWS VPC,
    you won't believe these combinations of CIDR prefixes within a single VPC! This
    means that, whenever you need to expand a network, you can feel free to assign
    another CIDR prefix to add to the VPC.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是以下Web控制台。如果您熟悉AWS VPC，您将不相信这些CIDR前缀的组合在单个VPC中！这意味着，每当您需要扩展网络时，您可以随意分配另一个CIDR前缀以添加到VPC中。
- en: '![](../images/00132.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00132.jpeg)'
- en: Firewall rules
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防火墙规则
- en: As mentioned previously, GCP firewall rule is important to achieve network security.
    But GCP firewall is more simple and flexible than AWS **security group** (**SG**).
    For example, in AWS, when you launch an EC2 instance, you have to assign at least
    one SG that is tight coupling with EC2 and SG. On the other hand, in GCP, you
    can't assign any firewall rules directly. Instead, firewall rule and VM instance
    are loosely coupled via **network tag**. Therefore, there is no direct association
    between firewall rule and VM instance. The following diagram is a comparison between
    AWS security group and GCP firewall rule. EC2 requires security group, on the
    other hand, GCP VM instance just sets a tag. This is regardless of whether the
    corresponding firewall has the same tag or not.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，GCP防火墙规则对于实现网络安全非常重要。但是GCP防火墙比AWS的安全组（SG）更简单和灵活。例如，在AWS中，当您启动一个EC2实例时，您必须至少分配一个与EC2和SG紧密耦合的SG。另一方面，在GCP中，您不能直接分配任何防火墙规则。相反，防火墙规则和VM实例是通过网络标签松散耦合的。因此，防火墙规则和VM实例之间没有直接关联。以下图表是AWS安全组和GCP防火墙规则之间的比较。EC2需要安全组，另一方面，GCP
    VM实例只需设置一个标签。这与相应的防火墙是否具有相同的标签无关。
- en: '![](../images/00133.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00133.jpeg)'
- en: 'For example, create a firewall rule for public host (use network tag `public`)
    and private host (use network tag `private`) as given in the following command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，根据以下命令为公共主机（使用网络标签`public`）和私有主机（使用网络标签`private`）创建防火墙规则：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It creates four firewall rules as shown in the following screenshot. Let''s
    create VM instances to use either the `public` or `private` network tag to see
    how it works:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它创建了四个防火墙规则，如下图所示。让我们创建VM实例，以使用`public`或`private`网络标签，看看它是如何工作的：
- en: '![](../images/00134.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00134.jpeg)'
- en: VM instance
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VM实例
- en: VM instance in GCP is quite similar to AWS EC2\. You can choose from a variety
    of machine (instance) types that have different hardware configurations. As well
    as OS images that are Linux or Windows–based OS or your customized OS, you can
    choose.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GCP中的VM实例与AWS EC2非常相似。您可以选择各种具有不同硬件配置的机器（实例）类型。以及Linux或基于Windows的OS镜像或您定制的OS，您可以选择。
- en: As mentioned when talking about firewall rules, you can specify zero or more
    network tags. A tag is not necessary to be created beforehand. This means you
    can launch VM instances with network tags first, even though a firewall rule is
    not created. It is still valid, but no firewall rule is applied in this case.
    Then create a firewall rule to have a network tag. Eventually a firewall rule
    will be applied to the VM instances afterwards. This is why VM instances and firewall
    rules are loosely coupled, which provides flexibility to the user.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如在讨论防火墙规则时提到的，您可以指定零个或多个网络标签。标签不一定要事先创建。这意味着您可以首先使用网络标签启动VM实例，即使没有创建防火墙规则。这仍然是有效的，但在这种情况下不会应用防火墙规则。然后创建一个防火墙规则来具有网络标签。最终，防火墙规则将应用于VM实例。这就是为什么VM实例和防火墙规则松散耦合的原因，这为用户提供了灵活性。
- en: '![](../images/00135.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00135.jpeg)'
- en: 'Before launching a VM instance, you need to create a ssh public key first,
    the same as AWS EC2\. The easiest way to do this is to run the following command
    to create and register a new key:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动VM实例之前，您需要首先创建一个ssh公钥，与AWS EC2相同。这样做的最简单方法是运行以下命令来创建和注册一个新密钥：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now let's get started to launch a VM instance on GCP.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始在GCP上启动一个VM实例。
- en: 'Deploy two instances on both `subnet-a` and `subnet-b` as public instances
    (use the `public` network tag) and then launch another instance on the `subnet-a`
    as private instance (with a `private` network tag):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在`subnet-a`和`subnet-b`上部署两个实例作为公共实例（使用`public`网络标签），然后在`subnet-a`上启动另一个实例作为私有实例（使用`private`网络标签）：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../images/00136.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00136.jpeg)'
- en: 'You can log in to those machines to check whether a firewall rule works as
    expected. First of all, you need to add a ssh key to the ssh-agent on your machine:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以登录到这些机器上检查防火墙规则是否按预期工作。首先，您需要将ssh密钥添加到您的机器上的ssh-agent中：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then check whether an ICMP firewall rule can reject from external, because
    ICMP allows only public or private tagged hosts, so it must not allow ping from
    your machine as shown in the following screenshot:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后检查ICMP防火墙规则是否可以拒绝来自外部的请求，因为ICMP只允许公共或私有标记的主机，因此不应允许来自您的机器的ping，如下面的屏幕截图所示：
- en: '![](../images/00137.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00137.jpeg)'
- en: On the other hand, the public host allows ssh from your machine, because public-ssh
    rule allows any (`0.0.0.0/0`).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，公共主机允许来自您的机器的ssh，因为public-ssh规则允许任何（`0.0.0.0/0`）。
- en: '![](../images/00138.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00138.jpeg)'
- en: Of course, this host can ping and ssh to private hosts on `subnet-a` (`10.0.1.2`)
    through a private IP address, because of the `internal-icmp` rule and `private-ssh`
    rule.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这台主机可以通过私有IP地址在`subnet-a`（`10.0.1.2`）上ping和ssh到私有主机，因为有`internal-icmp`规则和`private-ssh`规则。
- en: Let's ssh to a private host and then install `tomcat8` and `tomcat8-examples`
    package (it will install the `/examples/` application to Tomcat).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过ssh连接到私有主机，然后安装`tomcat8`和`tomcat8-examples`包（它将在Tomcat中安装`/examples/`应用程序）。
- en: '![](../images/00139.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00139.jpeg)'
- en: Remember that `subnet-a` is `10.0.1.0/24` CIDR prefix, but `subnet-b` is `172.16.1.0/24`
    CIDR prefix. But within the same VPC, there is connectivity with each other. This
    is a great benefit and advantage of using GCP whereby you can expand a network
    address block whenever you need.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`subnet-a`是`10.0.1.0/24`的CIDR前缀，但`subnet-b`是`172.16.1.0/24`的CIDR前缀。但在同一个VPC中，它们之间是可以互相连接的。这是使用GCP的一个巨大优势，您可以根据需要扩展网络地址块。
- en: 'Now, install nginx to public hosts (`public-on-subnet-a` and `public-on-subnet-b`):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在公共主机（`public-on-subnet-a`和`public-on-subnet-b`）上安装nginx：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'However, at this moment, you can''t access Tomcat on a private host. Even if
    it has a public IP address. This is because a private host doesn''t have any firewall
    rule that allows 8080/tcp yet:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，此时，您无法访问私有主机上的Tomcat。即使它有一个公共IP地址。这是因为私有主机还没有任何允许8080/tcp的防火墙规则：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Moving forward, not to just creating a firewall rule for Tomcat but will also
    be setting up a LoadBalancer to configure both nginx and Tomcat access from a
    single LoadBalancer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前进，不仅为Tomcat创建防火墙规则，还将设置一个负载均衡器，以配置nginx和Tomcat从单个负载均衡器访问。
- en: Load balancing
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'GCP provides several types of load balancers as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GCP提供以下几种类型的负载均衡器：
- en: Layer 4 TCP LoadBalancer
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4层TCP负载均衡器
- en: Layer 4 UDP LoadBalancer
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第4层UDP负载均衡器
- en: Layer 7 HTTP(S) LoadBalancer
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7层HTTP(S)负载均衡器
- en: Layer 4, both TCP and UDP, LoadBalancers are similar to AWS Classic ELB. On
    the other hand, Layer 7 HTTP(S) LoadBalancer has content (context) based routing.
    For example, URL /img will forward to instance-a, everything else will forward
    to instance-b. So, it is more like an application level LoadBalancer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第4层，无论是TCP还是UDP，负载均衡器都类似于AWS经典ELB。另一方面，第7层HTTP(S)负载均衡器具有基于内容（上下文）的路由。例如，URL
    /img将转发到实例a，其他所有内容将转发到实例b。因此，它更像是一个应用级别的负载均衡器。
- en: AWS also provides **Application Load Balancer** (**ALB** or **ELBv2**), which
    is quite similar to GCP Layer 7 HTTP(S) LoadBalancer. For details, please visit
    [https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: AWS还提供了**应用负载均衡器**（**ALB**或**ELBv2**），它与GCP第7层HTTP(S)负载均衡器非常相似。有关详细信息，请访问[https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/)。
- en: 'In order to set up LoadBalancer, unlike AWS ELB, there are several steps needed
    to configure some items beforehand:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置负载均衡器，与AWS ELB不同，需要在配置一些项目之前进行几个步骤：
- en: '| **Configuration item** | **Purpose** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **配置项** | **目的** |'
- en: '| Instance group | Determine group of VM instances or VM template (OS image).
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 实例组 | 确定VM实例或VM模板（OS镜像）的组。|'
- en: '| Health check | Set health threshold (interval, timeout, and so on) to determine
    instance group health status. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 健康检查 | 设置健康阈值（间隔、超时等）以确定实例组的健康状态。|'
- en: '| Backend service | Set load threshold (maximum CPU or request per second)
    and session affinity (sticky session) to the instance group and also associate
    to health check. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 后端服务 | 设置负载阈值（最大CPU或每秒请求）和会话亲和性（粘性会话）到实例组，并关联到健康检查。|'
- en: '| url-maps (LoadBalancer) | This is an actual place holder to represent an
    L7 LoadBalancer that associates backend services and target HTTP(S) proxy |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| url-maps（负载均衡器）| 这是一个实际的占位符，用于表示关联后端服务和目标HTTP(S)代理的L7负载均衡器|'
- en: '| Target HTTP(S) proxy | This is a connector that makes relationships between
    frontend forwarding rules to LoadBalancer |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 目标HTTP(S)代理 | 这是一个连接器，它建立了前端转发规则与负载均衡器之间的关系|'
- en: '| Frontend forwarding rule | Associate IP address (ephemeral or static), port
    number to the target HTTP proxy |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 前端转发规则 | 将IP地址（临时或静态）、端口号与目标HTTP代理关联起来|'
- en: '| External IP (static) | (Optional) Allocate static external IP address for
    LoadBalancer |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 外部IP（静态）|（可选）为负载均衡器分配静态外部IP地址|'
- en: 'The following diagram is for all the preceding components'' association that
    constructs L7 LoadBalancer:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了构建L7负载均衡器的所有前述组件的关联：
- en: '![](../images/00140.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00140.jpeg)'
- en: Let's set up an instance group first. In this example, there are three instance
    groups to create. One for private host Tomcat instance (8080/tcp) and another
    two instance groups for public HTTP instances per zones.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先设置一个实例组。在这个例子中，有三个要创建的实例组。一个用于私有主机Tomcat实例（8080/tcp），另外两个实例组用于每个区域的公共HTTP实例。
- en: 'To do that, execute the following command to group three of them:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，执行以下命令将它们三个分组：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Health check
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: 'Let''s set standard settings by executing the following commands:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下命令设置标准设置：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Backend service
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后端服务
- en: 'First of all, we need to create a backend service that specifies health check.
    And then add each instance group with threshold with CPU utilization that utilizes
    up to 80% and max capacity as 100% for both HTTP and Tomcat:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个指定健康检查的后端服务。然后为每个实例组添加阈值，CPU利用率最高为80%，HTTP和Tomcat的最大容量均为100%：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Creating a LoadBalancer
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建负载均衡器
- en: 'The LoadBalancer needs to bind both `my-http-backend-service` and `my-tomcat-backend-service`.
    In this scenario, only `/examples` and `/examples/*` will be the forwarded traffic
    to `my-tomcat-backend-service`. Other than that, every URI forwards traffic to
    `my-http-backend-service`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器需要绑定`my-http-backend-service`和`my-tomcat-backend-service`。在这种情况下，只有`/examples`和`/examples/*`将被转发到`my-tomcat-backend-service`。除此之外，每个URI都将转发流量到`my-http-backend-service`：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you don't specify an `--address` option, it will create and assign an ephemeral
    external IP address.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不指定`--address`选项，它将创建并分配一个临时的外部IP地址。
- en: Finally, LoadBalancer has been created. However, one missing configuration is
    remaining. Private hosts don't have any firewall rules to allow Tomcat traffic
    (8080/tcp). This is why when you see LoadBalancer status, healthy status of `my-tomcat-backend-service`
    is kept down (0).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，负载均衡器已经创建。但是，还有一个缺失的配置。私有主机没有任何防火墙规则来允许Tomcat流量（8080/tcp）。这就是为什么当您查看负载均衡器状态时，`my-tomcat-backend-service`的健康状态保持为下线（0）。
- en: '![](../images/00141.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00141.jpeg)'
- en: 'In this case, you need to add one more firewall rule that allows connection
    from LoadBalancer to a private subnet (use the `private` network tag). According
    to GCP documentation ([https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing](https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing)),
    health check heart beat will come from address range `130.211.0.0/22` and `35.191.0.0/16`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您需要添加一个允许从负载均衡器到私有子网的连接的防火墙规则（使用`private`网络标记）。根据GCP文档（[https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing](https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing)），健康检查心跳将来自地址范围`130.211.0.0/22`和`35.191.0.0/16`：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After a few minutes, `my-tomcat-backend-service` healthy status will be up
    (`1`); now you can access LoadBalancer from a web browser. When access to `/`
    it should route to `my-http-backend-service`, which has nginx application on public
    hosts:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，`my-tomcat-backend-service`的健康状态将变为正常（`1`）；现在您可以从Web浏览器访问负载均衡器。当访问`/`时，它应该路由到`my-http-backend-service`，该服务在公共主机上有nginx应用程序：
- en: '![](../images/00142.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00142.jpeg)'
- en: 'On the other hand, if you access `/examples/` URL with the same LoadBalancer
    IP address, it will route to `my-tomcat-backend-service`, which is a Tomcat application
    on a private host, as shown in the following screenshot:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您使用相同的负载均衡器IP地址访问`/examples/` URL，它将路由到`my-tomcat-backend-service`，该服务是私有主机上的Tomcat应用程序，如下面的屏幕截图所示：
- en: '![](../images/00143.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00143.jpeg)'
- en: Overall, there are some steps needed to be performed to set up LoadBalancer,
    but it is useful to integrate different HTTP applications onto a single LoadBalancer
    to deliver your service efficiently with minimum resources.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，需要执行一些步骤来设置负载均衡器，但将不同的HTTP应用集成到单个负载均衡器上，以最小的资源高效地提供服务是很有用的。
- en: Persistent Disk
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久磁盘
- en: GCE also has a storage service called **Persistent Disk** (**PD**) that is quite
    similar to AWS EBS. You can allocate desired size and types (either standard or
    SSD) on each zone and attach/detach to VM instances anytime.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: GCE还有一个名为**持久磁盘**（**PD**）的存储服务，它与AWS EBS非常相似。您可以在每个区域分配所需的大小和类型（标准或SSD），并随时附加/分离到VM实例。
- en: 'Let''s create one PD and then attach to the VM instance. Note that when attaching
    PD to the VM instance, both must be sat in the same zones. This limitation is
    the same as AWS EBS. So before creating PD, check the VM instance location once
    again:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个PD，然后将其附加到VM实例。请注意，将PD附加到VM实例时，两者必须位于相同的区域。这个限制与AWS EBS相同。因此，在创建PD之前，再次检查VM实例的位置：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s choose `us-west1-a` and then attach it to `public-on-subnet-a`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择`us-west1-a`，然后将其附加到`public-on-subnet-a`：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You may see PD has been attached at `/dev/sdb`. Similar to AWS EBS, you have
    to format this disk. Because this is a Linux OS operation, the steps are exactly
    the same as described in [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会看到PD已经附加到`/dev/sdb`。与AWS EBS类似，您必须格式化此磁盘。因为这是一个Linux操作系统操作，步骤与[第9章](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e)中描述的完全相同，*在AWS上的Kubernetes*。
- en: Google Container Engine (GKE)
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google容器引擎（GKE）
- en: Overall, there are some GCP components that have been introduced in previous
    sections. Now you can start to set up Kubernetes on GCP VM instances using those
    components. You can even use kops that was also introduced in [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS* too.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，一些GCP组件已经在前几节中介绍过。现在，您可以开始在GCP VM实例上使用这些组件设置Kubernetes。您甚至可以使用在[第9章](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e)中介绍的kops，*在AWS上的Kubernetes*也是如此。
- en: However, GCP has a managed Kubernetes service called GKE. Underneath, it uses
    some GCP components such as VPC, VM instances, PD, firewall rules, and LoadBalancers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GCP有一个名为GKE的托管Kubernetes服务。在底层，它使用了一些GCP组件，如VPC、VM实例、PD、防火墙规则和负载均衡器。
- en: 'Of course, as usual, you can use the `kubectl` command to control your Kubernetes
    cluster on GKE, which is included Cloud SDK. If you don''t install the `kubectl`
    command on your machine yet, type the following command to install `kubectl` via
    Cloud SDK:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，像往常一样，您可以使用`kubectl`命令来控制GKE上的Kubernetes集群，该命令包含在Cloud SDK中。如果您尚未在您的机器上安装`kubectl`命令，请输入以下命令通过Cloud
    SDK安装`kubectl`：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Setting up your first Kubernetes cluster on GKE
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在GKE上设置您的第一个Kubernetes集群
- en: 'You can set up a Kubernetes cluster on GKE using the `gcloud` command. It needs
    to specify several parameters to determine some configurations. One of the important
    parameters is network. You have to specify which VPC and subnet you will deploy.
    Although GKE supports multiple zones to deploy, you need to specify at least one
    zone for Kubernetes master node. This time, it uses the following parameters to
    launch a GKE cluster:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`gcloud`命令在GKE上设置Kubernetes集群。它需要指定几个参数来确定一些配置。其中一个重要的参数是网络。您必须指定将部署到哪个VPC和子网。虽然GKE支持多个区域进行部署，但您需要至少指定一个区域用于Kubernetes主节点。这次，它使用以下参数来启动GKE集群：
- en: '| **Parameter** | **Description** | **Value** |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **描述** | **值** |'
- en: '| `--cluster-version` | Specify Kubernetes version | `1.6.7` |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `--cluster-version` | 指定Kubernetes版本 | `1.6.7` |'
- en: '| `--machine-type` | VM instance type for Kubernetes Node | `f1-micro` |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `--machine-type` | Kubernetes节点的VM实例类型 | `f1-micro` |'
- en: '| `--num-nodes` | Initial number size of Kubernetes nodes | `3` |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `--num-nodes` | Kubernetes节点的初始数量 | `3` |'
- en: '| `--network` | Specify GCP VPC | `my-custom-network` |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `--network` | 指定GCP VPC | `my-custom-network` |'
- en: '| `--subnetwork` | Specify GCP Subnet if VPC is custom mode | `subnet-c` |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `--subnetwork` | 如果VPC是自定义模式，则指定GCP子网 | `subnet-c` |'
- en: '| `--zone` | Specify single zone | `asia-northeast1-a` |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| `--zone` | 指定单个区域 | `asia-northeast1-a` |'
- en: '| `--tags` | Network tags that will be assigned to Kubernetes nodes | `private`
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| `--tags` | 将分配给Kubernetes节点的网络标签 | `private` |'
- en: 'In this scenario, you need to type the following command to launch a Kubernetes
    cluster on GCP. It may take a few minutes to complete because, behind the scenes,
    it will launch several VM instances and set up Kubernetes master and nodes. Note
    that Kubernetes master and etcd will be fully managed by GCP. This means master
    node and etcd don''t consume your VM instances:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您需要键入以下命令在GCP上启动Kubernetes集群。这可能需要几分钟才能完成，因为在幕后，它将启动多个VM实例并设置Kubernetes主节点和节点。请注意，Kubernetes主节点和etcd将由GCP完全管理。这意味着主节点和etcd不会占用您的VM实例：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we specify the `--tags private` option, so Kubernetes node VM instance
    has a network tag as `private`. Therefore, it behaves the same as other regular
    VM instances that have `private` tags. Therefore you can't ssh from public Internet
    and you can't HTTP from internet either. But you can ping and ssh from another
    VM instance which has a `public` network tag.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们指定了`--tags private`选项，因此Kubernetes节点VM实例具有`private`网络标记。因此，它的行为与具有`private`标记的其他常规VM实例相同。因此，您无法从公共互联网进行ssh，也无法从互联网进行HTTP。但是，您可以从具有`public`网络标记的另一个VM实例进行ping和ssh。
- en: 'Once all nodes are ready, let''s access Kubernetes UI, which is installed by
    default. To do that, use the `kubectl proxy` command to connect to your machine
    as a proxy. Then access the UI via proxy:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有节点准备就绪，让我们访问默认安装的Kubernetes UI。为此，使用`kubectl proxy`命令作为代理连接到您的计算机。然后通过代理访问UI：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../images/00144.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00144.jpeg)'
- en: Node pool
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点池
- en: When launching the Kubernetes cluster, you can specify the number of nodes using
    the `--num-nodes` option. GKE manages a Kubernetes node as node pool. Which means
    you can manage one or more node pools that attach to your Kubernetes cluster.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动Kubernetes集群时，您可以使用`--num-nodes`选项指定节点的数量。GKE将Kubernetes节点管理为节点池。这意味着您可以管理一个或多个附加到您的Kubernetes集群的节点池。
- en: 'What if you need to add more nodes or delete some nodes? GKE provides a functionality
    to resize the node pool by following the command to change Kubernetes node from
    3 to 5:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要添加更多节点或删除一些节点怎么办？GKE提供了一个功能，可以通过以下命令将Kubernetes节点从3更改为5来调整节点池的大小：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Increasing the number of nodes will help if you need to scale out your node
    capacity. However, in this scenario, it still uses the smallest instance type
    (`f1-micro`, which has only 0.6 GB memory). It might not help if a single container
    needs more than 0.6 GB memory. In this case you need to scale up, which means
    you need to add a larger size of VM instance type.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 增加节点数量将有助于扩展节点容量。但是，在这种情况下，它仍然使用最小的实例类型（`f1-micro`，仅具有0.6 GB内存）。如果单个容器需要超过0.6
    GB内存，则可能无法帮助。在这种情况下，您需要进行扩展，这意味着您需要添加更大尺寸的VM实例类型。
- en: In this case, you have to add another set of node pools onto your cluster. Because
    within the same node pool, all VM instances are configured the same. So you can't
    change the instance type in the same node pool.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您必须将另一组节点池添加到您的集群中。因为在同一个节点池中，所有VM实例都配置相同。因此，您无法在同一个节点池中更改实例类型。
- en: Therefore, add a new node pool that has two new sets of `g1-small` (1.7 GB memory)
    VM instance type to the cluster. Then you can expand Kubernetes nodes with different
    hardware configuration.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，添加一个新的节点池，该节点池具有两组新的`g1-small`（1.7 GB内存）VM实例类型到集群中。然后，您可以扩展具有不同硬件配置的Kubernetes节点。
- en: By default, there are some quotas that you can create a number limit of VM instances
    within one region (for example, up to eight cpu cores on `us-west1`). If you wish
    to increase this quota, you must change your account to be a paid account. Then
    request quota change to GCP. For more details, please read online documentation
    from [https://cloud.google.com/compute/quotas](https://cloud.google.com/compute/quotas)
    and [https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade](https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，您可以在一个区域内创建VM实例数量的一些配额限制（例如，在`us-west1`最多八个CPU核心）。如果您希望增加此配额，您必须将您的帐户更改为付费帐户。然后向GCP请求配额更改。有关更多详细信息，请阅读来自[https://cloud.google.com/compute/quotas](https://cloud.google.com/compute/quotas)和[https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade](https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade)的在线文档。
- en: 'Run the following command that adds an additional node pool that has two instances
    of `g1-small` instance:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令，添加一个具有两个`g1-small`实例的额外节点池：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now you have a total of seven CPU cores and 6.4 GB memory in your cluster that
    has more capacity. However, due to larger hardware types, Kubernetes scheduler
    will probably assign to deploy pod to the `large-mem-pool` first, because it has
    enough memory capacity.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的集群中总共有七个CPU核心和6.4GB内存的容量更大。然而，由于更大的硬件类型，Kubernetes调度器可能会首先分配部署pod到`large-mem-pool`，因为它有足够的内存容量。
- en: However, you may want to preserve `large-mem-pool` node in case a big application
    needs large heap memory size (for example, Java application). Therefore, you may
    want to differentiate `default-pool` and `large-mem-pool`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您可能希望保留`large-mem-pool`节点，以防一个大型应用程序需要大的堆内存大小（例如，Java应用程序）。因此，您可能希望区分`default-pool`和`large-mem-pool`。
- en: 'In this case, Kubernetes label `beta.kubernetes.io/instance-type` helps to
    distinguish instance type of node. Therefore, use `nodeSelector` to specify a
    desired node to the pod. For example, following `nodeSelector` parameter will
    force to use `f1-micro` node for nginx application:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Kubernetes标签`beta.kubernetes.io/instance-type`有助于区分节点的实例类型。因此，使用`nodeSelector`来指定pod的所需节点。例如，以下`nodeSelector`参数将强制使用`f1-micro`节点进行nginx应用程序：
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you want to specify a particular label instead of `beta.kubernetes.io/instance-type`,
    use `--node-labels` option to create a node pool. That assigns your desired label
    for the node pool.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想指定一个特定的标签而不是`beta.kubernetes.io/instance-type`，请使用`--node-labels`选项创建一个节点池。这将为节点池分配您所需的标签。
- en: 'For more details, please read the following online document:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请阅读以下在线文档：
- en: '[https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create](https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create](https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create)。'
- en: 'Of course, you can feel free to remove a node pool if you no longer need it.
    To do that, run the following command to delete `default-pool` (`f1-micro` x 5
    instances). This operation will involve pod migration (terminate pod on `default-pool`
    and re-launch on `large-mem-pool`) automatically, if there are some pods running
    at `default-pool`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您不再需要它，可以随时删除一个节点池。要做到这一点，请运行以下命令删除`default-pool`（`f1-micro` x 5个实例）。如果在`default-pool`上有一些正在运行的pod，此操作将自动涉及pod迁移（终止`default-pool`上的pod并重新在`large-mem-pool`上启动）：
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You may have noticed that all of the preceding operations happened in a single
    zone (`asia-northeast1-a`). Therefore, if `asia-northeast1-a` zone gets an outage,
    your cluster will be down. In order to avoid zone failure, you may consider setting
    up a multi zone cluster.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，所有前面的操作都发生在一个单一区域（`asia-northeast1-a`）中。因此，如果`asia-northeast1-a`区域发生故障，您的集群将会宕机。为了避免区域故障，您可以考虑设置一个多区域集群。
- en: Multi zone cluster
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多区域集群
- en: GKE supports multi zone cluster that allows you to launch Kubernetes nodes on
    multiple zones, but limits within the same region. In previous examples, it has
    been provisioned at `asia-northeast1-a` only, so let's re-provision a cluster
    that has `asia-northeast1-a`, `asia-northeast1-b` and `asia-northeast1-c` in a
    total of three zones.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: GKE支持多区域集群，允许您在多个区域上启动Kubernetes节点，但限制在同一地区内。在之前的示例中，它只在`asia-northeast1-a`上进行了配置，因此让我们重新配置一个集群，其中包括`asia-northeast1-a`，`asia-northeast1-b`和`asia-northeast1-c`，总共三个区域。
- en: It is very simple; you just append an `--additional-zones` parameter when creating
    a new cluster.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单；在创建新集群时，只需添加一个`--additional-zones`参数。
- en: 'As of August, 2017, there is a beta feature that supports to update existing
    clusters from single zones to multi zones. Use a beta command as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2017年8月，有一个测试功能支持将现有集群从单个区域升级到多个区域。使用以下测试命令：
- en: '`$ gcloud beta container clusters update my-k8s-cluster --additional-zones=asia-northeast1-b,asia-northeast1-c`.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud beta container clusters update my-k8s-cluster --additional-zones=asia-northeast1-b,asia-northeast1-c`。'
- en: To change an existing cluster to multi zone, it may need an additional SDK tool
    installation, but out of SLA.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要将现有集群更改为多区域，可能需要安装额外的SDK工具，但不在SLA范围内。
- en: 'Let''s delete the previous cluster, and create a new cluster with an `--additional-zones`
    option:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们删除之前的集群，并使用`--additional-zones`选项创建一个新的集群：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this example, it will create two nodes per zones (`asia-northeast1-a`, `b`
    and `c`); therefore, a total of six nodes will be added:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，它将在每个区域（`asia-northeast1-a`，`b`和`c`）创建两个节点；因此，总共将添加六个节点：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You may also distinguish node zone by Kubernetes label `failure-domain.beta.kubernetes.io/zone`
    so that you can specify desired zones to deploy a pod.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过Kubernetes标签`failure-domain.beta.kubernetes.io/zone`区分节点区域，以便指定要部署Pod的所需区域。
- en: Cluster upgrade
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群升级
- en: Once you start to manage Kubernetes, you may encounter some difficulty when
    upgrading Kubernetes clusters. Because the Kubernetes project is very aggressive,
    around every three months, there is a new release, such as 1.6.0 (released on
    March 28^(th) 2017) to 1.7.0 (released on June 29^(th) 2017).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦开始管理Kubernetes，您可能会在升级Kubernetes集群时遇到一些困难。因为Kubernetes项目非常积极，大约每三个月就会有一个新版本发布，例如1.6.0（2017年3月28日发布）到1.7.0（2017年6月29日发布）。
- en: 'GKE also keeps adding new version support in a timely manner. It allows us
    to upgrade both master and nodes via the `gcloud` command. You can run the following
    command to see which Kubernetes version is supported by GKE:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: GKE还会及时添加新版本支持。它允许我们通过`gcloud`命令升级主节点和节点。您可以运行以下命令查看GKE支持的Kubernetes版本：
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'So, you may see the latest supported version is 1.7.3 on both master and node
    at this moment. Since the previous example installed is version 1.6.7, let''s
    update to 1.7.3\. First of all, you need to upgrade master first:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可能会看到此时主节点和节点上支持的最新版本都是1.7.3。由于之前的示例安装的是1.6.7版本，让我们升级到1.7.3。首先，您需要先升级主节点：
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It takes around 10 minutes depending on environment, after that you can verify
    via the following command:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 根据环境，大约需要10分钟，之后您可以通过以下命令进行验证：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now you can upgrade all nodes to version 1.7.3\. Because GKE tries to perform
    rolling upgrade, it will perform the following steps per node one by one:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将所有节点升级到1.7.3版本。因为GKE尝试执行滚动升级，它将按照以下步骤逐个节点执行：
- en: Deregister a target node from the cluster.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从集群中注销目标节点。
- en: Delete old VM instance.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除旧的VM实例。
- en: Provision a new VM instance.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个新的VM实例。
- en: Set up the node with the 1.7.3 version.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置节点为1.7.3版本。
- en: Register to master.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册到主节点。
- en: 'Therefore, it takes much longer than a master upgrade:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它比主节点升级需要更长的时间：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'During rolling upgrade, you can see node status as follows and it shows a mid
    process of rolling update (two nodes have upgraded to 1.7.3, one node is upgrading,
    three nodes are pending):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在滚动升级期间，您可以看到节点状态如下，并显示滚动更新的中间过程（两个节点已升级到1.7.3，一个节点正在升级，三个节点处于挂起状态）：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Kubernetes cloud provider
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes云提供商
- en: GKE also integrates Kubernetes cloud provider out of box that deep integrate
    to GCP infrastructure; for example overlay network by VPC route, StorageClass
    by Persistent Disk, and Service by L4 LoadBalancer. The best part is ingress by
    L7 LoadBalancer. Let's take a look at how it works.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: GKE还集成了Kubernetes云提供商，可以深度整合到GCP基础设施；例如通过VPC路由的覆盖网络，通过持久磁盘的StorageClass，以及通过L4负载均衡器的服务。最好的部分是通过L7负载均衡器的入口。让我们看看它是如何工作的。
- en: StorageClass
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StorageClass
- en: 'As per as kops on AWS, GKE also sets up StorageClass by default, which uses
    Persistent Disk:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与AWS上的kops一样，GKE也默认设置了StorageClass，使用持久磁盘：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Therefore, when creating Persistent Volume Claim, it will allocate GCP Persistent
    Disk as Kubernetes Persistent Volume automatically. Regarding Persistent Volume
    Claim and Dynamic Provisioning, please refer to [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在创建持久卷索赔时，它将自动将GCP持久磁盘分配为Kubernetes持久卷。关于持久卷索赔和动态配置，请参阅[第4章](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e)，*使用存储和资源*：
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: L4 LoadBalancer
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L4负载均衡器
- en: Similar to AWS cloud provider, GKE also supports using L4 LoadBalancer for Kubernetes
    Service. Just specify `Service.spec.type` as LoadBalancer, and then GKE will set
    up and configure L4 LoadBalancer automatically.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与AWS云提供商类似，GKE还支持使用L4负载均衡器来为Kubernetes服务提供支持。只需将`Service.spec.type`指定为LoadBalancer，然后GKE将自动设置和配置L4负载均衡器。
- en: 'Note that the corresponding firewall rule between L4 LoadBalancer to Kubernetes
    node can be created by cloud provider automatically. It is simple but powerful
    enough if you want to expose your application to the internet quickly:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，L4负载均衡器到Kubernetes节点之间的相应防火墙规则可以由云提供商自动创建。如果您想快速将应用程序暴露给互联网，这种方法简单但足够强大：
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: L7 LoadBalancer (ingress)
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L7负载均衡器（入口）
- en: GKE also supports Kubernetes ingress that can set up GCP L7 LoadBalancer to
    dispatch HTTP requests to the target service based on URL. You just need to set
    up one or more NodePort services and then create ingress rules to point to services.
    Behind the scenes, Kubernetes creates and configures firewall rules, health check,
    backend service, forwarding rules, and URL maps automatically.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: GKE还支持Kubernetes入口，可以设置GCP L7负载均衡器，根据URL将HTTP请求分发到目标服务。您只需要设置一个或多个NodePort服务，然后创建入口规则指向服务。在幕后，Kubernetes会自动创建和配置防火墙规则、健康检查、后端服务、转发规则和URL映射。
- en: 'Let''s create same examples that use nginx and Tomcat to deploy to the Kubernetes
    cluster first. These are using Kubernetes Services that bind to NodePort instead
    of LoadBalancer:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建相同的示例，使用nginx和Tomcat部署到Kubernetes集群。这些示例使用绑定到NodePort而不是LoadBalancer的Kubernetes服务：
- en: '**![](../images/00145.jpeg)**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](../images/00145.jpeg)**'
- en: At this moment, you cannot access service, because there are no firewall rules
    that allow access to the Kubernetes node from the internet yet. So, let's create
    Kubernetes ingress that points to these services.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您无法访问服务，因为还没有防火墙规则允许从互联网访问Kubernetes节点。因此，让我们创建指向这些服务的Kubernetes入口。
- en: 'You can use `kubectl port-forward <pod name> <your machine available port><:
    service port number>` to access via the Kubernetes API server. For the preceding
    case, use `kubectl port-forward tomcat-670632475-l6h8q 10080:8080.`.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以使用`kubectl port-forward <pod name> <your machine available port><: service
    port number>`通过Kubernetes API服务器访问。对于前面的情况，请使用`kubectl port-forward tomcat-670632475-l6h8q
    10080:8080.`。'
- en: After that, open your web browser to `http://localhost:10080/` and then you
    can access Tomcat pod directly.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，打开您的Web浏览器到`http://localhost:10080/`，然后您可以直接访问Tomcat pod。
- en: 'Kubernetes ingress definition is quite similar to GCP backend service definition
    as it needs to specify a combination of URL path, Kubernetes service name, and
    service port number. So in this scenario, URL `/` and `/*` point to nginx service,
    also URL `/examples` and `/examples/*` point to the Tomcat service as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Ingress定义与GCP后端服务定义非常相似，因为它需要指定URL路径、Kubernetes服务名称和服务端口号的组合。因此，在这种情况下，URL
    `/` 和 `/*` 指向nginx服务，URL `/examples` 和 `/examples/*` 指向Tomcat服务，如下所示：
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It takes around 10 minutes to fully configure GCP components such as health
    check, forwarding rule, backend services, and url-maps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 大约需要10分钟来完全配置GCP组件，如健康检查、转发规则、后端服务和URL映射：
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You can also check the status on the web console as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过Web控制台检查状态，如下所示：
- en: '![](../images/00146.jpeg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: ！[](../images/00146.jpeg)
- en: Once you have completed the setup of L7 LoadBalancer, you can access the public
    IP address of LoadBalancer (`http://107.178.253.174/`) to see the nginx page.
    As well as access to `http://107.178.253.174/examples/` then you can see `tomcat
    example` page.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 完成L7负载均衡器的设置后，您可以访问负载均衡器的公共IP地址（`http://107.178.253.174/`）来查看nginx页面。以及访问`http://107.178.253.174/examples/`，然后您可以看到`tomcat示例`页面。
- en: In the preceding steps, we created and assigned an ephemeral IP address for
    L7 LoadBalancer. However, the best practice to use L7 LoadBalancer is to assign
    a static IP address instead, because you can also associate DNS (FQDN) to the
    static IP address.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们为L7负载均衡器创建并分配了临时IP地址。然而，使用L7负载均衡器的最佳实践是分配静态IP地址，因为您还可以将DNS（FQDN）关联到静态IP地址。
- en: 'To do that, update ingress setting to add an annotation `kubernetes.io/ingress.global-static-ip-name`
    to associate a GCP static IP address name as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，更新Ingress设置以添加注释`kubernetes.io/ingress.global-static-ip-name`，以关联GCP静态IP地址名称，如下所示：
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So, now you can access ingress via a static IP address as `http://35.186.227.252/`
    (nginx) and `http://35.186.227.252/examples/` (Tomcat) instead.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在您可以通过静态IP地址访问入口，如`http://35.186.227.252/`（nginx）和`http://35.186.227.252/examples/`（Tomcat）。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed Google Cloud Platform. The basic concept is similar
    to AWS, but some of the policies and concepts are different. Especially Google
    Container Engine, as it is a very powerful service to use Kubernetes as production
    grade. Kubernetes cluster and node management are quite easy, not only the installation,
    but also upgrade. Cloud provider is also fully integrated to GCP, especially Ingress
    as it can configure L7 LoadBalancer with one command. Therefore, it is highly
    recommended to try GKE if you plan to use Kubernetes on the public cloud.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Google Cloud Platform。基本概念类似于AWS，但一些政策和概念是不同的。特别是Google Container
    Engine，作为一个非常强大的服务，可以将Kubernetes用作生产级别。Kubernetes集群和节点管理非常容易，不仅安装，还有升级。云提供商也完全集成到GCP中，特别是Ingress，因为它可以通过一个命令配置L7负载均衡器。因此，如果您计划在公共云上使用Kubernetes，强烈建议尝试GKE。
- en: The next chapter will provide a sneak preview to some new features and alternative
    services to against Kubernetes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将提供一些新功能和替代服务的预览，以对抗Kubernetes。
