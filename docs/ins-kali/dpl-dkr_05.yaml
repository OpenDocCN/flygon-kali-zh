- en: Keeping the Data Persistent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保持数据持久性
- en: 'In this chapter, we will cover how to keep your important data persistent,
    safe, and independent of your containers by covering everything about Docker volumes.
    We will go through various topics, including the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何通过涵盖Docker卷的所有内容来保持您的重要数据持久、安全并独立于您的容器。我们将涵盖各种主题，包括以下内容：
- en: Docker image internals
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker镜像内部
- en: Deploying your own instance of a repository
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署您自己的存储库实例
- en: Transient storage
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瞬态存储
- en: Persistent storage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久存储
- en: Bind-mounts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绑定挂载
- en: Named volumes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名卷
- en: Relocatable volumes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可移动卷
- en: User and group ID handling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和组ID处理
- en: While we won't cover all the available storage options, especially ones that
    are specific to orchestration tooling, this chapter should give you a better understanding
    of how Docker handles data and what you can do to make sure it is kept in exactly
    the way you want it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会涵盖所有可用的存储选项，特别是那些特定于编排工具的选项，但本章应该让您更好地了解Docker如何处理数据，以及您可以采取哪些措施来确保数据被保持在您想要的方式。
- en: Docker image internals
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker镜像内部
- en: To understand even better why we need persistent data, we first need to understand
    in detail how Docker handles container layers. We covered this topic in some detail
    in previous chapters, but here, we will spend some time to understand what is
    going on under the covers. We will first discuss what Docker currently does for
    handling the written data within containers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解为什么我们需要持久性数据，我们首先需要详细了解Docker如何处理容器层。我们在之前的章节中已经详细介绍了这个主题，但在这里，我们将花一些时间来了解底层发生了什么。我们将首先讨论Docker目前如何处理容器内部的写入数据。
- en: How images are layered
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 镜像的分层方式
- en: As we covered earlier, Docker stores data that composes the images in a set
    of discrete, read-only filesystem layers that are stacked on top of each other
    when you build your image. Any changes done to the filesystem are stacked like
    transparent slides on top of each other to create the full tree, and any files
    that have newer content (including being completely removed) will mask the old
    ones with each new layer. Our former depth of understanding here would probably
    be sufficient for the basic handling of containers, but for advanced usage, we
    need to know the full internals on how the data gets handled.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所介绍的，Docker将组成镜像的数据存储在一组离散的只读文件系统层中，当您构建镜像时，这些层会堆叠在一起。对文件系统所做的任何更改都会像透明幻灯片一样堆叠在一起，以创建完整的树，任何具有更新内容的文件（包括完全删除的文件）都会用新层遮盖旧的文件。我们以前对此的理解深度可能已经足够用于基本的容器处理，但对于高级用法，我们需要了解数据的全部内部处理方式。
- en: 'When you start multiple containers with the same base image, all of them are
    given the same set of filesystem layers as the original image so they start from
    the exact same filesystem history (barring any mounted volumes or variables),
    as we''d expect. However, during the start up process, an extra writable layer
    is added to the top of the image, which persists any data written within that
    specific container:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用相同的基础镜像启动多个容器时，它们都会被赋予与原始镜像相同的一组文件系统层，因此它们从完全相同的文件系统历史开始（除了任何挂载的卷或变量），这是我们所期望的。然而，在启动过程中，会在镜像顶部添加一个额外的可写层，该层会保留容器内部写入的任何数据：
- en: '![](assets/29fbb07c-b4ce-4194-96e6-3a3cb9968a06.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/29fbb07c-b4ce-4194-96e6-3a3cb9968a06.png)'
- en: As you would expect, any new files are written to this top layer, but this layer
    is actually not the same type as the other ones but a special **copy-on-write** (**CoW**)
    type. If a file that you are writing to in a container is already part of one
    of the underlying layers, Docker will make a copy of it in the new layer, masking
    the old one and from that point forward if you read or write to that file, the
    CoW layer will return its content.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所期望的那样，任何新文件都将写入此顶层，但是这个层实际上不是与其他层相同的类型，而是特殊的**写时复制**（CoW）类型。如果您在容器中写入的文件已经是底层层之一的一部分，Docker将在新层中对其进行复制，掩盖旧文件，并从那时起，如果您读取或写入该文件，CoW层将返回其内容。
- en: If you destroy this container without trying to save this new CoW layer or without
    using volumes, as we have experienced this earlier but in a different context,
    this writable layer will get deleted and all the data written to the filesystem
    by that container will be effectively lost. In fact, if you generally think of
    containers as just images with a thin and writable CoW layer, you can see how
    simple yet effective this layering system is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在不尝试保存这个新的CoW层或不使用卷的情况下销毁此容器，就像我们之前在不同的上下文中经历过的那样，这个可写层将被删除，并且容器写入文件系统的所有数据将被有效删除。实际上，如果您通常将容器视为具有薄且可写的CoW层的镜像，您会发现这种分层系统是多么简单而有效。
- en: Persisting the writable CoW layer(s)
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化可写的CoW层
- en: 'At some point or another, you might want to save the writable container layer
    to use as a regular image later. While this type of image splicing is highly discouraged,
    and I would tend to mostly agree, you may find times where it could provides you
    with an invaluable debugging tooling when you are unable to investigate the container
    code in other ways. To create an image from an existing container, there is the
    `docker commit` command:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时候，您可能希望保存可写的容器层，以便以后用作常规镜像。虽然强烈不建议这种类型的镜像拼接，我大多数情况下也会同意，但您可能会发现在其他方式无法调查容器代码时，它可以为您提供一个宝贵的调试工具。要从现有容器创建镜像，有`docker
    commit`命令：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, we just need some basic information, and Docker will take care
    of the rest. How about we try this out on our own:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们只需要一些基本信息，Docker会处理其余的部分。我们自己试一下如何：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `docker commit -c` switch is very useful and adds a command to the image
    just like the Dockerfile would and accepts the same directives that the Dockerfile
    does, but since this form is so rarely used, we have decided to skip it. If you
    would like to know more about this particular form and/or more about `docker commit`,
    feel free to explore [https://docs.docker.com/engine/reference/commandline/commit/#commit-a-container-with-new-configurations](https://docs.docker.com/engine/reference/commandline/commit/#commit-a-container-with-new-configurations)
    at leisure.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker commit -c`开关非常有用，并且像Dockerfile一样向镜像添加命令，并接受Dockerfile接受的相同指令，但由于这种形式很少使用，我们决定跳过它。如果您想了解更多关于这种特定形式和/或更多关于`docker
    commit`的信息，请随意在闲暇时探索[https://docs.docker.com/engine/reference/commandline/commit/#commit-a-container-with-new-configurations](https://docs.docker.com/engine/reference/commandline/commit/#commit-a-container-with-new-configurations)。'
- en: Running your own image registry
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行您自己的镜像注册表
- en: In our previous chapter, during Swarm deploys, we were getting warnings about
    not using a registry for our images and for a good reason. All the work we did
    was based on our images being available only to our local Docker Engine so multiple
    nodes could not have been able to use any of the images that we built. For absolutely
    bare-bones setups, you can use Docker Hub ([https://hub.docker.com/](https://hub.docker.com/))
    as an option to host your public images, but since practically every **Virtual
    Private Cloud (VPC)** cluster uses their own internal instance of a private registry
    for security, speed, and privacy, we will leave Docker Hub as an exercise for
    you if you want to explore it and we will cover how to run our own registry here.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的章节中，在Swarm部署期间，我们收到了有关不使用注册表来存储我们的镜像的警告，而且理由充分。我们所做的所有工作都是基于我们的镜像仅对我们本地的Docker引擎可用，因此多个节点无法使用我们构建的任何镜像。对于绝对基本的设置，您可以使用Docker
    Hub（[https://hub.docker.com/](https://hub.docker.com/)）作为托管公共镜像的选项，但由于几乎每个**虚拟私有云（VPC）**集群都使用其自己的内部私有注册表实例来确保安全、速度和隐私，我们将把Docker
    Hub作为一个探索的练习留给您，如果您想探索它，我们将介绍如何在这里运行我们自己的注册表。
- en: Docker has recently come out with a service called Docker Cloud ([https://cloud.docker.com/](https://cloud.docker.com/)),
    which has private registry hosting and continuous integration and may cover a
    decent amount of use cases for small-scale deployments, though the service is
    not free past a single private repository at this time. Generally, though, the
    most preferred way of setting up scalable Docker-based clusters is a privately
    hosted registry, so we will focus on that approach, but keep an eye on Docker
    Cloud's developing feature set as it may fill some operational gaps in your clusters
    that you can defer as you build other parts of your infrastructure.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Docker最近推出了一个名为Docker Cloud的服务（[https://cloud.docker.com/](https://cloud.docker.com/)），其中包括私有注册表托管和持续集成，可能涵盖了小规模部署的相当多的用例，尽管目前该服务在单个私有存储库之外并不免费。一般来说，建立可扩展的基于Docker的集群的最受欢迎的方式是使用私有托管的注册表，因此我们将专注于这种方法，但要密切关注Docker
    Cloud正在开发的功能集，因为它可能填补了集群中的一些运营空白，您可以在构建基础设施的其他部分时推迟处理这些空白。
- en: 'To host a registry locally, Docker has provided a Docker Registry image (`registry:2`)
    that you can run as a regular container with various backends, including the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在本地托管注册表，Docker提供了一个Docker Registry镜像（`registry:2`），您可以将其作为常规容器运行，包括以下后端：
- en: '`inmemory`: A temporary image storage with a local in-memory map. This is only
    recommended for testing.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inmemory`：使用本地内存映射的临时镜像存储。这仅建议用于测试。'
- en: '`filesystem`: Stores images using a regular filesystem tree.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filesystem`：使用常规文件系统树存储镜像。'
- en: '`s3`, `azure`, `swift`, `oss`, `gcs`: Cloud vendor-specific implementations
    of storage backends.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s3`，`azure`，`swift`，`oss`，`gcs`：云供应商特定的存储后端实现。'
- en: Let us deploy a registry with a local filesystem backend and see how it can
    be used.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署一个具有本地文件系统后端的注册表，并看看它如何被使用。
- en: Warning! The following section does not use TLS-secured or authenticated registry
    configuration. While this configuration might be acceptable in some rare circumstances
    in isolated VPCs, generally, you would want to both secure the transport layer
    with TLS certificates and add some sort of authentication. Luckily, since the
    API is HTTP-based, you can do most of this with an unsecured registry with a reverse-proxied
    web server in front of it, like we did earlier with NGINX. Since the certificates
    need to be "valid" as evaluated by your Docker client and this procedure is different
    for pretty much every operating system out there, doing the work here would generally
    not be portable in most configurations, which is why we are skipping it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 警告！以下部分不使用TLS安全或经过身份验证的注册表配置。虽然在一些孤立的VPC中，这种配置可能是可以接受的，但通常情况下，您希望使用TLS证书来保护传输层，并添加某种形式的身份验证。幸运的是，由于API是基于HTTP的，您可以在不安全的注册表上使用反向代理的Web服务器，就像我们之前使用NGINX一样。由于证书需要被您的Docker客户端评估为“有效”，而这个过程对于几乎每个操作系统来说都是不同的，因此在大多数配置中，这里的工作通常不具备可移植性，这就是为什么我们跳过它的原因。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, using the local registry actually seems to be pretty easy! The
    only new thing introduced here that might need a bit of coverage outside of the
    registry itself is `--restart=always`, which makes sure that the containers automatically
    restarts if it exits unexpectedly. The tagging is required to associate an image
    with the registry, so with doing `docker tag [<source_registry>/]<original_tag_or_id>
    [<target_registry>/]<new_tag>`, we can effectively assign a new tag to either
    an existing image tag, or we can create a new tag. As indicated in this small
    code snippet, both the source and the target can be prefixed with an optional
    repository location that defaults to `docker.io` (Docker Hub) if not specified.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用本地注册表似乎非常容易！这里引入的唯一新事物可能需要在注册表本身之外进行一些覆盖的是`--restart=always`，它确保容器在意外退出时自动重新启动。标记是必需的，以将图像与注册表关联起来，因此通过执行`docker
    tag [<source_registry>/]<original_tag_or_id> [<target_registry>/]<new_tag>`，我们可以有效地为现有图像标签分配一个新标签，或者我们可以创建一个新标签。正如在这个小的代码片段中所示，源和目标都可以以可选的存储库位置为前缀，如果未指定，则默认为`docker.io`（Docker
    Hub）。
- en: 'Sadly, from personal experience, even though this example has made things look
    real easy, real deployments of the registry are definitely not easy since appearances
    can be deceiving and there are a few things you need to keep in mind when using
    it:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，根据个人经验，尽管这个例子让事情看起来很容易，但实际的注册表部署绝对不容易，因为外表可能具有欺骗性，而在使用它时需要牢记一些事情：
- en: 'If you use an insecure registry, to access it from a different machine, you
    must add `"insecure-registries" : ["<ip_or_dns_name>:<port>"]` to `/etc/docker/daemon.json`
    to every Docker Engine that will be using this registry''s images.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果您使用不安全的注册表，要从不同的机器访问它，您必须将`"insecure-registries" : ["<ip_or_dns_name>:<port>"]`添加到将使用该注册表的图像的每个Docker引擎的`/etc/docker/daemon.json`中。'
- en: 'Note: This configuration is not recommended for a vast number of security reasons.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：出于许多安全原因，不建议使用此配置。
- en: If you use an invalid HTTPS certificate, you have to also mark it as an insecure
    registry on all clients.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用无效的HTTPS证书，您还必须在所有客户端上将其标记为不安全的注册表。
- en: This configuration is also not recommended as it is only marginally better than
    the unsecured registry due to possible transport downgrade **Man-in-the-Middle
    (MITM)** attacks
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种配置也不建议，因为它只比不安全的注册表稍微好一点，可能会导致传输降级**中间人攻击（MITM）**。
- en: 'The final word of advice that I would give you regarding the registry is that
    the cloud provider backend documentation for the registry has been, in my experience,
    notoriously and persistently (dare I say intentionally?) incorrect. I would highly
    recommend that you go through the source code if the registry rejects your settings
    since setting the right variables is pretty unintuitive. You can also use a mounted
    file to configure the registry, but if you don''t want to build a new image when
    your cluster is just starting up, environmental variables are the way to go. The
    environment variables are all-capital names with "`_`" segment-joined names and
    match up to the hierarchy of the available options:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我要给你的最后一条建议是，根据我的经验，注册表的云提供商后端文档一直都是错误的，并且一直（我敢说是故意的吗？）错误。我强烈建议，如果注册表拒绝了你的设置，你应该查看源代码，因为设置正确的变量相当不直观。你也可以使用挂载文件来配置注册表，但如果你不想在集群刚启动时构建一个新的镜像，环境变量是一个不错的选择。环境变量都是全大写的名称，用“_”连接起来，并与可用选项的层次结构相匹配：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This field for the registry would then be set with `-e PARENT_CHILD_OPTION_SOME_SETTING=<value>`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，注册表的这个字段将设置为“-e PARENT_CHILD_OPTION_SOME_SETTING=<value>”。
- en: For a complete list of the available registry options, you can visit [https://github.com/docker/docker-registry/blob/master/config/config_sample.yml](https://github.com/docker/docker-registry/blob/master/config/config_sample.yml)
    and see which ones you would need to run your registry. As mentioned earlier,
    I have found the main documentation on [docs.docker.com](https://docs.docker.com/)
    and a large percentage of documentation on the code repository itself extremely
    unreliable for configurations, so don't be afraid to read the source code in order
    to find out what the registry is actually expecting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有关可用注册表选项的完整列表，您可以访问[https://github.com/docker/docker-registry/blob/master/config/config_sample.yml](https://github.com/docker/docker-registry/blob/master/config/config_sample.yml)，并查看您需要运行注册表的选项。正如前面提到的，我发现[docs.docker.com](https://docs.docker.com/)上的主要文档以及代码存储库本身的大部分文档在配置方面极不可靠，因此不要害怕阅读源代码以找出注册表实际期望的内容。
- en: 'To help people who will deploy the registry with the most likely backing storage
    outside of `filesystem`, which is `s3`, I will leave you a working (at the time
    of writing this) configuration:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助那些将使用最有可能的后备存储（在“文件系统”之外）部署注册表的人，即“s3”，我将留下一个可用的（在撰写本文时）配置：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Underlying storage driver
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 底层存储驱动程序
- en: This section may be a bit too advanced for some readers and does not strictly
    require reading, but in the interest of fully understanding how Docker handles
    images and what issues you might encounter on large-scale deployments, I would
    encourage everyone to at least skim through it as the identification of backing-storage
    driver issues may be of use. Also, be aware that issues mentioned here may not
    age gracefully as the Docker code base evolves, so check out their website for
    up-to-date information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分对一些读者来说可能有点高级，并且并不严格要求阅读，但为了充分理解Docker如何处理镜像以及在大规模部署中可能遇到的问题，我鼓励每个人至少浏览一下，因为识别后备存储驱动程序问题可能会有用。另外，请注意，这里提到的问题可能随着Docker代码库的演变而变得不太适用，因此请查看他们的网站以获取最新信息。
- en: Unlike what you might have expected from the Docker daemon, the handling of
    the image layers locally is actually done in a very modular way so that almost
    any layering filesystem driver can be plugged into the daemon. The storage driver
    controls how images are stored and retrieved on your docker host(s), and while
    there may not be any difference from the client's perspective, each one is unique
    in many aspects.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与您可能从Docker守护程序期望的不同，本地图像层的处理实际上是以非常模块化的方式进行的，因此几乎可以将任何分层文件系统驱动程序插入到守护程序中。存储驱动程序控制着图像在您的Docker主机上的存储和检索方式，虽然从客户端的角度看可能没有任何区别，但每个驱动程序在许多方面都是独一无二的。
- en: To start, all of the available storage drivers we will mention are provided
    by the underlying containerization technology used by Docker, called `containerd`.
    While knowing anything beyond that last sentence about it is generally overkill
    for most Docker usages, suffice it to say that it is just one of underlying modules
    that Docker uses as the image handling API. `containerd` provides a stable API
    for storing and retrieving images and their designated layers so that any software
    built on top of it (such as Docker and Kubernetes) can worry about just tying
    it all together.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将提到的所有可用存储驱动程序都是由Docker使用的底层容器化技术`containerd`提供的。虽然了解它之外的任何内容通常对大多数Docker用途来说都是多余的，但可以说它只是Docker用作图像处理API的底层模块之一。`containerd`提供了一个稳定的API，用于存储和检索图像及其指定的层，以便构建在其之上的任何软件（如Docker和Kubernetes）只需担心将其全部整合在一起。
- en: You may see references in code and/or documentation about things called graphdrivers,
    which is pedantically the high-level API that interacts with storage drivers,
    but in most cases, when it is written, it is used to describe a storage driver
    that implements the graphdriver API; for example, when a new type of storage driver
    is talked about, you will often see it referred to as a new graphdriver.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会在代码和/或文档中看到有关称为图形驱动程序的内容，这在学究式上是与存储驱动程序进行交互的高级API，但在大多数情况下，当它被写入时，它用于描述实现图形驱动程序API的存储驱动程序；例如，当谈论新类型的存储驱动程序时，您经常会看到它被称为新的图形驱动程序。
- en: 'To see which backing filesystem you are using, you can type `docker info` and
    look for the `Storage Driver` section:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看您正在使用的后备文件系统，可以输入`docker info`并查找`Storage Driver`部分：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Warning! Changing the storage driver will, in most cases, remove access to any
    and all images and layers from your machine that were stored by the old driver,
    so proceed with care! Also, I believe that by changing the storage driver without
    manually cleaning images and containers either through CLI and/or by deleting
    things from `/var/lib/docker/` will leave those images and containers dangling,
    so make sure to clean things up if you consider these changes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 警告！在大多数情况下，更改存储驱动程序将删除您的计算机上由旧驱动程序存储的任何和所有图像和层的访问权限，因此请谨慎操作！此外，我相信通过更改存储驱动程序而不通过CLI手动清理图像和容器，或者通过从`/var/lib/docker/`中删除内容，将使这些图像和容器悬空，因此请确保在考虑这些更改时清理一下。
- en: 'If you would like to change your storage driver to any of the options we will
    discuss here, you can edit (or create if missing) `/etc/docker/daemon.json` and
    add the following to it, after which you should restart the docker service:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想将存储驱动程序更改为我们将在此处讨论的任何选项，您可以编辑（或创建缺失的）`/etc/docker/daemon.json`并在其中添加以下内容，之后应重新启动docker服务：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If `daemon.json` does not work, you can also try changing `/etc/default/docker`
    by adding a `-s` flag to `DOCKER_OPTS` and restarting the service:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`daemon.json`不起作用，您还可以尝试通过向`DOCKER_OPTS`添加`-s`标志并重新启动服务来更改`/etc/default/docker`：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In general, Docker is transitioning from `/etc/default/docker` (the path dependent
    on distribution) to `/etc/docker/daemon.json` as its configuration file, so if
    you see somewhere on the Internet or other documentation that the former file
    is referenced, see whether you can find the equivalent configuration for `daemon.json`
    as I believe that it will fully replace the other one at some point in the future
    (as with all books, probably under a week after this book gets released).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Docker正在从`/etc/default/docker`（取决于发行版的路径）过渡到`/etc/docker/daemon.json`作为其配置文件，因此，如果您在互联网或其他文档中看到引用了前者文件，请查看是否可以找到`daemon.json`的等效配置，因为我相信它将在将来的某个时候完全取代另一个（就像所有的书籍一样，可能是在这本书发布后的一周内）。
- en: So now that we know what storage drivers are and how to change them, what our
    the options that we can use here?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们知道了存储驱动程序是什么，以及如何更改它们，我们可以在这里使用哪些选项呢？
- en: aufs
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: aufs
- en: '`aufs` (also known as `unionfs`) is the oldest but probably the most mature
    and stable layered filesystem available for Docker. This storage driver is generally
    fast to start and efficient in terms of storage and memory overhead. If your kernel
    has been built with support for this driver, Docker will default to it, but generally,
    outside of Ubuntu and only with the `linux-image-extra-$(uname -r)` package installed,
    most distributions do not add that driver to their kernels, nor do they have it
    available, so most likely your machine will not be able to run it. You could download
    the kernel source and recompile it with `aufs` support, but generally, this is
    such a nightmare of a maintenance that you might as well choose a different storage
    driver if it is not readily available. You can use `grep aufs /proc/filesystems`
    to check whether your machine has the `aufs` kernel module enabled and available.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`aufs`（也称为`unionfs`）是Docker可用的最古老但可能也是最成熟和稳定的分层文件系统。这种存储驱动程序通常启动快速，并且在存储和内存开销方面非常高效。如果您的内核已构建支持此驱动程序，Docker将默认使用它，但通常情况下，除了Ubuntu并且只有安装了`linux-image-extra-$(uname
    -r)`软件包的情况下，大多数发行版都不会将该驱动程序添加到其内核中，也不会提供该驱动程序，因此您的计算机很可能无法运行它。您可以下载内核源代码并重新编译以支持`aufs`，但通常情况下，这是一个维护的噩梦，如果它不容易获得，您可能会选择不同的存储驱动程序。您可以使用`grep
    aufs /proc/filesystems`来检查您的计算机是否启用并可用`aufs`内核模块。'
- en: Note that the `aufs` driver can only be used on `ext4` and `xfs` filesystems.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`aufs`驱动程序只能用于`ext4`和`xfs`文件系统。
- en: btrfs / zfs
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: btrfs / zfs
- en: These are conceptually less of drivers than actual filesystems that you mount
    under `/var/lib/docker` and each comes with its own set of pros and cons. Generally,
    they both have performance impacts as opposed to some of the other options and
    have a high memory overhead but may provide you with easier management tooling
    and/or higher density storage. Since these drivers currently have marginal support
    and I have heard of many critical bugs still affecting them, I would not advise
    using them in production unless you have very good reasons to do so. If the system
    has the appropriate drive mounted at `/var/lib/docker` and the related kernel
    modules are available, Docker will pick these next after `aufs`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在概念上不太像驱动程序，而更像是您在`/var/lib/docker`下挂载的实际文件系统，每个都有其自己的一套优缺点。一般来说，它们都会对性能产生影响，与其他选项相比具有较高的内存开销，但可能为您提供更容易的管理工具和/或更高密度的存储。由于这些驱动程序目前的支持有限，我听说仍然存在许多影响它们的关键错误，所以我不建议在生产环境中使用它们，除非您有非常充分的理由这样做。如果系统在`/var/lib/docker`下挂载了适当的驱动，并且相关的内核模块可用，Docker将在`aufs`之后选择这些驱动程序。
- en: Note that the order of preference here doesn't mean that these two storage drivers
    are more desirable than the other ones mentioned in this section but purely that
    if the drive is mounted with the appropriate (and uncommon) filesystem is at the
    expected Docker location, Docker will assume that this is the configuration that
    the user wanted.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的优先顺序并不意味着这两个存储驱动程序比本节中提到的其他存储驱动程序更可取，而纯粹是如果驱动器已挂载到适当（且不常见）的文件系统位置，则Docker将假定这是用户想要的配置。
- en: overlay and overlay2
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: overlay和overlay2
- en: These particular storage drivers are slowly becoming a favorite for Docker installations.
    They are very similar to `aufs` but are much faster and simpler implementation.
    Like `aufs`, both `overlay` and `overlay2` require a kernel overlay module included
    and loaded, which in general should be available on kernels 3.18 and higher. Also,
    both can run only on top of `ext4` or `xfs` filesystems. The difference between
    `overlay` and `overlay2` is that the newer version has improvements that were
    added in kernel 4.0 to reduce `inode` usage, but the older one has a longer track
    record in the field. If you have any doubt, `overlay2` is a rock-solid choice
    in almost any circumstance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特定的存储驱动程序正在逐渐成为Docker安装的首选。它们与`aufs`非常相似，但实现速度更快，更简单。与`aufs`一样，`overlay`和`overlay2`都需要包含和加载内核叠加模块，一般来说应该在3.18及更高版本的内核上可用。此外，两者只能在`ext4`或`xfs`文件系统上运行。`overlay`和`overlay2`之间的区别在于较新版本在内核4.0中添加了减少`inode`使用的改进，而较旧版本在领域中有更长的使用记录。如果您有任何疑问，`overlay2`几乎在任何情况下都是一个非常可靠的选择。
- en: If you have not worked with inodes before, note that they contain the metadata
    about each individual file on the filesystem and the maximum count allowed is
    in most cases hardcoded when the filesystem is created. While this hardcoded maximum
    is fine for most general usages, there are edge cases where you may run out of
    them, in which case the filesystem will give you errors on any new file creation
    even though you will have available space to store the file. If you want to learn
    more about these structures, you can visit [http://www.linfo.org/inode.html](http://www.linfo.org/inode.html)
    for more information.Both `overlay` and `overlay2` backing storage driver have
    been known to cause heavy inode usage due to how they handle file copies internally.
    While `overlay2` is advertised not to have these issues, I have personally run
    into inode problems numerous times, with large Docker volumes built with default
    inode maximums. If you ever use these drivers and notice that the disk is full
    with messages but you still have space on the device, check your inodes for exhaustion
    with `df -i` to ensure it is not the docker storage that is causing issues.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您以前没有使用过inode，请注意它们包含有关文件系统上每个单独文件的元数据，并且在创建文件系统时大多数情况下都是硬编码的最大计数。虽然这种硬编码的最大值对于大多数一般用途来说是可以的，但也有一些边缘情况，您可能会用尽它们，这种情况下文件系统将在任何新文件创建时给出错误，即使您有可用空间来存储文件。如果您想了解更多关于这些结构的信息，您可以访问[http://www.linfo.org/inode.html](http://www.linfo.org/inode.html)。`overlay`和`overlay2`支持的存储驱动程序由于其内部处理文件复制的方式而被认为会导致大量的inode使用。虽然`overlay2`被宣传为不会出现这些问题，但我个人在使用默认inode最大值构建大型Docker卷时多次遇到inode问题。如果您曾经使用这些驱动程序并注意到磁盘已满但设备上仍有空间，请使用`df
    -i`检查inode是否已用尽，以确保不是Docker存储引起的问题。
- en: devicemapper
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: devicemapper
- en: Instead of working on file-level devices, this driver operates directly on the
    block device where your Docker instance is. While the default setup generally
    sets up a loopback device and is mostly fine for local testing, this particular
    setup is extremely not suggested for production systems due to the sparse files
    it creates in the loopback device. For production systems, you are encouraged
    to combine it with `direct-lvm`, but that kind of intricate setup requires a configuration
    that is particularly tricky and slower than the `overlay` storage driver, so I
    would generally not recommend its use unless you are unable to use `aufs` or `overlay`/`overlay2`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个驱动程序不是在文件级设备上工作，而是直接在Docker实例所在的块设备上操作。虽然默认设置通常会设置一个回环设备，并且在本地测试时大多数情况下都很好，但由于在回环设备中创建的稀疏文件，这种特定设置极不建议用于生产系统。对于生产系统，建议您将其与`direct-lvm`结合使用，但这种复杂的设置需要特别棘手且比`overlay`存储驱动慢，因此我通常不建议使用它，除非您无法使用`aufs`或`overlay`/`overlay2`。
- en: Cleanup of Docker storage
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker存储的清理
- en: If you work with Docker images and containers, you will notice that, in general, Docker
    will chew through any storage you give it relatively quickly, so proper maintenance
    is recommended every now and then to ensure that you do not end up with useless
    garbage on your hosts or run out of inodes for some storage drivers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Docker镜像和容器，您会注意到，一般来说，Docker会相对快速地消耗您提供的任何存储空间，因此建议定期进行适当的维护，以确保您的主机上不会积累无用的垃圾或者某些存储驱动程序的inode用尽。
- en: Manual cleanup
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动清理
- en: 'First up is the cleanup of all containers that you have run but have forgotten
    to use `--rm` by using `docker rm`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是清理您运行过但忘记使用`--rm`的所有容器，使用`docker rm`：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This command effectively finds all containers (`docker ps`), even the ones that
    you stopped (the `-a` flag), and only returns their IDs (the `-q` flag). This
    is then passed on to `docker rm`, which will try to remove them one by one. If
    any containers are still running, it will give you a warning and skip them. Generally,
    this is just a good thing to do as often as you want if your containers are stateless
    or have a state stored outside of the container itself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令有效地找到所有容器（`docker ps`），甚至是您停止的容器（`-a`标志），并且只返回它们的ID（`-q`标志）。然后将其传递给`docker
    rm`，它将尝试逐个删除它们。如果有任何容器仍在运行，它将给出警告并跳过它们。一般来说，如果您的容器是无状态的或者具有在容器本身之外存储的状态，这通常是一个很好的做法，您可以随时执行。
- en: 'The next thing up, though potentially much more destructive and more space-saving,
    is deleting Docker images you have accumulated. If your space issues are frequent,
    manual removal can be pretty effective. A good rule of thumb is that any images
    with `<none>` as their tag (also called dangling) can usually be removed using
    `docker rmi` as they, in most cases, indicate that this image was superseded by
    a newer build of a `Dockerfile`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，尽管可能更具破坏性和节省空间，但要删除您积累的Docker镜像。如果您经常遇到空间问题，手动删除可能非常有效。一个经验法则是，任何标签为`<none>`的镜像（也称为悬空）通常可以使用`docker
    rmi`来删除，因为在大多数情况下，这些镜像表明该镜像已被`Dockerfile`的新版本取代：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Automatic cleanup
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动清理
- en: 'All of the things we have just done seem pretty painful to do and are hard
    to remember so Docker recently added `docker image prune` to help out in this
    aspect. By using `docker image prune`, all dangling images will be removed with
    a single command:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚做的所有事情似乎都很痛苦，很难记住，所以Docker最近添加了`docker image prune`来帮助解决这个问题。通过使用`docker
    image prune`，所有悬空的镜像将被一条命令删除：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you are intent on cleaning any and all images not tied to containers, you
    can also run `docker image prune -a`. Given that this command is pretty destructive
    I would not recommend it in most cases other than maybe running it on Docker slave
    nodes in clusters on a nighty/weekly timer to reduce space usage.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打算清理与容器无关的所有镜像，还可以运行`docker image prune -a`。鉴于这个命令相当具有破坏性，除了在Docker从属节点上夜间/每周定时器上运行它以减少空间使用之外，在大多数情况下我不建议这样做。
- en: Something to note here, as you might have noticed, deleting all references to
    an image layer also cascades onto child layers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，正如您可能已经注意到的，删除对镜像层的所有引用也会级联到子层。
- en: 'Last but not least is volume clean-up, which can be managed with the `docker
    volume` command. I would recommend that you exercise extreme caution when doing
    this in order to avoid deleting data that you might need and only use manual volume
    selection or `prune`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是卷的清理，可以使用`docker volume`命令进行管理。我建议在执行此操作时要极度谨慎，以避免删除您可能需要的数据，并且只使用手动卷选择或`prune`。
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As a reference, I have been running Docker rather lightly the week I wrote this
    chapter and the removal of stale containers, images, and volumes has reduced my
    filesystem usage by about 3 GB. While that number is mostly anecdotal and may
    not seem much, on cloud nodes with small instance hard disks and on clusters with
    continuous integration added, leaving these things around will get you out of
    disk space faster than you might realize, so expect to spend some time either
    doing this manually or automating this process for your nodes in something such
    as `systemd` timers or `crontab`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，我在写这一章的那周对Docker的使用相当轻，清理了陈旧的容器、镜像和卷后，我的文件系统使用量减少了大约3GB。虽然这个数字大部分是个人经验，并且可能看起来不多，但在具有小实例硬盘的云节点和添加了持续集成的集群上，保留这些东西会比你意识到的更快地耗尽磁盘空间，因此期望花一些时间手动执行这个过程，或者为您的节点自动化这个过程，比如使用`systemd`定时器或`crontab`。
- en: Persistent storage
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久存储
- en: Since we have covered transient local storage, we can now consider what other
    options we have for keeping data safe when the container dies or is moved. As
    we talked about previously, without being able to save data from the container
    in some fashion to an outside source if a node or the container unexpectedly dies
    while it is serving up something (such as your database), you will most likely
    lose some or all your data contained on it, which is definitively something we
    would like to avoid. Using some form of container-external storage for your data,
    like we did in earlier chapters with mounted volumes, we can begin to make the
    cluster really resilient and containers that run on it stateless.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了瞬态本地存储，现在我们可以考虑当容器死亡或移动时，我们还有哪些选项可以保护数据安全。正如我们之前讨论过的，如果不能以某种方式将容器中的数据保存到外部源，那么当节点或容器在提供服务时意外死机时（比如您的数据库），您很可能会丢失其中包含的一些或全部数据，这绝对是我们想要避免的。使用一些形式的容器外部存储来存储您的数据，就像我们在之前的章节中使用挂载卷一样，我们可以开始使集群真正具有弹性，并且在其上运行的容器是无状态的。
- en: By making containers stateless, you gain confidence to not worry much about
    exactly what container is running on which Docker Engine as long as they can pull
    the right image and run it with the right parameters. If you think about it for
    a minute, you may even notice how this approach has a huge number of similarities
    with threading, but on steroids. You can imagine Docker Engine like a virtual
    CPU core, each service as a process, and each task as a thread. With this in mind,
    if everything is stateless in your system then your cluster is effectively stateless
    too, and by inference, you must utilize some form of data storage outside of the
    containers to keep your data safe.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使容器无状态，您可以放心地不用太担心容器在哪个Docker引擎上运行，只要它们可以拉取正确的镜像并使用正确的参数运行即可。如果您仔细考虑一下，您甚至可能会注意到这种方法与线程有很多相似之处，但是更加强大。您可以将Docker引擎想象成虚拟CPU核心，每个服务作为一个进程，每个任务作为一个线程。考虑到这一点，如果您的系统中的一切都是无状态的，那么您的集群也是无状态的，因此，您必须利用容器外的某种形式的数据存储来保护您的数据。
- en: Caution! Lately, I have noticed a number of sources online that have been recommending
    that you should keep data through massive replication of services with sharding
    and clustering of backing databases without persisting data on disk, relying on
    the cloud provider's distributed availability zones and trusting **Service Level
    Agreements** (**SLA**) to provide you with resilience and self-healing properties
    for your cluster. While I would agree that these clusters are somewhat resilient,
    without some type of permanent physical representation of your data on some type
    of a volume, you may hit cascade outages on your clusters that will chain before
    the data is replicated fully and risk losing data with no way to restore it. As
    a personal advice here, I would highly recommend that at least one node in your
    stateful services uses storage that is on physical media that is not liable to
    be wiped when issues arise (e.g. NAS, AWS EBS storage, and so on).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！最近，我注意到一些在线来源一直在建议您通过大规模复制服务、分片和集群化后端数据库来保留数据，而不将数据持久化在磁盘上，依赖于云提供商的分布式可用区和信任**服务级别协议**（SLA）来为您的集群提供弹性和自愈特性。虽然我同意这些集群在某种程度上是有弹性的，但如果没有某种形式的永久物理表示您的数据的存储，您可能会在数据完全复制之前遇到集群的级联故障，并且有风险丢失数据而无法恢复。在这里，我个人建议至少有一个节点在您的有状态服务中使用存储，这种存储是在出现问题时不会被擦除的物理介质（例如NAS、AWS
    EBS存储等）。
- en: Node-local storage
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点本地存储
- en: This type of storage that is external to the container is specifically geared
    toward keeping data separate from your container instances, as we would expect,
    but is limited to usability only within containers deployed to the same node.
    Such storage allows a stateless container setup and has many development-geared
    uses, such as isolated builds and reading of configuration files, but for clustered
    deployments it is severely limited, as containers that run on other nodes will
    not have any access to data created on the original node. In either case, we will
    cover all of these node-local storage types here since most large clusters use
    some combination of node-local storage and relocatable storage.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种存储类型是外部于容器的，专门用于将数据与容器实例分开，但仅限于在部署到同一节点的容器内使用。这种存储允许无状态容器设置，并具有许多面向开发的用途，例如隔离构建和读取配置文件，但对于集群部署来说，它受到严重限制，因为在其他节点上运行的容器将无法访问在原始节点上创建的数据。无论哪种情况，我们将在这里涵盖所有这些节点本地存储类型，因为大多数大型集群都使用节点本地存储和可重定位存储的某种组合。
- en: Bind mounts
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绑定挂载
- en: 'We have seen these earlier, but maybe we did not know what they are. Bind mounts
    take a specific file or folder and mount it within the container sandbox at a
    specified location, separated by `:`. The general syntax that we have used so
    far for this should look similar to the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前见过这些，但也许我们不知道它们是什么。绑定挂载将特定文件或文件夹挂载到容器沙箱中的指定位置，用`:`分隔。到目前为止，我们使用的一般语法应该类似于以下内容：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Newer Docker syntax for this functionality is making its way into becoming a
    standard where the `-v` and `--volume` is now being replaced with `--mount`, so
    you should get used to that syntax too. In fact, from here on out, we will use
    both as much as we can so that you are comfortable with either style, but at the
    time of writing this book, `--mount` is not yet as fully functional as the alternative
    so expect some interchanging depending on what works and what does not.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能的新的Docker语法正在逐渐成为标准，其中`-v`和`--volume`现在正在被`--mount`替换，所以你也应该习惯这种语法。事实上，从现在开始，我们将尽可能多地使用两种语法，以便你能够熟悉任何一种风格，但在撰写本书时，`--mount`还没有像替代方案那样完全功能，所以根据工作情况和不工作情况，可能会有一些交替。
- en: In particular here, at this time, a simple bind mount volume with an absolute
    path source just does not work with `--mount` style which is almost all the examples
    we have used so far which is why we have not introduced this form earlier.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在这里，在这个时候，一个简单的绑定挂载卷，带有绝对路径源，与几乎所有我们迄今为止使用的`--mount`样式都不起作用，这就是为什么我们之前没有介绍这种形式的原因。
- en: 'With all that said and out of the way, unlike `--volume`, `--mount` is a `<key>=<value>`
    comma-separated list of parameters:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，不像`--volume`，`--mount`是一个`<key>=<value>`逗号分隔的参数列表：
- en: '`type`: The type of the mount, which can be `bind`, `volume`, or `tmpfs`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`类型`：挂载的类型，可以是`bind`，`volume`或`tmpfs`。'
- en: '`source`: The source for the mount.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`源`：挂载的源。'
- en: '`target`: The path to the location in the container where the source will be
    mounted.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`：容器中源将被挂载的位置。'
- en: '`readonly`: Causes the mount to be mounted as read-only.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readonly`：使挂载为只读。'
- en: '`volume-opt`: Extra options for the volume. May be entered more than once.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`volume-opt`：卷的额外选项。可以输入多次。'
- en: 'This is a comparative version to the one we used for `--volume`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们用于`--volume`的比较版本：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Read-only bind mounts
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 只读绑定挂载
- en: 'Another type of a bind mount that we did not really cover earlier is a read-only
    bind mount. This configuration is used when the data mounted into the container
    needs to remain read-only, which is very useful when passing configuration files
    into multiple containers from the host. This form of mounting a volume looks a
    bit like this for both of the two syntax styles:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有真正涵盖的另一种绑定挂载类型是只读绑定挂载。当容器中挂载的数据需要保持只读时，这种配置非常有用，尤其是从主机向多个容器传递配置文件时。这种挂载卷的形式看起来有点像这样，适用于两种语法风格：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As mentioned a bit earlier, something that a read-only volume can provide us
    as opposed to a regular mount is passing configuration files to the containers
    from the host. This is generally used when the Docker Engine host has something
    in their configuration that impacts the containers running code (that is, path
    prefixes for storing or fetching data, which host we're running on, what DNS resolvers
    the machine is using from `/etc/resolv.conf`, and many others) so in big deployments,
    it is used extensively and expect to see it often.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如稍早提到的，只读卷相对于常规挂载可以为我们提供一些东西，这是从主机传递配置文件到容器的。这通常在Docker引擎主机有一些影响容器运行代码的配置时使用（即，用于存储或获取数据的路径前缀，我们正在运行的主机，机器从`/etc/resolv.conf`使用的DNS解析器等），因此在大型部署中广泛使用，并且经常会看到。
- en: As a good rule of thumb, unless you explicitly need to write data to a volume,
    always mount it as read-only to the container. This will prevent the inadvertent
    opening of security holes from a compromised container spreading onto the other
    containers and the host itself.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个很好的经验法则，除非你明确需要向卷写入数据，否则始终将其挂载为只读到容器中。这将防止从一个受损的容器传播到其他容器和主机本身的安全漏洞的意外打开。
- en: Named volumes
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名卷
- en: Another form of volume mounting is using named volumes. Unlike bind-mounts,
    named data volumes (often referred to as data volume containers) provide a more
    portable way to refer to volumes as they do not depend on knowing anything about
    the host. Under the covers, they work almost exactly the same way as bind-mounts,
    but they are much easier to handle due to their simpler usage. Also, they have
    an added benefit of being able to be easily shared among containers and even be managed
    by host-independent solutions or a completely separate backend.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种卷挂载的形式是使用命名卷。与绑定挂载不同，命名数据卷（通常称为数据卷容器）提供了一种更便携的方式来引用卷，因为它们不依赖于对主机的任何了解。在底层，它们的工作方式几乎与绑定挂载完全相同，但由于使用更简单，它们更容易处理。此外，它们还有一个额外的好处，就是可以很容易地在容器之间共享，甚至可以由与主机无关的解决方案或完全独立的后端进行管理。
- en: Caution! If the named data volume is created by simply running the container,
    unlike bind-mounts that literally replace all content the container had at that
    mounted path, the named data volume will copy the content that the container image
    had at that location into the named data volume when the container launches. This
    difference is very subtle but can cause serious issues, as you might end up with
    unexpected content in the volume if you forget about this detail or assume that
    it behaves the same way as bind-mounts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！如果命名的数据卷是通过简单地运行容器创建的，与字面上替换容器在挂载路径上的所有内容的绑定挂载不同，当容器启动时，命名的数据卷将把容器镜像在该位置的内容复制到命名的数据卷中。这种差异非常微妙，但可能会导致严重的问题，因为如果你忘记了这个细节或者假设它的行为与绑定挂载相同，你可能会在卷中得到意外的内容。
- en: 'Now that we know what named data volumes are, let us create one by using the
    early-configuration approach (as opposed to creating one by directly running a
    container):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了命名数据卷是什么，让我们通过使用早期配置方法（而不是直接运行容器创建一个）来创建一个。
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Manually creating the volume before you use it (using `docker volume create`)
    is generally unnecessary but was done here to demonstrate the long-form of doing
    it but we could have just launched our container as the first step and Docker
    would have created the volume on its own:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用之前手动创建卷（使用`docker volume create`）通常是不必要的，但在这里这样做是为了演示这样做的长格式，但我们可以只是启动我们的容器作为第一步，Docker
    将自行创建卷。
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You may have noticed here, though, we ended up with two volumes instead of
    just our expected `mongodb_data` and if you followed the previous example with
    this one, you might actually have three (one named, two with random names). This
    is because every container launched will create all the local volumes defined
    in the `Dockerfile` regardless of whether you name them or not, and our MongoDB
    image actually defines two volumes:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在这里，我们最终得到了两个卷，而不仅仅是我们预期的`mongodb_data`，如果你按照前面的例子进行了这个例子，你可能实际上有三个（一个命名，两个随机命名）。这是因为每个启动的容器都会创建`Dockerfile`中定义的所有本地卷，无论你是否给它们命名，而且我们的
    MongoDB 镜像实际上定义了两个卷：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We only gave a name to the first one so the `/data/configdb` volume received
    a random one. Be aware of such things as you might encounter space exhaustion
    issues if you are not attentive enough. Running `docker volume prune` every once
    in a while can help reclaim that space, but be careful with this command as it
    will destroy all volumes not tied to containers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只给第一个命名，所以`/data/configdb`卷收到了一个随机的名称。要注意这样的事情，因为如果你不够注意，你可能会遇到空间耗尽的问题。偶尔运行`docker
    volume prune`可以帮助回收空间，但要小心使用这个命令，因为它会销毁所有未绑定到容器的卷。
- en: Relocatable volumes
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可移动卷
- en: All of these options that we discussed earlier are fine when working on a single
    host, but what they lack is real data portability between different physical hosts.
    For example, the current methods of keeping data persistent can realistically
    scale up to but not beyond (without some extreme hacking) a single physical server
    with single Docker Engine and shared attached storage. This might be fine for
    a powerful server but starts to lack any sort of use in a true clustering configuration
    since you might be dealing with an unknown number of servers, mixed virtual and
    physical hosts, different geographic areas, and so on.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的所有这些选项在单个主机上工作时都很好，但它们缺乏不同物理主机之间的真正数据可移植性。例如，当前的保持数据持久性的方法实际上可以扩展到但不能超出（没有一些极端的黑客行为）单个物理服务器与单个Docker引擎和共享附加存储。这对于强大的服务器可能还可以，但在真正的集群配置中开始缺乏任何形式的用途，因为您可能会处理未知数量的服务器，混合虚拟和物理主机，不同的地理区域等等。
- en: 'Also when a container is restarted, you most likely will not be able to easily
    predict where it is going to get launched to have the volume backend there for
    it when it starts. For this use case, there are things called relocatable volumes.
    These go by various different names, such as "shared multi-host storage", "orchestrated
    data volume", and many others, but the idea is pretty much the same across the
    board: have a data volume that will follow the container wherever it goes.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当容器重新启动时，您很可能无法轻易预测它将在何处启动，以便在其启动时为其提供卷后端。对于这种用例，有一些被称为可移动卷的东西。它们有各种不同的名称，比如“共享多主机存储”，“编排数据卷”等等，但基本上想法在各方面都是相同的：拥有一个数据卷，无论容器去哪里，它都会跟随。
- en: 'To illustrate the example, here, we have three hosts with two stateful services
    all connected using the same relocatable volume storage driver:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明，在这里，我们有三个主机，连接着两个有状态服务，它们都使用相同的可移动卷存储驱动程序：
- en: '**Stateful Container 1** with **Volume D** on **Host 1**'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有** **卷D** 的**有状态容器1**在**主机1**上'
- en: '**Stateful** **Container 2** with **Volume G** on **Host 3**'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有** **卷G** 的**有状态容器2**在**主机3**上'
- en: '![](assets/93a9f826-7fc4-4a70-8c6b-6e60a163e9c1.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/93a9f826-7fc4-4a70-8c6b-6e60a163e9c1.png)'
- en: 'For the purpose of this example, assume that **Host 3** has died. In the normal
    volume driver case, all your data from **Stateful** **Container 2** would be lost,
    but because you would be using relocatable storage:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个例子，假设**主机3**已经死机。在正常的卷驱动程序情况下，**有状态** **容器2**的所有数据都会丢失，但因为您将使用可移动存储：
- en: The orchestration platform will notify your storage driver that the container
    has died.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排平台将通知您的存储驱动程序容器已经死亡。
- en: The orchestration platform will indicate that it wants to restart the killed
    services on a host with available resources.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排平台将指示它希望在具有可用资源的主机上重新启动被杀死的服务。
- en: The volume driver will mount the same volume to the new host that will run the
    service.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷驱动程序将将相同的卷挂载到将运行服务的新主机上。
- en: The orchestration platform will start the service, passing the volume details
    into the new container.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排平台将启动服务，并将卷详细信息传递到新容器中。
- en: 'In our hypothetical example, the new system state should look a little bit
    like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的假设示例中，新系统状态应该看起来有点像这样：
- en: '![](assets/a4f48824-a92d-4373-ba82-6cbcf6c55d0a.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a4f48824-a92d-4373-ba82-6cbcf6c55d0a.png)'
- en: As you can see from an external point of view, nothing has changed and the data
    was seamlessly transitioned to the new container and kept its state, which is
    exactly what we wanted. For this specific purpose, there are a number of Docker
    volume drivers that one can choose, and each one has its own configuration method
    for various storage backends, but the only one included with Docker pre-built
    images for Azure and AWS out of the box is CloudStor, and it is only for Docker
    Swarm, making it super-specific and completely non-portable.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从外部观点来看，没有任何变化，数据无缝地过渡到新容器并保持其状态，这正是我们想要的。对于这个特定目的，有许多Docker卷驱动程序可供选择，每个驱动程序都有其自己的配置方法用于各种存储后端，但Docker预构建的Azure和AWS镜像中唯一包含的是CloudStor，它仅适用于Docker
    Swarm，使其非常特定且完全不可移植。
- en: For various reasons, including the age of technology and lackluster support
    by Docker and plugin developers, having to do this type of volume handling is
    most likely going to be the part that you sink a lot of time into when building
    your infrastructure. I do not want to discourage you, but at the time of writing
    this, the state of things is really dire regardless of what easy tutorials may
    like you to believe.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 出于各种原因，包括技术的老化和Docker以及插件开发人员的支持不力，不得不进行这种类型的卷处理很可能会是在构建基础设施时您要花费大量时间的部分。我不想打击你的积极性，但在撰写本文时，无论易于教程可能让您相信的是，事实情况确实非常严峻。
- en: 'You can find a majority of the drivers at [https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins](https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins).
    After configuration, use them in the following manner if you are doing it manually
    without orchestration in order to manage mounting:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins](https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins)找到大多数驱动程序。配置后，如果您手动进行管理挂载而没有编排，可以按以下方式使用它们：
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For reference, currently, I believe that the most popular plugins for handling
    relocatable volumes are Flocker, REX-Ray ([https://github.com/codedellemc/rexray](https://github.com/codedellemc/rexray)),
    and GlusterFS though there are many to choose from, with many of them having similar
    functionality. As mentioned earlier, the state of this ecosystem is rather abysmal
    for such an important feature and it seems that almost every big player running
    their clustering either forks and builds their own storage solution, or they make
    their own and keep it closed-sourced. Some deployments have even opted to using
    labels for their nodes to avoid this topic completely and force specific containers
    to go to specific hosts so that they can use locally mounted volumes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 供参考，目前我认为处理可移动卷最受欢迎的插件是Flocker、REX-Ray ([https://github.com/codedellemc/rexray](https://github.com/codedellemc/rexray))和GlusterFS，尽管有许多可供选择的插件，其中许多具有类似的功能。如前所述，对于如此重要的功能，这个生态系统的状态相当糟糕，似乎几乎每个大型参与者都在运行他们的集群时要么分叉并构建自己的存储解决方案，要么他们自己制作并保持封闭源。一些部署甚至选择使用标签来避免完全避开这个话题，并强制特定容器去特定主机，以便它们可以使用本地挂载的卷。
- en: Flocker's parent company, ClusterHQ, shut down its operations in December 2016
    for financial reasons, and while the lack of support would give a bit of a push
    to not be mentioned here, it is still the most popular one by an order of magnitude
    for this type of volume management at the time of writing this book. All the code
    is open sourced on GitHub at [https://github.com/ClusterHQ](https://github.com/ClusterHQ)
    so you can build, install, and run it even without official support. If you want
    to use this plugin in an enterprise environment and would like to have support
    for it, some of the original developers are available for hire through a new company
    called ScatterHQ at [https://www.scatterhq.com/](https://www.scatterhq.com/) and
    they have their own source code repositories at [https://github.com/ScatterHQ](https://github.com/ScatterHQ).GlusterFS
    is unmaintained in its original source like Flocker, but just like Flocker, you
    can build, install, and run the full code from the source repository located at
    [https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs).
    If you would like code versions that have received updates, you can find a few
    in the fork network at [https://github.com/calavera/docker-volume-glusterfs/network](https://github.com/calavera/docker-volume-glusterfs/network).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Flocker的母公司ClusterHQ因财务原因于2016年12月停止运营，虽然缺乏支持会给不予提及提供一点推动力，但在撰写本书时，它仍然是最受欢迎的一种卷管理方式。所有代码都在GitHub上开源[https://github.com/ClusterHQ](https://github.com/ClusterHQ)，因此即使没有官方支持，你也可以构建、安装和运行它。如果你想在企业环境中使用这个插件，并希望得到支持，一些原始开发人员可以通过一个名为ScatterHQ的新公司进行雇佣[https://www.scatterhq.com/](https://www.scatterhq.com/)，他们在[https://github.com/ScatterHQ](https://github.com/ScatterHQ)上有自己的源代码库。GlusterFS在其原始源中没有维护，就像Flocker一样，但你可以从源代码库[https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs)中构建、安装和运行完整的代码。如果你想要已经接收更新的代码版本，你可以在分支网络中找到一些[https://github.com/calavera/docker-volume-glusterfs/network](https://github.com/calavera/docker-volume-glusterfs/network)。
- en: On top of all this ecosystem fragmentation, this particular way of integrating
    with Docker is starting to be deprecated in favor of the `docker plugin` system
    which manages and installs these plugins as Docker images from Docker Hub but
    due to lack of availability of these new-style plugins, you might have to use
    a legacy plugin depending on your specific use cases.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所有这些生态系统的分裂，这种与Docker集成的特定方式开始被弃用，而更倾向于管理和安装这些插件的“docker插件”系统，这些插件是从Docker
    Hub作为Docker镜像安装的，但由于这些新风格插件的可用性不足，根据你的具体用例，你可能需要使用遗留插件。
- en: Sadly at the time of writing this book, `docker plugin` system is, like many
    of these features, so new that there are barely any available plugins for it.
    For example, the only plugin from the ones earlier mentioned in legacy plugins
    that is built using this new system is REX-Ray but the most popular storage backend
    (EBS) plugin does not seem to install cleanly. By the time you get to read this
    book, things will probably have changed here but be aware that there is a significant
    likelihood that in your own implementation you will be using the tried-and-tested
    legacy plugins.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，在撰写本书时，“docker插件”系统像许多其他功能一样是全新的，几乎没有可用的插件。例如，在早期提到的遗留插件中，唯一使用这个新系统构建的插件是REX-Ray，但最流行的存储后端（EBS）插件似乎无法干净地安装。当你阅读本书时，这里的情况可能已经改变，但请注意，在你自己的实现中，你可能会使用经过验证的遗留插件。
- en: 'So with all of these caveats mentioned, let''s actually try to get one of the
    only plugins (`sshfs`) that can be found working using the new `docker plugin
    install` system:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在提到所有这些警告之后，让我们实际尝试获取唯一一个可以使用新的“docker插件安装”系统找到的插件（sshfs）：
- en: To duplicate this work, you will need access to a secondary machine (though
    you can run it loopback too) with SSH enabled and reachable from wherever you
    have Docker Engine running from, since that is the backing storage system that
    it uses. You will also need the target folder `ssh_movable_volume` made on the
    device and possibly the addition of `-o odmap=user` to the `sshfs` volume parameters
    depending on your setup.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要复制这项工作，您需要访问一个启用了SSH并且可以从Docker引擎运行的地方到达的辅助机器（尽管您也可以在回环上运行），因为它使用的是支持存储系统。您还需要在设备上创建目标文件夹`ssh_movable_volume`，可能还需要根据您的设置在`sshfs`卷参数中添加`-o
    odmap=user`。
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Due to the way the volume is used, this volume is mostly portable and could
    allow us the relocatable features we need, though most other plugins use a process
    that runs outside of Docker and in parallel on each host in order to manage the
    volume mounting, un-mounting, and moving, so instructions for those will be vastly
    different.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷的使用方式，这个卷大多是可移动的，并且可以允许我们需要的可移动特性，尽管大多数其他插件使用一个在Docker之外并行在每个主机上运行的进程来管理卷的挂载、卸载和移动，因此这些指令将大不相同。
- en: Relocatable volume sync loss
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可移动卷同步丢失
- en: One last thing that must be mentioned in this section as well is the fact that
    most of plugins that handle the moving of volumes can only handle being attached
    to a single node at any one time due to the volume being writable by multiple
    sources is going to generally cause serious issues so most drivers disallow it.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中还必须提到的最后一件事是，大多数处理卷移动的插件通常只能处理连接到单个节点，因为卷被多个源写入通常会导致严重问题，因此大多数驱动程序不允许这样做。
- en: This however is in conflict with the main feature of most orchestration engines
    which, on changes to Docker services, will leave the original service running
    until the new one is started and passes health checks, causing the need to mount
    the same volume on both the old and new service task in effect, creating a chicken-egg
    paradox.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这与大多数编排引擎的主要特性相冲突，即对Docker服务的更改将使原始服务保持运行，直到新服务启动并通过健康检查，从而需要在旧服务和新服务任务上挂载相同的卷，实际上产生了一个鸡蛋-鸡的悖论。
- en: In most cases, this can be worked around by making sure that Docker completely
    kills the old service before starting the new one, but even then, you can expect
    that occasionally the old volume will not be unmounted quickly enough from the
    old node, so the new service will fail to start.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，这可以通过确保Docker在启动新服务之前完全终止旧服务来解决，但即使这样，您也可以预期偶尔旧卷将无法从旧节点快速卸载，因此新服务将无法启动。
- en: UID/GID and security considerations with volumes
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷的UID/GID和安全考虑
- en: 'This section is not in a small informational box like I would have put it elsewhere,
    because it is a big enough issue and problematic enough to deserve its own section.
    To understand what happens with container **user ID** (**UID**) and **group ID**
    (**GID**), we need to understand how the host''s system permission works. When
    you have a file with group and user permissions, they are internally all actually
    mapped to numbers and not kept as usernames or group names that you see when listing
    things with regular `ls` switches:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分不像我在其他地方放置的小信息框那样，因为这是一个足够大的问题，足够棘手，值得有自己的部分。要理解容器**用户ID**（**UID**）和**组ID**（**GID**）发生了什么，我们需要了解主机系统权限是如何工作的。当你有一个带有组和用户权限的文件时，它们实际上都被映射为数字，而不是保留为用户名或组名，当你使用常规的`ls`开关列出东西时，你会看到它们：
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When you do `ls`, the system reads in `/etc/passwd` and `/etc/group` to display
    the actual username and group name for permissions, and it is the only way in
    which the UID/GID is mapped to permissions but the underlying values are UIDs
    and GIDs.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当您执行`ls`时，系统会读取`/etc/passwd`和`/etc/group`以显示权限的实际用户名和组名，这是UID/GID映射到权限的唯一方式，但底层值是UID和GID。
- en: 'As you might have guessed, this user-to-UID and group-to-GID mapping might
    not (and often does not) translate well to a containerized system as the container(s)
    will not have the same `/etc/passwd` and `/etc/group` files but the permissions
    of files on external volumes are stored with the data. For example, if the container
    has a group with a GID of `1001`, it will match the group permission bits `-rw`
    on our `foofile` and if it has a user has a UID of `1001`, it will match our `-rw`
    user permissions on the file. Conversely, if your UIDs and GIDs do not match up,
    even if you have a group or user with the same name in the container and on the
    host, you will not have the right UIDs and GID for proper permission processing.
    Time to check out what kind of a mess we can do with this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的那样，这种用户到UID和组到GID的映射在容器化系统中可能无法很好地转换，因为容器将不具有相同的`/etc/passwd`和`/etc/group`文件，但外部卷上的文件权限是与数据一起存储的。例如，如果容器有一个GID为`1001`的组，它将匹配我们的`foofile`上的组权限位`-rw`，如果它有一个UID为`1001`的用户，它将匹配我们文件上的`-rw`用户权限。相反，如果您的UID和GID不匹配，即使容器和主机上有相同名称的组或用户，您也不会拥有正确的UID和GID以进行适当的权限处理。是时候看看我们可以用这个做成什么样的混乱了：
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Warning! The ability to set the `setuid` flag on files is a really big security
    hole that executes the file with the file owner's permissions. If we decided to
    compile a program and set this flag on it, we could have done a massive amount
    of damage on the host. Refer to [https://en.wikipedia.org/wiki/Setuid](https://en.wikipedia.org/wiki/Setuid)
    for more information on this flag.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 警告！在文件上设置`setuid`标志是一个真正的安全漏洞，它以文件所有者的权限执行文件。如果我们决定编译一个程序并在其上设置此标志，我们可能会对主机造成大量的破坏。有关此标志的更多信息，请参阅[https://en.wikipedia.org/wiki/Setuid](https://en.wikipedia.org/wiki/Setuid)。
- en: As you can see, this can be a serious issue if we decided to be more malicious
    with our `setuid` flag. This issue extends to any mounted volumes we use, so make
    sure that you exercise proper caution when dealing with them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，如果我们决定更加恶意地使用我们的`setuid`标志，这可能是一个严重的问题。这个问题也延伸到我们使用的任何挂载卷，因此在处理它们时，请确保您要谨慎行事。
- en: Docker has been working on getting user namespaces working in order to avoid
    some of these security issues, which work by re-mapping the UIDs and GIDs to something
    else within the container through `/etc/subuid` and `/etc/subgid` files so that
    there is no `root` UID clashing between the host and the container, but they're
    not without their problems (and there's plenty of them at the time of writing
    this book). For more information on using user namespaces, you can find more information
    at [https://docs.docker.com/engine/security/userns-remap/](https://docs.docker.com/engine/security/userns-remap/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Docker一直在努力使用户命名空间工作，以避免一些这些安全问题，它通过`/etc/subuid`和`/etc/subgid`文件重新映射UID和GID到容器内的其他内容，以便在主机和容器之间没有`root`
    UID冲突，但它们并不是没有问题的（在撰写本书时存在大量问题）。有关使用用户命名空间的更多信息，您可以在[https://docs.docker.com/engine/security/userns-remap/](https://docs.docker.com/engine/security/userns-remap/)找到更多信息。
- en: 'Compounding this UID/GID problem is another issue that happens with such separate
    environments: even if you install all the same packages in the same order between
    two containers, due to users and groups usually being created by name and not
    a specific UID/GID, you are not guaranteed to have these consistent between the
    container runs, which is a serious problems if you want to remount the same volume
    between a container that was upgraded or rebuilt. For this reason, you must ensure
    that UIDs and GIDs are stable on volumes by doing something similar to the following,
    as we have done in some earlier examples, before you install the package(s) with
    the users and groups that will be dealing with the volume data:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 加剧这个UID/GID问题的是另一个问题，即在这样的独立环境中会发生的问题：即使在两个容器之间以相同的顺序安装了所有相同的软件包，由于用户和组通常是按名称而不是特定的UID/GID创建的，你不能保证在容器运行之间这些一致，如果你想重新挂载已升级或重建的容器之间的相同卷，这是一个严重的问题。因此，你必须确保卷上的UID和GID是稳定的，方法类似于我们在一些早期示例中所做的，在安装包之前：
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, we create a group `mongodb` with GID `910` and a user `mongodb` with UID
    `910` and then make sure that our data directory is owned by it before we install
    MongoDB. By doing this, when the `mongodb-org` package is installed, the group
    and user for running the database is already there and with the exact UID/GID
    that will not change. With a stable UID/GID, we can mount and remount the volume
    on any built container with the same configuration as both of the numbers will
    match and it should work on any machine that we move the volume to.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个GID为`910`的组`mongodb`和一个UID为`910`的用户`mongodb`，然后确保我们的数据目录由它拥有，然后再安装MongoDB。通过这样做，当安装`mongodb-org`软件包时，用于运行数据库的组和用户已经存在，并且具有不会更改的确切UID/GID。有了稳定的UID/GID，我们可以在任何具有相同配置的构建容器上挂载和重新挂载卷，因为这两个数字将匹配，并且它应该在我们将卷移动到的任何机器上工作。
- en: 'The only final thing to possibly worry about (which is also somewhat of a problem
    in that last example) is that mounting a folder will overlay itself over an already
    created folder on the host and replace its permissions. This means that if you
    mount a new folder onto the container, either you have to manually change the
    volume''s permissions or change the ownership when the container starts. Let''s
    see what I mean by that:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后可能需要担心的一件事（在上一个示例中也是一个问题）是，挂载文件夹将覆盖主机上已创建的文件夹并替换其权限。这意味着，如果你将一个新文件夹挂载到容器上，要么你必须手动更改卷的权限，要么在容器启动时更改所有权。让我们看看我是什么意思：
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As you can see, whatever permissions were already set on the folder within
    the container got completely trampled by our mounted directory volume. As mentioned
    earlier, the best way to avoid having permission errors with limited users running
    services in the container and mounted volumes is to change the permissions on
    the mounted paths on container start with a wrapper script or start the container
    with a mounted volume and change it manually, with the former being the much preferable
    option. The simplest wrapper script goes something like this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，容器内文件夹上已经设置的任何权限都被我们挂载的目录卷完全覆盖了。如前所述，避免在容器中运行服务的有限用户出现权限错误的最佳方法是在容器启动时使用包装脚本更改挂载路径上的权限，或者使用挂载卷启动容器并手动更改权限，前者是更可取的选项。最简单的包装脚本大致如下：
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Placing this in `/usr/bin/wrapper.sh` of the container and adding the following
    snippet somewhere to the `Dockerfile` where it runs as root should be good enough
    to fix the issue:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 将此脚本放在容器的`/usr/bin/wrapper.sh`中，并在`Dockerfile`中以root身份运行的地方添加以下代码片段应该足以解决问题：
- en: '[PRE26]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When the container starts, the volume will be mounted already and the script
    will change the user and group of the volume to the proper one before passing
    the command to the original runner for the container, fixing our issue.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器启动时，卷将已经挂载，并且脚本将在将命令传递给容器的原始运行程序之前，更改卷的用户和组为正确的用户和组，从而解决了我们的问题。
- en: The biggest takeaway from this section should be that you should be mindful
    of user permissions when you deal with volumes as they may cause usability and
    security issues if you are not careful. As you develop your services and infrastructure,
    these types of pitfalls can cause everything from minor headaches to catastrophic
    failures but now that you know more about them, we have hopefully prevented the
    worst.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从本节中最重要的收获应该是，在处理卷时，您应该注意用户权限，因为如果不小心，它们可能会导致可用性和安全性问题。当您开发您的服务和基础设施时，这些类型的陷阱可能会导致从轻微头痛到灾难性故障的一切，但是现在您对它们了解更多，我们希望已经预防了最坏的情况。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned a massive amount of new stuff revolving around
    Docker's data handling including Docker image internals and running your own Docker
    Registry. We have also covered transient, node-local, and relocatable data storage
    and the associated volume management that you will need to effectively deploy
    your services in the cloud. Later we have spent some time covering the volume
    orchestration ecosystem to help you navigate the changing landscape of Docker
    volume drivers as things have been changing quickly in this space. As we got to
    the end, coverage of various pitfalls (like UID/GID issues) was included so that
    you can avoid them in your own deployments.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经学到了大量关于Docker数据处理的新知识，包括Docker镜像内部和运行自己的Docker注册表。我们还涵盖了瞬态、节点本地和可重定位数据存储以及相关的卷管理，这将帮助您有效地在云中部署您的服务。随后，我们花了一些时间来介绍卷编排生态系统，以帮助您在Docker卷驱动程序的不断变化中进行导航，因为在这个领域事情变化得很快。最后，我们还涵盖了各种陷阱（如UID/GID问题），以便您可以在自己的部署中避免它们。
- en: As we continue into the next chapter, we will cover cluster hardening and how
    to pass data between a large volume of services in an orderly fashion.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们继续进入下一章时，我们将介绍集群加固以及如何以有序的方式在大量服务之间传递数据。 '
