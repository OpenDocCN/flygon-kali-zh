- en: '*Chapter 12*: Kubernetes Security and Compliance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：Kubernetes安全性和合规性'
- en: In this chapter, you will learn about some of the key pieces of Kubernetes security.
    We'll discuss some recent Kubernetes security issues, and the finding of a recent
    audit that was performed on Kubernetes. Then, we'll look at implementing security
    at each level of our cluster, starting with the security of Kubernetes resources
    and their configurations, moving on to container security, and then finally, runtime
    security with intrusion detection. To start, we will discuss some key security
    concepts as they relate to Kubernetes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解一些关键的Kubernetes安全性要点。我们将讨论一些最近的Kubernetes安全问题，以及对Kubernetes进行的最近审计的发现。然后，我们将从我们集群的每个级别开始实施安全性，从Kubernetes资源及其配置的安全性开始，然后是容器安全，最后是入侵检测的运行时安全。首先，我们将讨论一些与Kubernetes相关的关键安全概念。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding security on Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解Kubernetes上的安全性
- en: Reviewing CVEs and security audits for Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查Kubernetes的CVE和安全审计
- en: Implementing tools for cluster configuration and container security
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施集群配置和容器安全的工具
- en: Handling intrusion detection, runtime security, and compliance on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理Kubernetes上的入侵检测、运行时安全性和合规性
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool, along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章详细介绍的命令，您需要一台支持`kubectl`命令行工具的计算机，以及一个正常运行的Kubernetes集群。请参阅[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*，了解快速启动Kubernetes的几种方法，以及如何安装`kubectl`工具的说明。
- en: Additionally, you will need a machine that supports the Helm CLI tool, which
    typically has the same prerequisites as `kubectl` – for details, check the Helm
    documentation at [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还需要一台支持Helm CLI工具的机器，通常具有与`kubectl`相同的先决条件-有关详细信息，请查看Helm文档[https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在书籍的GitHub存储库中找到[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter12)。
- en: Understanding security on Kubernetes
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Kubernetes上的安全性
- en: When discussing security on Kubernetes, it is very important to note security
    boundaries and shared responsibility. The *Shared Responsibility Model* is a common
    term used to describe how security is handled in public cloud services. It states
    that the customer is responsible for the security of their applications, and the
    security of their configuration of public cloud components and services. The public
    cloud provider, on the other hand, is responsible for the security of the services
    themselves as well as the infrastructure they run on, all the way to the data
    center and physical layer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论Kubernetes上的安全性时，非常重要的是要注意安全边界和共享责任。*共享责任模型*是一个常用术语，用于描述公共云服务中的安全处理方式。它指出客户对其应用程序的安全性以及公共云组件和服务的配置的安全性负责。另一方面，公共云提供商负责服务本身的安全性以及其运行的基础设施，一直到数据中心和物理层。
- en: Similarly, security on Kubernetes is shared. Though upstream Kubernetes is not
    a commercial product, the thousands of Kubernetes contributors and significant
    organizational heft from large tech companies ensure that the security of Kubernetes
    components is maintained. Additionally, the large ecosystem of individual contributors
    and companies using the technology ensures that it gets better as CVEs are reported
    and handled. Unfortunately, as we will discuss in the next section, the complexity
    of Kubernetes means that there are many possible attack vectors.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Kubernetes的安全性是共享的。尽管上游Kubernetes不是商业产品，但成千上万的Kubernetes贡献者和来自大型科技公司的重要组织力量确保了Kubernetes组件的安全性得到维护。此外，大量的个人贡献者和使用该技术的公司构成了庞大的生态系统，确保了在CVE报告和处理时的改进。不幸的是，正如我们将在下一节讨论的那样，Kubernetes的复杂性意味着存在许多可能的攻击向量。
- en: Applying the shared responsibility model then, as a developer you are responsible
    for the security of how you configure Kubernetes components, the security of the
    applications that you run on Kubernetes, and access-level security in your cluster
    configuration. While the security of your applications and containers themselves
    are not quite in scope for this book, they are definitely important to Kubernetes
    security. We will spend most of our time discussing configuration-level security,
    access security, and runtime security.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为开发人员，根据共享责任模型，你需要负责配置Kubernetes组件的安全性，你在Kubernetes上运行的应用程序的安全性，以及集群配置中的访问级别安全性。虽然你的应用程序和容器本身的安全性不在本书的范围内，但它们对Kubernetes的安全性绝对重要。我们将花大部分时间讨论配置级别的安全性，访问安全性和运行时安全性。
- en: Either Kubernetes itself or the Kubernetes ecosystem provides tooling, libraries,
    and full-blown products to handle security at each of these levels – and we'll
    be reviewing some of these options in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes本身或Kubernetes生态系统提供了工具、库和完整的产品来处理这些级别的安全性 - 我们将在本章中审查其中一些选项。
- en: Now, before we discuss these solutions, it's best to start with a base understanding
    of why they may be needed in the first place. Let's move on to the next section,
    where we'll detail some issues that Kubernetes has encountered in the realm of
    security.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们讨论这些解决方案之前，最好先从为什么可能需要它们的基本理解开始。让我们继续下一节，我们将详细介绍Kubernetes在安全领域遇到的一些问题。
- en: Reviewing CVEs and security audits for Kubernetes
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查Kubernetes的CVE和安全审计
- en: Kubernetes has encountered several **Common Vulnerabilities and Exposures**
    (**CVEs**) in its storied history. The MITRE CVE database, at the time of writing,
    lists 73 CVE announcements from 2015 to 2020 when searching for `kubernetes`.
    Each one of these is related either directly to Kubernetes, or to a common open
    source solution that runs on Kubernetes (like the NGINX ingress controller, for
    instance).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在其悠久历史中遇到了几个**通用漏洞和暴露**（**CVEs**）。在撰写本文时，MITRE CVE数据库在搜索`kubernetes`时列出了2015年至2020年间的73个CVE公告。其中每一个要么直接与Kubernetes相关，要么与在Kubernetes上运行的常见开源解决方案相关（例如NGINX入口控制器）。
- en: Several of these were critical enough to require hotfixes to the Kubernetes
    source, and thus they list the affected versions in the CVE description. A full
    list of all CVEs related to Kubernetes can be found at [https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes).
    To give you an idea of some of the issues that have been found, let's review a
    few of these CVEs in chronological order.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些攻击向量足够严重，需要对Kubernetes源代码进行热修复，因此它们在CVE描述中列出了受影响的版本。关于Kubernetes相关的所有CVE的完整列表可以在[https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes)找到。为了让你了解一些已经发现的问题，让我们按时间顺序回顾一些这些CVE。
- en: Understanding CVE-2016-1905 – Improper admission control
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解CVE-2016-1905 – 不正确的准入控制
- en: This CVE was one of the first major security issues with production Kubernetes.
    The National Vulnerability Database (a NIST website) gives this issue a base score
    of 7.7, putting it in the high-impact category.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个CVE是生产Kubernetes中的第一个重大安全问题。国家漏洞数据库（NIST网站）给出了这个问题的基础评分为7.7，将其归类为高影响类别。
- en: With this issue, a Kubernetes admission controller would not ensure that a `kubectl
    patch` command followed admission rules, allowing users to completely work around
    the admission controller – a nightmare in a multitenant scenario.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个问题，Kubernetes准入控制器不会确保`kubectl patch`命令遵循准入规则，允许用户完全绕过准入控制器 - 在多租户场景中是一场噩梦。
- en: Understanding CVE-2018-1002105 – Connection upgrading to the backend
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解CVE-2018-1002105 – 连接升级到后端
- en: This CVE was likely the most critical to date in the Kubernetes project. In
    fact, NVD gives it a 9.8 criticality score! In this CVE, it was found that it
    was possible in some versions of Kubernetes to piggyback on an error response
    from the Kubernetes API server and then upgrade the connection. Once the connection
    was upgraded, it was possible to send authenticated requests to any backend server
    in the cluster. This allowed a malicious user to essentially emulate a perfectly
    authenticated TLS request without proper credentials.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个CVE很可能是迄今为止Kubernetes项目中最关键的。事实上，NVD给出了它9.8的严重性评分！在这个CVE中，发现在某些版本的Kubernetes中，可以利用Kubernetes
    API服务器的错误响应进行连接升级。一旦连接升级，就可以向集群中的任何后端服务器发送经过身份验证的请求。这允许恶意用户在没有适当凭据的情况下模拟完全经过身份验证的TLS请求。
- en: In addition to these CVEs (and likely partially driven by them), the CNCF sponsored
    a third-party security audit of Kubernetes in 2019\. The results of the audit
    are open source and publicly available and are worth a review.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些CVE（很可能部分受它们驱动），CNCF在2019年赞助了Kubernetes的第三方安全审计。审计的结果是开源的，公开可用，值得一看。
- en: Understanding the 2019 security audit results
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解2019年安全审计结果
- en: As we mentioned in the previous section, the 2019 Kubernetes security audit
    was performed by a third party, and the results of the audit are completely open
    source. The full audit report with all sections can be found at [https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/](https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中提到的，2019年Kubernetes安全审计是由第三方进行的，审计结果完全是开源的。所有部分的完整审计报告可以在[https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/](https://www.cncf.io/blog/2019/08/06/open-sourcing-the-kubernetes-security-audit/)找到。
- en: 'In general, this audit focused on the following pieces of Kubernetes functionality:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这次审计关注了以下Kubernetes功能的部分：
- en: '`kube-apiserver`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-apiserver`'
- en: '`etcd`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`'
- en: '`kube-scheduler`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`'
- en: '`kube-controller-manager`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-controller-manager`'
- en: '`cloud-controller-manager`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud-controller-manager`'
- en: '`kubelet`'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubelet`'
- en: '`kube-proxy`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy`'
- en: The Container Runtime
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时
- en: The intent was to focus on the most important and relevant pieces of Kubernetes
    when it came to security. The results of the audit included not just a full security
    report, but also a threat model and a penetration test, as well as a whitepaper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 意图是在涉及安全性时专注于Kubernetes最重要和相关的部分。审计的结果不仅包括完整的安全报告，还包括威胁模型和渗透测试，以及白皮书。
- en: Diving deep into the audit results is not in the scope of this book, but there
    are some major takeaways that are great windows into the crux of many of the biggest
    Kubernetes security issues.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解审计结果不在本书的范围内，但有一些重要的收获是对许多最大的Kubernetes安全问题的核心有很好的了解。
- en: In short, the audit found that since Kubernetes is a complex, highly networked
    system with many different settings, there are many possible configurations that
    inexperienced engineers may perform and in doing so, open their cluster to outside
    attackers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，审计发现，由于Kubernetes是一个复杂的、高度网络化的系统，具有许多不同的设置，因此有许多可能的配置，经验不足的工程师可能会执行，并在这样做的过程中，打开他们的集群给外部攻击者。
- en: This idea of Kubernetes being complex enough that an insecure configuration
    could happen easily is important to note and take to heart.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的这个想法足够复杂，以至于不安全的配置很容易发生，这一点很重要，需要注意和牢记。
- en: The entire audit is worth a read – for those with significant knowledge of network
    security and containers, it is an excellent view of some of the security decisions
    that were made as part of the development of Kubernetes as a platform.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 整个审计值得一读-对于那些具有重要的网络安全和容器知识的人来说，这是对Kubernetes作为平台开发过程中所做的一些安全决策的极好的视角。
- en: Now that we have discussed where Kubernetes security issues have been found,
    we can start looking into ways to increase the security posture of your clusters.
    Let's start with some default Kubernetes functionality for security.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了Kubernetes安全问题的发现位置，我们可以开始研究如何增加集群的安全姿态。让我们从一些默认的Kubernetes安全功能开始。
- en: Implementing tools for cluster configuration and container security
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施集群配置和容器安全的工具
- en: 'Kubernetes gives us many inbuilt options for the security of cluster configurations
    and container permissions. Since we''ve already talked about RBAC, TLS Ingress,
    and encrypted Kubernetes Secrets, let''s discuss a few concepts that we haven''t
    had time to review yet: admission controllers, Pod security policies, and network
    policies.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为我们提供了许多内置选项，用于集群配置和容器权限的安全性。由于我们已经讨论了RBAC、TLS Ingress和加密的Kubernetes
    Secrets，让我们讨论一些我们还没有时间审查的概念：准入控制器、Pod安全策略和网络策略。
- en: Using admission controllers
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用准入控制器
- en: Admission controllers are an often overlooked but extremely important Kubernetes
    feature. Many of Kubernetes' advanced features use admission controllers under
    the hood. In addition, you can create new admission controller rules in order
    to add custom functionality to your cluster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 准入控制器经常被忽视，但它是一个极其重要的Kubernetes功能。许多Kubernetes的高级功能都在幕后使用准入控制器。此外，您可以创建新的准入控制器规则，以添加自定义功能到您的集群中。
- en: 'There are two general types of admission controllers:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种一般类型的准入控制器：
- en: Mutating admission controllers
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变异准入控制器
- en: Validating admission controllers
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证准入控制器
- en: Mutating admission controllers take in Kubernetes resource specifications and
    return an updated resource specification. They also perform side-effect calculations
    or make external calls (in the case of custom admission controllers).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 变异准入控制器接受Kubernetes资源规范并返回更新后的资源规范。它们还执行副作用计算或进行外部调用（在自定义准入控制器的情况下）。
- en: On the other hand, validating admission controllers simply accept or deny Kubernetes
    resource API requests. It is important to know that both types of controllers
    only act on create, update, delete, or proxy requests. These controllers cannot
    mutate or change requests to list resources.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，验证准入控制器只是接受或拒绝Kubernetes资源API请求。重要的是要知道，这两种类型的控制器只对创建、更新、删除或代理请求进行操作。这些控制器不能改变或更改列出资源的请求。
- en: When a request of one of those types comes into the Kubernetes API server, it
    will first run the request through all the relevant mutating admission controllers.
    Then, the output, which may be mutated, will pass through the validating admission
    controllers, before finally being acted upon (or not, if the call is denied by
    an admission controller) in the API server.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些类型的请求进入 Kubernetes API 服务器时，它将首先通过所有相关的变异准入控制器运行请求。然后，输出（可能已经变异）将通过验证准入控制器，最后在
    API 服务器中被执行（或者如果被准入控制器拒绝，则不会被执行）。
- en: 'Structurally, the Kubernetes-provided admission controllers are functions or
    "plugins," which run as part of the Kubernetes API server. They rely on two webhook
    controllers (which are admission controllers themselves, just special ones): **MutatingAdmissionWebhook**
    and **ValidatingAdmissionWebhook**. All other admission controllers use either
    one of these webhooks under the hood, depending on their type. In addition, any
    custom admission controllers you write can be attached to either one of these
    webhooks.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构上，Kubernetes 提供的准入控制器是作为 Kubernetes API 服务器的一部分运行的函数或“插件”。它们依赖于两个 webhook
    控制器（它们本身就是准入控制器，只是特殊的准入控制器）：**MutatingAdmissionWebhook** 和 **ValidatingAdmissionWebhook**。所有其他准入控制器在底层都使用这两个
    webhook 中的一个，具体取决于它们的类型。此外，您编写的任何自定义准入控制器都可以附加到这两个 webhook 中的任一个。
- en: Before we look at the process of creating a custom admission controller, let's
    review a few of the default admission controllers that Kubernetes provides. For
    a full list, check the Kubernetes official documentation at [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看创建自定义准入控制器的过程之前，让我们回顾一下 Kubernetes 提供的一些默认准入控制器。有关完整列表，请查看 Kubernetes 官方文档
    [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do)。
- en: Understanding default admission controllers
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解默认准入控制器
- en: There are quite a few default admission controllers present in a typical Kubernetes
    setup – many of which are required for some fairly important basic functionality.
    Here are some examples of default admission controllers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的 Kubernetes 设置中有许多默认的准入控制器，其中许多对一些非常重要的基本功能是必需的。以下是一些默认准入控制器的示例。
- en: The NamespaceExists admission controller
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**NamespaceExists** 准入控制器'
- en: The **NamespaceExists** admission controller checks any incoming Kubernetes
    resource (other than namespaces themselves). This is to check whether the namespace
    attached to the resource exists. If not, it denies the resource request at the
    admission controller level.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**NamespaceExists** 准入控制器检查任何传入的 Kubernetes 资源（除了命名空间本身）。这是为了检查资源所附加的命名空间是否存在。如果不存在，它将在准入控制器级别拒绝资源请求。'
- en: The PodSecurityPolicy admission controller
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PodSecurityPolicy 准入控制器
- en: The **PodSecurityPolicy** admission controller supports Kubernetes Pod security
    policies, which we will learn about momentarily. This controller prevents resources
    that do not follow Pod security policies from being created.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**PodSecurityPolicy** 准入控制器支持 Kubernetes Pod 安全策略，我们马上就会了解到。该控制器阻止不符合 Pod 安全策略的资源被创建。'
- en: In addition to the default admission controllers, we can create custom admission
    controllers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了默认准入控制器之外，我们还可以创建自定义准入控制器。
- en: Creating custom admission controllers
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义准入控制器
- en: 'Creating a custom admission controller can be done dynamically using one of
    the two webhook controllers. The way this works is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用两个 webhook 控制器之一动态地创建自定义准入控制器。其工作方式如下：
- en: You must write your own server or script that runs separately to the Kubernetes
    API server.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您必须编写自己的服务器或脚本，以独立于Kubernetes API服务器运行。
- en: Then, you configure one of the two previously mentioned webhook triggers to
    make a request with resource data to your custom server controller.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以配置前面提到的两个webhook触发器之一，向您的自定义服务器控制器发送带有资源数据的请求。
- en: Based on the result, the webhook controller will then tell the API server whether
    or not to proceed.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于结果，webhook控制器将告诉API服务器是否继续。
- en: 'Let''s start with the first step: writing a quick admission server.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始：编写一个快速的准入服务器。
- en: Writing a server for a custom admission controller
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自定义准入控制器的服务器
- en: To create our custom admission controller server (which will accept webhooks
    from the Kubernetes control plane), we can use any programming language. As with
    most extensions to Kubernetes, Go has the best support and libraries that make
    the task of writing a custom admission controller easier. For now, we will use
    some pseudocode.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的自定义准入控制器服务器（它将接受来自Kubernetes控制平面的webhook），我们可以使用任何编程语言。与大多数对Kubernetes的扩展一样，Go语言具有最好的支持和库，使编写自定义准入控制器更容易。现在，我们将使用一些伪代码。
- en: 'The control flow for our server will look something like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务器的控制流将看起来像这样：
- en: Admission-controller-server.pseudo
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Admission-controller-server.pseudo
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have a simple server for our custom admission controller, we can
    configure a Kubernetes admission webhook to call it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个简单的服务器用于我们的自定义准入控制器，我们可以配置一个Kubernetes准入webhook来调用它。
- en: Configuring Kubernetes to call a custom admission controller server
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置Kubernetes调用自定义准入控制器服务器
- en: In order to tell Kubernetes to call our custom admission server, it needs a
    place to call. We can run our custom admission controller anywhere – it doesn't
    need to be on Kubernetes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了告诉Kubernetes调用我们的自定义准入服务器，它需要一个地方来调用。我们可以在任何地方运行我们的自定义准入控制器 - 它不需要在Kubernetes上。
- en: 'That being said, it''s easy for the purposes of this chapter to run it on Kubernetes.
    We won''t go through the full manifest, but let''s assume we have a Service and
    a Deployment that it is pointed at, running a container that is our server. The
    Service would look something like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，出于本章的目的，在Kubernetes上运行它很容易。我们不会详细介绍清单，但让我们假设我们有一个Service和一个Deployment指向它，运行着我们的服务器的容器。Service看起来会像这样：
- en: Service-webhook.yaml
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Service-webhook.yaml
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It's important to note that our server needs to use HTTPS in order for Kubernetes
    to accept webhook responses. There are many ways to configure this, and we won't
    get into it in this book. The certificate can be self-signed, but the common name
    of the certificate and CA needs to match the one used when setting up the Kubernetes
    cluster.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，我们的服务器需要使用HTTPS，以便Kubernetes接受webhook响应。有许多配置的方法，我们不会在本书中详细介绍。证书可以是自签名的，但证书的通用名称和CA需要与设置Kubernetes集群时使用的名称匹配。
- en: Now that we have our server running and accepting HTTPS requests, let's tell
    Kubernetes where to find it. To do this, we use `MutatingWebhookConfiguration`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的服务器正在运行并接受HTTPS请求，让我们告诉Kubernetes在哪里找到它。为此，我们使用`MutatingWebhookConfiguration`。
- en: 'An example of `MutatingWebhookConfiguration` is shown in the following code
    block:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块显示了`MutatingWebhookConfiguration`的一个示例：
- en: Mutating-webhook-config-service.yaml
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Mutating-webhook-config-service.yaml
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's pick apart the YAML for our `MutatingWebhookConfiguration`. As you can
    see, we can configure more than one webhook in this configuration – though we've
    only done one in this example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下我们的`MutatingWebhookConfiguration`的YAML。正如你所看到的，我们可以在这个配置中配置多个webhook
    - 尽管在这个示例中我们只做了一个。
- en: For each webhook, we set `name`, `rules`, and a `configuration`. The `name`
    is simply the identifier for the webhook. The `rules` allow us to configure exactly
    in which cases Kubernetes should make a request to our admission controller. In
    this case, we have configured our webhook to fire whenever a `CREATE` event for
    resources of the types `pods`, `deployments`, and `configmaps` occurs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个webhook，我们设置`name`，`rules`和`configuration`。`name`只是webhook的标识符。`rules`允许我们精确配置Kubernetes应该在哪些情况下向我们的准入控制器发出请求。在这种情况下，我们已经配置了我们的webhook，每当发生`pods`、`deployments`和`configmaps`类型资源的`CREATE`事件时触发。
- en: Finally, we have the `clientConfig`, where we specify exactly where and how
    Kubernetes should make the webhook request. Since we're running our custom server
    on Kubernetes, we specify the Service name as in the previous YAML, in addition
    to the path on our server to hit (`"/mutate"` is a best practice here), and the
    CA of the cluster to compare to the certificate of the HTTPS termination. If your
    custom admission server is running somewhere else, there are other possible configuration
    fields – check the docs if you need them ([https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`clientConfig`，在其中我们指定Kubernetes应该如何在哪里进行webhook请求。由于我们在Kubernetes上运行我们的自定义服务器，我们指定了服务名称，以及在我们的服务器上要命中的路径（`"/mutate"`在这里是最佳实践），以及要与HTTPS终止证书进行比较的集群CA。如果您的自定义准入服务器在其他地方运行，还有其他可能的配置字段-如果需要，可以查看文档（[https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)）。
- en: Once we create the `MutatingWebhookConfiguration` in Kubernetes, it is easy
    to test the validation. All we need to do is create a Pod, Deployment, or ConfigMap
    as normal, and check whether our requests are denied or patched according to the
    logic in our server.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在Kubernetes中创建了`MutatingWebhookConfiguration`，就很容易测试验证。我们所需要做的就是像平常一样创建一个Pod、Deployment或ConfigMap，并检查我们的请求是否根据服务器中的逻辑被拒绝或修补。
- en: Let's assume for now that our server is set to deny any Pod with a name that
    includes the string `deny-me`. It is also set up to add an error response to the
    `AdmissionReviewResponse`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的服务器目前设置为拒绝任何包含字符串`deny-me`的Pod。它还设置了在`AdmissionReviewResponse`中添加错误响应。
- en: 'Let''s use a Pod spec as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下的Pod规范：
- en: To-deny-pod.yaml
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: To-deny-pod.yaml
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can create our Pod to check the admission controller. We can use the
    following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建我们的Pod来检查准入控制器。我们可以使用以下命令：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And that's it! Our custom admission controller has successfully denied a Pod
    that doesn't match the conditions we specified in our server. For resources that
    are patched (not denied, but altered), `kubectl` will not show any special response.
    You will need to fetch the resource in question to see the patch in action.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们的自定义准入控制器成功拒绝了一个不符合我们在服务器中指定条件的Pod。对于被修补（而不是被拒绝但被更改）的资源，`kubectl`不会显示任何特殊响应。您需要获取相关资源以查看修补的效果。
- en: Now that we've explored custom admission controllers, let's look at another
    way to impose cluster security practices – Pod security policies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了自定义准入控制器，让我们看看另一种实施集群安全实践的方法- Pod安全策略。
- en: Enabling Pod security policies
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用Pod安全策略
- en: The basics of Pod security policies are that they allow a cluster administrator
    to create rules that Pods must follow in order to be scheduled onto a node. Technically,
    Pod security policies are just another type of admission controller. However,
    this feature is officially supported by Kubernetes and is worth an in-depth discussion,
    since many options are available.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Pod安全策略的基本原则是允许集群管理员创建规则，Pod必须遵循这些规则才能被调度到节点上。从技术上讲，Pod安全策略只是另一种准入控制器。然而，这个功能得到了Kubernetes的官方支持，并值得深入讨论，因为有许多选项可用。
- en: Pod security policies can be used to prevent Pods from running as root, put
    limits on ports and volumes used, restrict privilege escalation, and much more.
    We will review a subset of Pod security policy capabilities now, but for a full
    list of Pod security policy configuration types, check the official PSP documentation
    at [https://kubernetes.io/docs/concepts/policy/pod-security-policy/](https://kubernetes.io/docs/concepts/policy/pod-security-policy/).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Pod安全策略可用于防止Pod以root身份运行，限制端口和卷的使用，限制特权升级等等。我们现在将回顾一部分Pod安全策略的功能，但要查看完整的Pod安全策略配置类型列表，请查阅官方PSP文档[https://kubernetes.io/docs/concepts/policy/pod-security-policy/]。
- en: As a final note, Kubernetes also supports low-level primitives for controlling
    container permissions – namely *AppArmor*, *SELinux*, and *Seccomp*. These configurations
    are outside the scope of this book, but they can be useful for highly secure environments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Kubernetes还支持用于控制容器权限的低级原语 - 即*AppArmor*，*SELinux*和*Seccomp*。这些配置超出了本书的范围，但对于高度安全的环境可能会有用。
- en: Steps to create a Pod security policy
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Pod安全策略的步骤
- en: 'There are several steps to implementing Pod security policies:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实施Pod安全策略有几个步骤：
- en: First, the Pod security policy admission controller must be enabled.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，必须启用Pod安全策略准入控制器。
- en: This will prevent all Pods from being created in your cluster since it requires
    a matched Pod security policy and role to be able to create a Pod. You will likely
    want to create your Pod security policies and roles before enabling the admission
    controller for this reason.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将阻止在您的集群中创建所有Pod，因为它需要匹配的Pod安全策略和角色才能创建Pod。出于这个原因，您可能希望在启用准入控制器之前创建您的Pod安全策略和角色。
- en: After the admission controller is enabled, the policy itself must be created.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用准入控制器后，必须创建策略本身。
- en: Then, a `Role` or `ClusterRole` object must be created with access to the Pod
    security policy.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，必须创建具有对Pod安全策略访问权限的`Role`或`ClusterRole`对象。
- en: Finally, that role can be bound with a **ClusterRoleBinding** or **RoleBinding**
    to a user or service `accountService` account, allowing Pods created with that
    service account to use permissions available to the Pod security policy.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，该角色可以与**ClusterRoleBinding**或**RoleBinding**绑定到用户或服务`accountService`帐户，允许使用该服务帐户创建的Pod使用Pod安全策略可用的权限。
- en: In some cases, you may not have the Pod security policy admission controller
    enabled by default on your cluster. Let's look at how to enable it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您的集群可能默认未启用Pod安全策略准入控制器。让我们看看如何启用它。
- en: Enabling the Pod security policy admission controller
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用Pod安全策略准入控制器
- en: In order to enable the PSP admission controller, the `kube-apiserver` must be
    started with a flag that specifies admission controllers to start with. On managed
    Kubernetes (EKS, AKS, and others), the PSP admission controller will likely be
    enabled by default, along with a privileged Pod security policy created for use
    by the initial admin user. This prevents the PSP from causing any issues with
    creating Pods in the new cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用PSP准入控制器，`kube-apiserver`必须使用指定准入控制器的标志启动。在托管的Kubernetes（EKS、AKS等）上，PSP准入控制器可能会默认启用，并且为初始管理员用户创建一个特权Pod安全策略。这可以防止PSP在新集群中创建Pod时出现任何问题。
- en: 'If you''re self-managing Kubernetes and you haven''t yet enabled the PSP admission
    controller, you can do so by restarting the `kube-apiserver` component with the
    following flags:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在自行管理Kubernetes，并且尚未启用PSP准入控制器，您可以通过使用以下标志重新启动`kube-apiserver`组件来启用它：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If your Kubernetes API server is run using a `systemd` file (as it would be
    if following *Kubernetes: The Hard Way*), you should update the flags there instead.
    Typically, `systemd` files are placed in the `/etc/systemd/system/` folder.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的Kubernetes API服务器是使用`systemd`文件运行的（如果遵循*Kubernetes：困难的方式*，它将是这样），则应该在那里更新标志。通常，`systemd`文件放置在`/etc/systemd/system/`文件夹中。
- en: 'In order to find out which admission plugins are already enabled, you can run
    the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出已经启用了哪些准入插件，您可以运行以下命令：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This command will present a long list of admission plugins that are enabled.
    For instance, you will see the following admission plugins in the output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将显示已启用的准入插件的长列表。例如，您将在输出中看到以下准入插件：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we are sure the PSP admission controller is enabled, we can actually
    create a PSP.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们确定了PSP准入控制器已启用，我们实际上可以创建PSP了。
- en: Creating the PSP resource
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建PSP资源
- en: 'Pod security policies themselves can be created using typical Kubernetes resource
    YAML. Here''s a YAML file for a privileged Pod security policy:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Pod安全策略本身可以使用典型的Kubernetes资源YAML创建。以下是一个特权Pod安全策略的YAML文件：
- en: Privileged-psp.yaml
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Privileged-psp.yaml
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This Pod security policy allows the user or service account (via a **RoleBinding**
    or **ClusterRoleBinding**) to create Pods that have privileged capabilities. For
    instance, the Pod using this `PodSecurityPolicy` would be able to bind to the
    host network on ports `2000`-`65535`, run as any user, and bind to any volume
    type. In addition, we have an annotation for a `seccomp` restriction on `allowedProfileNames`
    – to give you an idea of how `Seccomp` and `AppArmor` annotations work with `PodSecurityPolicies`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此Pod安全策略允许用户或服务账户（通过**RoleBinding**或**ClusterRoleBinding**）创建具有特权功能的Pod。例如，使用此`PodSecurityPolicy`的Pod将能够绑定到主机网络的端口`2000`-`65535`，以任何用户身份运行，并绑定到任何卷类型。此外，我们还有一个关于`allowedProfileNames`的`seccomp`限制的注释-这可以让您了解`Seccomp`和`AppArmor`注释与`PodSecurityPolicies`的工作原理。
- en: As we mentioned previously, just creating the PSP does nothing. For any service
    account or user that will be creating privileged Pods, we need to give them access
    to the Pod security policy via a **Role** and **RoleBinding** (or `ClusterRole`
    and `ClusterRoleBinding`).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，仅仅创建PSP是没有任何作用的。对于将创建特权Pod的任何服务账户或用户，我们需要通过**Role**和**RoleBinding**（或`ClusterRole`和`ClusterRoleBinding`）为他们提供对Pod安全策略的访问权限。
- en: 'In order to create a `ClusterRole` that has access to this PSP, we can use
    the following YAML:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建具有对此PSP访问权限的`ClusterRole`，我们可以使用以下YAML：
- en: Privileged-clusterrole.yaml
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Privileged-clusterrole.yaml
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can bind our newly created `ClusterRole` to the user or service account
    with which we intend to create privileged Pods. Let''s do this with a `ClusterRoleBinding`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将新创建的`ClusterRole`绑定到我们打算创建特权Pod的用户或服务账户上。让我们使用`ClusterRoleBinding`来做到这一点：
- en: Privileged-clusterrolebinding.yaml
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Privileged-clusterrolebinding.yaml
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In our case, we want to let every authenticated user on the cluster create privileged
    Pods, so we bind to the `system:authenticated` group.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们希望让集群上的每个经过身份验证的用户都能创建特权Pod，因此我们绑定到`system:authenticated`组。
- en: Now, it is likely that we do not want all our users or Pods to be privileged.
    A more realistic Pod security policy places restrictions on what Pods are capable
    of.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可能不希望所有用户或Pod都具有特权。一个更现实的Pod安全策略会限制Pod的能力。
- en: 'Let''s take a look at some example YAML of a PSP that has these restrictions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下具有这些限制的PSP的一些示例YAML：
- en: unprivileged-psp.yaml
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: unprivileged-psp.yaml
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can tell, this Pod security policy is vastly different in the restrictions
    it imposes on created Pods. No Pods under this policy are allowed to run as root
    or escalate to root. They also have restrictions on the types of volumes they
    can bind to (this section has been highlighted in the preceding code snippet)
    – and they cannot use host networking or bind directly to host ports.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这个Pod安全策略在其对创建的Pod施加的限制方面大不相同。在此策略下，不允许任何Pod以root身份运行或升级为root。它们还对它们可以绑定的卷的类型有限制（在前面的代码片段中已经突出显示了这一部分）-它们不能使用主机网络或直接绑定到主机端口。
- en: In this YAML, both the `runAsUser` and `supplementalGroups` sections control
    the Linux user ID and group IDs that can run or be added by the container, while
    the `fsGroup` key controls the filesystem groups that can be used by the container.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个YAML中，`runAsUser`和`supplementalGroups`部分都控制可以运行或由容器添加的Linux用户ID和组ID，而`fsGroup`键控制容器可以使用的文件系统组。
- en: In addition to using rules like `MustRunAsNonRoot`, it is possible to directly
    specify which user ID a container can run with – and any Pods not running specifically
    with that ID in their spec will not be able to schedule onto a Node.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用诸如`MustRunAsNonRoot`之类的规则，还可以直接指定容器可以使用的用户ID - 任何未在其规范中明确使用该ID运行的Pod将无法调度到节点上。
- en: 'For a sample PSP that restricts users to a specific ID, look at the following
    YAML:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看限制用户特定ID的示例PSP，请查看以下YAML：
- en: Specific-user-id-psp.yaml
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Specific-user-id-psp.yaml
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This Pod security policy, when applied, will prevent any Pods from running as
    user ID `0` or `3001`, or higher. In order to create a Pod that satisfies this
    condition, we use the `runAs` option in the `securityContext` in a Pod spec.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 应用此Pod安全策略后，将阻止任何以用户ID“0”或“3001”或更高的身份运行的Pod。为了创建一个满足这个条件的Pod，我们在Pod规范的`securityContext`中使用`runAs`选项。
- en: 'Here is an example Pod that satisfies this constraint and would be successfully
    scheduled even with this Pod security policy in place:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个满足这一约束的示例Pod，即使有了这个Pod安全策略，它也可以成功调度：
- en: Specific-user-pod.yaml
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Specific-user-pod.yaml
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, in this YAML, we give our Pod a specific user to run with, ID
    `1000`. We also disallowed our Pod from escalating to root. This Pod spec would
    successfully schedule even when `specific-user-psp` is in place.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这个YAML中看到的，我们为我们的Pod指定了一个特定的用户ID`1000`来运行。我们还禁止我们的Pod升级为root。即使`specific-user-psp`已经生效，这个Pod规范也可以成功调度。
- en: Now that we've discussed how Pod security policies can secure Kubernetes by
    placing restrictions on how a Pod runs, we can move onto network policies, where
    we can restrict how Pods network.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了Pod安全策略如何通过对Pod运行方式施加限制来保护Kubernetes，我们可以转向网络策略，我们可以限制Pod的网络。
- en: Using network policies
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用网络策略
- en: Network policies in Kubernetes work similar to firewall rules or route tables.
    They allow users to specify a group of Pods via a selector and then determine
    how and where those Pods can communicate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的网络策略类似于防火墙规则或路由表。它们允许用户通过选择器指定一组Pod，然后确定这些Pod可以如何以及在哪里进行通信。
- en: For network policies to work, your chosen Kubernetes network plugin (such as,
    *Weave*, *Flannel*, or *Calico*) must support the network policy spec. Network
    policies can be created as all other Kubernetes resources are – through a YAML
    file. Let's start with a very simple network policy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使网络策略工作，您选择的Kubernetes网络插件（如*Weave*、*Flannel*或*Calico*）必须支持网络策略规范。网络策略可以像其他Kubernetes资源一样通过一个YAML文件创建。让我们从一个非常简单的网络策略开始。
- en: 'Here is a network policy spec that restricts access to Pods with the label
    `app=server`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个限制访问具有标签`app=server`的Pod的网络策略规范。
- en: Label-restriction-policy.yaml
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Label-restriction-policy.yaml
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, let's pick apart this network policy YAML since it will help us explain
    some more complicated network policies as we progress.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐步解析这个网络策略的YAML，因为这将帮助我们解释随着我们的进展一些更复杂的网络策略。
- en: First, in our spec, we have a `podSelector`, which works similarly to node selectors
    in functionality. Here, we are using `matchLabels` to specify that this network
    policy will only affect Pods with the label `app=server`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在我们的规范中，我们有一个`podSelector`，它在功能上类似于节点选择器。在这里，我们使用`matchLabels`来指定这个网络策略只会影响具有标签`app=server`的Pod。
- en: 'Next, we specify a policy type for our network policy. There are two policy
    types: `ingress` and `egress`. A network policy can specify one or both types.
    `ingress` refers to making network rules that come into effect for connections
    to the matched Pods, and `egress` refers to network rules that come into effect
    for connections leaving the matched Pods.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为我们的网络策略指定一个策略类型。有两种策略类型：`ingress`和`egress`。一个网络策略可以指定一个或两种类型。`ingress`指的是制定适用于连接到匹配的Pod的网络规则，而`egress`指的是制定适用于离开匹配的Pod的连接的网络规则。
- en: 'In this specific network policy, we are simply dictating a single `ingress`
    rule: the only traffic that will be accepted by Pods with the label `app=server`
    is traffic that originates from Pods with the label `app:frontend`. Additionally,
    the only port that will accept traffic on Pods with the label `app=server` is
    `80`.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的网络策略中，我们只是规定了一个单一的`ingress`规则：只有来自具有标签`app=server`的Pod的流量才会被接受，这些流量是源自具有标签`app:frontend`的Pod。此外，唯一接受具有标签`app=server`的Pod上的流量的端口是`80`。
- en: There can be multiple `from` blocks in an `ingress` policy set that correspond
    to multiple traffic rules. Similarly, with `egress`, there can be multiple `to`
    blocks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ingress`策略集中可以有多个`from`块对应多个流量规则。同样，在`egress`中也可以有多个`to`块。
- en: It is important to note that network policies work by namespace. By default,
    if there isn't a single network policy in a namespace, there are no restrictions
    on Pod-to-Pod communication within that namespace. However, as soon as a specific
    Pod is selected by a single network policy, all traffic to and from that Pod must
    explicitly match a network policy rule. If it doesn't match a rule, it will be
    blocked.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，网络策略是按命名空间工作的。默认情况下，如果在命名空间中没有单个网络策略，那么在该命名空间中的Pod之间的通信就没有任何限制。然而，一旦一个特定的Pod被单个网络策略选中，所有到该Pod的流量和从该Pod出去的流量都必须明确匹配一个网络策略规则。如果不匹配规则，它将被阻止。
- en: 'With this in mind, we can easily create policies that enforce broad restrictions
    on Pod networking. Let''s take a look at the following network policy:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，我们可以轻松地创建强制执行广泛限制的Pod网络策略。让我们来看看以下网络策略：
- en: Full-restriction-policy.yaml
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Full-restriction-policy.yaml
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this `NetworkPolicy`, we specify that we will be including both an `Ingress`
    and `Egress` policy, but we don't write a block for either of them. This has the
    effect of automatically denying any traffic for both `Egress` and `Ingress` since
    there are no rules for traffic to match against.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`NetworkPolicy`中，我们指定我们将包括`Ingress`和`Egress`策略，但我们没有为它们写一个块。这样做的效果是自动拒绝任何`Egress`和`Ingress`的流量，因为没有规则可以匹配流量。
- en: Additionally, our `{}` Pod selector value corresponds to selecting every Pod
    in the namespace. The end result of this rule is that every Pod in the `development`
    namespace will not be able to accept ingress traffic or send egress traffic.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们的`{}` Pod选择器值对应于选择命名空间中的每个Pod。这条规则的最终结果是，`development`命名空间中的每个Pod将无法接受入口流量或发送出口流量。
- en: Important note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is also important to note that network policies are interpreted by combining
    all the separate network policies that affect a Pod and then applying the combination
    of all those rules to Pod traffic.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，网络策略是通过结合影响Pod的所有单独的网络策略，然后将所有这些规则的组合应用于Pod流量来解释的。
- en: This means that even though we have restricted all ingress and egress traffic
    in the `development` namespace in our preceding example, we can still enable it
    for specific Pods by adding another network policy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，即使在我们先前的示例中限制了`development`命名空间中的所有入口和出口流量，我们仍然可以通过添加另一个网络策略来为特定的Pod启用它。
- en: Let's assume that now our `development` namespace has complete traffic restriction
    for Pods, we want to allow a subset of Pods to receive network traffic on port
    `443` and send traffic on port `6379` to a database Pod. In order to do this,
    we simply need to create a new network policy that, by the additive nature of
    policies, allows this traffic.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现在我们的`development`命名空间对Pod有完全的流量限制，我们希望允许一部分Pod在端口`443`上接收网络流量，并在端口`6379`上向数据库Pod发送流量。为了做到这一点，我们只需要创建一个新的网络策略，通过策略的叠加性质，允许这种流量。
- en: 'This is what the network policy looks like:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是网络策略的样子：
- en: Override-restriction-network-policy.yaml
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖限制网络策略.yaml
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this network policy, we are allowing our server Pods in the `development`
    namespace to receive traffic from frontend Pods on port `443` and send traffic
    to database Pods on port `6379`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络策略中，我们允许`development`命名空间中的服务器Pod在端口`443`上接收来自前端Pod的流量，并在端口`6379`上向数据库Pod发送流量。
- en: 'If instead, we wanted to open up all Pod-to-Pod communication without any restrictions,
    while still actually instituting a network policy, we could do so with the following
    YAML:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要打开所有Pod之间的通信而没有任何限制，同时实际上还要制定网络策略，我们可以使用以下YAML来实现：
- en: All-open-network-policy.yaml
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 全开放网络策略.yaml
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we have discussed how we can use network policies to set rules on Pod-to-Pod
    traffic. However, it is also possible to use network policies as an external-facing
    firewall of sorts. To do this, we create network policy rules based not on Pods
    as origin or destination, but external IPs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何使用网络策略来设置Pod之间的流量规则。然而，也可以将网络策略用作外部防火墙。为了做到这一点，我们创建基于外部IP而不是Pod作为源或目的地的网络策略规则。
- en: 'Let''s look at an example network policy where we are restricting communication
    to and from a Pod, with a specific IP range as the target:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个限制与特定IP范围作为目标的Pod之间通信的网络策略的示例：
- en: External-ip-network-policy.yaml
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 外部IP网络策略.yaml
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this network policy, we are specifying a single `Ingress` rule and a single
    `Egress` rule. Each of these rules accepts or denies traffic based not on which
    Pod it is coming from but on the source IP of the network request.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络策略中，我们指定了一个`Ingress`规则和一个`Egress`规则。每个规则根据网络请求的源IP而不是来自哪个Pod来接受或拒绝流量。
- en: In our case, we have selected a `/16` subnet mask range (with a specified `/24`
    CIDR exception) for both our `Ingress` and `Egress` rules. This has the side effect
    of preventing any traffic from within our cluster from reaching these Pods since
    none of our Pod IPs will match the rules in a default cluster networking setup.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们已经为我们的“Ingress”和“Egress”规则选择了一个`/16`子网掩码范围（带有指定的`/24` CIDR异常）。这会产生一个副作用，即阻止集群内部的任何流量到达这些Pod，因为我们的Pod
    IP都不会匹配默认集群网络设置中的规则。
- en: However, traffic from outside the cluster in the specified subnet mask (and
    not in the exception range) will be able to both send traffic to the `worker`
    Pods and also be able to accept traffic from the `worker` Pods.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在指定的子网掩码中来自集群外部的流量（并且不在异常范围内）将能够向“worker”Pod发送流量，并且还能够接受来自“worker”Pod的流量。
- en: With the end of our discussion on network policies, we can move onto a completely
    different layer of the security stack – runtime security and intrusion detection.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们讨论网络策略的结束，我们可以转向安全堆栈的一个完全不同的层面 - 运行时安全和入侵检测。
- en: Handling intrusion detection, runtime security, and compliance on Kubernetes
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理Kubernetes上的入侵检测、运行时安全和合规性
- en: Once you have set your Pod security policies and network policies – and generally
    ensured that your configuration is as watertight as possible – there are still
    many attack vectors that are possible in Kubernetes. In this section, we will
    focus on attacks from within a Kubernetes cluster. Even with highly specific Pod
    security policies in place (which definitely do help, to be clear), it is possible
    for containers and applications running in your cluster to perform unexpected
    or malicious operations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您设置了Pod安全策略和网络策略，并且通常确保您的配置尽可能牢固 - Kubernetes仍然存在许多可能的攻击向量。在本节中，我们将重点关注来自Kubernetes集群内部的攻击。即使在具有高度特定的Pod安全策略的情况下（这确实有所帮助，需要明确），您的集群中运行的容器和应用程序仍可能执行意外或恶意操作。
- en: In order to solve this problem, many professionals look to runtime security
    tools, which allow constant monitoring and alerting of application processes.
    For Kubernetes, a popular open source tool that can accomplish this is *Falco*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，许多专业人士寻求运行时安全工具，这些工具允许对应用程序进程进行持续监控和警报。对于Kubernetes来说，一个流行的开源工具就是*Falco*。
- en: Installing Falco
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Falco
- en: Falco bills itself as a *behavioral activity monitor* for processes on Kubernetes.
    It can monitor both your containerized applications running on Kubernetes as well
    as the Kubernetes components themselves.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Falco自称为Kubernetes上进程的*行为活动监视器*。它可以监视在Kubernetes上运行的容器化应用程序以及Kubernetes组件本身。
- en: How does Falco work? In real time, Falco parses system calls from the Linux
    kernel. It then filters these system calls through rules – which are sets of configurations
    that can be applied to the Falco engine. Whenever a rule is broken by a system
    call, Falco triggers an alert. It's that simple!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Falco是如何工作的？在实时中，Falco解析来自Linux内核的系统调用。然后，它通过规则过滤这些系统调用 - 这些规则是可以应用于Falco引擎的一组配置。每当系统调用违反规则时，Falco就会触发警报。就是这么简单！
- en: Falco ships with an extensive set of default rules that add significant observability
    at the kernel level. Custom rules are of course supported by Falco – and we will
    show you how to write them.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Falco附带了一套广泛的默认规则，可以在内核级别增加显著的可观察性。当然，Falco支持自定义规则 - 我们将向您展示如何编写这些规则。
- en: First, however, we need to install Falco on our cluster! Luckily, Falco can
    be installed using Helm. However, it is very important to note that there are
    a few different ways to install Falco, and they differ significantly in how effective
    they can be in the event of a breach.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们需要在我们的集群上安装Falco！幸运的是，Falco可以使用Helm进行安装。但是，非常重要的是要注意，有几种不同的安装Falco的方式，在事件发生时它们在有效性上有很大的不同。
- en: We're going to be installing Falco using the Helm chart, which is simple and
    works well for managed Kubernetes clusters, or any scenario where you may not
    have direct access to the worker nodes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Helm图表安装Falco，这对于托管的Kubernetes集群或者您可能无法直接访问工作节点的任何情况都非常简单且有效。
- en: However, for the best possible security posture, Falco should be installed directly
    onto the Kubernetes nodes at the Linux level. The Helm chart, which uses a DaemonSet
    is great for ease of use but is inherently not as secure as a direct Falco installation.
    To install Falco directly to your nodes, check the installation instructions at
    [https://falco.org/docs/installation/](https://falco.org/docs/installation/).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了获得最佳的安全姿态，Falco应该直接安装到Kubernetes节点的Linux级别。使用DaemonSet的Helm图表非常易于使用，但本质上不如直接安装Falco安全。要直接将Falco安装到您的节点上，请查看[https://falco.org/docs/installation/](https://falco.org/docs/installation)上的安装说明。
- en: 'With that caveat out of the way, we can install Falco using Helm:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个警告，我们可以使用Helm安装Falco：
- en: 'First, we need to add the `falcosecurity` repo to our local Helm:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将`falcosecurity`存储库添加到我们本地的Helm中：
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we can proceed with actually installing Falco using Helm.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以继续使用Helm实际安装Falco。
- en: Important note
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Falco Helm chart has many possible variables that can be changed in the
    values file – for a full review of those, you can check the official Helm chart
    repo at [https://github.com/falcosecurity/charts/tree/master/falco](https://github.com/falcosecurity/charts/tree/master/falco).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Falco Helm图表有许多可能可以在values文件中更改的变量-要全面审查这些变量，您可以在官方Helm图表存储库[https://github.com/falcosecurity/charts/tree/master/falco](https://github.com/falcosecurity/charts/tree/master/falco)上查看。
- en: 'To install Falco, run the following:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装Falco，请运行以下命令：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This command will install Falco using the default values, which you can see
    at [https://github.com/falcosecurity/charts/blob/master/falco/values.yaml](https://github.com/falcosecurity/charts/blob/master/falco/values.yaml).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将使用默认值安装Falco，您可以在[https://github.com/falcosecurity/charts/blob/master/falco/values.yaml](https://github.com/falcosecurity/charts/blob/master/falco/values.yaml)上查看默认值。
- en: Next, let's dive into what Falco offers a security-conscious Kubernetes administrator.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解Falco为安全意识的Kubernetes管理员提供了什么。
- en: Understanding Falco's capabilities
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解Falco的功能
- en: As mentioned previously, Falco ships with a set of default rules, but we can
    easily add more rules using new YAML files. Since we're using the Helm version
    of Falco, passing custom rules to Falco is as simple as either creating a new
    values file or editing the default one with custom rules.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Falco附带一组默认规则，但我们可以使用新的YAML文件轻松添加更多规则。由于我们使用的是Helm版本的Falco，因此将自定义规则传递给Falco就像创建一个新的values文件或编辑具有自定义规则的默认文件一样简单。
- en: 'Adding custom rules looks like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 添加自定义规则看起来像这样：
- en: Custom-falco.yaml
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Custom-falco.yaml
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now is a good time to discuss the structure of a Falco rule. To illustrate,
    let's borrow a few lines of rules from the `Default` Falco ruleset that ships
    with the Falco Helm chart.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是讨论Falco规则结构的好时机。为了说明，让我们借用一些来自随Falco Helm图表一起提供的`Default` Falco规则集的规则。
- en: When specifying Falco configuration in YAML, we can use three different types
    of keys to help compose our rules. These are macros, lists, and rules themselves.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在YAML中指定Falco配置时，我们可以使用三种不同类型的键来帮助组成我们的规则。这些是宏、列表和规则本身。
- en: The specific rule we're looking at in this example is called `Launch Privileged
    Container`. This rule will detect when a privileged container has been started
    and log some information about the container to `STDOUT`. Rules can do all sorts
    of things when it comes to alerts, but logging to `STDOUT` is a good way to increase
    observability when high-risk events happen.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们正在查看的具体规则称为`启动特权容器`。这个规则将检测特权容器何时被启动，并记录一些关于容器的信息到`STDOUT`。规则在处理警报时可以做各种事情，但记录到`STDOUT`是在发生高风险事件时增加可观察性的好方法。
- en: 'First, let''s look at the rule entry itself. This rule uses a few helper entries,
    several macros, and lists – but we''ll get to those in a second:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一下规则条目本身。这个规则使用了一些辅助条目，几个宏和列表 - 但我们将在稍后讨论这些：
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, a Falco rule has several parts. First, we have the rule name
    and description. Then, we specify the triggering condition for the rule – which
    acts as a filter for Linux system calls. If a system call matches all the logic
    filters in the `condition` block, the rule is triggered.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Falco规则有几个部分。首先，我们有规则名称和描述。然后，我们指定规则的触发条件 - 这充当Linux系统调用的过滤器。如果系统调用匹配`condition`块中的所有逻辑过滤器，规则就会被触发。
- en: When a rule is triggered, the output key allows us to set a format for how the
    text of the output appears. The `priority` key lets us assign a priority, which
    can be one of `emergency`, `alert`, `critical`, `error`, `warning`, `notice`,
    `informational`, and `debug`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 当触发规则时，`output`键允许我们设置输出文本的格式。`priority`键让我们分配一个优先级，可以是`emergency`、`alert`、`critical`、`error`、`warning`、`notice`、`informational`和`debug`中的一个。
- en: Finally, the `tags` key applies tags to the rule in question, making it easier
    to categorize rules. This is especially important when using alerts that aren't
    simply plain text `STDOUT` entries.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`tags`键将标签应用于相关的规则，使得更容易对规则进行分类。当使用不仅仅是简单文本`STDOUT`条目的警报时，这一点尤为重要。
- en: The syntax for `condition` is especially important here, and we will focus on
    how this system of filtering works.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`condition`的语法特别重要，我们将重点关注过滤系统的工作原理。
- en: First off, since the filters are essentially logical statements, you will see
    some familiar syntax (if you have ever programmed or written pseudocode) – and,
    and not, and so on. This syntax is pretty simple to learn, and a full discussion
    of it – the *Sysdig* filter syntax – can be found at [https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering](https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于过滤器本质上是逻辑语句，您将看到一些熟悉的语法（如果您曾经编程或编写伪代码） - `and`、`and not`、`and so on`。这种语法很容易学习，可以在[https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering](https://github.com/draios/sysdig/wiki/sysdig-user-guide#filtering)找到关于它的全面讨论
    - *Sysdig*过滤器语法。
- en: As a note, the Falco open source project was originally created by *Sysdig*,
    which is why it uses the common *Sysdig* filter syntax.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Falco开源项目最初是由*Sysdig*创建的，这就是为什么它使用常见的*Sysdig*过滤器语法。
- en: Next, you will see reference to `container_started` and `container`, as well
    as `falco_privileged_containers` and `user_privileged_containers`. These are not
    plain strings but the use of macros – references to other blocks in the YAML that
    specify additional functionality, and generally make it much easier to write rules
    without repeating a lot of configuration.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将看到对`container_started`和`container`的引用，以及`falco_privileged_containers`和`user_privileged_containers`的引用。这些不是简单的字符串，而是宏的使用
    - 引用YAML中其他块的引用，指定了额外的功能，并且通常使得编写规则变得更加容易，而不需要重复大量的配置。
- en: 'To see how this rule really works, let''s look at a full reference for all
    the macros that were referenced in the preceding rule:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这个规则是如何真正工作的，让我们看一下在前面规则中引用的所有宏的完整参考：
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You will see in the preceding YAML that each macro is really just a reusable
    block of `Sysdig` filter syntax, often using other macros to accomplish the rule
    functionality. Lists, not pictured here, are like macros except that they do not
    describe filter logic. Instead, they include a list of string values that can
    be used as part of a comparison using the filter syntax.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在前面的YAML中看到，每个宏实际上只是一块可重用的`Sysdig`过滤器语法块，通常使用其他宏来完成规则功能。列表在这里没有显示，它们类似于宏，但不描述过滤逻辑。相反，它们包括一个字符串值列表，可以作为使用过滤器语法的比较的一部分。
- en: 'For instance, `(``trusted_images)` in the `falco_privileged_containers` macro
    references a list called `trusted_images`. Here''s the source for that list:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在`falco_privileged_containers`宏中的`(``trusted_images)`引用了一个名为`trusted_images`的列表。以下是该列表的来源：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, this particular list is empty in the default rules, but a custom
    ruleset could use a list of trusted images in this list, which would then automatically
    be consumed by all the other macros and rules that use the `trusted_image` list
    as part of their filter rules.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，在默认规则中，这个特定列表是空的，但自定义规则集可以在这个列表中使用一组受信任的镜像，然后这些受信任的镜像将自动被所有使用`trusted_image`列表作为其过滤规则一部分的其他宏和规则所使用。
- en: As mentioned previously, in addition to tracking Linux system calls, Falco can
    also track Kubernetes control plane events as of Falco v0.13.0.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，除了跟踪Linux系统调用之外，Falco在版本v0.13.0中还可以跟踪Kubernetes控制平面事件。
- en: Understanding Kubernetes audit event rules in Falco
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解Falco中的Kubernetes审计事件规则
- en: 'Structurally, these Kubernetes audit event rules work the same way as Falco''s
    Linux system call rules. Here''s an example of one of the default Kubernetes rules
    in Falco:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构上，这些Kubernetes审计事件规则的工作方式与Falco的Linux系统调用规则相同。以下是Falco中默认Kubernetes规则的示例：
- en: '[PRE26]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This rule acts on Kubernetes audit events in Falco (essentially, control plane
    events) to alert when a Pod is created that isn't on the list `allowed_k8s_containers`.
    The default `k8s` audit rules contain many similar rules, most of which output
    formatted logs when triggered.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则在Falco中针对Kubernetes审计事件（基本上是控制平面事件），在创建不在`allowed_k8s_containers`列表中的Pod时发出警报。默认的`k8s`审计规则包含许多类似的规则，大多数在触发时输出格式化日志。
- en: 'Now, we talked about Pod security policies a bit earlier in this chapter –
    and you may be seeing some similarities between PSPs and Falco Kubernetes audit
    event rules. For instance, take this entry from the default Kubernetes Falco rules:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在本章的前面谈到了一些Pod安全策略，你可能会发现PSPs和Falco Kubernetes审计事件规则之间有一些相似之处。例如，看看默认的Kubernetes
    Falco规则中的这个条目：
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This rule, which is triggered when a Pod is attempting to start using the host
    network, maps directly to host network PSP settings.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则在尝试使用主机网络启动Pod时触发，直接映射到主机网络PSP设置。
- en: Falco capitalizes on this similarity by letting us use Falco as a way to `trial`
    new Pod security policies without applying them cluster-wide and causing issues
    with running Pods.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Falco利用这种相似性，让我们可以使用Falco作为一种`试验`新的Pod安全策略的方式，而不会在整个集群中应用它们并导致运行中的Pod出现问题。
- en: For this purpose, `falcoctl` (the Falco command-line tool) comes with the `convert
    psp` command. This command takes in a Pod security policy definition and turns
    it into a set of Falco rules. These Falco rules will just output logs to `STDOUT`
    when triggered (instead of causing Pod scheduling failures like a PSP mismatch),
    which makes it much easier to test out new Pod security policies in an existing
    cluster.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，`falcoctl`（Falco命令行工具）带有`convert psp`命令。该命令接受一个Pod安全策略定义，并将其转换为一组Falco规则。这些Falco规则在触发时只会将日志输出到`STDOUT`（而不会像PSP不匹配那样导致Pod调度失败），这样就可以更轻松地在现有集群中测试新的Pod安全策略。
- en: To learn how to use the `falcoctl` conversion tool, check out the official Falco
    documentation at [https://falco.org/docs/psp-support/](https://falco.org/docs/psp-support/).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何使用`falcoctl`转换工具，请查看官方Falco文档[https://falco.org/docs/psp-support/](https://falco.org/docs/psp-support/)。
- en: Now that we have a good grounding on the Falco tool, let's discuss how it can
    be used to implement compliance controls and runtime security.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Falco工具有了很好的基础，让我们讨论一下它如何用于实施合规性控制和运行时安全。
- en: Mapping Falco to compliance and runtime security use cases
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将Falco映射到合规性和运行时安全用例
- en: Because of its extensibility and ability to audit low-level Linux system calls,
    Falco is a great tool for continuous compliance and runtime security.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其可扩展性和审计低级别的Linux系统调用的能力，Falco是持续合规性和运行时安全的绝佳工具。
- en: On the compliance side, it is possible to leverage Falco rulesets that map specifically
    to the requirements of a compliance standard – for instance, PCI or HIPAA. This
    allows users to quickly detect and act on any processes that do not comply with
    the standard in question. There are open and closed source Falco rulesets for
    several standards.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在合规性方面，可以利用Falco规则集，这些规则集专门映射到合规性标准的要求-例如PCI或HIPAA。这使用户能够快速检测并采取行动，处理不符合相关标准的任何进程。有几个标准的开源和闭源Falco规则集。
- en: Similarly, for runtime security, Falco exposes an alerting/eventing system,
    which means that any runtime events that trigger an alert can also trigger automated
    intervention and remediation processes. This can work for both security and compliance.
    As an example, if a Pod triggers a Falco alert for non-compliance, a process can
    work off that alert and delete the offending Pod immediately.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于运行时安全，Falco公开了一个警报/事件系统，这意味着任何触发警报的运行时事件也可以触发自动干预和补救过程。这对安全性和合规性都适用。例如，如果一个Pod触发了Falco的不合规警报，一个进程可以立即处理该警报并删除有问题的Pod。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about security in the context of Kubernetes. First,
    we reviewed the basics of security on Kubernetes – which layers of the security
    stack are relevant to our cluster and some broad strokes of how to manage that
    complexity. Next, we learned about some of the major security issues that Kubernetes
    has encountered, as well as discussing the results of the 2019 security audit.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了Kubernetes上下文中的安全性。首先，我们回顾了Kubernetes上的安全性基础知识-安全堆栈的哪些层对我们的集群相关，以及如何管理这种复杂性的一些基本概念。接下来，我们了解了Kubernetes遇到的一些主要安全问题，以及讨论了2019年安全审计的结果。
- en: Then, we implemented security at two different levels of the stack in Kubernetes
    – first, in configuration with Pod security policies and network policies, and
    finally, runtime security with Falco.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在Kubernetes的两个不同级别实施了安全性-首先是使用Pod安全策略和网络策略进行配置，最后是使用Falco进行运行时安全。
- en: In the next chapter, we will learn how to make Kubernetes your own by building
    custom resources. This will allow you to add significant new functionality to
    your cluster.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何通过构建自定义资源使Kubernetes成为您自己的。这将允许您为集群添加重要的新功能。
- en: Questions
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the names of the two webhook controllers that a custom admission controller
    can use?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义准入控制器可以使用哪两个Webhook控制器的名称？
- en: What effect does a blank `NetworkPolicy` for ingress have?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 空的`NetworkPolicy`对入口有什么影响？
- en: What sort of Kubernetes control plane events would be valuable to track in order
    to prevent attackers from altering Pod functionality?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了防止攻击者更改Pod功能，哪种类型的Kubernetes控制平面事件对于跟踪是有价值的？
- en: Further reading
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Kubernetes CVE Database: [https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes CVE数据库：[https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes](https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes)
