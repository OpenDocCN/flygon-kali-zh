- en: Collecting and Querying Logs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和查询日志
- en: In critical moments, men sometimes see exactly what they wish to see.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键时刻，人们有时会看到他们希望看到的东西。
- en: '- *Spock*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- *Spock*'
- en: So far, our primary focus was on metrics. We used them in different forms and
    for different purposes. In some cases, we used metrics to scale Pods and nodes.
    In others, metrics were used to create alerts that would notify us when there
    is an issue that cannot be fixed automatically. We also created a few dashboards.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的主要重点是指标。我们以不同的形式和不同的目的使用它们。在某些情况下，我们使用指标来扩展Pod和节点。在其他情况下，指标被用来创建警报，以便在出现无法自动修复的问题时通知我们。我们还创建了一些仪表板。
- en: However, metrics are often not enough. That is especially true when dealing
    with issues that require manual interventions. When metrics alone are insufficient,
    we usually need to consult logs hoping that they will reveal the cause of the
    problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，指标通常是不够的。特别是在处理需要手动干预的问题时。当仅仅依靠指标是不够的时，我们通常需要查看日志，希望它们能揭示问题的原因。
- en: Logging is often misunderstood or, to be more precise, mixed with metrics. For
    many, the line between logs and metrics is blurred. Some are extracting metrics
    from logs. Others are treating metrics and logs as the same source of information.
    Both approaches are wrong. Metrics and logs are separate entities, they serve
    different purposes, and there is a clear distinction between them. We store them
    separately, and we use them to solve different types of issues. We'll park that
    and a few other discussions. Instead of going into details based on theory, we'll
    explore them through hands-on examples. For that, we need a cluster.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 日志经常被误解，或者更准确地说，与指标混淆。对许多人来说，日志和指标之间的界限变得模糊。有些人从日志中提取指标。其他人则将指标和日志视为相同的信息来源。这两种方法都是错误的。指标和日志是独立的实体，它们有不同的用途，它们之间有明显的区别。我们将它们分开存储，并且我们使用它们来解决不同类型的问题。我们将暂时搁置这些讨论和其他一些讨论。我们不会基于理论细节进行探讨，而是通过实际示例来探索它们。为此，我们需要一个集群。
- en: Creating a cluster
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个集群
- en: You know the drill. We'll move into the directory with the `vfarcic/k8s-specs`
    ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository, we'll pull the latest version of the code just in case I pushed something
    recently, and we'll create a new cluster unless you already have one at hand.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道该怎么做。我们将进入`vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))存储库的目录，我们将拉取最新版本的代码，以防我最近推送了一些东西，然后我们将创建一个新的集群，除非您已经有一个可用。
- en: All the commands from this chapter are available in the `07-logging.sh` ([https://gist.github.com/vfarcic/74774240545e638b6cf0e01460894f34](https://gist.github.com/vfarcic/74774240545e638b6cf0e01460894f34))
    Gist.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在`07-logging.sh` ([https://gist.github.com/vfarcic/74774240545e638b6cf0e01460894f34](https://gist.github.com/vfarcic/74774240545e638b6cf0e01460894f34))
    Gist中找到。
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This time, the requirements for the cluster changed. We need much more memory
    than before. The main culprit is ElasticSearch which is very resource hungry.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，集群的要求发生了变化。我们需要比以前更多的内存。主要问题是ElasticSearch非常耗资源。
- en: If you're using **Docker for Desktop** or **minikube**, you'll need to increase
    the memory dedicated to the cluster to **10 GB**. If that's too much for your
    laptop, you might choose to read the *Exploring Centralized Logging Through Elasticsearch,
    Fluentd, and Kibana* without running the examples or you might have to switch
    to one of the Cloud providers (AWS, GCP, or Azure).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用**Docker for Desktop**或**minikube**，您需要将集群的内存增加到**10 GB**。如果这对您的笔记本来说太多，您可以选择阅读*通过Elasticsearch、Fluentd和Kibana探索集中式日志记录*，而不运行示例，或者您可能需要切换到云提供商（AWS、GCP或Azure）之一。
- en: In the case of **EKS** and **AKS**, we'll need bigger nodes. For EKS we'll use
    **t2.large** and for AKS **Standard_B2ms**. Both are based on **2 CPUs** and **8
    GB RAM**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **EKS** 和 **AKS**，我们将需要更大的节点。对于 EKS，我们将使用 **t2.large**，对于 AKS，我们将使用 **Standard_B2ms**。两者都基于
    **2 个 CPU** 和 **8 GB RAM**。
- en: '**GKE** requirements are the same as before.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**GKE** 的要求与以前相同。'
- en: On top of new requirements, it should be noted that we do NOT need Prometheus
    in this chapter, so I removed it from the Gists.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 除了新的要求之外，还应该注意到在本章中我们不需要 Prometheus，所以我从 Gists 中删除了它。
- en: Feel free to use one of the Gists that follow to create a new cluster, or to
    validate that the one you're planning to use meets the requirements.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意使用以下其中一个 Gist 来创建一个新的集群，或者验证您计划使用的集群是否符合要求。
- en: '`gke-monitor.sh`: **GKE** with 3 n1-standard-1 worker nodes, **nginx Ingress**,
    **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4](https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4)).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gke-monitor.sh`：**GKE** 有 3 个 n1-standard-1 工作节点，**nginx Ingress**，**tiller**，并且集群
    IP 存储在环境变量 **LB_IP** 中（[https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4](https://gist.github.com/vfarcic/10e14bfbec466347d70d11a78fe7eec4)）。'
- en: '`eks-logging.sh`: **EKS** with 3 t2.large worker nodes, **nginx Ingress**,
    **tiller**, **Metrics Server**, **Cluster Autoscaler**, and cluster IP stored
    in environment variable **LB_IP** ([https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7](https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7)).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eks-logging.sh`：**EKS** 有 3 个 t2.large 工作节点，**nginx Ingress**，**tiller**，**Metrics
    Server**，**Cluster Autoscaler**，并且集群 IP 存储在环境变量 **LB_IP** 中（[https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7](https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7)）。'
- en: '`aks-logging.sh`: **AKS** with 3 Standard_B2ms worker nodes, **nginx Ingress**,
    and **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/c4a63b92c03a0a1c721cb63b07d2ddfc](https://gist.github.com/vfarcic/c4a63b92c03a0a1c721cb63b07d2ddfc)).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aks-logging.sh`：**AKS** 有 3 个 Standard_B2ms 工作节点，**nginx Ingress**，和 **tiller**，并且集群
    IP 存储在环境变量 **LB_IP** 中（[https://gist.github.com/vfarcic/c4a63b92c03a0a1c721cb63b07d2ddfc](https://gist.github.com/vfarcic/c4a63b92c03a0a1c721cb63b07d2ddfc)）。'
- en: '`docker-logging.sh`: **Docker for Desktop** with **2 CPUs** and **10 GB RAM**,
    **nginx Ingress**, **tiller**, **Metrics Server**, and cluster IP stored in environment
    variable **LB_IP** ([https://gist.github.com/vfarcic/17d4f11ec53eed74e4b5e73debb4a590](https://gist.github.com/vfarcic/17d4f11ec53eed74e4b5e73debb4a590)).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker-logging.sh`：**Docker for Desktop** 有 **2 个 CPU** 和 **10 GB RAM**，**nginx
    Ingress**，**tiller**，**Metrics Server**，并且集群 IP 存储在环境变量 **LB_IP** 中（[https://gist.github.com/vfarcic/17d4f11ec53eed74e4b5e73debb4a590](https://gist.github.com/vfarcic/17d4f11ec53eed74e4b5e73debb4a590)）。'
- en: '`minikube-logging.sh`: **minikube** with **2 CPUs** and **10 GB RAM**, **ingress**,
    **storage-provisioner**, **default-storageclass**, and **metrics-server** addons
    enabled, **tiller**, and cluster IP stored in environment variable **LB_IP** ([https://gist.github.com/vfarcic/9f72c8451e1cca71758c70195c1c9f07](https://gist.github.com/vfarcic/9f72c8451e1cca71758c70195c1c9f07)).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minikube-logging.sh`：**minikube** 有 **2 个 CPU** 和 **10 GB RAM**，启用了 **ingress**，**storage-provisioner**，**default-storageclass**，和
    **metrics-server** 插件，**tiller**，并且集群 IP 存储在环境变量 **LB_IP** 中（[https://gist.github.com/vfarcic/9f72c8451e1cca71758c70195c1c9f07](https://gist.github.com/vfarcic/9f72c8451e1cca71758c70195c1c9f07)）。'
- en: Now that we have a working cluster, we'll explore how to use logs through `kubectl`.
    That will provide a base for more comprehensive solutions that follow.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个可用的集群，我们将探索如何通过 `kubectl` 使用日志。这将为后续更全面的解决方案提供一个基础。
- en: Exploring logs through kubectl
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 kubectl 探索日志
- en: The first contact most people have with logs in Kubernetes is through `kubectl`.
    It is almost unavoidable not to use it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人在 Kubernetes 中接触日志的第一次是通过 `kubectl`。几乎不可避免地会使用它。
- en: As we're learning how to tame the Kubernetes beast, we are bound to check logs
    when we get stuck. In Kubernetes, the term "logs" is reserved for the output produced
    by our and third-party applications running inside a cluster. However, those exclude
    the events generated by different Kubernetes resources. Even though many would
    call them logs as well, Kubernetes separates them from logs and calls them events.
    I'm sure that you already know how to retrieve logs from the applications and
    how to see Kubernetes events. Nevertheless, we'll explore them briefly here as
    well since that will add relevance to the discussion we'll have later on. I promise
    to keep it short, and you are free to skip this section if a brief overview of
    logging and events baked into Kubernetes is too basic for you.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们学习如何驯服Kubernetes时，我们必须在遇到困难时检查日志。在Kubernetes中，“日志”一词是指集群内运行的我们和第三方应用程序产生的输出。然而，这些不包括不同Kubernetes资源生成的事件。尽管许多人也称它们为日志，但Kubernetes将它们与日志分开，并称其为事件。我相信你已经知道如何从应用程序中检索日志以及如何查看Kubernetes事件。尽管如此，我们也会在这里简要探讨它们，因为这将使我们后面的讨论更有关联。我保证会简短地介绍，如果你认为Kubernetes中日志和事件的简要概述对你来说太基础，你可以跳过这一部分。
- en: We'll install the already familiar `go-demo-5` application. It should generate
    enough logs for us to explore them. Since it consists of a few resources, we are
    bound to create some Kubernetes events as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装已经熟悉的`go-demo-5`应用程序。它应该会产生足够的日志供我们探索。由于它由几个资源组成，我们也必然会创建一些Kubernetes事件。
- en: Off we go.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们出发了。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We rolled out `go-demo-5` and sent a `curl` request to confirm that it is indeed
    working.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署了`go-demo-5`并发送了一个`curl`请求来确认它确实在运行。
- en: The outputs and screenshots in this chapter are taken from minikube, except
    inside the sections dedicated to exclusively GKE, EKS, and AKS. There might be
    slight differences between what you see here and what you can observe on your
    screen.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的输出和截图来自minikube，除了专门用于GKE、EKS和AKS的部分。你在这里看到的内容可能与你在屏幕上观察到的有轻微差异。
- en: To see "logs" generated by Kubernetes and limited to a specific resource, we
    need to retrieve the events.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看由Kubernetes生成并限于特定资源的“日志”，我们需要检索事件。
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output, limited to the messages in the `Events` section, is as follows.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 输出仅限于“事件”部分的消息如下。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The events you see in front of you are, in a way, Kubernetes logs generated
    by, in this case, the `go-demo-5-db` StatefulSet.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你在面前看到的事件，在某种程度上，是由Kubernetes生成的日志，这里是`go-demo-5-db` StatefulSet。
- en: While those events are useful, they are often insufficient. More often than
    not, we do not know in advance where the problem is. If one of our Pods misbehaves,
    the cause might be in that Pod, but it might also be in the ReplicaSet that created
    it, or it might be in the Deployment that created the ReplicaSet, or maybe the
    node got detached from the cluster, or it might be something completely different.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些事件很有用，但通常还不够。往往我们事先不知道问题出在哪里。如果我们的Pod之一表现不佳，问题可能在于该Pod，但也可能在创建它的ReplicaSet中，或者可能在创建ReplicaSet的Deployment中，或者可能节点从集群中分离了，或者可能是完全不同的原因。
- en: For any but the smallest systems, going from one resource to another and from
    one node to another to find the cause of an issue is anything but practical, reliable,
    and fast.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于除了最小的系统之外的任何系统来说，从一个资源到另一个资源，从一个节点到另一个节点去找到问题的原因绝非实际、可靠和快速。
- en: Simply put, looking at events by describing a resource is not the way to go
    and we need to find an alternative.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，通过描述资源来查看事件并不是解决问题的方法，我们需要找到替代方案。
- en: But, before we do that, let's see what happens with logs from applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，让我们看看应用程序的日志发生了什么。
- en: We deployed a few replicas of the `go-demo-5` API and a few replicas of the
    MongoDB. How can we explore their logs if we suspect that there is a problem with
    one of them? We can execute `kubectl logs` command like the one that follows.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署了几个`go-demo-5` API的副本和几个MongoDB的副本。如果我们怀疑其中一个存在问题，我们如何探索它们的日志？我们可以执行像下面这样的`kubectl
    logs`命令。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output shows the logs of the `db` container inside the `go-demo-5-db-0`
    Pod.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了`go-demo-5-db-0` Pod内`db`容器的日志。
- en: While the previous output is limited to a single container and a single Pod,
    we can use labels to retrieve logs from multiple Pods.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然先前的输出仅限于单个容器和单个Pod，但我们可以使用标签从多个Pod中检索日志。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This time, the output comes from all the Pods with the label `app` set to `go-demo-5`.
    We broadened our results, and that is often all we need. If we know that there
    is something wrong with, let's say, `go-demo-5` Pods, we need to figure out whether
    the issue is present in multiple Pods or it is limited to a single one. While
    the previous command allowed us to broaden our search, if there were something
    suspicious in those logs, we would not know where that comes from. Retrieving
    logs from multiple Pods does not get us any closer to knowing which Pods are misbehaving.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，输出来自所有标签为`app`设置为`go-demo-5`的Pod。我们扩大了我们的结果，这通常是我们所需要的。如果我们知道`go-demo-5`的Pod存在问题，我们需要弄清楚问题是存在于多个Pod中还是仅限于一个Pod。虽然先前的命令允许我们扩大搜索范围，但如果日志中有可疑的内容，我们将不知道其来源。从多个Pod中检索日志并不能让我们更接近知道哪些Pod的行为不端。
- en: Using labels is still very limiting. They are by no means a substitute for more
    complex querying. We might need to filter the results based on timestamps, nodes,
    keywords, and so on. While we could accomplish some of those things with additional
    `kubectl logs` arguments and creative usage of `grep`, `sed`, and other Linux
    commands, this approach to retrieving, filtering, and outputting logs is far from
    optimum.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标签仍然非常有限。它们绝对不能替代更复杂的查询。我们可能需要根据时间戳、节点、关键字等来过滤结果。虽然我们可以通过额外的`kubectl logs`参数和对`grep`、`sed`和其他Linux命令的创造性使用来实现其中一些功能，但这种检索、过滤和输出日志的方法远非最佳。
- en: More often than not, `kubectl logs` command does not provide us with enough
    options to perform anything but simplest retrieval of logs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 往往`kubectl logs`命令并不能为我们提供足够的选项来执行除了最简单的日志检索之外的任何操作。
- en: We need a few things to improve our debugging capabilities. We need a potent
    query language that will allow us to filter log entries, we need sufficient information
    about the origin of those logs, we need queries to be fast, and we need access
    to logs created in any part of the cluster. We'll try to accomplish that, and
    a few other things, by setting up a centralized logging solution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些东西来提高我们的调试能力。我们需要一个强大的查询语言，可以让我们过滤日志条目，我们需要足够的关于这些日志来源的信息，我们需要查询速度快，我们需要访问集群中任何部分创建的日志。我们将尝试通过设置集中式日志解决方案来实现这一点，以及其他一些事情。
- en: Choosing a centralized logging solution
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择集中式日志解决方案
- en: The first thing we need to do is to find a place where we'll store logs. Given
    that we want to have the ability to filter log entries, storing them in files
    should be discarded from the start. What we need is a database, of sorts. It is
    more important that it is fast than transactional, so we are most likely looking
    into a solution that is an in-memory database. But, before we take a look at the
    choices, we should discuss the location of our database. Should we run it inside
    our cluster, or should we use a service? Instead of making that decision right
    away, we'll explore both options, before we make a choice.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是找到一个存储日志的地方。考虑到我们希望能够过滤日志条目，将它们存储在文件中应该从一开始就被排除。我们需要的是一种数据库。它更重要的是快速而不是事务性，所以我们很可能在寻找一种内存数据库解决方案。但在我们看选择之前，我们应该讨论一下我们的数据库位置。我们应该在集群内运行它，还是应该使用一个服务？在立即做出决定之前，我们将探讨两种选择，然后再做出选择。
- en: There are two major groups of logging-as-a-service types. If we are running
    our cluster with one of the Cloud providers, an obvious choice might be to use
    a logging solution they provide. EKS has AWS CloudWatch, GKE has GCP Stackdriver,
    and AKS has Azure Log Analytics. If you choose to use one of the Cloud vendors,
    that might make a lot of sense. Why bother with setting up your own solution of
    looking for a third-party service, if everything is already set up and waiting
    for you? We'll explore them soon.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 日志服务有两个主要类型。如果我们正在使用云提供商之一的集群，一个明显的选择可能是使用他们提供的日志解决方案。EKS有AWS CloudWatch，GKE有GCP
    Stackdriver，AKS有Azure Log Analytics。如果您选择使用其中一个云供应商，这可能是很有意义的。如果一切都已经设置好并等待您，为什么要费心设置自己的解决方案或寻找第三方服务呢？我们很快将探索它们。
- en: Since my mission is to provide instructions that work for (almost) anyone, we'll
    also explore one of the logging-as-a-service solutions found outside hosting vendors.
    But, which one should we select? There are too many solutions in the market. We
    could, for example, choose *Splunk* ([https://www.splunk.com/](https://www.splunk.com/))
    or *DataDog* ([https://www.datadoghq.com/](https://www.datadoghq.com/)). Both
    are excellent choices, and both are much more than only logging solutions. We
    can use them to collect metrics (like with Prometheus). They provide dashboards
    (like Grafana), and a few other things. Later on, we'll discuss whether we should
    combine logs and metrics in a single tool. For now, our focus is only on logging,
    and that's the main reason we'll skip Splunk, DataDog, and similar comprehensive
    tools that offer much more than what we need. That does not mean that you should
    discard them, but rather that this chapter tries to maintain the focus on logging.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我的使命是为（几乎）任何人提供有效的指导，我们还将探讨一些在托管供应商之外找到的日志服务解决方案。但是，我们应该选择哪一个？市场上有太多的解决方案。例如，我们可以选择*Splunk*
    ([https://www.splunk.com/](https://www.splunk.com/)) 或 *DataDog* ([https://www.datadoghq.com/](https://www.datadoghq.com/))。两者都是很好的选择，而且都不仅仅是日志解决方案。我们可以使用它们来收集指标（就像使用Prometheus一样）。它们提供仪表板（就像Grafana一样），还有其他一些功能。稍后，我们将讨论是否应该将日志和指标合并到一个工具中。目前，我们的重点只是日志记录，这也是我们跳过Splunk、DataDog和类似综合工具的主要原因，因为它们提供的远远超出我们所需的。这并不意味着你应该放弃它们，而是本章试图保持对日志记录的关注。
- en: There are many logging services available, with *Scalyr* ([https://www.scalyr.com/pricing](https://www.scalyr.com/pricing)),
    [*logdna*](https://logdna.com/) ([https://logdna.com/](https://logdna.com/)),
    *sumo logic* ([https://www.sumologic.com/](https://www.sumologic.com/)) being
    only a few. We won't go through all of them since would take much more time and
    space than I feel is useful. Given that most services are very similar when logging
    is involved, I'll skip a detailed comparison and jump straight into *Papertrail*
    ([https://papertrailapp.com/](https://papertrailapp.com/)), my favorite logging
    service. Please bear in mind that we'll use it only as an example. I will assume
    that you'll check at least a few others and make your own choice based on your
    needs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多日志记录服务可用，包括*Scalyr* ([https://www.scalyr.com/pricing](https://www.scalyr.com/pricing)),
    [*logdna*](https://logdna.com/) ([https://logdna.com/](https://logdna.com/)),
    *sumo logic* ([https://www.sumologic.com/](https://www.sumologic.com/)) 等等。我们不会逐一介绍它们，因为这将花费比我认为有用的时间和空间更多。鉴于大多数服务在涉及日志记录时非常相似，我将跳过详细比较，直接介绍*Papertrail*
    ([https://papertrailapp.com/](https://papertrailapp.com/))，这是我最喜欢的日志记录服务。请记住，我们只会将其用作示例。我假设您至少会检查其他一些服务，并根据自己的需求做出选择。
- en: Logging-as-a-service might not be a good fit for all. Some might prefer a self-hosted
    solution, while others might not even be allowed to send data outside their clusters.
    In those cases, a self-hosted logging solution is likely the only choice.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为服务的日志记录可能并不适合所有人。有些人可能更喜欢自托管的解决方案，而其他人甚至可能不被允许将数据发送到集群外部。在这些情况下，自托管的日志记录解决方案很可能是唯一的选择。
- en: Even if you are not restrained to your own cluster, there might be other reasons
    to keep it inside, latency being only one of many. We'll explore a self-hosted
    solution as well, so let us pick one. Which one will it be?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您不受限于自己的集群，也可能有其他原因将其保留在内部，延迟只是其中之一。我们也将探讨自托管的解决方案，因此让我们选择一个。它将是哪一个？
- en: Given that we need a place to store our logs, we might look into traditional
    databases. However, most of them would not fit our needs. Transactional databases
    like MySQL require fixed schemas so we can discard them immediately. NoSQL is
    a better fit so we might choose something like MongoDB. But, that would be a poor
    choice since we require the ability to perform very fast free-text searches. For
    that we probably need an in-memory database. MongoDB is not one of those. We could
    use Splunk Enterprise, but this book is dedicated to free (mostly open source)
    solutions. The only exception we made so far is with Cloud providers, and I intend
    to keep it that way.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们需要一个存储日志的地方，我们可能会研究传统数据库。然而，大多数传统数据库都不符合我们的需求。像MySQL这样的事务性数据库需要固定的模式，因此我们可以立即将其排除。NoSQL更适合，因此我们可能会选择类似MongoDB的东西。但是，这将是一个糟糕的选择，因为我们需要能够执行非常快速的自由文本搜索。为此，我们可能需要一个内存数据库。MongoDB不是其中之一。我们可以使用Splunk
    Enterprise，但本书致力于免费（大多数是开源）的解决方案。到目前为止，我们唯一的例外是云提供商，我打算保持这种方式。
- en: The few requirements we mentioned (fast, free-text, in-memory, and a free solution)
    limit our potential candidates to only a few. *Solr* ([http://lucene.apache.org/solr/](http://lucene.apache.org/solr/))
    is one of those, but its usage has been dropping, and it is rarely used today
    (for a good reason). The solution that sticks from a tiny crowd is *Elasticsearch*
    ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)).
    If you have a preference for a different on-prem solution, consider the examples
    we'll go through as a set of practices that you should be able to apply to other
    centralized logging solutions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到的少数要求（快速、自由文本、内存中、免费解决方案）将我们的潜在候选者限制在很少的几个。*Solr*（[http://lucene.apache.org/solr/](http://lucene.apache.org/solr/)）是其中之一，但它的使用量一直在下降，如今很少被使用（有充分的理由）。从一小群人中脱颖而出的解决方案是*Elasticsearch*（[https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)）。如果你对本地解决方案有偏好，可以将我们将要介绍的示例视为一套实践，你应该能够将其应用到其他集中式日志记录解决方案上。
- en: All in all, we'll explore an example of independent logging-as-a-service offering
    (Papertrail), we'll explore the solutions provided with Cloud hosting vendors
    (AWS CloudWatch, GCP Stackdriver, and Azure Log Analytics), and we'll try to set
    up ElasticSearch with a few friends. Those should provide enough examples for
    you to choose which type of a solution fits your use case the best.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们将探索一个独立的日志服务提供示例（Papertrail），我们将探索云托管供应商提供的解决方案（AWS CloudWatch、GCP Stackdriver和Azure
    Log Analytics），并尝试与一些朋友设置ElasticSearch。这些示例应该足够让你选择哪种类型的解决方案最适合你的用例。
- en: But, before we explore the tools where we will store logs, we need to figure
    out how to collect them and ship them to their final destination.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在我们探索存储日志的工具之前，我们需要弄清楚如何收集它们并将它们发送到最终目的地。
- en: Exploring logs collection and shipping
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索日志收集和发送
- en: For a long time now, there are two major contestants for the "logs collection
    and shipping" throne. Those are *Logstash* ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash))
    and *Fluentd* ([https://www.fluentd.org/](https://www.fluentd.org/)). Both are
    open source, and both are widely accepted and actively maintained. While both
    have their pros and cons, Fluentd turned up to have an edge with cloud-native
    distributed systems. It consumes fewer resources and, more importantly, it is
    not tied to a single destination (Elasticsearch). While Logstash can push logs
    to many different targets, it is primarily designed to work with Elasticsearch.
    For that reason, other logging solutions adopted Fluentd.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，有两个主要竞争者争夺“日志收集和发送”王位。它们分别是*Logstash*（[https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)）和*Fluentd*（[https://www.fluentd.org/](https://www.fluentd.org/)）。两者都是开源的，都得到了广泛的接受和积极的维护。虽然两者都有各自的优缺点，但Fluentd在云原生分布式系统中表现出了优势。它消耗更少的资源，更重要的是，它不受限于单一目的地（Elasticsearch）。虽然Logstash可以将日志推送到许多不同的目标，但它主要设计用于与Elasticsearch一起工作。因此，其他日志解决方案采用了Fluentd。
- en: As of today, no matter which logging product you embrace, the chances are that
    it will support Fluentd. The culmination of that adoption can be seen by Fluentd's
    entry into the list of *Cloud Native Computing Foundation* ([https://www.cncf.io/](https://www.cncf.io/))
    projects. Even Elasticsearch users are adopting Fluentd over Logstash. What was
    previously commonly referred to as **ELK** (**Elasticsearch**, **Logstash**, **Kibana**)
    stack, is now called **EFK** (**Elasticsearch**, **Fluentd**, **Kibana**).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，无论你采用哪种日志产品，它都很可能支持Fluentd。这种采用的高潮可以从Fluentd进入*Cloud Native Computing Foundation*（[https://www.cncf.io/](https://www.cncf.io/)）项目列表中看出。甚至Elasticsearch用户也在采用Fluentd而不是Logstash。以前通常被称为**ELK**（**Elasticsearch**、**Logstash**、**Kibana**）堆栈的东西，现在被称为**EFK**（**Elasticsearch**、**Fluentd**、**Kibana**）。
- en: We'll follow the trend and adopt Fluentd as the solution for collecting and
    shipping logs, no matter whether the destination is Papertrail, Elasticsearch,
    or something else.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跟随潮流，采用Fluentd作为收集和传送日志的解决方案，无论目的地是Papertrail、Elasticsearch还是其他什么。
- en: We'll install Fluentd soon. But, since Papertrail is our first target, we need
    to create and set up an account. For now, remember that we need to collect logs
    from all the nodes of the cluster and, as you already know, Kubernetes' DaemonSet
    will ensure that a Fluentd Pod will run in each of our servers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将安装Fluentd。但是，由于Papertrail是我们的第一个目标，我们需要创建和设置一个账户。现在，记住我们需要从集群的所有节点收集日志，正如您已经知道的那样，Kubernetes的DaemonSet将确保在我们的每个服务器上运行一个Fluentd
    Pod。
- en: Exploring centralized logging through Papertrail
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Papertrail探索集中式日志记录
- en: The first centralized logging solution we'll explore is *Papertrail* ([https://papertrailapp.com/](https://papertrailapp.com/)).
    We'll use it as a representative of a logging-as-a-service solution that can save
    us from installing and, more importantly, maintaining a self-hosted alternative.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索的第一个集中式日志记录解决方案是*Papertrail* ([https://papertrailapp.com/](https://papertrailapp.com/))。我们将把它作为一种日志即服务解决方案的代表，它可以使我们摆脱安装和更重要的是维护自托管的替代方案。
- en: Papertrail features live trailing, filtering by timestamps, powerful search
    queries, pretty colors, and quite a few other things that might (or might not)
    be essential when skimming through logs produced inside our clusters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Papertrail具有实时跟踪、按时间戳过滤、强大的搜索查询、漂亮的颜色，以及在浏览我们集群内产生的日志时可能（或可能不）必不可少的其他一些功能。
- en: The first thing we need to do is to register or, if this is not the first time
    you tried Papertrail, to log in.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是注册，或者，如果这不是您第一次尝试Papertrail，那就登录。
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Please follow the instructions to register or to log in if you already have
    a user in their system.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照说明注册或登录，如果您已经在他们的系统中有用户。
- en: You will be glad to find out that Papertrail provides a free plan that allows
    storage of 50 MB of logs searchable for one day, as well as a full year of downloadable
    archives. That should be more than enough for running the examples we are about
    to explore. If you have a relatively small cluster, that should keep you going
    indefinitely. Their prices are reasonable, even if your cluster is bigger and
    you have more monthly logs than 50 MB.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您会高兴地发现，Papertrail提供了一个免费计划，允许存储50MB的日志，可搜索一天，以及一整年的可下载存档。这应该足够运行我们即将探索的示例。如果您有一个相对较小的集群，那应该可以无限期地继续下去。即使您的集群更大，每月的日志超过50MB，他们的价格也是合理的。
- en: Arguably, they are so cheap that we can say that it provides a better return
    on investment than if we'd run an alternative solution inside our own cluster. After
    all, nothing is free. Even self-hosted solutions based on open source create costs
    in maintenance time as well as in compute power.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，他们的价格如此便宜，以至于我们可以说它提供了比在我们自己的集群内运行替代解决方案更好的投资回报。毕竟，没有什么是免费的。即使基于开源的自托管解决方案也会在维护时间和计算能力方面产生成本。
- en: For now, what matters is that the examples we'll run with Papertrail will be
    well within their free plan.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，重要的是我们将使用Papertrail运行的示例将完全在他们的免费计划范围内。
- en: If you have a small operation, Papertrail will work well. But, if you're having
    many applications and a bigger cluster, you might be wondering whether Papertrail
    scales to suit your needs. Worry not. One of their customers is GitHub, and they
    are likely bigger than you are. Papertrail can handle (almost) any load. Whether
    it is a good solution for you is yet to be discovered. Read on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个小型操作，Papertrail将运作良好。但是，如果您有许多应用程序和一个更大的集群，您可能会想知道Papertrail是否能够满足您的需求。不用担心。他们的一个客户是GitHub，他们可能比您更大。Papertrail可以处理（几乎）任何负载。它是否对您是一个好的解决方案还有待发现。继续阅读。
- en: Let's go to the start screen unless you are already there.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去到起始屏幕，除非您已经在那里。
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you were redirected to the welcome screen, you are not authenticated (your
    session might have expired). Login and repeat the previous command to get to the
    start screen.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您被重定向到欢迎屏幕，则表示您未经过身份验证（您的会话可能已过期）。登录并重复上一个命令以返回到起始屏幕。
- en: Click the Add systems button.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“添加系统”按钮。
- en: If you read the instructions, you'll probably think that the setup is relatively
    easy. It is. However, Kubernetes is not available as one of the options. If you
    change the value of the *from* drop-down list to *something else...*, you'll see
    a fairly big list of log sources that can be plugged into Papertrail. Still, there
    is no sign of Kubernetes. The closest one on that list is *Docker*. Even that
    one will not do. Don't worry. I prepared instructions for you or, to be more precise,
    I extracted them from the documentation buried in Papertrail's site.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您阅读了说明，您可能会认为设置相对容易。的确如此。然而，Kubernetes不作为选项之一。如果您将*from*下拉列表的值更改为*something
    else...*，您将看到一个相当大的日志来源列表，可以连接到Papertrail。但是，没有Kubernetes的迹象。列表中最接近的是*Docker*。即使那个也不行。不用担心。我已经为您准备好了说明，更准确地说，我从Papertrail网站的文档中提取了它们。
- en: Please note the `Your logs will go to logsN.papertrailapp.com:NNNNN and appear
    in Events` message at the top of the screen. We'll need that address soon, so
    we better store the values in environment variables.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意屏幕顶部的`Your logs will go to logsN.papertrailapp.com:NNNNN and appear in Events`消息。我们很快就会需要那个地址，所以最好将这些值存储在环境变量中。
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Please replace the first `[...]` with the host. It should be something like
    `logsN.papertrailapp.com`, where `N` is the number assigned to you by Papertrail.
    The second `[...]` should be replaced with the port from the before mentioned
    message.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请用主机替换第一个`[...]`。它应该类似于`logsN.papertrailapp.com`，其中`N`是Papertrail分配给您的数字。第二个`[...]`应该用前面提到的消息中的端口替换。
- en: Now that we have the host and the port stored in environment variables, we can
    explore the mechanism we'll use to collect and ship the logs to Papertrail.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将主机和端口存储在环境变量中，我们可以探索我们将用来收集和发送日志到Papertrail的机制。
- en: Since I already claimed that most vendors adopted Fluentd for collecting and
    shipping logs to their solutions, it should come as no surprise that Papertrail
    recommends it as well. Folks from SolarWinds (Papertrail's parent company) created
    an image with customized Fluentd that we can use. In turn, I created a YAML file
    with all the resources we'll need to run their image.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我已经声称大多数供应商都采用了Fluentd来收集和发送日志到他们的解决方案，因此Papertrail也推荐使用它。SolarWinds（Papertrail的母公司）的人员创建了一个带有定制Fluentd的镜像，我们可以使用。反过来，我创建了一个YAML文件，其中包含我们运行其镜像所需的所有资源。
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, the YAML defines a DaemonSet with ServiceAccount, SolarWind's
    Fluentd, and a ConfigMap that uses a few environment variables to specify the
    host and the port where logs should be shipped.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，YAML定义了一个带有ServiceAccount、SolarWind的Fluentd和使用一些环境变量来指定日志应该发送到的主机和端口的ConfigMap的DaemonSet。
- en: We'll have to change the `logsN.papertrailapp.com` and `NNNNN` entries in that
    YAML before we apply it. Also, I prefer running all logs-related resources in
    `logging` Namespace, so we'll need to change that as well.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们应用之前，我们需要更改YAML中的`logsN.papertrailapp.com`和`NNNNN`条目。此外，我更喜欢在`logging`命名空间中运行所有与日志相关的资源，所以我们也需要更改那个。
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we're running Fluentd in our cluster and that it is configured to forward
    logs to our Papertrail account, we should turn back to its UI.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在集群中运行Fluentd，并且它配置为将日志转发到我们的Papertrail帐户，我们应该转回到它的UI。
- en: Please switch back to Papertrail console in your browser. You should see a green
    box stating that logs were received. Click the Events link.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请在浏览器中切换回Papertrail控制台。您应该看到一个绿色框，指出已接收到日志。点击事件链接。
- en: '![](assets/cbe13042-656d-4b4f-9703-0cc7617f5aed.png)Figure 7-1: Papertrail''s
    Setup Logging screen'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/cbe13042-656d-4b4f-9703-0cc7617f5aed.png)图7-1：Papertrail的设置日志屏幕'
- en: Next, we'll produce a few logs and explore how they appear in Papertrail.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将生成一些日志，并探索它们在Papertrail中的显示方式。
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: That Pod uses `chentex/random-logger` image that has a single purpose. It periodically
    outputs random log entries.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 该Pod使用`chentex/random-logger`图像，其目的是单一的。它定期输出随机日志条目。
- en: Let's create `random-logger`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建`random-logger`。
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Please wait for a minute or two to accumulate a few logs entries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请等待一两分钟积累一些日志条目。
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output should be similar to the one that follows.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于接下来的内容。
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, the container is outputting random entries, some of them as
    `ERROR`, and others as `DEBUG`, `WARN`, and `INFO`. Messages are random as well.
    After all, that is not a real application, but a simple image that produces log
    entries we can use to explore our logging solution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，容器正在输出随机条目，其中一些是`ERROR`，其他的是`DEBUG`，`WARN`和`INFO`。消息也是随机的。毕竟，这不是一个真正的应用程序，而是一个产生日志条目的简单图像，我们可以用来探索我们的日志解决方案。
- en: Please go back to Papertrail UI. You should notice that all the logs from our
    system are available. Some are coming from Kubernetes, while others are from system-level
    services.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请返回到Papertrail UI。您应该注意到我们系统中的所有日志都可用。一些来自Kubernetes，而其他来自系统级服务。
- en: Those from `go-demo-5` are also there, together with the `random-logger` we
    just installed. We'll focus on the latter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`go-demo-5`的日志也在那里，与我们刚刚安装的`random-logger`一起。我们将专注于后者。'
- en: Let's imagine that we found out through alerts that there is an issue and that
    we limited the scope to the `random-logger` application. Alerts helped us detect
    the problem and we narrowed it down to a single application by digging through
    metrics. We still need to consult logs to find the cause. Given what we know (or
    invented), the logical next step would be to retrieve only the log entries related
    to the `random-logger`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们通过警报发现了问题，并将范围限定在`random-logger`应用程序上。警报帮助我们检测到问题，并通过挖掘指标将其缩小到单个应用程序。我们仍然需要查看日志以找出原因。根据我们所知道的（或者虚构的），逻辑上的下一步将是仅检索与`random-logger`相关的日志条目。
- en: Please type `random-logger` in the Search field at the bottom of the screen,
    and press the enter key.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请在屏幕底部的搜索字段中键入`random-logger`，然后按回车键。
- en: '![](assets/10d4a911-f534-4f12-9cc7-111825990d08.png)Figure 7-2: Papertrail''s
    Events screen'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/10d4a911-f534-4f12-9cc7-111825990d08.png)图7-2：Papertrail的事件屏幕'
- en: From now on, we'll see only log entries that contain the word `random-logger`.
    That does not necessarily mean that only the log entries from that application
    are displayed. Instead, any mention of that word is shown on the screen. What
    we did was to instruct Papertrail to perform a free-text search inside all the
    log entries and retrieve only those that contain the beforementioned word.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们将只看到包含单词`random-logger`的日志条目。这并不一定意味着只显示来自该应用程序的日志条目。相反，屏幕上显示了该单词的任何提及。我们所做的是指示Papertrail在所有日志条目中执行自由文本搜索，并仅检索包含上述单词的日志条目。
- en: While free-text search across all the records is probably the most commonly
    used query, there are a few other ways we could filter logs. We won't go through
    all of them. Instead, click the Search tips button in the right-hand side of the
    Search field and explore the syntax yourself. If those few examples are not enough,
    click the Full Syntax Guide link.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管跨所有记录的自由文本搜索可能是最常用的查询方式，但我们还有一些其他过滤日志的方法。我们不会逐一介绍所有这些方法。相反，点击搜索字段右侧的“搜索提示”按钮，自行探索语法。如果这些示例不够用，点击“完整语法指南”链接。
- en: '![](assets/3649c719-cc5c-41d6-8527-a99601654bc3.png)Figure 7-3: Papertrail''s
    Syntax & Examples screen'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/3649c719-cc5c-41d6-8527-a99601654bc3.png)图7-3：Papertrail的语法和示例屏幕'
- en: There's probably no need to explore Papertrail in more detail. It is intuitive,
    easy to use, and well-documented service. I'm sure you'll figure out the details
    if you choose to use it. For now, we'll remove the DaemonSet and the ConfigMap
    before we move into exploring alternatives.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 可能没有必要更详细地探索Papertrail。它是直观的，易于使用，并且有很好的文档服务。我相信您如果选择使用它，会弄清楚细节。现在，我们将在进入探索替代方案之前删除DaemonSet和ConfigMap。
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, we'll explore logging solutions available in Cloud providers. Feel free
    to jump directly to *GCP Stackdriver*, *AWS CloudWatch*, or *Azure Log Analytics*. If
    you do not use any of the three providers, you can skip them altogether and go
    directly to the *Exploring centralized logging through Elasticsearch, Fluentd,
    and Kibana* sub-chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨云提供商中可用的日志记录解决方案。随时可以直接跳转到*GCP Stackdriver*、*AWS CloudWatch*或*Azure
    Log Analytics*。如果您不使用这三个提供商中的任何一个，可以完全跳过它们，直接转到*通过Elasticsearch、Fluentd和Kibana探索集中式日志记录*子章节。
- en: Combining GCP Stackdriver with a GKE cluster
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将GCP Stackdriver与GKE集群结合使用
- en: If you're using GKE cluster, logging is already set up, even though you might
    not know about it. By default, every GKE cluster comes by default with a Fluentd
    DaemonSet that is configured to forward logs to GCP Stackdriver. It is running
    in the `kube-system` Namespace.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用GKE集群，日志记录已经设置好了，尽管您可能不知道。默认情况下，每个GKE集群都默认配备了一个配置为将日志转发到GCP Stackdriver的Fluentd
    DaemonSet。它正在`kube-system`命名空间中运行。
- en: Let's describe GKE's Fluentd DaemonSet and see whether there is any useful information
    we might find.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下GKE的Fluentd DaemonSet，并看看我们可能会找到的一些有用信息。
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限相关部分，如下所示。
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see that, among others, the DaemonSet's Pod Template has the label `k8s-app=fluentd-gcp`.
    We'll need it soon. Also, we can see that one of the containers is based on the
    `stackdriver-logging-agent` image. Just as Papertrail extended Fluentd, Google
    did the same.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，除其他外，DaemonSet的Pod模板具有标签`k8s-app=fluentd-gcp`。我们很快就会需要它。此外，我们可以看到其中一个容器是基于`stackdriver-logging-agent`镜像的。就像Papertrail扩展了Fluentd一样，Google也做了同样的事情。
- en: Now that we know that Stackdriver-specific Fluentd is running in our cluster
    as a DaemonSet, the logical conclusion would be that there is already a UI we
    can use to explore the logs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了在我们的集群中运行的特定于Stackdriver的Fluentd是作为DaemonSet运行的，逻辑结论将是已经有一个我们可以用来探索日志的UI。
- en: UI is indeed available but, before we see it in action, we'll output the logs
    of the Fluentd containers and verify that everything is working as expected.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: UI确实可用，但在我们看到它实际运行之前，我们将输出Fluentd容器的日志，并验证一切是否按预期工作。
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Unless you already enabled Stackdriver Logging API, the output should contain
    at least one message similar to the one that follows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您已经启用了Stackdriver Logging API，否则输出应该至少包含一个类似于以下内容的消息。
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Fortunately, the warning already tells us not only what the issue is, but also
    what to do. Open the link from the log entry in your favorite browser, and click
    the ENABLE button.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，警告已经告诉我们不仅问题是什么，还告诉了我们该怎么做。在您喜欢的浏览器中打开日志条目中的链接，然后单击“启用”按钮。
- en: Now that we enabled Stackdriver Logging API, Fluentd will be able to ship log
    entries there. All we have to do is wait for a minute or two until the action
    propagates.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启用了Stackdriver Logging API，Fluentd将能够将日志条目传送到那里。我们所要做的就是等待一两分钟，直到操作传播。
- en: Let's see the Stackdriver UI.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Stackdriver用户界面。
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Please type `random-logger` in the Filter by label or text search field and
    select GKE Container from the drop-down list.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请在标签或文本搜索字段中键入`random-logger`，然后从下拉列表中选择GKE容器。
- en: The output should display all the logs that contain `random-logger` text.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应显示包含`random-logger`文本的所有日志。
- en: '![](assets/6e1b7844-b066-4a75-8bf5-3e131aba34ff.png)Figure 7-4: GCP Stackdriver
    logs screen'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/6e1b7844-b066-4a75-8bf5-3e131aba34ff.png)图7-4：GCP Stackdriver日志屏幕'
- en: We won't go into details how to use Stackdriver. It is easy and, hopefully,
    intuitive. So, I'll leave it to you to explore it in more detail. What matters
    is that it is very similar to what we experienced with Papertrail. Most of the
    differences are cosmetic.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细介绍如何使用Stackdriver。这很容易，希望也很直观。所以，我会留给你更详细地探索它。重要的是它与我们在Papertrail中体验到的非常相似。大部分差异是表面的。
- en: If you are using GCP, Stackdriver is ready and waiting for you. As such, it
    probably makes sense to use it over any other third-party solution. Stackdriver
    contains not only the logs coming from the cluster but also logs of all GCP services
    (for example, load balancers). That is probably the significant difference between
    the two solutions. It is a massive bonus in favor of Stackdriver. Still, check
    the pricing before making a decision.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用GCP，Stackdriver已经准备好等待您。因此，使用它可能是有道理的，而不是使用任何其他第三方解决方案。Stackdriver不仅包含来自集群的日志，还包括所有GCP服务的日志（例如负载均衡器）。这可能是两种解决方案之间的重大区别。这是Stackdriver的巨大优势。但是，在做出决定之前，请检查定价。
- en: Combining AWS CloudWatch with an EKS cluster
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将AWS CloudWatch与EKS集群结合使用
- en: Unlike GKE that has a logging solution baked into a cluster, EKS requires us
    to set up a solution. It does provide CloudWatch service, but we need to ensure
    that the logs are shipped there from our cluster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 与GKE相比，EKS需要我们设置日志解决方案，而不是在集群中内置日志解决方案。它确实提供了CloudWatch服务，但我们需要确保日志从我们的集群中传送到那里。
- en: Just as before, we'll use Fluentd to collect logs and ship them to CloudWatch.
    Or, to be more precise, we'll use a Fluentd tag built specifically for CloudWatch.
    As you probably already know, we'll also need an IAM policy that will allow Fluentd
    to communicate with CloudWatch.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们将使用Fluentd来收集日志并将其传送到CloudWatch。更准确地说，我们将使用专门为CloudWatch构建的Fluentd标签。您可能已经知道，我们还需要一个IAM策略，允许Fluentd与CloudWatch通信。
- en: All in all, the setup we are about to make will be very similar to the one we
    did with Papertrail, except that we'll store the logs in CloudWatch, and that
    we'll have to put some effort into creating AWS permissions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们即将进行的设置将与我们在Papertrail中进行的设置非常相似，只是我们将把日志存储在CloudWatch中，并且我们将不得不花一些精力创建AWS权限。
- en: Before we proceed, I'll assume that you still have the environment variables
    `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_DEFAULT_REGION` used in
    the `eks-logging.sh` ([https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7](https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7))
    Gist. If you don't, please create them.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我假设您仍然拥有`eks-logging.sh`（[https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7](https://gist.github.com/vfarcic/a783351fc9a3637a291346dd4bc346e7)）Gist中使用的环境变量`AWS_ACCESS_KEY_ID`，`AWS_SECRET_ACCESS_KEY`和`AWS_DEFAULT_REGION`。如果没有，请创建它们。
- en: Off we go.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始吧。
- en: We need to create a new AWS **Identity and Access Management** (**IAM**) ([https://aws.amazon.com/iam/](https://aws.amazon.com/iam/))
    policy. For that, we need to find out the IAM role, and for that we need the IAM
    profile. If you're confused with that, it might help to know that you're not the
    only one. AWS permissions are anything but straightforward. Nevertheless, that's
    not the subject of this chapter (nor the book), so I will assume at least a basic
    understanding of how IAM works.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个新的AWS **身份和访问管理**（**IAM**）([https://aws.amazon.com/iam/](https://aws.amazon.com/iam/))政策。为此，我们需要找出IAM角色，而这需要IAM配置文件。如果你对此感到困惑，知道你并不是唯一一个会这样的人可能会有所帮助。AWS权限绝非简单。尽管如此，这并不是本章（也不是本书）的主题，所以我会假设至少有基本的IAM工作原理的理解。
- en: If we reverse engineer the route to creating an IAM policy, the first thing
    we need is the profile.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们逆向工程IAM政策的创建路径，我们首先需要的是配置文件。
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output should be similar to the one that follows.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该与接下来的内容类似。
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we know the profile, we can use it to retrieve the role.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了配置文件，我们可以使用它来检索角色。
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: With the role at hand, we can finally create the policy. I already created one
    we can use, so let's take a quick look at it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有了角色，我们终于可以创建政策了。我已经创建了一个我们可以使用的，让我们快速看一下。
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output is as follows.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, there's nothing special about that policy. It defines permissions
    required for interaction with `logs` (CloudWatch) from inside our cluster.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这个政策并没有什么特别之处。它定义了在我们集群内部与`logs`（CloudWatch）交互所需的权限。
- en: So, let's move on and create it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续并创建它。
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Finally, to be on the safe side, we'll retrieve the `eks-logs` policy and confirm
    that it was indeed created correctly.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了安全起见，我们将检索`eks-logs`政策并确认它确实被正确创建了。
- en: '[PRE27]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `PolicyDocument` section of the output should be the same as the JSON file
    we used to create the policy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的`PolicyDocument`部分应该与我们用来创建政策的JSON文件相同。
- en: Now that we have the policy in place, we can turn our attention to Fluentd.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经制定了政策，我们可以把注意力转向Fluentd。
- en: Unfortunately, at this moment (December 2018), there is no CloudWatch-friendly
    Fluentd Helm Chart. So, we'll fall back to good old YAML. I prepared one, so let's
    take a quick look at it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前（2018年12月），还没有适用于CloudWatch的Fluentd Helm Chart。因此，我们将退而求其次，使用老式的YAML。我已经准备好了一个，让我们快速看一下。
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: I won't go into the details of the YAML. You should be able to understand what
    it does by exploring it on your own. The key resources are the `fluentd-cloudwatch`
    ConfigMap that contains the configuration and the DaemonSet with the same name
    that will run Fluentd Pod in each node of your cluster. The only difficulty you
    might have with that YAML is to understand the Fluentd configuration, especially
    if that is the first time you're working with it. Nevertheless, we won't go into
    details, and I'll let you explore Fluentd's documentation on your own. Instead,
    we'll `apply` that YAML hoping that everything works as expected.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细介绍YAML。你应该能够通过自己探索来理解它的作用。关键资源是包含配置的`fluentd-cloudwatch` ConfigMap和具有相同名称的DaemonSet，它将在集群的每个节点上运行Fluentd
    Pod。你可能会在理解Fluentd配置方面遇到困难，特别是如果这是你第一次使用它。尽管如此，我们不会深入细节，我会让你自己去探索Fluentd的文档。相反，我们将`apply`这个YAML，希望一切都能如预期般工作。
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Before we move into Cloudwatch UI, we'll retrieve Fluentd Pods and confirm that
    there is one in each node of the cluster.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入Cloudwatch UI之前，我们将检索Fluentd Pods并确认集群中每个节点都有一个。
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In my case, the output shows three `fluentd-cloudwatch` Pods matching the number
    of nodes in my EKS cluster.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，输出显示了三个与我的EKS集群中节点数量相匹配的`fluentd-cloudwatch` Pods。
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now that everything seems to be working inside our cluster, the time has come
    to move into CloudWatch UI.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一切似乎都在我们的集群内正常运行，是时候进入CloudWatch UI了。
- en: '[PRE32]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Please type `random-logger` in the Log Stream Name Prefix field and press the
    enter key. As a result, only one stream should be available. Click it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请在Log Stream Name Prefix字段中输入`random-logger`并按下回车键。结果，只会有一个流可用。点击它。
- en: Once inside the `random-logger` screen, you should see all the logs generated
    by that Pod. I'll leave it to you to explore the available options (there aren't
    many).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入`random-logger`屏幕，你应该能看到该Pod生成的所有日志。我会留给你去探索可用的选项（并不多）。
- en: '![](assets/c6a05184-dcf3-41ce-a1a4-6d2fc4839889.png)Figure 7-5: AWS CloudWatch
    events screen'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c6a05184-dcf3-41ce-a1a4-6d2fc4839889.png)图7-5：AWS CloudWatch事件屏幕'
- en: Once you're done exploring CloudWatch, we'll proceed by deleting the Fluentd
    resources as well as the policy and the log group. We still have more logging
    solutions to explore. If you choose to use CloudWatch with Fluentd, you should
    be able to replicate the same installation steps in your "real" cluster.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你探索完CloudWatch，我们将继续删除Fluentd资源以及策略和日志组。我们还有更多日志解决方案要探索。如果你选择在Fluentd中使用CloudWatch，你应该能够在你的“真实”集群中复制相同的安装步骤。
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Combining Azure Log Analytics with an AKS cluster
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Azure Log Analytics与AKS集群结合使用
- en: Just like GKE (and unlike EKS), AKS comes with an integrated logging solution.
    All we have to do is enable one of the AKS addons. To be more precise, we'll enable
    the `monitoring` addon. As the name indicates, the addon does not fulfill only
    the needs to collect logs, but it also handles metrics. However, we are interested
    just in logs. I believe that nothing beats Prometheus for metrics, especially
    since it integrates with HorizontalPodAutoscaler. Still, you should explore AKS
    metrics as well and reach your own conclusion. For now, we'll explore only the
    logging part of the addon.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 就像GKE（而不像EKS）一样，AKS带有集成的日志解决方案。我们所要做的就是启用其中一个AKS插件。更准确地说，我们将启用`monitoring`插件。正如其名称所示，该插件不仅满足了收集日志的需求，还处理指标。然而，我们只对日志感兴趣。我相信在指标方面没有什么能比得上Prometheus，特别是因为它与HorizontalPodAutoscaler集成。不过，你也应该探索一下AKS的指标，并得出自己的结论。目前，我们只会探索插件的日志部分。
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The output is a rather big JSON with all the information about the newly enabled
    `monitoring` addon. There's nothing exciting in it.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个相当庞大的JSON，包含了关于新启用的`monitoring`插件的所有信息。里面没有什么令人兴奋的东西。
- en: It's important to note that we could have enabled the addon when we created
    the cluster by adding `-a monitoring` argument to the `az aks create` command.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们本可以在创建集群时通过在`az aks create`命令中添加`-a monitoring`参数来启用插件。
- en: If you're curious what we got, we can list the Deployments in the `kube-system`
    Namespace.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你好奇我们得到了什么，我们可以列出`kube-system`命名空间中的部署。
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output is as follows.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The new addition is the `omsagent-rs` Deployment that will ship the logs (and
    metrics) to Azure Log Analytics. If you `describe` it, you'll see that it is based
    on `microsoft/oms` image. That makes it the first and the only time we switched
    from Fluentd to a different log shipping solution. We'll use it simply because
    Azure recommends it.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 新增的是`omsagent-rs`部署，它将日志（和指标）发送到Azure Log Analytics。如果你`describe`它，你会发现它是基于`microsoft/oms`镜像的。这使得它成为我们从Fluentd切换到不同日志发送解决方案的第一次，也是唯一一次。我们将使用它，只是因为Azure推荐它。
- en: Next, we need to wait for a few minutes until the logs are propagated to Log
    Analytics. This is the perfect moment for you to take a short break. Go fetch
    a cup of coffee.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要等待几分钟，直到日志传播到Log Analytics。这是你休息片刻的绝佳时机。去冲杯咖啡吧。
- en: Let's open Azure portal and see Log Analytics in action.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开Azure门户并看看Log Analytics的运行情况。
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Please click the All services item from the left-hand menu, type `log analytics`
    in the Filter field, and click the Log Analytics item.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请从左侧菜单中单击“所有服务”项目，在过滤字段中键入“日志分析”，然后单击“日志分析”项目。
- en: '![](assets/7fb76fbc-fb0a-4ec2-827b-8e4bbdb12ca8.png)Figure 7-6: Azure portal
    All services screen with log analytics filter'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-6：Azure门户所有服务屏幕，带有日志分析过滤器
- en: Unless you are already using Log Analytics, there should be only one active
    workspace. If that's the case, click it. Otherwise, if there are multiple workspaces,
    choose the one with the ID that matches the *id* entry of the `az aks enable-addons`
    output.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您已经在使用Log Analytics，否则应该只有一个活动的工作区。如果是这种情况，请单击它。否则，如果有多个工作区，请选择与`az aks enable-addons`输出的*id*条目匹配的工作区。
- en: Click the menu item Logs in the *General* section.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 单击*常规*部分中的菜单项日志。
- en: Next, we'll try to limit the output entries only to those that contain `random-logger`.
    Please type the query that follows in the Type your query here... field.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试将输出条目限制为仅包含“random-logger”的条目。请在“在此处键入查询”字段中输入以下查询。
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Click the Run button, and you'll be presented with all the `random-logger` entries.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 单击运行按钮，您将看到所有“random-logger”条目。
- en: By default, all the fields are shown in the table, and many of them are either
    not used, or not very useful. The extra columns probably distract us from absorbing
    the logs, so we'll change the output.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，表中显示所有字段，其中许多字段要么未被使用，要么没有多大用处。额外的列可能会分散我们吸收日志的注意力，因此我们将更改输出。
- en: It's easier to specify which columns we need, than which ones we don't. Please
    expand the Columns list, and click the SELECT NONE button. Next, select **LogEntry**,
    **Name**, and **TimeGenerated** fields and, once you're finished, contract the
    **Columns** list.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 指定我们需要哪些列比指定我们不需要哪些列更容易。请展开“列”列表，并单击“选择无”。然后，选择**LogEntry**、**Name**和**TimeGenerated**字段，完成后，收缩**列**列表。
- en: What you see in front of you are logs limited to `random-logger` and presented
    only through the three columns we selected.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您面前看到的是限定为“random-logger”的日志，并且仅通过我们选择的三列呈现。
- en: '![](assets/eff379dc-1093-4212-8efa-5731ad7a3ba5.png)Figure 7-7: Azure Log Analytics
    screen with filtered entries'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-7：带有过滤条目的Azure日志分析屏幕
- en: I'll let you explore Log Analytics features on your own. Even though Azure portal's
    UI is not as intuitive as it could be, I'm sure you'll manage to get your way
    around it. If you choose to adopt AKS integration with Log Analytics, you should
    probably explore *Log Analytics query language* ([https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-language](https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-language))
    documentation that will help you write more complex queries than the one we used.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我会让您自己探索日志分析功能。尽管Azure门户的用户界面可能不够直观，但我相信您能够熟悉它。如果您选择采用AKS与Log Analytics集成，您可能需要探索*Log
    Analytics查询语言*（[https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-language](https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-language)）文档，该文档将帮助您编写比我们使用的更复杂的查询。
- en: Given that there is at least one more solution we should explore before we choose
    the one that fits your needs the best, we'll disable the addon. Later on, if you
    do like Log Analytics more than the alternatives, all you'll have to do is to
    enable it again.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到在选择最适合您需求的解决方案之前，我们应该探索至少还有一个解决方案，我们将禁用插件。稍后，如果您确实更喜欢Log Analytics而不是其他替代方案，您只需再次启用它即可。
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Exploring centralized logging through Elasticsearch, Fluentd, and Kibana
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Elasticsearch、Fluentd和Kibana探索集中式日志记录
- en: Elasticsearch is probably the most commonly used in-memory database. At least,
    if we narrow the scope to self-hosted databases. It is designed for many other
    scenarios, and it can be used to store (almost) any type of data. As such, it
    is almost perfect for storing logs which could come in many different formats.
    Given its flexibility, some use it for metrics as well and, as such, Elasticsearch
    competes with Prometheus. We'll leave metrics aside, for now, and focus only on
    logs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch可能是最常用的内存数据库。至少，如果我们将范围缩小到自托管数据库。它设计用于许多其他场景，并且可以用于存储（几乎）任何类型的数据。因此，它几乎完美地用于存储可能以许多不同格式出现的日志。鉴于其灵活性，一些人也将其用于指标，并且因此，Elasticsearch与Prometheus竞争。我们暂时将指标放在一边，只关注日志。
- en: The **EFK** (**Elasticsearch**, **Fluentd**, and **Kibana**) stack consists
    of three components. Data is stored in Elasticsearch, logs are collected, transformed,
    and pushed to the DB by Fluentd, and Kibana is used as UI through which we can
    explore data stored in Elasticsearch. If you are used to ELK (Logstash instead
    of Fluentd), the setup that follows should be familiar.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**EFK**（**Elasticsearch**，**Fluentd**和**Kibana**）堆栈由三个组件组成。数据存储在Elasticsearch中，日志由Fluentd收集，转换并推送到数据库，并且Kibana用作UI，通过它我们可以探索存储在Elasticsearch中的数据。如果您习惯于ELK（Logstash而不是Fluentd），那么接下来的设置应该很熟悉。'
- en: The first components we'll install is Elasticsearch. Without it, Fluentd would
    not have a destination to ship logs, and Kibana would not have a source of data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装的第一个组件是Elasticsearch。没有它，Fluentd将没有日志的目的地，Kibana也将没有数据来源。
- en: As you might have guessed, we'll continue using Helm and, fortunately, *Elasticsearch
    Chart* ([https://github.com/helm/charts/tree/master/stable/elasticsearch](https://github.com/helm/charts/tree/master/stable/elasticsearch))
    is already available in the stable channel. I'm confident that you know how to
    find the chart and explore all the values you can use. So, we'll jump straight
    into the values I prepared. They are the bare minimum and contain only the `resources`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经猜到的，我们将继续使用Helm，幸运的是，*Elasticsearch Chart*（[https://github.com/helm/charts/tree/master/stable/elasticsearch](https://github.com/helm/charts/tree/master/stable/elasticsearch)）已经在稳定通道中可用。我相信您知道如何找到图表并探索您可以使用的所有值。因此，我们将直接跳转到我准备的值。它们是最低限度的，只包含`资源`。
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The output is as follows.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, there are three sections (`client`, `master`, and `data`) that
    correspond with ElasticSearch components that will be installed. All we're doing
    is setting up resource requests and limits, and leaving the rest to the Chart's
    default values.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，有三个部分（`client`，`master`和`data`），对应于将要安装的ElasticSearch组件。我们所做的就是设置资源请求和限制，然后将其余部分留给图表的默认值。
- en: Before we proceed, please note that you should NOT use those values in production.
    You should know by now that they differ from one case to another and that you
    should adjust resources depending on the actual usage that you can retrieve from
    tools like `kubectl top`, Prometheus, and others.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，请注意您不应该在生产中使用这些值。您现在应该知道它们在不同情况下有所不同，您应该根据实际使用情况调整资源，您可以从`kubectl top`，Prometheus和其他工具中获取。
- en: Let's install Elasticsearch.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装Elasticsearch。
- en: '[PRE42]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: It might take a while until all the resources are created. On top of that, if
    you're using GKE, new nodes might need to be created to accommodate requested
    resources. Be patient.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要一段时间才能创建所有资源。此外，如果您正在使用GKE，可能需要创建新节点来容纳所请求的资源。请耐心等待。
- en: Now that Elasticsearch is rolled out, we can turn our attention to the second
    component in the EFK stack. We'll install Fluentd. Just as Elasticsearch, Fluentd
    is also available in Helm's stable channel.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Elasticsearch已经推出，我们可以把注意力转向EFK堆栈中的第二个组件。我们将安装Fluentd。就像Elasticsearch一样，Fluentd也可以在Helm的稳定通道中找到。
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: There's no much to say about Fluentd. It is running as DaemonSet and, as the
    name of the chart suggests, it is already preconfigured to work with Elasticsearch.
    I did not even bother showing you the contents of the values file `logging/fluentd-values.yml`
    since it contains only the resources.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Fluentd没有太多可说的。它作为DaemonSet运行，并且正如图表的名称所暗示的那样，它已经预先配置好以与Elasticsearch一起工作。我甚至都没有打扰向你展示`logging/fluentd-values.yml`值文件的内容，因为它只包含资源。
- en: To be on the safe side, we'll check Fluentd's logs to confirm that it managed
    to connect to Elasticsearch.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，我们将检查Fluentd的日志，以确认它是否成功连接到Elasticsearch。
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The output, limited to the messages, is as follows.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于消息，如下所示。
- en: '[PRE45]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: A note to Docker for Desktop users You will likely see much more than the few
    log entries presented above. There will be a lot of warnings due to the differences
    in Docker for Desktop API when compared to other Kubernetes flavors. Feel free
    to ignore those warnings since they do not affect the examples we are about to
    explore and you are not going to use Docker for Desktop in production but only
    for practice and local development.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 给Docker for Desktop用户的一条注释：您可能会看到比上面呈现的少得多的日志条目。由于Docker for Desktop API与其他Kubernetes版本的差异，会有很多警告。请随意忽略这些警告，因为它们不会影响我们即将探索的示例，并且您不会在生产中使用Docker
    for Desktop，而只会用于练习和本地开发。
- en: That was simple and beautiful. The only thing left is to install the K from
    EFK.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单也很美丽。唯一剩下的就是安装EFK中的K。
- en: Let's take a look at the values file we'll use for the Kibana chart.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们将用于Kibana图表的值文件。
- en: '[PRE46]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The output is as follows.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE47]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Again, this is a relatively straightforward set of values. This time, we are
    specifying not only the resources but also the Ingress host, as well as the environment
    variable `ELASTICSEARCH_URL` that'll tell Kibana where to find Elasticsearch.
    As you might have guessed, I did not know in advance what will be your host, so
    we'll need to overwrite `hosts` at runtime. But, before we do that, we need to
    define it.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这是一组相对简单的值。这一次，我们不仅指定了资源，还指定了Ingress主机，以及环境变量`ELASTICSEARCH_URL`，告诉Kibana在哪里找到Elasticsearch。正如你可能已经猜到的，我事先不知道你的主机是什么，所以我们需要在运行时覆盖`hosts`。但在我们这样做之前，我们需要定义它。
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Off we go towards installing the last component in the EFK stack.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续安装EFK堆栈中的最后一个组件。
- en: '[PRE49]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now we can finally open Kibana and confirm that all three EFK components indeed
    work together and that they are fulfilling our centralized logging objectives.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以打开Kibana并确认所有三个EFK组件确实一起工作，并且它们正在实现我们的集中日志记录目标。
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: If you do not see Kibana just yet, wait for a few moments and refresh the screen.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有看到Kibana，请等待片刻并刷新屏幕。
- en: You should see the *Welcome* screen. Ignore the offer to try their sample data
    by clicking the link to Explore on my own. You'll be presented with the screen
    that allows you to add data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到*欢迎*屏幕。忽略尝试使用他们的示例数据的建议，点击链接自己探索。您将看到允许您添加数据的屏幕。
- en: '![](assets/f1a8a6ea-94ef-40e8-861b-d7d04f0a6dc5.png)Figure 7-8: Kibana''s home
    screen'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/f1a8a6ea-94ef-40e8-861b-d7d04f0a6dc5.png)图7-8：Kibana的主屏幕'
- en: The first thing we need to do is create a new Elasticsearch index that will
    match the one created by Fluentd. The version we're running is already pushing
    data to Elasticsearch, and it's doing that by using LogStash indexing pattern
    as a way to simplify things since that's what Kibana expects to see.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一个新的Elasticsearch索引，以匹配Fluentd创建的索引。我们正在运行的版本已经将数据推送到Elasticsearch，并且它是通过使用LogStash索引模式来简化事情的，因为这是Kibana期望看到的。
- en: Click the Management item from the left-hand menu, followed with the Index Patterns
    link.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从左侧菜单中点击管理项目，然后点击索引模式链接。
- en: All the logs Fluentd is sending to Elasticsearch are indexes with the *logstash*
    prefix followed with the date. Since we want Kibana to retrieve all the logs,
    type `logstash-*` into the Index pattern field, and click the > Next step button.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd发送到Elasticsearch的所有日志都是以*logstash*前缀和日期为后缀的索引。由于我们希望Kibana检索所有日志，所以在索引模式字段中键入“logstash-*”，然后单击“>
    下一步”按钮。
- en: Next, we need to specify which field contains timestamps. It is an easy choice.
    Select @timestamp from the Time Filter field name, and click the Create index
    pattern button.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要指定包含时间戳的字段。这是一个简单的选择。从时间过滤器字段名称中选择@timestamp，并单击“创建索引模式”按钮。
- en: '![](assets/884ab586-e6e9-4fa6-bc02-d15e6a91b682.png)Figure 7-9: Kibana''s Create
    index pattern screen'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/884ab586-e6e9-4fa6-bc02-d15e6a91b682.png)图7-9：Kibana的创建索引模式屏幕'
- en: That's it. All we have to do now is wait for a few moments until the index is
    created, and explore the logs collected from across the whole cluster.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。现在我们所要做的就是等待一段时间，直到索引创建完成，并探索从整个集群收集的日志。
- en: Please click the Discover item from the left-hand menu.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请从左侧菜单中点击“发现”项。
- en: What you see in front of you are all the logs generated in the last fifteen
    minutes (can be extended to any period). The list of fields is available on the
    left-hand side.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您在面前看到的是过去十五分钟内生成的所有日志（可以延长到任何时期）。字段列表位于左侧。
- en: There is a silly (and useless) graph on the top and the logs themselves are
    in the main body of the screen.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部有一个愚蠢（且无用）的图表，日志本身位于屏幕主体中。
- en: '![](assets/20ca4884-36f5-465c-8fe2-f1f2af4b5abe.png)Figure 7-10: Kibana''s
    Discover screen'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/20ca4884-36f5-465c-8fe2-f1f2af4b5abe.png)图7-10：Kibana的发现屏幕'
- en: Just as with Papertrail, we won't go into all the options available in Kibana.
    I trust you can figure them out yourself. We'll just go through a few basic operations
    in case this is your first contact with Kibana.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Papertrail一样，我们不会涉及Kibana中的所有可用选项。我相信你可以自己弄清楚它们。我们只会介绍一些基本操作，以防这是您第一次接触Kibana。
- en: Our scenario is the same as before. We'll try to find all the log entries generated
    from the `random-logger` application.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的情景与以前相同。我们将尝试找到从“random-logger”应用程序生成的所有日志条目。
- en: 'Please type `kubernetes.pod_name: "random-logger"` into the Search field and
    click the Refresh (or Update) button located on the right.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '请在搜索字段中键入`kubernetes.pod_name: "random-logger"`，然后单击右侧的刷新（或更新）按钮。'
- en: More often than not, we want to customize the fields presented by default. For
    example, it would be more useful to see the log entry only, instead of the full
    source.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 往往我们希望自定义默认显示的字段。例如，仅查看日志条目会更有用，而不是完整的源。
- en: Click the Add button next to the log field, and it will replace the default
    *_source* column.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 点击日志字段旁边的“添加”按钮，它将替换默认的*_source*列。
- en: If you'd like to see an entry with all the fields, please expand one by clicking
    the arrow on the left side of the row.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想看到带有所有字段的条目，请通过单击行左侧的箭头来展开。
- en: '![](assets/20ca4884-36f5-465c-8fe2-f1f2af4b5abe.png)Figure 7-11: Kibana''s
    Discover screen with filtered entries'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/20ca4884-36f5-465c-8fe2-f1f2af4b5abe.png)图7-11：Kibana的带有过滤条目的发现屏幕'
- en: I'll leave you to explore the rest of Kibana on your own. But, before you do
    that, there's a word of warning. Do not get fooled by all the flashy options.
    If all we're having are logs, there's probably no point creating visualizations,
    dashboards, timelines, and other nice looking, but useless things we can do with
    logs. Those might be useful with metrics, but we do not have any. For now, they
    are in Prometheus. Later on, we'll discuss the option of pushing metrics to Elasticsearch
    instead of pulling them from Prometheus.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我会让你自己去探索Kibana的其余部分。但在你这样做之前，有一个警告。不要被所有花哨的选项所迷惑。如果我们只有日志，可能没有必要创建可视化、仪表板、时间轴和其他看起来不错但无用的东西。这些可能对指标有用，但我们现在没有。目前，它们在Prometheus中。以后，我们将讨论将指标推送到Elasticsearch而不是从Prometheus中拉取的选项。
- en: Now, take your time and see what else you can do in Kibana, at least within
    the *Discover* screen.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，花点时间看看你在Kibana中还能做些什么，至少在*Discover*屏幕中。
- en: We're done with the EFK stack and, given that we did not yet make a decision
    which solution to use, we'll purge it from the system. Later on, if you do choose
    EFK, you should not have any trouble creating it in your "real" cluster.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了EFK堆栈，并且鉴于我们尚未做出使用哪种解决方案的决定，我们将从系统中清除它。以后，如果你选择了EFK，你在你的“真实”集群中创建它应该不会有任何麻烦。
- en: '[PRE51]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Switching to Elasticsearch for storing metrics
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切换到Elasticsearch存储指标。
- en: Now that we had Elasticsearch running in our cluster and knowing that it can
    handle almost any data type, a logical question could be whether we can use it
    to store our metrics besides logs. If you explore *elastic.co* ([https://www.elastic.co/](https://www.elastic.co/)),
    you'll see that metrics are indeed something they advertise. If it could replace
    Prometheus, it would undoubtedly be beneficial to have a single tool that can
    handle not only logs but also metrics. On top of that, we could ditch Grafana
    and keep Kibana as a single UI for both data types.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们的集群中已经运行了Elasticsearch，并且知道它几乎可以处理任何数据类型，一个合乎逻辑的问题可能是我们是否可以用它来存储除日志之外的指标。如果你探索*elastic.co*（[https://www.elastic.co/](https://www.elastic.co/)），你会发现指标确实是他们宣传的东西。如果它可以取代Prometheus，那么拥有一个既可以处理日志又可以处理指标的单一工具无疑是有益的。除此之外，我们可以放弃Grafana，保留Kibana作为两种数据类型的单一UI。
- en: Nevertheless, I would strongly advise against using Elasticsearch for metrics.
    It is a general-purpose free-text no-SQL database. That means that it can handle
    almost any data but, at the same time, it does not excel at any specific format.
    Prometheus, on the other hand, is designed to store time-series data which are
    the preferred way of exposing metrics. As such, it is more limited in what it
    does. But, it handles metrics much better than Elasticsearch. I believe that using
    the right tool for the job is better than having a single tool that does too many
    things, and if you believe the same, Prometheus is a much better choice for metrics.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我强烈建议不要使用Elasticsearch来存储指标。它是一个通用的自由文本非SQL数据库。这意味着它几乎可以处理任何数据，但与此同时，它并不擅长处理任何特定格式。另一方面，Prometheus被设计用于存储时间序列数据，这是暴露指标的首选方式。因此，它在所做的事情上更受限制。但是，它比Elasticsearch更擅长处理指标。我相信使用合适的工具比拥有一个可以做太多事情的单一工具更好，如果你也持相同观点，那么Prometheus是处理指标的更好选择。
- en: When compared to Elasticsearch, and focused only on metrics, Prometheus' requires
    much fewer resources (as you already noticed), it is faster, and it has a much
    better query language. That shouldn't come as a surprise given that both tools
    are great, but only Prometheus is designed to work exclusively with metrics. The
    increased cost of maintaining an additional tool is well paid off by having a
    better (and more focused) solution.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 与Elasticsearch相比，仅专注于指标的Prometheus需要更少的资源（正如您已经注意到的），它更快，而且具有更好的查询语言。这并不奇怪，因为这两个工具都很棒，但只有Prometheus被设计为专门处理指标。额外工具的维护成本增加得很值得，因为它提供了更好（更专注）的解决方案。
- en: Did I mention that notifications generated through Prometheus and Alertmanager
    and better than those through Elasticsearch?
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我是否提到通过Prometheus和Alertmanager生成的通知比通过Elasticsearch生成的更好？
- en: There's one more important thing to note. Prometheus integration with Kubernetes
    is way better than what Elasticsearch offers. That is not a surprise since Prometheus
    is based on the same cloud-native principles as Kubernetes and both belong to
    *Cloud Native Computing Foundation* ([https://www.cncf.io/](https://www.cncf.io/)).
    Elasticsearch, on the other hand, comes from a more traditional background.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件重要的事情需要注意。Prometheus与Kubernetes的集成要比Elasticsearch提供的要好得多。这并不奇怪，因为Prometheus基于与Kubernetes相同的云原生原则，并且两者都属于*Cloud
    Native Computing Foundation*（[https://www.cncf.io/](https://www.cncf.io/)）。另一方面，Elasticsearch来自更传统的背景。
- en: Elasticsearch is excellent, but it does too much. Its lack of focus makes it
    inferior to Prometheus for storing and querying metrics, as well as sending alerts
    based on such data.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch很棒，但它做得太多了。它缺乏专注性，使其在存储和查询指标以及基于这些数据发送警报方面不如Prometheus。
- en: If replacing Prometheus with Elasticsearch is not a good idea, can we invert
    the question? Can we use Prometheus for logs? The answer is a definite no. As
    already stated, Prometheus is focused only on metrics. If you do adopt it, you
    need a different tool for storing logs. That can be Elasticsearch, Papertrail,
    or any other solution that fits your needs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用Elasticsearch替换Prometheus不是一个好主意，那我们是否可以反过来提问？我们可以使用Prometheus来处理日志吗？答案是绝对不行。正如前面所述，Prometheus只专注于指标。如果您采用它，您需要另一个工具来存储日志。这可以是Elasticsearch、Papertrail或任何其他符合您需求的解决方案。
- en: How about Kibana? Can we ditch it in favor of Grafana? The answer is yes, but
    don't do that. While we could create a table in Grafana and attach it to Elasticsearch
    as a data source, its capability to display and filter logs is inferior. On the
    other hand, Grafana is much more flexible than Kibana for displaying graphs based
    on metrics. So, the answer is similar to the Elasticsearch vs. Prometheus dilemma.
    Keep Grafana for metrics and use Kibana for logs, if you chose to store them in
    Elasticsearch.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 那么Kibana呢？我们可以放弃它，转而使用Grafana吗？答案是可以，但不要这样做。虽然我们可以在Grafana中创建一个表，并将其附加到Elasticsearch作为数据源，但它显示和过滤日志的能力不如Kibana。另一方面，相对于Kibana来说，Grafana在显示基于指标的图形方面更加灵活。因此，答案与Elasticsearch与Prometheus的困境类似。保留Grafana用于指标，并在选择将日志存储在Elasticsearch中时使用Kibana。
- en: Should you add Elasticsearch as yet another data source in Grafana? If you took
    previous recommendations, the answer is most likely no. There is not much value
    in presenting logs as graphs. Even the pre-defined graph available in Kibana's
    *Explore* section is, in my opinion, a waste of space. There is no point in showing
    how many logs entries we have in total, nor even how many are error entries. We
    use metrics for that.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否应该将Elasticsearch作为Grafana中的另一个数据源？如果您采纳了之前的建议，答案很可能是否定的。将日志呈现为图形并没有太大价值。即使在Kibana的*Explore*部分中提供的预定义图表，在我看来也是浪费空间。显示我们总共有多少日志条目，甚至有多少错误条目是没有意义的。我们使用指标来做这些。
- en: Logs themselves are too expensive to parse, and most of the time they do not
    provide enough data to act as metrics.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 日志本身解析起来太昂贵，而且大多数时候它们并不能提供足够的数据作为指标。
- en: We saw several tools in action, but we did not yet discuss what we truly need
    from a centralized logging solution. We'll explore that in more detail next.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了几种工具的运行情况，但我们还没有讨论我们真正需要从集中式日志记录解决方案中得到什么。我们将在接下来更详细地探讨这个问题。
- en: What should we expect from centralized logging?
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们应该从集中式日志记录中期待什么？
- en: We explored several products that can be used to centralize logging. As you
    saw, all are very similar, and we can assume that most of the other solutions
    follow the same principles. We need to collect logs across the cluster. We used
    Fluentd for that, which is the most widely accepted solution that you will likely
    use no matter which database receives those logs (Azure being an exception).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了几种可用于集中日志记录的产品。正如你所看到的，它们都非常相似，我们可以假设大多数其他解决方案都遵循相同的原则。我们需要跨集群收集日志。我们使用Fluentd来做到这一点，这是最广泛接受的解决方案，无论哪个数据库接收这些日志，你都很可能会使用它（Azure是一个例外）。
- en: Log entries collected with Fluentd are shipped to a database which, in our case,
    is Papertrail, Elasticsearch, or one of the solutions provided by hosting vendors.
    Finally, all solutions offer a UI that allows us to explore the logs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Fluentd收集的日志条目被传送到一个数据库，我们的情况下是Papertrail、Elasticsearch或者由托管供应商提供的解决方案之一。最后，所有解决方案都提供了一个允许我们探索日志的用户界面。
- en: I usually provide a single solution for a problem but, in this case, there are
    quite a few candidates for your need for centralized logging. Which one should
    you choose? Will it be Papertrail, Elasticsearch-Fluentd-Kibana stack (EFK), AWS
    CloudWatch, GCP Stackdriver, Azure Log Analytics, or something else?
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常为问题提供一个解决方案，但在这种情况下，有很多候选项可以满足你对集中式日志记录的需求。你应该选择哪一个？是Papertrail，Elasticsearch-Fluentd-Kibana堆栈（EFK），AWS
    CloudWatch，GCP Stackdriver，Azure Log Analytics，还是其他什么？
- en: When possible and practical, I prefer a centralized logging solution provided
    as a service, instead of running it inside my clusters. Many things are easier
    when others are making sure that everything works. If we use Helm to install EFK,
    it might seem like an easy setup. However, maintenance is far from trivial. Elasticsearch
    requires a lot of resources. For smaller clusters, compute required to run Elasticsearch
    alone is likely higher than the price of Papertrail or similar solutions. If I
    can get a service managed by others for the same price as running the alternative
    inside my own cluster, service wins most of the time. But, there are a few exceptions.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能和实际的情况下，我更喜欢作为服务提供的集中式日志记录解决方案，而不是在我的集群内部运行它。许多事情在其他人确保一切正常时会更容易。如果我们使用Helm来安装EFK，这可能看起来是一个简单的设置。然而，维护远非微不足道。Elasticsearch需要大量资源。对于较小的集群，仅运行Elasticsearch所需的计算量可能高于Papertrail或类似解决方案的价格。如果我可以以与在自己的集群内运行替代方案相同的价格获得由他人管理的服务，大多数情况下服务会获胜。但也有一些例外。
- en: I do not want to lock my business into a service provider. Or, to be more precise,
    I think it's crucial that core components are controlled by me, while as much
    of the rest is given to others. A good example is VMs. I do not honestly care
    who creates them, as long as the price is competitive and the service is reliable.
    I can easily move my VMs from on-prem to AWS, and from there to, let's say, Azure.
    I can even go back to on-prem. There's not much logic in the creation and maintenance
    of VMs. Or, at least, there shouldn't be.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想将我的业务锁定在一个服务提供商上。更准确地说，我认为核心组件由我控制，而尽可能多的其他部分交给他人是至关重要的。一个很好的例子是虚拟机。我并不真的在乎谁创建它们，只要价格有竞争力，服务可靠。我可以很容易地将我的虚拟机从本地转移到AWS，然后再从那里转移到，比如说，Azure。我甚至可以回到本地。虚拟机的创建和维护并没有太多的逻辑。或者，至少，不应该有。
- en: What I genuinely care are my applications. As long as they are running, they
    are fault-tolerant, they are highly available, and their maintenance is not costly,
    it does not matter where they run. But, I need to make sure that the system is
    done in a way that allows me to switch from one provider to another, without spending
    months in refactoring. That's one of the big reasons why Kubernetes is so widely
    adopted. It abstracts everything below it, thus allowing us to run our applications
    in (almost) any Kubernetes cluster. I believe the same can be applied to logs.
    We need to be clear what we expect, and any solution that meets our requirements
    is as good as any other. So, what do we need from a logging solution?
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我真正关心的是我的应用程序。只要它们在运行，它们是容错的，它们是高可用的，它们的维护成本不高，它们在哪里运行并不重要。但是，我需要确保系统以一种允许我在不花费数月进行重构的情况下从一个提供商切换到另一个提供商的方式完成。这就是为什么Kubernetes如此广泛被采用的一个重要原因。它将其下方的所有内容抽象化，从而使我们能够在（几乎）任何Kubernetes集群中运行我们的应用程序。我相信同样的方法也可以应用于日志。我们需要明确我们的期望，任何满足我们要求的解决方案都和其他任何解决方案一样好。那么，我们需要从日志解决方案中得到什么？
- en: We need logs centralized in a single location so that we can explore logs from
    any part of the system.We need a query language that will allow us to filter the
    results.We need the solution to be fast.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将日志集中在一个位置，以便我们可以从系统的任何部分探索日志。我们需要一种查询语言，可以让我们过滤结果。我们需要解决方案快速。
- en: All of the solutions we explored meets those requirements. Papertrail, EFK,
    AWS CloudWatch, GCP Stackdriver, and Azure Log Analytics all fulfill those requirements.
    Kibana might be a bit prettier, and Elasticsearch's query language might be a
    bit richer than those provided by the other solutions. The importance of prettiness
    is up to you to establish. As for Elasticsearch' query language being more powerful...
    It does not really matter. Most of the time, we need simple operations with logs.
    Find me all the entries that have specific keywords. Return all logs from that
    application. Limit the result to the last thirty minutes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索的所有解决方案都满足这些要求。Papertrail、EFK、AWS CloudWatch、GCP Stackdriver和Azure Log Analytics都满足这些要求。Kibana可能更漂亮一些，Elasticsearch的查询语言可能比其他解决方案提供的更丰富一些。漂亮的重要性取决于您的建立。至于Elasticsearch的查询语言更强大……这并不重要。大多数时候，我们只需要对日志进行简单的操作。找到所有包含特定关键字的条目。返回该应用程序的所有日志。将结果限制在最近三十分钟内。
- en: When possible and practical, logging-as-a-service provided by a third party
    like Papertrail, AWS, GCP, or Azure is a better option than to host it inside
    our clusters.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能和实际的情况下，由Papertrail、AWS、GCP或Azure等第三方提供的日志即服务比在我们的集群内部托管更好。
- en: With a service, we accomplish the same goals, while getting rid of one of the
    things we need to worry about. The reasoning behind that statement is similar
    to the logic that makes me believe that managed Kubernetes services (for example,
    EKS, AKS, GKE) are a better choice than Kubernetes maintained by us. Nevertheless,
    there might be many reasons why using a third-party something-as-a-service solution
    is not possible. Regulations might not allow us to go outside the internal network.
    Latency might be too big. Decision makers are stubborn. No matter the reasons,
    when we can not use something-as-a-service, we have to host that something ourselves.
    In such a case, EFK is likely the best solution, excluding enterprise offerings
    that are out of the scope of this book.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过服务，我们实现了相同的目标，同时摆脱了我们需要担心的一件事。这种说法背后的推理类似于使我相信托管的Kubernetes服务（例如EKS、AKS、GKE）比我们自己维护的Kubernetes更好的逻辑。然而，使用第三方即服务解决方案可能有很多原因。法规可能不允许我们走出内部网络。延迟可能太大。决策者固执。无论原因如何，当我们无法使用即服务时，我们必须自己托管该服务。在这种情况下，EFK可能是最佳解决方案，不包括超出本书范围的企业提供的解决方案。
- en: If EFK is likely one of the best solutions for self-hosted centralized logging,
    which one should be our choice when we can use logging-as-a-service? Is Papertrail
    a good choice?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果EFK很可能是自托管集中日志的最佳解决方案之一，那么当我们可以使用日志作为服务时，我们应该选择哪一个？Papertrail是一个好选择吗？
- en: If our cluster is running inside one of the Cloud providers, there is likely
    already a good solution offered by it. For example, EKS has AWS CloudWatch, GKE
    has GCP Stackdriver, and AKS has Azure Log Analytics. Using one of those makes
    perfect sense. It's already there, it is likely already integrated with the cluster
    you are running, and all you have to do is say yes. When a cluster is running
    with one of the Cloud providers, the only reason to choose some other solution
    could be the price.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的集群在云提供商之一内运行，很可能已经有一个很好的解决方案。例如，EKS有AWS CloudWatch，GKE有GCP Stackdriver，AKS有Azure
    Log Analytics。使用其中之一是完全合理的。它已经存在，很可能已经与您运行的集群集成在一起，您只需要说是。当集群与云提供商之一一起运行时，选择其他解决方案的唯一原因可能是价格。
- en: Use a service provided by your Cloud provider, unless it is more expensive than
    alternatives. If your cluster is on-prem, use a third-party service like Papertrail,
    unless there are rules that prevent you from sending logs outside your internal
    network. If everything else fails, use EFK.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云提供商提供的服务，除非它比其他选择更昂贵。如果您的集群在本地，使用像Papertrail这样的第三方服务，除非有规则阻止您将日志发送到内部网络之外。如果其他一切都失败了，使用EFK。
- en: At this point, you might be wondering why do I suggest to use a service for
    logs, while I proposed that our metrics should be hosted inside our clusters.
    Isn't that contradictory? Following that logic, shouldn't we use metrics-as-a-service
    as well?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能会想知道为什么我建议使用日志服务，而我提出我们的指标应该托管在我们的集群内。这不是矛盾的吗？按照这种逻辑，我们难道不应该也使用指标作为服务吗？
- en: Our system does not need to interact with our logs storage. The system needs
    to ship logs, but it does not need to retrieve them. As an example, there is no
    need for HorizontalPodAutoscaler to hook into Elasticsearch and use logs to decide
    whether to scale the number of Pods. If the system does not need logs to make
    decisions, can we say the same for humans? What do we need logs for? We need logs
    for debugging. We need them to find the cause of a problem. What we do NOT need
    are alerts based on logs. Logs do not help us discover that there is an issue,
    but to find the cause of a problem detected through alerts based on metrics.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统不需要与日志存储进行交互。系统需要发送日志，但不需要检索它们。例如，HorizontalPodAutoscaler无需连接到Elasticsearch并使用日志来决定是否扩展Pod的数量。如果系统不需要日志来做决定，我们是否也可以对人类说同样的话？我们需要日志来做什么？我们需要日志来调试。我们需要它们来找出问题的原因。我们不需要基于日志的警报。日志不能帮助我们发现问题，但可以帮助我们找到基于指标警报检测到的问题的原因。
- en: Wait a minute! Shouldn't we create an alert when the number of log entries with
    the word *ERROR* goes over a certain threshold? The answer is no. We can (and
    should) accomplish the same objective through metrics. We already explored how
    to fetch errors from exporters as well as through instrumentation.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下！当包含*ERROR*这个词的日志条目数量超过一定阈值时，我们不应该创建警报吗？答案是否定的。我们可以（而且应该）通过指标实现相同的目标。我们已经探讨了如何从出口商和仪器中获取错误。
- en: What happens when we detect that there is an issue through a metrics-based notification?
    Is that the moment we should start exploring logs? Most of the time, the first
    steps towards finding the cause of a problem does not lie in exploring logs, but
    in querying metrics. Is the application down? Does it have a memory leak? Is there
    a problem with networking? Is there a high number of error responses? Those and
    countless other questions are answered through metrics. Sometimes metrics reveal
    the cause of the problem, and in other cases, they help us narrow it down to a
    specific part of the system. Logs are becoming useful only in the latter case.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过基于指标的通知检测到存在问题时，会发生什么？这是我们应该开始探索日志的时刻吗？大多数情况下，寻找问题原因的第一步并不在于探索日志，而是在于查询指标。应用程序是否宕机？它是否有内存泄漏？网络是否有问题？错误响应的数量是否很高？这些以及无数其他问题都可以通过指标得到答案。有时指标会揭示问题的原因，而在其他情况下，它们会帮助我们将问题范围缩小到系统的特定部分。只有在后一种情况下，日志才变得有用。
- en: We should start exploring logs only when metrics reveal the culprit, but not
    the cause of the issue.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当指标揭示了罪魁祸首，而不是问题的原因时，我们才应该开始探索日志。
- en: If we do have comprehensive metrics, and they do reveal most (if not all) of
    the information we need to solve an issue, we do not need much from a logging
    solution. We need logs to be centralized so that we can find them all in one place,
    we need to be able to filter them by application or a specific replica, we need
    to be able to narrow the scope to a particular time frame, and we need to be able
    to search for specific keywords.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有全面的指标，并且它们确实揭示了我们解决问题所需的大部分（如果不是全部）信息，我们对日志解决方案的需求就不是很大。我们需要将日志集中起来，这样我们就可以在一个地方找到它们，我们需要能够按应用程序或特定副本进行过滤，我们需要能够将范围缩小到特定的时间范围，我们需要能够搜索特定的关键词。
- en: That's all we need. As it happens, almost all solutions offer those features.
    As such, the choice should be based on simplicity and the cost of ownership.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的全部。恰巧的是，几乎所有的解决方案都提供了这些功能。因此，选择应该基于简单性和所有权成本。
- en: Whatever you choose, do not fall into the trap of getting impressed with shiny
    features that you are not going to use. I prefer solutions that are simple to
    use and manage. Papertrail fulfills all the requirements, and its cheap. It's
    the perfect choice for both on-prem and Cloud clusters. The same can be said for
    CloudWatch (AWS), Stackdriver (GCP), and Log Analytics (Azure). Even though I
    have a slight preference towards Papertrail, those three do, more or less, the
    same job, and they are already part of the offer.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择什么，都不要陷入对闪亮功能的印象，而你并不打算使用。我更喜欢简单易用且易管理的解决方案。Papertrail满足所有要求，而且价格便宜。它是本地和云集群的完美选择。CloudWatch（AWS）、Stackdriver（GCP）和Log
    Analytics（Azure）也可以这样说。尽管我对Papertrail有点偏爱，但这三个基本上做着同样的工作，并且它们已经是服务的一部分。
- en: If you are not allowed to store your data outside your cluster, or you have
    some other impediment towards one of those solutions, EFK is a good choice. Just
    be aware that it'll eat your resources for breakfast, and still complain that
    it's hungry. Elasticsearch alone requires a few GB of RAM as a minimum, and you
    will likely need much more. That, of course, is not that important if you're already
    using Elasticsearch for other purposes. If that's the case, EFK is a no-brainer.
    It's already there, so use it.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不允许将数据存储在集群外，或者对其中一个解决方案有其他障碍，EFK是一个不错的选择。只是要注意它会吃掉你的资源，而且还会抱怨自己饿了。单单Elasticsearch就需要几GB的RAM作为最低配置，而你可能需要更多。当然，如果你已经在其他用途上使用Elasticsearch，那就不那么重要了。如果是这种情况，EFK是一个不需要动脑筋的选择。它已经在那里，所以就用它吧。
- en: What now?
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在怎么办？
- en: You know what to do. Destroy the cluster if you created it specifically for
    this chapter.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道该怎么做。如果你专门为这一章创建了集群，就销毁它。
- en: Before you leave, you might want to go over the main points of this chapter.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在你离开之前，你可能想要复习一下本章的要点。
- en: For any but the smallest systems, going from one resource to another and from
    one node to another to find the cause of an issue is anything but practical, reliable,
    and fast.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于除了最小系统之外的任何系统来说，从一个资源到另一个资源，从一个节点到另一个节点去找到问题的原因是不切实际、可靠和快速的。
- en: More often than not, `kubectl logs` command does not provide us with enough
    options to perform anything but simplest retrieval of logs.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很多时候，“kubectl logs”命令并不能提供足够的选项来执行除了最简单的日志检索之外的任何操作。
- en: Elasticsearch is excellent, but it does too much. Its lack of focus makes it
    inferior to Prometheus for storing and querying metrics, as well as sending alerts
    based on such data.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch很棒，但它做得太多了。它缺乏专注使得它在存储和查询指标以及基于这些数据发送警报方面不如Prometheus。
- en: Logs themselves are too expensive to parse, and most of the time they do not
    provide enough data to act as metrics.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志本身解析起来太昂贵，而且大多数时候它们并不能提供足够的数据来充当指标。
- en: We need logs centralized in a single location so that we can explore logs from
    any part of the system.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要将日志集中在一个地方，这样我们就可以从系统的任何部分探索日志。
- en: We need a query language that will allow us to filter the results.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要一种查询语言，可以让我们过滤结果。
- en: We need the solution to be fast.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要解决方案要快。
- en: Use a service provided by your Cloud provider, unless it is more expensive than
    alternatives. If your cluster is on-prem, use a third-party service like Papertrail,
    unless there are rules that prevent you from sending logs outside your internal
    network. If everything else fails, use EFK.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云提供商提供的服务，除非它比其他选择更昂贵。如果你的集群在本地，使用像Papertrail这样的第三方服务，除非有规定阻止你将日志发送到内部网络之外。如果其他方法都失败了，就使用EFK。
- en: We should start exploring logs only when metrics reveal the culprit, but not
    the cause of the issue.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当指标显示出罪魁祸首时，我们应该开始探索日志，而不是问题的原因。
