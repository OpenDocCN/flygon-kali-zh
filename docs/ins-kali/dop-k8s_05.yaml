- en: Network and Security
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络和安全
- en: We've learned how to deploy containers with different resources in Kubernetes
    in [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes*, and know how to use volume to persist the data, dynamic
    provisioning, and different storage classes. Next, we'll learn how Kubernetes
    routes the traffic to make all of this possible. Networking always plays an important
    role in the software world. We'll describe the networking from containers on a
    single host, multiple hosts and finally to Kubernetes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何在Kubernetes中部署具有不同资源的容器，在[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)
    *开始使用Kubernetes*中，以及如何使用卷来持久化数据，动态配置和不同的存储类。接下来，我们将学习Kubernetes如何路由流量，使所有这些成为可能。网络在软件世界中始终扮演着重要角色。我们将描述从单个主机上的容器到多个主机，最终到Kubernetes的网络。
- en: Docker networking
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker网络
- en: Kubernetes networking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络
- en: Ingress
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口
- en: Network policy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略
- en: Kubernetes networking
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络
- en: 'There are plenty of choices you can use to implement networking in Kubernetes.
    Kubernetes itself doesn''t care how you implement it, but you must meet its three
    fundamental requirements:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，您有很多选择来实现网络。Kubernetes本身并不关心您如何实现它，但您必须满足其三个基本要求：
- en: All containers should be accessible to each other without NAT, regardless of
    which nodes they are on
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有容器应该彼此可访问，无需NAT，无论它们在哪个节点上
- en: All nodes should communicate with all containers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有节点应该与所有容器通信
- en: The IP container should see itself the same way as the others see it
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP容器应该以其他人看待它的方式看待自己
- en: Before getting into anything further, we'll first review how does the default
    container networking works. That's the pillar of the network to make all of this
    possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论之前，我们首先会回顾默认容器网络是如何工作的。这是使所有这些成为可能的网络支柱。
- en: Docker networking
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker网络
- en: Let's review how Docker networking works before getting into Kubernetes networking.
    In [Chapter 2](part0047.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e), *DevOps
    with Container*, we learned three modes of container networking, bridge, none,
    and host.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究Kubernetes网络之前，让我们回顾一下Docker网络。在[第2章](part0047.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e)
    *使用容器进行DevOps*中，我们学习了容器网络的三种模式，桥接，无和主机。
- en: Bridge is the default networking model. Docker creates and attaches virtual
    Ethernet device (also known as veth) and assigns network namespace to each container.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接是默认的网络模型。Docker创建并附加虚拟以太网设备（也称为veth），并为每个容器分配网络命名空间。
- en: The **network namespace** is a feature in Linux, which is logically another
    copy of a network stack. It has its own routing tables, arp tables, and network
    devices. It's a fundamental concept of container networking.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络命名空间**是Linux中的一个功能，它在逻辑上是网络堆栈的另一个副本。它有自己的路由表、arp表和网络设备。这是容器网络的基本概念。'
- en: 'Veth always comes in a pair, one is in network namespace and the other is in
    the bridge. When the traffic comes into the host network, it will be routed into
    the bridge. The packet will be dispatched to its veth, and will go into the namespace
    inside the container, as shown in the following figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Veth总是成对出现，一个在网络命名空间中，另一个在桥接中。当流量进入主机网络时，它将被路由到桥接中。数据包将被分派到它的veth，并进入容器内部的命名空间，如下图所示：
- en: '![](../images/00088.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00088.jpeg)'
- en: 'Let''s take a closer look. In the following example, we''ll use a minikube
    node as the docker host. Firstly, we''ll have to use `minikube ssh` to ssh into
    the node because we''re not using Kubernetes yet. After we get into the minikube
    node, let''s launch a container to interact with us:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看。在以下示例中，我们将使用minikube节点作为docker主机。首先，我们必须使用`minikube ssh`来ssh进入节点，因为我们还没有使用Kubernetes。进入minikube节点后，让我们启动一个容器与我们进行交互：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s see the implementation of outbound traffic within a container. `docker
    exec <container_name or container_id>` can run a command in a running container.
    Lets use `ip link list` to list down all the interfaces:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看容器内部的出站流量实现。`docker exec <container_name or container_id>`可以在运行中的容器中运行命令。让我们使用`ip
    link list`列出所有接口：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can see that we have three interfaces inside the `busybox` container. One
    is with ID `53` with the name `eth0@if54`. The number after `if` is the other
    interface ID in the pair. In this case, the pair ID is `54`. If we run the same
    command on the host, we could see the veth in the host is pointing to the `eth0`
    inside the container:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`busybox`容器内有三个接口。其中一个是ID为`53`的接口，名称为`eth0@if54`。`if`后面的数字是配对中的另一个接口ID。在这种情况下，配对ID是`54`。如果我们在主机上运行相同的命令，我们可以看到主机中的veth指向容器内的`eth0`。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have a veth on the host named `vethfeec36a@if53`**.** It pairs with `eth0@if54`
    in the container network namespace. The veth 54 is attached to the `docker0` bridge,
    and eventually accesses the internet via eth0\. If we take a look at the iptables
    rules, we can find a masquerading rule (also known as SNAT) on the host that Docker
    creates for outbound traffic, which will make internet access available for containers:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 主机上有一个名为`vethfeec36a@if53`的veth**。**它与容器网络命名空间中的`eth0@if54`配对。veth 54连接到`docker0`桥接口，并最终通过eth0访问互联网。如果我们查看iptables规则，我们可以找到Docker为出站流量创建的伪装规则（也称为SNAT），这将使容器可以访问互联网：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'On the other hand, for the inbound traffic, Docker creates a custom filter
    chain on prerouting and creates forwarding rules in the `DOCKER` filter chain
    dynamically. If we expose a container port `8080` and map it to a host port `8000`,
    we can see we''re listening to port `8000` on any IP address (`0.0.0.0/0`), which
    will then be routed to container port `8080`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于入站流量，Docker在预路由上创建自定义过滤器链，并动态创建`DOCKER`过滤器链中的转发规则。如果我们暴露一个容器端口`8080`并将其映射到主机端口`8000`，我们可以看到我们正在监听任何IP地址（`0.0.0.0/0`）的端口`8000`，然后将其路由到容器端口`8080`：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we know how packet goes in/out of containers. Let's have a look at how containers
    in a pod communicates with each other.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道数据包如何进出容器。让我们看看pod中的容器如何相互通信。
- en: Container-to-container communications
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器间通信
- en: 'Pods in Kubernetes have their own real IP addresses. Containers within a pod
    share network namespace, so they see each other as *localhost*. This is implemented
    by the **network container** by default, which acts as a bridge to dispatch the
    traffic for every container in a pod. Let''s see how this works in the following
    example. Let''s use the first example from [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Getting Started with Kubernetes*, which includes two containers, `nginx` and
    `centos` inside one pod:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的Pod具有自己的真实IP地址。Pod中的容器共享网络命名空间，因此它们将彼此视为*localhost*。这是默认情况下由**网络容器**实现的，它充当桥接口以为pod中的每个容器分发流量。让我们看看以下示例中的工作原理。让我们使用[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)中的第一个示例，*开始使用Kubernetes*，其中包括一个pod中的两个容器，`nginx`和`centos`：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we will describe the pod and see its container ID:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将描述pod并查看其容器ID：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this example, `web` is with container ID `d9bd923572ab` and `centos` is
    with container ID `f4c019d289d4`. If we go into the node `minikube/192.168.99.100`
    using `docker ps`, we can check how many containers Kubernetes actually launches
    since we''re in minikube, which launches lots of other cluster containers. Check
    out the latest launch time by `CREATED` column, where we will find that there
    are three containers that have just been launched:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`web` 的容器 ID 是 `d9bd923572ab`，`centos` 的容器 ID 是 `f4c019d289d4`。如果我们使用
    `docker ps` 进入节点 `minikube/192.168.99.100`，我们可以检查 Kubernetes 实际启动了多少个容器，因为我们在
    minikube 中，它启动了许多其他集群容器。通过 `CREATED` 列可以查看最新的启动时间，我们会发现有三个刚刚启动的容器：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is an additional container `4ddd3221cc47` that was launched. Before digging
    into which container it is, let''s check the network mode of our `web` container.
    We will find that the containers in our example pod are running in containers
    with mapped container mode:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个额外的容器 `4ddd3221cc47` 被启动了。在深入了解它是哪个容器之前，让我们先检查一下我们的 `web` 容器的网络模式。我们会发现我们示例中的
    pod 中的容器是在映射容器模式下运行的：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`4ddd3221cc47` container is the so-called network container in this case, which
    holds network namespace to let `web` and `centos` containers join. Containers
    in the same network namespace share the same IP address and same network configuration.
    This is the default implementation in Kubernetes to achieve container-to-container
    communications, which is mapped to the first requirement.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`4ddd3221cc47` 容器在这种情况下被称为网络容器，它持有网络命名空间，让 `web` 和 `centos` 容器加入。在同一网络命名空间中的容器共享相同的
    IP 地址和网络配置。这是 Kubernetes 中实现容器间通信的默认方式，这也是对第一个要求的映射。'
- en: Pod-to-pod communications
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 间的通信
- en: Pod IP addresses are accessible from other pods no matter which nodes they're
    on. This fits the second requirement. We'll describe the pods' communication within
    the same node and across nodes in the upcoming section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 无论它们位于哪个节点，Pod IP 地址都可以从其他 Pod 中访问。这符合第二个要求。我们将在接下来的部分描述同一节点内和跨节点的 Pod 通信。
- en: Pod communication within the same node
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同一节点内的 Pod 通信
- en: 'Pod-to-pod communication within the same node goes through the bridge by default.
    Let''s say we have two pods, which have their own network namespaces. When pod1
    wants to talk to pod2, the packet passes through pod1''s namespace to the corresponding
    veth pair **vethXXXX** and eventually goes to the bridge. The bridge then broadcasts
    the destination IP to help the packet find its way, **vethYYYY** responses. The
    packet then arrives at pod2:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同一节点内的 Pod 间通信默认通过桥接完成。假设我们有两个拥有自己网络命名空间的 pod。当 pod1 想要与 pod2 通信时，数据包通过 pod1
    的命名空间传递到相应的 veth 对 **vethXXXX**，最终到达桥接设备。桥接设备然后广播目标 IP 以帮助数据包找到它的路径，**vethYYYY**
    响应。数据包然后到达 pod2：
- en: '![](../images/00089.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00089.jpeg)'
- en: However, Kubernetes is all about clusters. How does traffic get routed when
    the pods are in different nodes?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kubernetes 主要是关于集群。当 pod 在不同的节点上时，流量是如何路由的呢？
- en: Pod communication across nodes
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点间的 Pod 通信
- en: 'According to the second requirement, all nodes must communicate with all containers.
    Kubernetes delegates the implementation to the **container network interface**
    (**CNI**). Users could choose different implementations, by L2, L3, or overlay.
    Overlay networking is one of the common solutions, known as **packet encapsulation**.
    It wraps a message before leaving the source, gets delivered, and unwraps the
    message at the destination. This leads to a situation where overlay increases
    the network latency and complexity. As long as all the containers can access each
    other across nodes, you''re free to use any technology, such as L2 adjacency or
    L3 gateway. For more information about CNI, refer to its spec ([https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md)):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第二个要求，所有节点必须与所有容器通信。Kubernetes将实现委托给**容器网络接口**（**CNI**）。用户可以选择不同的实现，如L2、L3或覆盖。覆盖网络是常见的解决方案之一，被称为**数据包封装**。它在离开源之前包装消息，然后传递并在目的地解包消息。这导致覆盖增加了网络延迟和复杂性。只要所有容器可以跨节点相互访问，您可以自由使用任何技术，如L2邻接或L3网关。有关CNI的更多信息，请参阅其规范（[https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md)）：
- en: '![](../images/00090.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00090.jpeg)'
- en: Let's say we have a packet from pod1 to pod4\. The packet leaves from container
    interface and reaches to the veth pair, then passes through the bridge and node's
    network interface. Network implementation comes into play in step 4\. As long
    as the packet could be routed to the target node, you are free to use any options.
    In the following example, we'll launch minikube with the `--network-plugin=cni`
    option. With CNI enabled, the parameters will be passed through kubelet in the
    node. Kubelet has a default network plugin, but you could probe any supported
    plugin when it starts up. Before starting minikube, you could use `minikube stop`
    first if it's been started or `minikube delete` to delete the whole cluster thoroughly
    before doing anything further. Although minikube is a single node environment,
    which might not completely represent the production scenario we'll encounter,
    this just gives you a basic idea of how all of this works. We will learn the deployment
    of networking options in the real world in [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS* and [Chapter 10](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on GCP*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个从pod1到pod4的数据包。数据包从容器接口离开并到达veth对，然后通过桥接和节点的网络接口。网络实现在第4步发挥作用。只要数据包能够路由到目标节点，您可以自由使用任何选项。在下面的示例中，我们将使用`--network-plugin=cni`选项启动minikube。启用CNI后，参数将通过节点中的kubelet传递。Kubelet具有默认的网络插件，但在启动时可以探测任何支持的插件。在启动minikube之前，如果已经启动，您可以首先使用`minikube
    stop`，或者在进一步操作之前使用`minikube delete`彻底删除整个集群。尽管minikube是一个单节点环境，可能无法完全代表我们将遇到的生产场景，但这只是让您对所有这些工作原理有一个基本的了解。我们将在[第9章](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e)的*在AWS上的Kubernetes*和[第10章](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e)的*在GCP上的Kubernetes*中学习网络选项的部署。
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When we specify the `network-plugin` option, it will use the directory specified
    in `--network-plugin-dir` for plugins on startup. In the CNI plugin, the default
    plugin directory is `/opt/cni/net.d`. After the cluster comes up, let''s log in
    to the node and see the setting inside via `minikube ssh`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们指定`network-plugin`选项时，它将在启动时使用`--network-plugin-dir`中指定的目录中的插件。在CNI插件中，默认的插件目录是`/opt/cni/net.d`。集群启动后，让我们登录到节点并通过`minikube
    ssh`查看内部设置：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will find that there is one new bridge in the node, and if we create the
    example pod again by `5-1-1_pod.yml`, we will find that the IP address of the
    pod becomes `10.1.0.x`, which is attaching to `mybridge` instead of `docker0`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会发现节点中有一个新的桥接，如果我们再次通过`5-1-1_pod.yml`创建示例pod，我们会发现pod的IP地址变成了`10.1.0.x`，它连接到了`mybridge`而不是`docker0`。
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Why is that? That''s because we specify that we''ll use CNI as the network
    plugin, and `docker0` will not be used (also known as **container network model**
    or **libnetwork**). CNI creates a virtual interface, attaches it to the underlay
    network, and sets the IP address and routes and maps it to the pods'' namespace
    eventually. Let''s take a look at the configuration located at `/etc/cni/net.d/`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？因为我们指定了要使用CNI作为网络插件，而不使用`docker0`（也称为**容器网络模型**或**libnetwork**）。CNI创建一个虚拟接口，将其连接到底层网络，并最终设置IP地址和路由，并将其映射到pod的命名空间。让我们来看一下位于`/etc/cni/net.d/`的配置：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this example, we use the bridge CNI plugin to reuse the L2 bridge for pod
    containers. If the packet is from `10.1.0.0/16`, and its destination is to anywhere,
    it'll go through this gateway. Just like the diagram we saw earlier, we could
    have another node with CNI enabled with `10.1.2.0/16` subnet, so that ARP packets
    could go out to the physical interface on the node that the target pod is located
    at. It then achieves pod-to-pod communication across nodes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用桥接CNI插件来重用用于pod容器的L2桥接。如果数据包来自`10.1.0.0/16`，目的地是任何地方，它将通过这个网关。就像我们之前看到的图表一样，我们可以有另一个启用了CNI的节点，使用`10.1.2.0/16`子网，这样ARP数据包就可以传输到目标pod所在节点的物理接口上。然后实现节点之间的pod到pod通信。
- en: 'Let''s check the rules in iptables:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查iptables中的规则：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: All the related rules have been switched to `10.1.0.0/16` CIDR.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有相关规则都已切换到`10.1.0.0/16` CIDR。
- en: Pod-to-service communications
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pod到service的通信
- en: Kubernetes is dynamic. Pods are created and deleted all the time. The Kubernetes
    service is an abstraction to define a set of pods by label selectors. We normally
    use the service to access pods instead of specifying a pod explicitly. When we
    create a service, an `endpoint` object will be created, which describes a set
    of pod IPs that the label selector in that service has selected.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是动态的。Pod不断地被创建和删除。Kubernetes服务是一个抽象，通过标签选择器定义一组pod。我们通常使用服务来访问pod，而不是明确指定一个pod。当我们创建一个服务时，将创建一个`endpoint`对象，描述了该服务中标签选择器选择的一组pod
    IP。
- en: In some cases, `endpoint` object will not be created with service creation.
    For example, services without selectors will not create a corresponding `endpoint`
    object. For more information, refer to the service without selectors section in
    [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting Started
    with Kubernetes*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，创建服务时不会创建`endpoint`对象。例如，没有选择器的服务不会创建相应的`endpoint`对象。有关更多信息，请参阅[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)中没有选择器的服务部分，*开始使用Kubernetes*。
- en: Then, how does traffic get from pod to the pod behind service? By default, Kubernetes
    uses iptables to perform the magic by `kube-proxy`. This is explained in the following
    figure.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，流量是如何从一个pod到service后面的pod的呢？默认情况下，Kubernetes使用iptables通过`kube-proxy`执行这个魔术。这在下图中有解释。
- en: '![](../images/00091.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00091.jpeg)'
- en: 'Let''s reuse the `3-2-3_rc1.yaml` and `3-2-3_nodeport.yaml` examples from [Chapter
    3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting Started with
    Kubernetes*, to observe the default behavior:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)中的`3-2-3_rc1.yaml`和`3-2-3_nodeport.yaml`的例子，*开始使用Kubernetes*，来观察默认行为：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let's observe iptable rules and see how this works. As shown next, our service
    IP is `10.0.0.167`, two pods IP addresses underneath are `10.1.0.4` and `10.1.0.5`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们观察iptables规则，看看它是如何工作的。如下所示，我们的服务IP是`10.0.0.167`，下面的两个pod IP地址分别是`10.1.0.4`和`10.1.0.5`。
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s get into minikube node by `minikube ssh` and check its iptable rules:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过`minikube ssh`进入minikube节点并检查其iptables规则：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The key point here is that the service exposes the cluster IP to outside traffic
    from the target `KUBE-SVC-37ROJ3MK6RKFMQ2B`, which links to two custom chains
    `KUBE-SEP-SVVBOHTYP7PAP3J5` and `KUBE-SEP-AYS7I6ZPYFC6YNNF` with statistic mode
    random probability 0.5\. This means, iptables will generate a random number and
    tune it based on the probability distribution 0.5 to the destination. These two
    custom chains have the `DNAT` target set to the corresponding pod IP. The `DNAT`
    target is responsible for changing the packets' destination IP address. By default,
    conntrack is enabled to track the destination and source of connection when the
    traffic comes in. All of this results in a routing behavior. When the traffic
    comes to service, iptables will randomly pick one of the pods to route, and modify
    the destination IP from service IP to real pod IP, and un-DNAT to go all the way
    back.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键点是服务将集群IP暴露给来自目标`KUBE-SVC-37ROJ3MK6RKFMQ2B`的外部流量，该目标链接到两个自定义链`KUBE-SEP-SVVBOHTYP7PAP3J5`和`KUBE-SEP-AYS7I6ZPYFC6YNNF`，统计模式为随机概率0.5。这意味着，iptables将生成一个随机数，并根据概率分布0.5调整目标。这两个自定义链的`DNAT`目标设置为相应的pod
    IP。`DNAT`目标负责更改数据包的目标IP地址。默认情况下，当流量进入时，启用conntrack来跟踪连接的目标和源。所有这些都导致了一种路由行为。当流量到达服务时，iptables将随机选择一个pod进行路由，并将目标IP从服务IP修改为真实的pod
    IP，并取消DNAT以返回全部路由。
- en: External-to-service communications
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部到服务的通信
- en: 'The ability to serve external traffic to Kubernetes is critical. Kubernetes
    provides two API objects to achieve this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够为Kubernetes提供外部流量是至关重要的。Kubernetes提供了两个API对象来实现这一点：
- en: '**Service**: External network LoadBalancer or NodePort (L4)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务**: 外部网络负载均衡器或NodePort（L4）'
- en: '**Ingress:** HTTP(S) LoadBalancer (L7)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**入口:** HTTP(S)负载均衡器（L7）'
- en: 'For ingress, we''ll learn more in the next section. We''ll focus on L4 first.
    Based on what we''ve learned about pod-to-pod communication across nodes, how
    the packet goes in and out between service and pod. The following figure shows
    how it works. Let''s say we have two services, one service A has three pods (pod
    a, pod b, and pod c) and another service B gets only one pod (pod d). When the
    traffic comes in from LoadBalancer, the packet will be dispatched to one of the
    nodes. Most of the cloud LoadBalancer itself is not aware of pods or containers.
    It only knows about the node. If the node passes the health check, then it will
    be the candidate for the destination. Assume that we want to access service B,
    it currently only has one pod running on one node. However, LoadBalancer sends
    the packet to another node that doesn''t have any of our desired pods running.
    The traffic route will look like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于入口，我们将在下一节中学到更多。我们先专注于L4。根据我们对节点间pod到pod通信的了解，数据包在服务和pod之间进出的方式。下图显示了它的工作原理。假设我们有两个服务，一个服务A有三个pod（pod
    a，pod b和pod c），另一个服务B只有一个pod（pod d）。当流量从负载均衡器进入时，数据包将被分发到其中一个节点。大多数云负载均衡器本身并不知道pod或容器。它只知道节点。如果节点通过了健康检查，那么它将成为目的地的候选者。假设我们想要访问服务B，它目前只在一个节点上运行一个pod。然而，负载均衡器将数据包发送到另一个没有我们想要的任何pod运行的节点。流量路由将如下所示：
- en: '![](../images/00092.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00092.jpeg)'
- en: 'The packet routing journey will be:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包路由的过程将是：
- en: LoadBalancer will choose one of the nodes to forward the packet. In GCE, it
    selects the instance based on a hash of the source IP and port, destination IP
    and port, and protocol. In AWS, it's based on a round-robin algorithm.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负载均衡器将选择一个节点来转发数据包。在GCE中，它根据源IP和端口、目标IP和端口以及协议的哈希选择实例。在AWS中，它基于循环算法。
- en: Here, the routing destination will be changed to pod d (DNAT) and forward it
    to the other node similar to pod-to-pod communication across nodes.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，路由目的地将被更改为pod d（DNAT），并将其转发到另一个节点，类似于节点间的pod到pod通信。
- en: Then, comes service-to-pod communication. The packet arrives at pod d with the
    response accordingly.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，服务到Pod的通信。数据包到达Pod d，响应相应地。
- en: Pod-to-service communication is manipulated by iptables as well.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod到服务的通信也受iptables控制。
- en: The packet will be forwarded to the original node.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包将被转发到原始节点。
- en: The source and destination will be un-DNAT to LoadBalancer and client, and sent
    all the way back.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源和目的地将被解除DNAT并发送回负载均衡器和客户端。
- en: In Kubernetes 1.7, there is a new attribute in service called **externalTrafficPolicy**.
    You can set its value to local, then after the traffic goes into a node, Kubernetes
    will route the pods on that node, if any.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes 1.7中，服务中有一个名为**externalTrafficPolicy**的新属性。您可以将其值设置为local，然后在流量进入节点后，Kubernetes将路由该节点上的Pod（如果有）。
- en: Ingress
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress
- en: Pods and services in Kubernetes have their own IP; however, it is normally not
    the interface you'd provide to the external internet. Though there is service
    with node IP configured, the port in the node IP can't be duplicated among the
    services. It is cumbersome to decide which port to manage with which service.
    Furthermore, the node comes and goes, it wouldn't be clever to provide a static
    node IP to external service.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的Pod和服务都有自己的IP；然而，通常不是您提供给外部互联网的接口。虽然有配置了节点IP的服务，但节点IP中的端口不能在服务之间重复。决定将哪个端口与哪个服务管理起来是很麻烦的。此外，节点来去匆匆，将静态节点IP提供给外部服务并不明智。
- en: 'Ingress defines a set of rules that allows the inbound connection to access
    Kubernetes cluster services. It brings the traffic into the cluster at L7, allocates
    and forwards a port on each VM to the service port. This is shown in the following
    figure. We define a set of rules and post them as source type ingress to the API
    server. When the traffic comes in, the ingress controller will then fulfill and
    route the ingress by the ingress rules. As shown in the following figure, ingress
    is used to route external traffic to the kubernetes endpoints by different URLs:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress定义了一组规则，允许入站连接访问Kubernetes集群服务。它将流量带入集群的L7层，在每个VM上分配和转发一个端口到服务端口。这在下图中显示。我们定义一组规则，并将它们作为源类型ingress发布到API服务器。当流量进来时，ingress控制器将根据ingress规则履行和路由ingress。如下图所示，ingress用于通过不同的URL将外部流量路由到kubernetes端点：
- en: '![](../images/00093.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00093.jpeg)'
- en: 'Now, we will go through an example and see how this works. In this example,
    we''ll create two services named `nginx` and `echoserver` with ingress path `/welcome`
    and `/echoserver` configured. We can run this in minikube. The old version of
    minikube doesn''t enable ingress by default; we''ll have to enable it first:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个示例来看看这是如何工作的。在这个例子中，我们将创建两个名为`nginx`和`echoserver`的服务，并配置ingress路径`/welcome`和`/echoserver`。我们可以在minikube中运行这个。旧版本的minikube默认不启用ingress；我们需要先启用它：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Enabling ingress in minikube will create an nginx ingress controller and a
    `ConfigMap` to store nginx configuration (refer to [https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md))),
    and a RC and service as default HTTP backend for handling unmapped requests. We
    could observe them by adding `--namespace=kube-system` in the `kubectl` command.
    Next, let''s create our backend resources. Here is our nginx `Deployment` and
    `Service`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在minikube中启用ingress将创建一个nginx ingress控制器和一个`ConfigMap`来存储nginx配置（参考[https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md)），以及一个RC和一个服务作为默认的HTTP后端，用于处理未映射的请求。我们可以通过在`kubectl`命令中添加`--namespace=kube-system`来观察它们。接下来，让我们创建我们的后端资源。这是我们的nginx
    `Deployment`和`Service`：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We''ll then create another service with RS:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建另一个带有RS的服务：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we''ll create the ingress resource. There is an annotation named `ingress.kubernetes.io/rewrite-target`.
    This is required if the service requests are coming from the root URL. Without
    a rewrite annotation, we''ll get 404 as response. Refer to [https://github.com/kubernetes/ingress/blob/master/controllers/nginx/configuration.md#annotations](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/configuration.md#annotations)
    for more supported annotation in nginx ingress controller:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建ingress资源。有一个名为`ingress.kubernetes.io/rewrite-target`的注释。如果服务请求来自根URL，则需要此注释。如果没有重写注释，我们将得到404作为响应。有关nginx
    ingress控制器中更多支持的注释，请参阅[https://github.com/kubernetes/ingress/blob/master/controllers/nginx/configuration.md#annotations](https://github.com/kubernetes/ingress/blob/master/controllers/nginx/configuration.md#annotations)。
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In some cloud providers, service LoadBalancer controller is supported. It could
    be integrated with ingress via the `status.loadBalancer.ingress` syntax in the
    configuration file. For more information, refer to [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些云提供商中，支持服务负载均衡器控制器。它可以通过配置文件中的`status.loadBalancer.ingress`语法与ingress集成。有关更多信息，请参阅[https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer)。
- en: 'Since our host is set to `devops.k8s`, it will only return if we access it
    from that hostname. You could either configure the DNS record in the DNS server,
    or modify the hosts file in local. For simplicity, we''ll just add a line with
    the `ip hostname` format in the host file:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的主机设置为`devops.k8s`，只有在从该主机名访问时才会返回。您可以在DNS服务器中配置DNS记录，或者在本地修改hosts文件。为简单起见，我们将在主机文件中添加一行，格式为`ip
    hostname`：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we should be able to access our service by the URL directly:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应该能够直接通过URL访问我们的服务：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The pod ingress controller dispatches the traffic based on the URL path. The
    routing path is similar to external-to-service communication. The packet hops
    between nodes and pods. Kubernetes is pluggable. Lots of third-party implementation
    is going on. We only scratch the surface here while iptables is just a default
    and common implementation. Networking evolves a lot in every single release. At
    the time of this writing, Kubernetes had just released version 1.7.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Pod ingress控制器根据URL路径分发流量。路由路径类似于外部到服务的通信。数据包在节点和Pod之间跳转。Kubernetes是可插拔的。正在进行许多第三方实现。我们在这里只是浅尝辄止，而iptables只是一个默认和常见的实现。网络在每个发布版本中都有很大的发展。在撰写本文时，Kubernetes刚刚发布了1.7版本。
- en: Network policy
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络策略
- en: Network policy works as a software firewall to the pods. By default, every pod
    could communicate with each other without any boundaries. Network policy is one
    of the isolations you could apply to the pods. It defines who can access which
    pods in which port by namespace selector and pod selector. Network policy in a
    namespace is additive, and once a pod has policy on, it denies any other ingress
    (also known as default deny all).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略作为pod的软件防火墙。默认情况下，每个pod都可以在没有任何限制的情况下相互通信。网络策略是您可以应用于pod的隔离之一。它通过命名空间选择器和pod选择器定义了谁可以访问哪个端口的哪个pod。命名空间中的网络策略是累加的，一旦pod启用了策略，它就会拒绝任何其他入口（也称为默认拒绝所有）。
- en: Currently, there are multiple network providers that support network policy,
    such as Calico ([https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/](https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/)),
    Romana ([https://github.com/romana/romana](https://github.com/romana/romana))),
    Weave Net ([https://www.weave.works/docs/net/latest/kube-addon/#npc)](https://www.weave.works/docs/net/latest/kube-addon/#npc)),
    Contiv ([http://contiv.github.io/documents/networking/policies.html)](http://contiv.github.io/documents/networking/policies.html))
    and Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)).
    Users are free to choose any options. For simplicity, we're going to use Calico
    with minikube. To do that, we'll have to launch minikube with the `--network-plugin=cni`
    option. Network policy is still pretty new in Kubernetes at this point. We're
    running Kubernetes version v.1.7.0 with v.1.0.7 minikube ISO to deploy Calico
    by self-hosted solution ([http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/](http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/)).
    First, we'll have to download a `calico.yaml` ([https://github.com/projectcalico/calico/blob/master/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml](https://github.com/projectcalico/calico/blob/master/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml)))
    file to create Calico nodes and policy controller. `etcd_endpoints` needs to be
    configured. To find out the IP of etcd, we need to access localkube resources.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有多个网络提供商支持网络策略，例如Calico ([https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/](https://www.projectcalico.org/calico-network-policy-comes-to-kubernetes/))、Romana
    ([https://github.com/romana/romana](https://github.com/romana/romana)))、Weave
    Net ([https://www.weave.works/docs/net/latest/kube-addon/#npc)](https://www.weave.works/docs/net/latest/kube-addon/#npc))、Contiv
    ([http://contiv.github.io/documents/networking/policies.html)](http://contiv.github.io/documents/networking/policies.html))和Trireme
    ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes))。用户可以自由选择任何选项。为了简单起见，我们将使用Calico与minikube。为此，我们将不得不使用`--network-plugin=cni`选项启动minikube。在这一点上，Kubernetes中的网络策略仍然是相当新的。我们正在运行Kubernetes版本v.1.7.0，使用v.1.0.7
    minikube ISO来通过自托管解决方案部署Calico ([http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/](http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/hosted/))。首先，我们需要下载一个`calico.yaml`
    ([https://github.com/projectcalico/calico/blob/master/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml](https://github.com/projectcalico/calico/blob/master/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml)))文件来创建Calico节点和策略控制器。需要配置`etcd_endpoints`。要找出etcd的IP，我们需要访问localkube资源。
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The default port of etcd is `2379`. In this case, we modify `etcd_endpoint`
    in `calico.yaml` from `http://127.0.0.1:2379` to `http://10.0.2.15:2379`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: etcd的默认端口是`2379`。在这种情况下，我们将在`calico.yaml`中修改`etcd_endpoint`，从`http://127.0.0.1:2379`改为`http://10.0.2.15:2379`：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s reuse `5-2-1_nginx.yaml` as the example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用`5-2-1_nginx.yaml`作为示例：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will find that our nginx service has IP `10.0.0.42`. Let''s launch a simple
    bash and use `wget` to see if we can access our nginx:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将发现我们的nginx服务的IP是`10.0.0.42`。让我们启动一个简单的bash并使用`wget`来看看我们是否可以访问我们的nginx：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `--spider` parameter is used to check whether the URL exists. In this case,
    busybox can access nginx successfully. Next, let''s apply a `NetworkPolicy` to
    our nginx pods:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`--spider`参数用于检查URL是否存在。在这种情况下，busybox可以成功访问nginx。接下来，让我们将`NetworkPolicy`应用到我们的nginx
    pod中：'
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can see some important syntax here. The `podSelector` is used to select
    pods, which should match the labels of the target pod. Another one is `ingress[].from[].podSelector`,
    which is used to define who can access these pods. In this case, all the pods
    with `project=chapter5` labels are eligible to access the pods with `server=nginx`
    labels. If we go back to our busybox pod, we''re unable to contact nginx anymore
    because right now, the nginx pod has NetworkPolicy on. By default, it is deny
    all, so busybox won''t be able to talk to nginx:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里看到一些重要的语法。`podSelector`用于选择pod，应该与目标pod的标签匹配。另一个是`ingress[].from[].podSelector`，用于定义谁可以访问这些pod。在这种情况下，所有具有`project=chapter5`标签的pod都有资格访问具有`server=nginx`标签的pod。如果我们回到我们的busybox
    pod，现在我们无法再联系nginx，因为nginx pod现在已经有了NetworkPolicy。默认情况下，它是拒绝所有的，所以busybox将无法与nginx通信。
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We could use `kubectl edit deployment busybox` to add the label `project=chaper5`
    into busybox pods.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`kubectl edit deployment busybox`将标签`project=chaper5`添加到busybox pod中。
- en: Refer to the labels and selectors section in [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Getting Started with Kubernetes* if you forget how to do so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果忘记如何操作，请参考[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)中的标签和选择器部分，*开始使用Kubernetes*。
- en: 'After that, we can contact nginx pod again:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以再次联系nginx pod：
- en: '[PRE29]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'With the help of the preceding example, we have an idea how to apply network
    policy. We could also apply some default polices to deny all or allow all by tweaking
    the selector to select nobody or everybody. For example, deny all behavior could
    be achieved as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的例子，我们了解了如何应用网络策略。我们还可以通过调整选择器来应用一些默认策略，拒绝所有或允许所有。例如，拒绝所有的行为可以通过以下方式实现：
- en: '[PRE30]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This way, all pods that don't match labels will deny all other traffic. Alternatively,
    we could create a `NetworkPolicy` whose ingress is listed from everywhere. Then
    the pods running in this namespace could be accessed by anyone else.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，所有不匹配标签的pod将拒绝所有其他流量。或者，我们可以创建一个`NetworkPolicy`，其入口列表来自任何地方。然后，运行在这个命名空间中的pod可以被任何其他人访问。
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned how containers communicate with each other
    as it is essential, and we introduced how pod-to-pod communication works. Service
    is an abstraction to route the traffic to any of the pods underneath, if label
    selectors match. We learned how service works with pod by iptables magic. We got
    to know how packet routes from external to a pod and the DNAT, un-DAT tricks.
    We also learned new API objects such as *ingress*, which allow us to use the URL
    path to route to different services in the backend. In the end, another object
    `NetworkPolicy` was introduced. It provides a second layer of security, acting
    as a software firewall rule. With network policy, we can make certain pods communicate
    only with certain pods. For example, only data retrieval service can talk to the
    database container. All of these things make Kubernetes more flexible, secure,
    and powerful.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了容器之间如何进行通信是至关重要的，并介绍了pod与pod之间的通信工作原理。Service是一个抽象概念，可以将流量路由到任何匹配标签选择器的pod下面。我们学习了service如何通过iptables魔术与pod配合工作。我们了解了数据包如何从外部路由到pod以及DNAT、un-DAT技巧。我们还学习了新的API对象，比如*ingress*，它允许我们使用URL路径来路由到后端的不同服务。最后，还介绍了另一个对象`NetworkPolicy`。它提供了第二层安全性，充当软件防火墙规则。通过网络策略，我们可以使某些pod只与某些pod通信。例如，只有数据检索服务可以与数据库容器通信。所有这些都使Kubernetes更加灵活、安全和强大。
- en: Until now, we've learned the basic concepts of Kubernetes. Next, we'll get a
    clearer understanding of what is happening inside your cluster by monitoring cluster
    metrics and analyzing applications and system logs for Kubernetes. Monitoring
    and logging tools are essential for every DevOps, which also play an extremely
    important role in dynamic clusters such as Kubernetes. So we'll get an insight
    into the activities of the cluster, such as scheduling, deployment, scaling, and
    service discovery. The next chapter will help you better understand the act of
    operating Kubernetes in the real world.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了Kubernetes的基本概念。接下来，我们将通过监控集群指标和分析Kubernetes的应用程序和系统日志，更清楚地了解集群内部发生了什么。监控和日志工具对于每个DevOps来说都是必不可少的，它们在Kubernetes等动态集群中也扮演着极其重要的角色。因此，我们将深入了解集群的活动，如调度、部署、扩展和服务发现。下一章将帮助您更好地了解在现实世界中操作Kubernetes的行为。
