- en: Chapter 6. Securing Container Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。保护容器网络
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下示例：
- en: Enabling and disabling ICC
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用和禁用ICC
- en: Disabling outbound masquerading
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁用出站伪装
- en: Managing netfilter to Docker integration
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理netfilter到Docker集成
- en: Creating custom iptables rules
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自定义iptables规则
- en: Exposing services through a load balancer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过负载均衡器公开服务
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: As you move toward container-based applications, one of the items you'll want
    to put some serious consideration toward is network security. Containers, in particular,
    can lead to a proliferation in the number of network endpoints that need to be
    secured. Granted, not all endpoints are fully exposed to the network. However,
    those that aren't, by default, talk directly to each other, which can cause other
    concerns. There are many ways to tackle network security when it comes to container-based
    applications, and this chapter doesn't aim to address all possible solutions.
    Rather, this chapter aims to review configuration options and relevant network
    topologies that can be combined in a number of different ways based on your own
    network security requirements. We'll discuss in detail some features that we were
    exposed to in earlier chapters such as ICC mode and outbound masquerading. In
    addition, we'll cover a couple of different techniques to limit the network exposure
    of your containers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着您转向基于容器的应用程序，您需要认真考虑的一项内容是网络安全。特别是容器可能导致需要保护的网络端点数量激增。当然，并非所有端点都完全暴露在网络中。然而，默认情况下，那些没有完全暴露的端点会直接相互通信，这可能会引起其他问题。在涉及基于容器的应用程序时，有许多方法可以解决网络安全问题，本章并不旨在解决所有可能的解决方案。相反，本章旨在审查配置选项和相关网络拓扑，这些选项可以根据您自己的网络安全要求以多种不同的方式组合。我们将详细讨论一些我们在早期章节中接触到的功能，如ICC模式和出站伪装。此外，我们将介绍一些不同的技术来限制容器的网络暴露。
- en: Enabling and disabling ICC
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用和禁用ICC
- en: In earlier chapters, we were exposed to the concept of ICC mode, but didn't
    have much information on the mechanics of how it worked. ICC is a Docker-native
    way of isolating all containers connected to the same network. The isolation provided
    prevents containers from talking directly to each other while still allowing their
    exposed ports to be published as well as allowing outbound connectivity. In this
    recipe, we'll review our options for ICC-based configuration in both the default
    `docker0` bridge context as well as with user-defined networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期章节中，我们接触到了ICC模式的概念，但对其工作机制并不了解。ICC是Docker本地的一种方式，用于隔离连接到同一网络的所有容器。提供的隔离可以防止容器直接相互通信，同时允许它们的暴露端口被发布，并允许出站连接。在这个示例中，我们将审查在默认的`docker0`桥接上下文以及用户定义的网络中基于ICC的配置选项。
- en: Getting ready
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll be using two Docker hosts in this recipe to demonstrate how ICC works
    in different network configurations. It is assumed that both Docker hosts used
    in this lab are in their default configuration. In some cases, the changes we
    make may require you to have root-level access to the system.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用两个Docker主机来演示ICC在不同网络配置中的工作方式。假设本实验室中使用的两个Docker主机都处于默认配置。在某些情况下，我们所做的更改可能需要您具有系统的根级访问权限。
- en: How to do it…
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作方法…
- en: 'ICC mode can be configured on both the native `docker0` bridge as well as any
    user-defined networks that utilize the bridge driver. In this recipe, we''ll review
    how to configure ICC mode on the `docker0` bridge. As we''ve seen in earlier chapters,
    settings related to the `docker0` bridge need to be made at the service level.
    This is because the `docker0` bridge is created as part of service initialization.
    This also means that, to make changes to it, we''ll need to edit the Docker service
    configuration and then restart the service for them to take effect. Before we
    make any changes, let''s take the opportunity to review the default ICC configuration.
    To do this, let''s first view the `docker0` bridge configuration:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ICC模式可以在原生的`docker0`桥以及使用桥驱动的任何用户定义的网络上进行配置。在本教程中，我们将介绍如何在`docker0`桥上配置ICC模式。正如我们在前几章中看到的，与`docker0`桥相关的设置需要在服务级别进行。这是因为`docker0`桥是作为服务初始化的一部分创建的。这也意味着，要对其进行更改，我们需要编辑Docker服务配置，然后重新启动服务以使更改生效。在进行任何更改之前，让我们有机会审查默认的ICC配置。为此，让我们首先查看`docker0`桥的配置：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It's important to remember that the `docker network` subcommand is used to manage
    all Docker networks. A common misconception is that it can only be used to manage
    user-defined networks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，`docker network`子命令用于管理所有Docker网络。一个常见的误解是它只能用于管理用户定义的网络。
- en: 'As we can see, the `docker0` bridge is configured for ICC mode on (`true`).
    This means that Docker will not interfere or prevent containers connected to this
    bridge to talk directly to one another. To prove this out, let''s start two containers:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`docker0`桥配置为ICC模式（`true`）。这意味着Docker不会干预或阻止连接到这个桥的容器直接相互通信。为了证明这一点，让我们启动两个容器：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Notice that we didn''t specify the `-P` flag, which tells Docker to not publish
    any of the containers exposed ports. Now, let''s get each container''s IP address,
    so we can validate connectivity:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有指定`-P`标志，这告诉Docker不要发布任何容器暴露的端口。现在，让我们获取每个容器的IP地址，以便验证连接：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we know the IP addresses, we can verify that each container can access
    the other on any service in which the container is listening:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了IP地址，我们可以验证每个容器是否可以访问另一个容器在其上监听的任何服务：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Based on these tests, we can assume that the containers are allowed to talk
    to each other on any protocol that is listening. This is the expected behavior
    when ICC mode is enabled. Now, let''s change the service level setting and recheck
    our configuration. To do this, set the following configuration in your systemd
    drop in file for the Docker service:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些测试，我们可以假设容器被允许在任何监听的协议上相互通信。这是启用ICC模式时的预期行为。现在，让我们更改服务级别设置并重新检查我们的配置。为此，在Docker服务的systemd
    drop in文件中设置以下配置：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now reload the systemd configuration, restart the Docker service, and check
    the ICC setting:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在重新加载systemd配置，重新启动Docker服务，并检查ICC设置：
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we''ve confirmed that ICC is disabled, let''s start up our two containers
    once again and run the same connectivity tests:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确认了ICC被禁用，让我们再次启动我们的两个容器并运行相同的连接性测试：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, we have no connectivity between the two containers. However,
    the Docker host itself is still able to access the services:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的两个容器之间没有连接。但是，Docker主机本身仍然能够访问服务：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can inspect the netfilter rules that are used to implement ICC by looking
    at the `iptables` rules `FORWARD` chain of the filter table:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查用于实现ICC的netfilter规则，方法是查看过滤表的`iptables`规则`FORWARD`链：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding bolded rule is what prevents container–to-container communication
    on the `docker0` bridge. If we had inspected this `iptables` chain before disabling
    ICC, we would have seen this rule set to `ACCEPT` as shown following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 前面加粗的规则是防止在`docker0`桥上进行容器之间通信的。如果在禁用ICC之前检查了这个`iptables`链，我们会看到这个规则设置为`ACCEPT`，如下所示：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we saw earlier, linking containers allowed you to bypass this rule and allow
    a source container to access a target container. If we remove the two containers
    we can restart them with a link as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，链接容器允许您绕过这一规则，允许源容器访问目标容器。如果我们移除这两个容器，我们可以通过以下方式重新启动它们：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now if we examine the rules with `iptables`, we can see two new rules added
    to the filter table:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们用`iptables`检查规则，我们可以看到两个新规则添加到了过滤表中：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: These two new rules allow `web2` to access `web1` on any exposed port. Notice
    how the first rule defines the access from `web2` (`172.17.0.3`) to `web1` (`172.17.0.2`)
    with a destination port of `80`. The second rule flips the IPs and specifies port
    `80` as the source port, allowing the traffic to return to `web2`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个新规则允许`web2`访问`web1`的任何暴露端口。请注意，第一个规则定义了从`web2`(`172.17.0.3`)到`web1`(`172.17.0.2`)的访问，目的端口为`80`。第二个规则翻转了IP，并指定端口`80`作为源端口，允许流量返回到`web2`。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Earlier, when we discussed user-defined networks, you saw that we could pass
    the ICC flag to a user-defined bridge. However, disabling ICC mode is not currently
    supported with the overlay driver.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，当我们讨论用户定义的网络时，您看到我们可以将ICC标志传递给用户定义的桥接。然而，目前不支持使用覆盖驱动程序禁用ICC模式。
- en: Disabling outbound masquerading
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 禁用出站伪装
- en: By default, containers are allowed to access the outside network by masquerading
    or hiding their real IP address behind that of the Docker host. This is accomplished
    through netfilter `masquerade` rules that hide container traffic behind the Docker
    host interface referenced in the next hop. We saw a detailed example of this in
    [Chapter 2](ch02.html "Chapter 2. Configuring and Monitoring Docker Networks"),
    *Configuring and Monitoring Docker Networks*, when we discussed container-to-container
    connectivity across hosts. While this type of configuration is ideal in many respects,
    there are some cases when you might prefer to disable the outbound masquerading
    capability. For instance, if you prefer to not allow your containers to have outbound
    connectivity at all, disabling masquerading would prevent containers from talking
    to the outside network. This, however, only prevents outbound traffic due to lack
    of return routing. A better option might be to treat containers like any other
    individual network endpoint and use existing security appliances to define network
    policy. In this recipe, we'll discuss how to disable IP masquerading as well as
    how to provide containers with unique IP addressing as they traverse the outside
    network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，容器允许通过伪装或隐藏其真实IP地址在Docker主机的IP地址后访问外部网络。这是通过netfilter `masquerade`规则实现的，这些规则将容器流量隐藏在下一跳中引用的Docker主机接口后面。当我们讨论跨主机的容器之间的连接时，我们在[第2章](ch02.html
    "第2章。配置和监控Docker网络")*配置和监控Docker网络*中看到了这方面的详细示例。虽然这种类型的配置在许多方面都是理想的，但在某些情况下，您可能更喜欢禁用出站伪装功能。例如，如果您不希望容器完全具有出站连接性，禁用伪装将阻止容器与外部网络通信。然而，这只是由于缺乏返回路由而阻止了出站流量。更好的选择可能是将容器视为任何其他单独的网络端点，并使用现有的安全设备来定义网络策略。在本教程中，我们将讨论如何禁用IP伪装以及如何在容器在外部网络中进行遍历时提供唯一的IP地址。
- en: Getting ready
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll be using a single Docker host in this example. It is assumed that the
    Docker host used in this lab is in its default configuration. You'll also need
    access to change Docker service-level settings. In some cases, the changes we
    make may require you to have root-level access to the system. We'll also be making
    changes to the network equipment to which the Docker host connects.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用单个Docker主机。假设在此实验中使用的Docker主机处于其默认配置中。您还需要访问更改Docker服务级别设置。在某些情况下，我们所做的更改可能需要您具有系统的根级访问权限。我们还将对Docker主机连接的网络设备进行更改。
- en: How to do it…
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'You''ll recall that IP masquerading in Docker is handled through a netfilter
    `masquerade` rule. On a Docker host in its default configuration, we can see this
    rule by examining the ruleset with `iptables`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您会记得，Docker中的IP伪装是通过netfilter `masquerade`规则处理的。在其默认配置中的Docker主机上，我们可以通过使用`iptables`检查规则集来看到这个规则：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This rule specifies the source of the traffic as the `docker0` bridge subnet
    and only NAT traffic can be headed off the host. The `MASQUERADE` target tells
    the host to source NAT the traffic to the Docker host''s next hop interface. That
    is, if the host has multiple IP interfaces, the container''s traffic will source
    NAT to whichever interface is used as the next hop. This means that container
    traffic could potentially be hidden behind different IP addresses based on the
    Docker host interface and routing table configuration. For instance, consider
    a Docker host with two interfaces, as shown in the following figure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此规则指定流量的来源为`docker0`网桥子网，只有NAT流量可以离开主机。`MASQUERADE`目标告诉主机对Docker主机的下一跳接口的流量进行源NAT。也就是说，如果主机有多个IP接口，容器的流量将源NAT到下一跳使用的任何接口。这意味着根据Docker主机接口和路由表配置，容器流量可能潜在地隐藏在不同的IP地址后面。例如，考虑一个具有两个接口的Docker主机，如下图所示：
- en: '![How to do it…](graphics/B05453_06_01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![如何做…](graphics/B05453_06_01.jpg)'
- en: In the left-hand side example, traffic is taking the default route since the
    destination of `4.2.2.2` doesn't match a more specific prefix in the host's routing
    table. In this case, the host performs a source NAT and changes the source of
    the traffic from `172.17.0.2` to `10.10.10.101` as it traverses the Docker host
    to the outside network. However, if the destination falls into `172.17.0.0/16`,
    the container traffic will instead be hidden behind the `192.168.10.101` interface,
    as shown in the example on the right.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧示例中，流量正在采用默认路由，因为`4.2.2.2`的目的地在主机的路由表中没有更具体的前缀。在这种情况下，主机执行源NAT，并在流经Docker主机到外部网络时将流量的源从`172.17.0.2`更改为`10.10.10.101`。但是，如果目的地落入`172.17.0.0/16`，容器流量将被隐藏在右侧示例中所示的`192.168.10.101`接口后面。
- en: 'The default behavior of Docker can be changed by manipulating the `--ip-masq`
    Docker option. By default, the option is considered to be `true` and can be overridden
    by specifying the option and setting it to `false`. We can do this by specifying
    the option in our Docker systemd drop in file, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Docker的默认行为可以通过操纵`--ip-masq` Docker选项来更改。默认情况下，该选项被认为是`true`，可以通过指定该选项并将其设置为`false`来覆盖。我们可以通过在Docker
    systemd drop in文件中指定该选项来实现这一点：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now reload the systemd configuration, restart the Docker service, and check
    the ICC setting:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在重新加载systemd配置，重新启动Docker服务，并检查ICC设置：
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Notice that the `masquerade` rule is now gone. Traffic generated from a container
    on this host would attempt to route out through the Docker host with its actual
    source IP address. A `tcpdump` on the Docker host would capture this traffic exiting
    the host''s `eth0` interface with the original container IP address:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`masquerade`规则现在已经消失。在此主机上生成的容器流量将尝试通过其实际源IP地址路由到Docker主机外部。在Docker主机上进行`tcpdump`将捕获此流量通过原始容器IP地址退出主机的`eth0`接口：
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Since the outside network doesn't know where `172.17.0.0/16` is, this request
    will never receive a response effectively preventing the container from communicating
    to the outside world.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于外部网络不知道`172.17.0.0/16`在哪里，这个请求将永远不会收到响应，有效地阻止了容器与外部世界的通信。
- en: While this may be a useful means to prevent communication to the outside world,
    it's not entirely ideal. For starters, you're still allowing the traffic out;
    the response just won't know where to go as it attempts to return to the source.
    Also, you've impacted all of the containers, from all networks, on the Docker
    host. If the `docker0` bridge had a routable subnet allocated to it, and the outside
    network knew where that subnet lived, you could use existing security tooling
    to make security policy decisions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能是阻止与外部世界通信的一种有用手段，但并不完全理想。首先，您仍然允许流量出去；响应只是不知道要返回到哪里，因为它试图返回到源。此外，您影响了Docker主机上所有网络的所有容器。如果`docker0`桥接分配了一个可路由的子网，并且外部网络知道该子网的位置，您可以使用现有的安全工具来制定安全策略决策。
- en: 'For instance, let''s assume that the `docker0` bridge were to be allocated
    a subnet of `172.10.10.0/24` and we left IP masquerading disabled. We could do
    this by changing the Docker options to also specify a new bridge IP address:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设`docker0`桥接被分配了一个子网`172.10.10.0/24`，并且我们禁用了IP伪装。我们可以通过更改Docker选项来指定新的桥接IP地址来实现这一点：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As before, traffic leaving a container and destined for the outside network
    would be unchanged as it traversed the Docker host. Let''s assume a small network
    topology, as the one shown in the following figure:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，离开容器并前往外部网络的流量在穿过Docker主机时不会改变。假设一个小的网络拓扑，如下图所示：
- en: '![How to do it…](graphics/B05453_06_02.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![如何做…](graphics/B05453_06_02.jpg)'
- en: 'Let''s assume a flow from the container to `4.2.2.2`. In this case, egress
    traffic should work inherently:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设从容器到`4.2.2.2`的流量。在这种情况下，出口流量应该天然工作：
- en: Container generates traffic toward `4.2.2.2` and uses its default gateway that
    is the `docker0` bridge IP address
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器生成流量到`4.2.2.2`，并使用它的默认网关，即`docker0`桥接IP地址
- en: The Docker host does a route lookup, fails to find a specific prefix match,
    and forwards the traffic to its default gateway that is the switch.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker主机进行路由查找，未能找到特定的前缀匹配，并将流量转发到其默认网关，即交换机。
- en: The switch does a route lookup, fails to find a specific prefix match, and forwards
    the traffic to its default route that is the firewall.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交换机进行路由查找，未能找到特定的前缀匹配，并将流量转发到其默认路由，即防火墙。
- en: The firewall does a route lookup, fails to find a specific prefix match, ensures
    that the traffic is allowed in the policy, performs a hide NAT to a public IP
    address, and forwards the traffic to its default route that is the Internet.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防火墙进行路由查找，未能找到特定的前缀匹配，确保流量在策略中被允许，执行隐藏NAT到公共IP地址，并将流量转发到其默认路由，即互联网。
- en: So without any additional configuration, egress traffic should reach its destination.
    The problem is with the return traffic. When the response from the Internet destination
    gets back to the firewall, it will attempt to determine how to route back to the
    source. This route lookup will likely fail causing the firewall to drop the traffic.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，没有任何额外的配置，出口流量应该能够到达目的地。问题在于返回流量。当来自互联网目的地的响应返回到防火墙时，它将尝试确定如何返回到源。这个路由查找可能会失败，导致防火墙丢弃流量。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In some cases, edge network equipment (the firewall in this case) routes all
    private IP addressing back to the inside (the switch in this case). In those scenarios,
    the firewall might forward the return traffic to the switch, but the switch won't
    have a specific return route causing the same problem.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，边缘网络设备（在本例中是防火墙）将所有私有IP地址路由回内部（在本例中是交换机）。在这种情况下，防火墙可能会将返回流量转发到交换机，但交换机没有特定的返回路由，导致了同样的问题。
- en: 'In order for this to work, the firewall and the switch need to know how to
    return the traffic to the specific container. To do this, we need to add specific
    routes on each device pointing the `docker0` bridge subnet back to the `docker1`
    host:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其工作，防火墙和交换机需要知道如何将流量返回到特定的容器。为此，我们需要在每个设备上添加特定的路由，将`docker0`桥接子网指向`docker1`主机：
- en: '![How to do it…](graphics/B05453_06_03.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![操作步骤...](graphics/B05453_06_03.jpg)'
- en: 'Once these routes are in place, containers spun up on the Docker host should
    have connectivity to outside networks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些路由设置好，在Docker主机上启动的容器应该能够连接到外部网络：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'A `tcpdump` on the Docker host will show that the traffic is leaving with the
    original container IP address:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker主机上进行`tcpdump`将显示流量以原始容器IP地址离开：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This type of configuration offers the ability to use existing security appliances
    to decide whether containers can reach certain resources on the outside networks.
    However, this is also a function of how close the security appliance is to your
    Docker host. For instance, in this configuration the containers on the Docker
    host would be able to reach any other network endpoints connected to the switch.
    The enforcement point (the firewall, in this example) only allows you to limit
    the container's connectivity to the Internet. In addition, assigning routable
    IP space each Docker host might introduce IP assignment constraints if you have
    large scale.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的配置提供了使用现有安全设备来决定容器是否可以访问外部网络资源的能力。但是，这也取决于安全设备与您的Docker主机的距离。例如，在这种配置中，Docker主机上的容器可以访问连接到交换机的任何其他网络端点。执行点（在本例中是防火墙）只允许您限制容器与互联网的连接。此外，为每个Docker主机分配可路由的IP空间可能会引入IP分配约束，特别是在大规模情况下。
- en: Managing netfilter to Docker integration
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理netfilter到Docker的集成
- en: By default, Docker performs most of the netfilter configuration for you. It
    takes care of things such as publishing ports and outbound masquerading, as well
    as allows you to block or allow ICC. However, this is all optional and you can
    tell Docker not to modify or add to any of your existing `iptables` rules. If
    you do this, you'll need to generate your own rules to provide similar functionality.
    This may be appealing to you if you're already using `iptables` rules extensively
    and don't want Docker to automatically make changes to your configuration. In
    this recipe we'll discuss how to disable automatic `iptables` rule generation
    for Docker and show you how to manually create similar rules.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Docker会为您执行大部分netfilter配置。它会处理诸如发布端口和出站伪装之类的事情，并允许您阻止或允许ICC。但是，这都是可选的，您可以告诉Docker不要修改或添加任何现有的`iptables`规则。如果这样做，您将需要生成自己的规则来提供类似的功能。如果您已经广泛使用`iptables`规则，并且不希望Docker自动更改您的配置，这可能会对您有吸引力。在本教程中，我们将讨论如何禁用Docker自动生成`iptables`规则，并向您展示如何手动创建类似的规则。
- en: Getting ready
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll be using a single Docker host in this example. It is assumed that the
    Docker host used in this lab is in its default configuration. You'll also need
    access to change Docker service-level settings. In some cases, the changes we
    make may require you to have root-level access to the system.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用单个Docker主机。假设在本实验中使用的Docker主机处于其默认配置中。您还需要访问更改Docker服务级别的设置。在某些情况下，我们所做的更改可能需要您具有系统的根级访问权限。
- en: How to do it…
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'As we''ve already seen, Docker takes care of a lot of the heavy lifting for
    you when it comes to network configuration. It also allows you the ability to
    configure these things on your own if need be. Before we look at doing it ourselves,
    let''s confirm what Docker is actually configuring on our behalf with regard to
    `iptables` rules. Let''s run the following containers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的，当涉及到网络配置时，Docker会为您处理很多繁重的工作。它还允许您在需要时自行配置这些内容。在我们自己尝试配置之前，让我们确认一下Docker实际上在我们的`iptables`规则方面为我们配置了什么。让我们运行以下容器：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Running these containers would yield the following topology:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些容器将产生以下拓扑结构：
- en: '![How to do it…](graphics/B05453_06_04.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![如何做…](graphics/B05453_06_04.jpg)'
- en: Note
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The examples given later will not use the host's `eth1` interface directly.
    It is displayed to illustrate how the rules generated by Docker are written in
    a manner that encompasses all physical interfaces on the Docker host.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后给出的示例将不直接使用主机的`eth1`接口。它只是用来说明Docker生成的规则是以涵盖Docker主机上的所有物理接口的方式编写的。
- en: 'As we''ve mentioned before, Docker uses `iptables` to handle the following
    items:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Docker使用`iptables`来处理以下项目：
- en: Outbound container connectivity (masquerading)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出站容器连接（伪装）
- en: Inbound port publishing
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入站端口发布
- en: Container–to-container connectivity
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器之间的连接
- en: 'Since we''re using the default configuration and we have published ports on
    both containers, we should be able to see all three of these items configured
    in `iptables`. Let''s take a look at the NAT table to start with:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是默认配置，并且我们已经在两个容器上发布了端口，我们应该能够在`iptables`中看到这三个项目的配置。让我们首先查看NAT表：
- en: Note
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In most cases, I prefer to print the rules and interpret them rather than have
    them listed in formatted columns. There are trade-offs to each approach, but if
    you prefer the list mode, you can replace the `-S` with `-vL`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我更喜欢打印规则并解释它们，而不是将它们列在格式化的列中。每种方法都有权衡，但如果您喜欢列表模式，您可以用`-vL`替换`-S`。
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s review the importance of each of the bolded lines in the preceding output.
    The first bolded line takes care of the outbound hide NAT or `MASQUERADE`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下前面输出中每个加粗行的重要性。第一个加粗行处理了出站隐藏NAT或`MASQUERADE`：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The rule is looking for traffic that matches two characteristics:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则正在寻找符合两个特征的流量：
- en: The source IP address must match the IP address space of the `docker0` bridge
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源IP地址必须匹配`docker0`桥的IP地址空间
- en: The traffic is not exiting through the `docker0` bridge. That is, it's leaving
    through another interface such as `eth0` or `eth1`
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该流量不是通过`docker0`桥出口的。也就是说，它是通过其他接口如`eth0`或`eth1`离开的
- en: The jump statement at the end specifies a target of `MASQUERADE`, which will
    source NAT the container traffic to one of the host's IP interfaces based on the
    routing table.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结尾处的跳转语句指定了`MASQUERADE`的目标，它将根据路由表将容器流量源NAT到主机的IP接口之一。
- en: 'The next two bolded lines provide similar functionality and provide the NAT
    required for publishing ports on each respective container. Let''s examine one
    of them:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两行加粗的内容提供了类似的功能，并为每个容器提供了所需的NAT。让我们来看其中一个：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The rule is looking for traffic that matches three characteristics:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则正在寻找符合三个特征的流量：
- en: The traffic is not entering through the `docker0` bridge
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量不是通过`docker0`桥接进入的
- en: The traffic is TCP
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量是TCP
- en: The traffic has a destination port of `32768`
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量的目的端口是`32768`
- en: The jump statement at the end specifies a target of `DNAT` and a destination
    of the container with its real service port (`80`). Notice that both of these
    rules are generic in terms of the Docker host's physical interfaces. As we saw
    earlier, both port publishing and outbound masquerading can occur on any interface
    on the host unless we 'specifically limit the scope.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的跳转语句指定了`DNAT`的目标和容器的真实服务端口（`80`）的目的地。请注意，这两条规则在Docker主机的物理接口方面是通用的。正如我们之前看到的，主机上的任何接口都可以进行端口发布和出站伪装，除非我们“明确限制范围。
- en: 'The next table we want to review is the filter table:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要审查的下一个表是过滤表：
- en: '[PRE23]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Again, you'll note that the chain policy is set to `ACCEPT` for the default
    chains. In the case of the filter table, it has a more drastic impact on functionality.
    This means that everything is being allowed unless specifically denied in a rule.
    In other words, if there were no rules defined everything would still work. Docker
    inserts these rules in case your default policy is not set to `ACCEPT`. Later
    on, when we manually create the rules, we'll set the default policy to `DROP`
    so that you can see the impact the rules have. The preceding rules require a little
    more explaining, especially if you aren't familiar with how `iptables` rules work.
    Let's review the bolded lines one at a time.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您会注意到默认链的链策略设置为`ACCEPT`。在过滤表的情况下，这对功能有更严重的影响。这意味着除非在规则中明确拒绝，否则一切都被允许。换句话说，如果没有定义规则，一切仍然可以工作。Docker在默认策略未设置为`ACCEPT`的情况下插入这些规则。稍后，当我们手动创建规则时，我们将把默认策略设置为`DROP`，以便您可以看到规则的影响。前面的规则需要更多的解释，特别是如果您不熟悉`iptables`规则的工作原理。让我们逐一审查加粗的线。
- en: 'The first bolded line takes care of allowing traffic from the outside network
    back into the containers. In this case, the rule is specific to instances where
    the container itself is generating traffic toward, and expecting a response, from
    the outside network:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行加粗的线负责允许来自外部网络的流量返回到容器中。在这种情况下，规则是特定于容器本身生成流量并期望来自外部网络的响应的实例：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The rule is looking for traffic that matches two characteristics:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则正在寻找符合两个特征的流量：
- en: The traffic is exiting through the `docker0` bridge
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量是通过`docker0`桥接离开的
- en: The traffic has a connection state of `RELATED` or `ESTABLISHED`. This would
    include sessions that are part of an existing flow or related to it
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量具有`RELATED`或`ESTABLISHED`的连接状态。这将包括作为现有流或与之相关的会话
- en: The jump statement at the end references a target of `ACCEPT`, which will allow
    the flow through.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的跳转语句引用了`ACCEPT`的目标，这将允许流量通过。
- en: 'The second bolded line allows the container''s connectivity to the outside
    network:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行加粗的线允许容器与外部网络的连接：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The rule is looking for traffic that matches two characteristics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则正在寻找符合两个特征的流量：
- en: The traffic is entering through the `docker0` bridge
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量是通过`docker0`桥接进入的
- en: The traffic is not exiting through the `docker0` bridge
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量不是通过`docker0`桥接离开的
- en: This is a very generic way of identifying traffic that came from the containers
    and is leaving through any other interface than the `docker0` bridge. The jump
    statement at the end references a target of `ACCEPT`, which will allow the flow
    through. This rule, in conjunction with the first rule, will allow a flow generated
    from a container toward the outside network to work.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非常通用的方式来识别来自容器并且通过`docker0`桥接以外的任何其他接口离开的流量。最后的跳转语句引用了`ACCEPT`的目标，这将允许流量通过。与第一条规则结合起来，将允许从容器生成的流向外部网络的流量工作。
- en: 'The third bolded line allows inter-container connectivity:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 加粗的第三行允许容器间的连接：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The rule is looking for traffic that matches two characteristics:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则正在寻找符合两个特征的流量：
- en: The traffic is entering through the `docker0` bridge
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量是通过`docker0`桥进入的
- en: The traffic is exiting through the `docker0` bridge
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量通过`docker0`桥出口
- en: This is another generic means to identify traffic that is originated from a
    container on the `docker0` bridge as well as destined for a target on the `docker0`
    bridge. The jump statement at the end references a target of `ACCEPT`, which will
    allow the flow through. This is the same rule that's turned into a `DROP` target
    when you disable ICC mode as we saw in earlier chapters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一种通用的方法来识别源自`docker0`桥上容器的流量，以及目标是`docker0`桥上的流量。结尾处的跳转语句引用了一个`ACCEPT`目标，这将允许流量通过。这与我们在早期章节中看到的在禁用ICC模式时转换为`DROP`目标的规则相同。
- en: 'The last two bolded lines allow the published ports to reach the containers.
    Let''s examine one of them:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两行加粗的允许发布的端口到达容器。让我们检查其中一行：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The rule is looking for traffic that matches five characteristics:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则正在寻找符合五个特征的流量：
- en: The traffic is destined to the container whose port was published
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量是发送到已发布端口的容器
- en: The traffic is not entering through the `docker0` bridge
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量不是通过`docker0`桥进入的
- en: The traffic is exiting through the `docker0` bridge
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量通过`docker0`桥出口
- en: The protocol is TCP
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协议是TCP
- en: The port number is `80`
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端口号是`80`
- en: This rule specifically allows the published port to work by allowing access
    to the container's service port (`80`). The jump statement at the end references
    a target of `ACCEPT`, which will allow the flow through.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则特别允许发布的端口工作，通过允许访问容器的服务端口（`80`）。结尾处的跳转语句引用了一个`ACCEPT`目标，这将允许流量通过。
- en: Manually creating the required iptables rules
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动创建所需的iptables规则
- en: 'Now that we''ve seen how Docker automatically handles rule generation, let''s
    walk through an example of how to build this connectivity on our own. To do this,
    we first need to instruct Docker to not create any `iptables` rules. To do this,
    we set the `--iptables` Docker option to `false` in the Docker systemd drop in
    file:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到Docker如何自动处理规则生成，让我们通过一个示例来了解如何自己建立这种连接。为此，我们首先需要指示Docker不创建任何`iptables`规则。为此，在Docker
    systemd drop in文件中将`--iptables` Docker选项设置为`false`：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''ll need to reload the systemd drop in file and restart the Docker service
    for Docker to reread the service parameters. To ensure that you start with a blank
    slate, if possible, restart the server or flush all the `iptables` rules out manually
    (if you''re not comfortable with managing the `iptables` rules, the best approach
    is just to reboot the server to clear them out). We''ll assume for the rest of
    the example that we''re working with an empty ruleset. Once Docker is restarted,
    you can restart your two containers and ensure that there are no `iptables` rules
    present on the system:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重新加载systemd drop in文件并重新启动Docker服务，以便Docker重新读取服务参数。为了确保从空白状态开始，如果可能的话，重新启动服务器或手动清除所有`iptables`规则（如果您不熟悉管理`iptables`规则，最好的方法就是重新启动服务器以清除它们）。在接下来的示例中，我们假设我们正在使用空规则集。一旦Docker重新启动，您可以重新启动两个容器，并确保系统上没有`iptables`规则存在：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As you can see, there are no `iptables` rules currently defined. We can also
    see that our default chain policy in the filter table is set to `ACCEPT`. Let''s
    now change the default policy in the filter table to `DROP` for each chain. Along
    with that, let''s also include a rule to allow SSH to and from the host so as
    not to break our connectivity:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当前没有定义`iptables`规则。我们还可以看到过滤表中默认链策略设置为`ACCEPT`。现在让我们将过滤表中的默认策略更改为每个链的`DROP`。除此之外，让我们还包括一条规则，允许SSH进出主机，以免破坏我们的连接：
- en: '[PRE30]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s now check the filter table once again to make sure that the rules were
    accepted:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次检查过滤表，以确保规则已被接受：
- en: '[PRE31]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'At this point, the containers `web1` and `web2` will no longer be able to reach
    each other:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，容器`web1`和`web2`将不再能够相互到达：
- en: '[PRE32]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Depending on your operating system, you might notice that `web1` actually is
    able to ping `web2` at this point. The most likely reason for this is that the
    `br_netfilter` kernel module has not been loaded. Without this module bridged
    packets will not be inspected by netfilter. To resolve this, you can manually
    load the module by using the `sudo modprobe br_netfilter` command. To make the
    module load at each boot, you could add it to the `/etc/modules` file as well.
    When Docker is managing the `iptables` ruleset, it takes care of loading the module
    for you.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的操作系统，您可能会注意到此时`web1`实际上能够ping通`web2`。最有可能的原因是`br_netfilter`内核模块尚未加载。没有这个模块，桥接的数据包将不会被netfilter检查。要解决这个问题，您可以使用`sudo
    modprobe br_netfilter`命令手动加载模块。要使模块在每次启动时加载，您还可以将其添加到`/etc/modules`文件中。当Docker管理`iptables`规则集时，它会负责为您加载模块。
- en: 'Now, let''s start building the ruleset to recreate the connectivity that Docker
    previously built for us automatically. The first thing we want to do is allow
    containers inbound and outbound access. We''ll do that with these two rules:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始构建规则集，以重新创建Docker自动为我们构建的连接。我们要做的第一件事是允许容器的入站和出站访问。我们将使用以下两条规则来实现：
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Although these two rules will allow containers to generate and receive traffic
    from the outside network, the connectivity still won''t work at this point. In
    order for this to work, we need to apply the `masquerade` rule so that the container
    traffic will be hidden behind an interface on the `docker0` host. If we don''t
    do this, the traffic will never get returned as the outside network knows nothing
    about the `172.17.0.0/16` network in which the containers live:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两条规则将允许容器从外部网络生成和接收流量，但此时连接仍然无法工作。为了使其工作，我们需要应用`masquerade`规则，以便容器流量将被隐藏在`docker0`主机的接口后面。如果我们不这样做，流量将永远不会返回，因为外部网络对容器所在的`172.17.0.0/16`网络一无所知：
- en: '[PRE34]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With this in place, the containers will now be able to reach network endpoints
    on the outside network:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个设置，容器现在将能够到达外部网络的网络端点：
- en: '[PRE35]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'However, the containers still cannot communicate directly with each other:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，容器仍然无法直接相互通信：
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We need to add one final rule:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加最后一条规则：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Since traffic between containers both enters and leaves the `docker0` bridge,
    this will allow the inter-container connectivity:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于容器之间的流量既进入又离开`docker0`桥，这将允许容器之间的互联：
- en: '[PRE38]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The only configuration remaining is to provide a mechanism to publish ports.
    We can do that by first provisioning a destination NAT on the Docker host itself.
    Even though Docker is not provisioning the NAT rules, it''s still keeping track
    of the port allocations on your behalf. At container runtime if you choose to
    publish a port, Docker will allocate a port mapping for you even though it is
    not handling the publishing. It is wise to use the port Docker allocates to prevent
    overlaps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的配置是提供一个发布端口的机制。我们可以首先在Docker主机上配置目标NAT来实现这一点。即使Docker没有配置NAT规则，它仍然会代表你跟踪端口分配。在容器运行时，如果你选择发布一个端口，Docker会为你分配一个端口映射，即使它不处理发布。明智的做法是使用Docker分配的端口以防止重叠：
- en: '[PRE39]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Using the ports Docker allocated, we can define an inbound NAT rule for each
    container that translates inbound connectivity to an external port on the Docker
    host to the real container IP and service port. Finally, we just need to allow
    inbound traffic:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker分配的端口，我们可以为每个容器定义一个入站NAT规则，将入站连接转换为Docker主机上的外部端口到真实的容器IP和服务端口。最后，我们只需要允许入站流量：
- en: '[PRE40]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once these rules are configured, we can now test the connectivity from outside
    the Docker host on the published ports:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些规则配置好了，我们现在可以测试来自Docker主机外部的已发布端口的连接：
- en: '![Manually creating the required iptables rules](graphics/B05453_06_05.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![手动创建所需的iptables规则](graphics/B05453_06_05.jpg)'
- en: Creating custom iptables rules
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义iptables规则
- en: In the previous recipe, we covered how Docker handles `iptables` rules for the
    most common container networking needs. However, there may be cases where you
    wish to extend the default `iptables` configuration to either allow more access
    or limit the scope of connectivity. In this recipe, we'll walk through a couple
    of examples of how to implement custom `iptables` rules. We'll focus on limiting
    the scope of sources connecting to services running on your containers as well
    as allowing the Docker host itself to connect to those services.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们介绍了Docker如何处理最常见的容器网络需求的`iptables`规则。然而，可能会有一些情况，您希望扩展默认的`iptables`配置，以允许更多的访问或限制连接的范围。在这个配方中，我们将演示如何实现自定义的`iptables`规则的一些示例。我们将重点放在限制连接到运行在您的容器上的服务的源的范围，以及允许Docker主机本身连接到这些服务。
- en: Note
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The examples provided later are designed to demonstrate the options you have
    to configure `iptables` rulesets. The way they are implemented in these examples
    may or may not make sense in your environment and can be deployed in different
    ways and places based on your security needs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 后面提供的示例旨在演示您配置`iptables`规则集的选项。它们在这些示例中的实现方式可能或可能不适合您的环境，并且可以根据您的安全需求以不同的方式和位置部署。
- en: Getting ready
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We'll be using the same Docker host with the same configuration from the previous
    recipe. The Docker service should be configured with the `--iptables=false` service
    option, and there should be two containers defined—`web1` and `web2`. If you are
    unsure how to get to this state, please see the previous recipe. In order to define
    a new `iptables` policy, we'll also need to flush all the existing `iptables`
    rules out of the NAT and the FILTER table. The easiest way to do this is to reboot
    the host.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前一个配方相同的Docker主机和相同的配置。Docker服务应该配置为使用`--iptables=false`服务选项，并且应该定义两个容器——`web1`和`web2`。如果您不确定如何达到这种状态，请参阅前一个配方。为了定义一个新的`iptables`策略，我们还需要清除NAT和FILTER表中的所有现有`iptables`规则。这样做的最简单方法是重新启动主机。
- en: Note
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Flushing the `iptables` rules when your default policy is deny will disconnect
    any remote administration sessions. Be careful not to accidentally disconnect
    yourself if you are managing the system without console access!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当默认策略为拒绝时刷新`iptables`规则将断开任何远程管理会话。如果您没有控制台访问权限，要小心不要意外断开自己！
- en: 'If you prefer not to reboot, you can change the default filter policy back
    to `allow`. Then, flush the filter and NAT table as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想重新启动，可以将默认的过滤策略更改回`allow`。然后，按照以下步骤刷新过滤和NAT表：
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How to do it…
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 怎么做...
- en: 'At this point, you should once again have a Docker host with two containers
    running and an empty default `iptables` policy. To begin, let''s once again change
    the default filter policy to `deny` while ensuring that we still allow our management
    connection over SSH:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该再次拥有两个运行的容器和一个空的默认`iptables`策略的Docker主机。首先，让我们再次将默认的过滤策略更改为`deny`，同时确保我们仍然允许通过SSH进行管理连接：
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Because we''ll be focusing on policy around the filter table, let''s put in
    the NAT policy unchanged from the previous recipe. These NATs cover both outbound
    masquerading and inbound masquerading for the destination NATs for the service
    in each container:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将专注于过滤表周围的策略，让我们将NAT策略放在上一篇配方中未更改的状态下。这些NAT覆盖了每个容器中服务的出站伪装和入站伪装：
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'One of the items you might be interested in configuring is limiting the scope
    of what the containers can access on the outside network. You''ll notice that,
    in previous examples, the containers were allowed to talk to anything externally.
    This was allowed since the filter rule was rather generic:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能有兴趣配置的项目之一是限制容器在外部网络上可以访问的范围。您会注意到，在以前的示例中，容器被允许与外部任何东西通信。这是因为过滤规则相当通用：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This rule allows the containers to talk to anything out of any interface besides
    `docker0`. Rather than allowing this, we can specify only the ports we want to
    allow outbound. So for instance, if we publish port `80`, we can then define a
    reverse or outbound rule, which only allows that specific return traffic. Let''s
    first recreate the inbound rules we used in the previous example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此规则允许容器与除`docker0`之外的任何接口上的任何东西通信。与其允许这样做，我们可以指定我们想要允许出站的端口。因此，例如，如果我们发布端口`80`，然后我们可以定义一个反向或出站规则，只允许特定的返回流量。让我们首先重新创建我们在上一个示例中使用的入站规则：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now we can easily replace the more generic outbound rule with specific rules
    that only allow the return traffic on port `80`. For example, let''s put in a
    rule that allows the container `web1` to return traffic only on port `80`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以轻松地用特定规则替换更通用的出站规则，只允许端口`80`上的返回流量。例如，让我们放入一个规则，允许容器`web1`只在端口`80`上返回流量：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If we check, we should see that from the outside network we can get to the
    service on `web1`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查一下，我们应该能够从外部网络访问`web1`上的服务：
- en: '![How to do it…](graphics/B05453_06_06.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![操作步骤...](graphics/B05453_06_06.jpg)'
- en: 'However, the container `web1` is not able to talk to anything on the outside
    network except on port `80` at this point because we didn''t use the generic outbound
    rule:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，此时容器`web1`除了在端口`80`上无法与外部网络上的任何东西通信，因为我们没有使用通用的出站规则：
- en: '[PRE47]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To fix this, we can add specific rules to allow things like ICMP sourced from
    the `web1` container:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以添加特定的规则，允许来自`web1`容器的ICMP之类的东西：
- en: '[PRE48]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The above rule coupled with the state-aware return rule from the previous recipe
    will allow the web1 container to initiate and receive return ICMP traffic.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上述规则与前一篇配方中的状态感知返回规则相结合，将允许web1容器发起和接收返回的ICMP流量。
- en: '[PRE49]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In the case of the `web2` container, its web server can still not be accessed
    from the outside network. If we wish to limit the source of traffic which can
    talk to the web server we could do that by altering the inbound port `80` rule,
    or by specifying a destination in the outbound port `80` rule. We could for instance
    limit the source of the traffic to a single device on the outside network by specifying
    a destination in the egress rule:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在`web2`容器的情况下，其Web服务器仍然无法从外部网络访问。如果我们希望限制可以与Web服务器通信的流量源，我们可以通过更改入站端口`80`规则或指定出站端口`80`规则中的目的地来实现。例如，我们可以通过在出口规则中指定目标来将流量源限制为外部网络上的单个设备：
- en: '[PRE51]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now if we try from a lab device on the outside network with the IP address
    of `10.20.30.13`, we should be able to access the web server:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们尝试使用外部网络上IP地址为`10.20.30.13`的实验室设备，我们应该能够访问Web服务器：
- en: '[PRE52]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'But if we try from a different lab server with a different IP address, the
    connection will fail:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们尝试使用具有不同IP地址的不同实验室服务器，连接将失败：
- en: '[PRE53]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Again, this rule could be implemented either as an inbound or outbound rule.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这条规则可以作为入站规则或出站规则实现。
- en: 'When managing the `iptables` rules in this manner, you might have noticed that
    the Docker host itself no longer has connectivity to the containers and the services
    they are hosting:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式管理`iptables`规则时，您可能已经注意到Docker主机本身不再能够与容器及其托管的服务进行通信：
- en: '[PRE54]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This is because all of the rules we''ve been writing in the filter table have
    been in the forward chain. The forward chain only applies to traffic the host
    is forwarding, not traffic that is originated or destined to the host itself.
    To fix this, we can put rules in the `INPUT` and `OUTPUT` chains of the filter
    table. To allow ICMP to and from the containers, we can specify rules like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们一直在过滤表中编写的所有规则都在转发链中。转发链仅适用于主机正在转发的流量，而不适用于源自主机或目的地为主机本身的流量。为了解决这个问题，我们可以在过滤表的`INPUT`和`OUTPUT`链中放置规则。为了允许容器之间的ICMP流量，我们可以指定以下规则：
- en: '[PRE55]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The rule being added to the output chain looks for traffic headed out of the
    `docker0` bridge (toward the containers), that is of protocol ICMP, and is a new
    or established flow. The rule being added to the input chain looks for traffic
    headed into the `docker0` bridge (toward the host), that is of protocol ICMP,
    and is an established flow. Since the traffic is being originated from the Docker
    host, these rules will match and allow the ICMP traffic to the container to work:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到输出链的规则查找流向`docker0`桥（流向容器）的流量，协议为ICMP，并且是新的或已建立的流量。添加到输入链的规则查找流向`docker0`桥（流向主机）的流量，协议为ICMP，并且是已建立的流量。由于流量是从Docker主机发起的，这些规则将匹配并允许容器的ICMP流量工作：
- en: '[PRE56]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'However, this still will not allow the containers themselves to ping the default
    gateway. This is because the rule we added to the input chain matching traffic
    coming into the `docker0` bridge is only looking for established sessions. To
    have this work bidirectionally, you''d need to add the `NEW` flag to the second
    rule so that it would also match new flows generated by the containers toward
    the host:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仍然不允许容器本身对默认网关进行ping。这是因为我们添加到输入链的规则仅匹配进入`docker0`桥的流量，只寻找已建立的会话。为了使其双向工作，您需要向第二条规则添加`NEW`标志，以便它也可以匹配容器向主机生成的新流量：
- en: '[PRE57]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Since the rule we added to the output chain already specifies new or established
    flows, ICMP connectivity from the containers to the host will now also work:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们添加到输出链的规则已经指定了新的或已建立的流量，容器到主机的ICMP连接现在也将工作：
- en: '[PRE58]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Exposing services through a load balancer
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过负载均衡器公开服务
- en: Another way to isolate your containers is to frontend them with a load balancer.
    This mode of operation offers several advantages. First, the load balancer can
    provide intelligent load balancing to multiple backend nodes. If a container dies,
    the load balancer can remove it from the load balancing pool. Second, you're effectively
    hiding your containers behind a load balancing **Virtual IP** (**VIP**) address.
    Clients believe that they are interacting directly with the application running
    in the container while they are actually interacting with the load balancer. In
    many cases, a load balancer can provide or offload security features, such as
    SSL and web application firewall that make scaling a container-based application
    easier to accomplish in a secure fashion. In this recipe, we'll learn how this
    can be done and some of the features available in Docker that make this easier
    to do.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离容器的另一种方法是使用负载均衡器作为前端。这种操作模式有几个优点。首先，负载均衡器可以为多个后端节点提供智能负载均衡。如果一个容器死掉，负载均衡器可以将其从负载均衡池中移除。其次，您实际上是将容器隐藏在负载均衡**虚拟IP**（**VIP**）地址后面。客户端认为他们直接与容器中运行的应用程序进行交互，而实际上他们是在与负载均衡器进行交互。在许多情况下，负载均衡器可以提供或卸载安全功能，如SSL和Web应用程序防火墙，使基于容器的应用程序更容易以安全的方式进行扩展。在本教程中，我们将学习如何做到这一点以及Docker中可用的一些功能，使这更容易实现。
- en: Getting ready
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll be using multiple Docker hosts in the following examples. We'll also be
    using a user-defined overlay network. It will be assumed that you know how to
    configure the Docker hosts for overlay networking. If you do not, please see the
    *Creating a user-defined overlay network* recipe in [Chapter 3](ch03.html "Chapter 3. User-Defined
    Networks"), *User-Defined Networks*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用多个Docker主机。我们还将使用用户定义的覆盖网络。假设您知道如何为覆盖网络配置Docker主机。如果不知道，请参阅[第3章](ch03.html
    "第3章。用户定义的网络")中的*创建用户定义的覆盖网络*教程，*用户定义的网络*。
- en: How to do it…
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Load balancing is not a new concept and is one that is well-understood in the
    physical and virtual machine space. However, load balancing with containers adds
    in an extra layer of complexity, which can make things drastically more complicated.
    To start with, let''s look how load balancing typically works without containers:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡不是一个新概念，在物理和虚拟机空间中是一个众所周知的概念。然而，使用容器进行负载均衡增加了额外的复杂性，这可能会使事情变得更加复杂。首先，让我们看看在没有容器的情况下负载均衡通常是如何工作的：
- en: '![How to do it…](graphics/B05453_06_07.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![如何做…](graphics/B05453_06_07.jpg)'
- en: 'In this case, we have a simple load balancer configuration where the load balancer
    is providing VIP for a single backend pool member (`192.168.50.150`). The flow
    works like this:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有一个简单的负载均衡器配置，其中负载均衡器为单个后端池成员（`192.168.50.150`）提供VIP。流程如下：
- en: The client generates a request toward the VIP (10.10.10.150) hosted on the load
    balancer
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端向托管在负载均衡器上的VIP（10.10.10.150）发出请求
- en: The load balancer receives the request, ensures that it has VIP for that IP,
    and then generates a request to the backend pool member(s) for that VIP on behalf
    of the client
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器接收请求，确保它具有该IP的VIP，然后代表客户端向后端池成员发出请求
- en: The server receives the request sourced from the load balancer and responds
    directly back to the load balancer
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器接收来自负载均衡器的请求，并直接回应负载均衡器
- en: The load balancer then responds back to the client
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器然后回应客户端
- en: In most cases, the conversation involves two distinct sessions, one between
    the client and the load balancer and another between the load balancer and the
    server. Each of these is a distinct TCP session.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，对话涉及两个不同的会话，一个是客户端和负载均衡器之间的会话，另一个是负载均衡器和服务器之间的会话。每个都是一个独立的TCP会话。
- en: 'Now, let''s show an example of how this might work in the container space.
    Examine the topology shown in the following figure:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们展示一个在容器空间中可能如何工作的示例。查看以下图中显示的拓扑：
- en: '![How to do it…](graphics/B05453_06_08.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![操作步骤…](graphics/B05453_06_08.jpg)'
- en: 'In this example, we''ll be using both container-based application servers as
    backend pool members as well as a container-based load balancer. Let''s make the
    following assumptions:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用基于容器的应用服务器作为后端池成员，以及基于容器的负载均衡器。让我们做出以下假设：
- en: The hosts `docker2` and `docker3` will provide hosting for many different web
    presentation containers that support many different VIPs
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机`docker2`和`docker3`将为许多支持许多不同VIP的不同网络演示容器提供托管
- en: We will use one load balancer container (`haproxy` instance) for each VIP we
    wish to define
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将为每个要定义的VIP使用一个负载均衡器容器（`haproxy`实例）
- en: Each presentation server exposes port `80`
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个演示服务器都公开端口`80`
- en: Given this, we can assume that host network mode is out of the question for
    both the load balancer host (`docker1`) as well as the hosting hosts (`docker2`
    and `docker3`) since it would require containers exposing services on a large
    number of ports. Before the introduction of user-defined networks, this would
    leave us with having to deal with port mapping on the `docker0` bridge.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我们可以假设主机网络模式对于负载均衡器主机（`docker1`）以及托管主机（`docker2`和`docker3`）都不可行，因为它需要容器在大量端口上公开服务。在用户定义网络引入之前，这将使我们不得不处理`docker0`桥上的端口映射。
- en: 'That would quickly become a problem both to manage as well as troubleshoot.
    For instance, the topology might really look like this:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这很快就会成为一个管理和故障排除的问题。例如，拓扑可能真的是这样的：
- en: '![How to do it…](graphics/B05453_06_09.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![操作步骤…](graphics/B05453_06_09.jpg)'
- en: 'In this case, the load balancer VIP would be a published port on the host `docker1`,
    that is, `32769`. The web servers themselves are also publishing ports to expose
    their web servers. Let''s walk through what a load balancing request might look
    like:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，负载均衡器VIP将是主机`docker1`上的发布端口，即`32769`。Web服务器本身也在发布端口以公开其Web服务器。让我们看一下负载均衡请求可能是什么样子：
- en: A client from the outside network generates a request to `http://docker1.lab.lab:32769`.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部网络的客户端生成对`http://docker1.lab.lab:32769`的请求。
- en: The `docker1` host receives the request and translates the packet through the
    published port on the `haproxy` container. This changes the destination IP and
    port to `172.17.0.2:80`.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker1`主机接收请求并通过`haproxy`容器上的发布端口转换数据包。这将把目的地IP和端口更改为`172.17.0.2:80`。'
- en: The `haproxy` container receives the request and determines that the VIP being
    accessed has a backend pool containing `docker2:23770` and `docker3:32771`. It
    selects the `docker3` host for this session and sends a request towards `docker3:32771`.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`haproxy`容器接收请求并确定被访问的VIP具有包含`docker2:23770`和`docker3:32771`的后端池。它选择`docker3`主机进行此会话，并向`docker3:32771`发送请求。'
- en: As the request traverses the host `docker1`, it performs an outbound `MASQUERADE`
    and hides the container behind the host's IP interface.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当请求穿过主机`docker1`时，它执行出站`MASQUERADE`并隐藏容器在主机的IP接口后面。
- en: The request is sent to the host's default gateway (the MLS), which, in turn,
    forwards the request down to the host `docker3`.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求被发送到主机的默认网关（MLS），然后转发请求到主机`docker3`。
- en: The `docker3` host receives the request and translates the packet through the
    published port on the `web2` container. This changes the destination IP and port
    to `172.17.0.3:80`.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker3`主机接收请求并通过`web2`容器上的发布端口转换数据包。这将把目的地IP和端口更改为`172.17.0.3:80`。'
- en: The `web2` container receives the request and responds back toward `docker1`
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`web2`容器接收请求并向`docker1`回复'
- en: The `docker3` host receives the reply and translates the packet back through
    the inbound published port.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker3`主机接收到回复，并通过入站发布端口将数据包翻译回去。'
- en: The request is received at `docker1` translated back through the outbound `MASQUERADE`,
    and is delivered at the `haproxy` container.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求在`docker1`接收，通过出站`MASQUERADE`进行翻译，并传递到`haproxy`容器。
- en: The `haproxy` container then responds back to the client. The `docker1` host
    translates the `haproxy` container's response back to its own IP address on port
    `32769` and the response makes its way back to the client.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后`haproxy`容器回应客户端。`docker1`主机将`haproxy`容器的响应翻译回自己的IP地址和端口`32769`，响应返回到客户端。
- en: While doable, it's a lot to keep track of. In addition, the load balancer node
    needs to be aware of the published port and IP address of each backend container.
    If a container gets restarted, the published port can change effectively making
    it unreachable. Troubleshooting this with a large backend pool would be a headache
    as well.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可行，但要跟踪这些内容是很多的。此外，负载均衡器节点需要知道每个后端容器的发布端口和IP地址。如果容器重新启动，发布端口可能会改变，从而使其无法访问。在大型后端池中进行故障排除也会是一个头痛的问题。
- en: 'So while this is certainly doable, the introduction of user-defined networks
    can make this much more manageable. For instance, we could leverage an overlay
    type network for the backend pool members and completely remove the need for much
    of the port publishing and outbound masquerading. That topology would look more
    like this:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然这当然是可行的，但引入用户定义的网络可以使这更加可管理。例如，我们可以利用覆盖类型网络来进行后端池成员的管理，并完全消除大部分端口发布和出站伪装的需要。这种拓扑结构看起来更像这样：
- en: '![How to do it…](graphics/B05453_06_10.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![如何做…](graphics/B05453_06_10.jpg)'
- en: 'Let''s see what it would take to build this kind of configuration. The first
    thing we need to do is to define a user-defined overlay type network on one of
    the nodes. We''ll define it on `docker1` and call it `presentation_backend`:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看构建这种配置需要做些什么。我们需要做的第一件事是在其中一个节点上定义一个用户定义的覆盖类型网络。我们将在`docker1`上定义它，并称之为`presentation_backend`：
- en: '[PRE59]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that how I passed the `--internal` flag when I created this network. You'll
    recall from [Chapter 3](ch03.html "Chapter 3. User-Defined Networks"), *User-Defined
    Networks*, that this means that only containers connected to this network will
    be able to access it.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我创建这个网络时，我传递了`--internal`标志。你会记得在[第3章](ch03.html "第3章。用户定义的网络")中，*用户定义的网络*，这意味着只有连接到这个网络的容器才能访问它。
- en: 'The next thing we want to do is to create the two web containers which will
    serve as the backend pool members for the load balancer. We''ll do that on hosts
    `docker2` and `docker3`:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要做的是创建两个Web容器，它们将作为负载均衡器的后端池成员。我们将在`docker2`和`docker3`主机上进行操作：
- en: '[PRE60]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The remaining component left to deploy is the load balancer. As mentioned,
    `haproxy` has a container image of their load balancer, so we''ll use that for
    this example. Before we run the container we need to come up with a configuration
    that we can pass into the container for `haproxy` to use. This is done through
    mounting a volume into the container as we''ll see shortly. The configuration
    file is named `haproxy.cfg` and my example configuration looks like this:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下要部署的组件是负载均衡器。如前所述，`haproxy`有一个负载均衡器的容器镜像，所以我们将在这个例子中使用它。在运行容器之前，我们需要准备一个配置，以便将其传递给`haproxy`使用。这是通过将一个卷挂载到容器中来完成的，我们很快就会看到。配置文件名为`haproxy.cfg`，我的示例配置看起来像这样：
- en: '[PRE61]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'A couple of items are worth pointing out in the preceding configuration:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配置中有几个值得指出的地方：
- en: We bind the `haproxy` service to all interfaces on port `80`
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将`haproxy`服务绑定到端口`80`上的所有接口
- en: Any request hitting the container on port `80` will get load balanced to a pool
    named `pres_containers`
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何命中端口`80`的容器的请求都将被负载均衡到名为`pres_containers`的池中
- en: 'The `pres_containers` pool load balances in a round-robin method between two
    servers:'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pres_containers`池以循环轮询的方式在两个服务器之间进行负载均衡：'
- en: '`web1` on port `80`'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`web1`的端口`80`'
- en: '`web2` on port `80`'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`web2`的端口`80`'
- en: One of the interesting items here is that we can define the pool members by
    name. This is a huge advantage that comes along with user-defined networks and
    means that we don't need to worry about tracking container IP addressing.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个有趣的地方是，我们可以按名称定义池成员。这是与用户定义的网络一起出现的一个巨大优势，这意味着我们不需要担心跟踪容器IP地址。
- en: 'I put this config file in a folder in my home directory named `haproxy`:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我将这个配置文件放在了我的主目录中名为`haproxy`的文件夹中：
- en: '[PRE62]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Once the configuration file is in pace, we can run the container as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置文件就位，我们可以按照以下方式运行容器：
- en: '[PRE63]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: You might be wondering why I'm specifying a port mapping when connecting the
    container to an `internal` type network. Recall from earlier chapters that port
    mappings are global across all network types. In other words, even though I'm
    not using it currently, it's still a characteristic of the container. So if I
    ever connect a network type to the container that can use the port mapping, it
    will. In this case, I first need to connect the container to the overlay network
    to ensure that it has reachability to the backend web servers. If the `haproxy`
    container is unable to resolve the pool member names when it starts, it will fail
    to load.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我在连接容器到“内部”类型网络时指定了端口映射。回想一下前几章中提到的端口映射在所有网络类型中都是全局的。换句话说，即使我目前没有使用它，它仍然是容器的一个特性。因此，如果我将来连接一个可以使用端口映射的网络类型到容器中，它就会使用端口映射。在这种情况下，我首先需要将容器连接到覆盖网络，以确保它可以访问后端web服务器。如果`haproxy`容器在启动时无法解析池成员名称，它将无法加载。
- en: 'At this point, the `haproxy` container has reachability to its pool members,
    but we have no way to access the `haproxy` container externally. To do that, we''ll
    connect another interface to the container that can use the port mapping. In this
    case, that will be the `docker0` bridge:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`haproxy`容器已经可以访问其池成员，但我们无法从外部访问`haproxy`容器。为了做到这一点，我们将连接另一个可以使用端口映射的接口到容器中。在这种情况下，这将是`docker0`桥：
- en: '[PRE64]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'At this point, the `haproxy` container should be available externally at the
    following URLs:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，`haproxy`容器应该可以在以下URL外部访问：
- en: 'Load balanced VIP: `http://docker1.lab.lab`'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡VIP：`http://docker1.lab.lab`
- en: 'HAProxy stats: `http://docker1.lab.lab/lbstats`'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy统计信息：`http://docker1.lab.lab/lbstats`
- en: 'If we check the stats page, we should see that the `haproxy` container can
    reach each backend web server across the overlay. We can see that the health check
    for each is coming back with a `200 OK` status:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查统计页面，我们应该看到`haproxy`容器可以通过覆盖网络访问每个后端web服务器。我们可以看到每个的健康检查都返回`200 OK`状态：
- en: '![How to do it…](graphics/B05453_06_11.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![如何做到这一点…](graphics/B05453_06_11.jpg)'
- en: 'Now if we check VIP itself and hit refresh a couple of times, we should see
    the web page presented from each container:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们检查VIP本身并刷新几次，我们应该看到来自每个容器的网页：
- en: '![How to do it…](graphics/B05453_06_12.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![如何做到这一点…](graphics/B05453_06_12.jpg)'
- en: This type of topology provides us several notable advantages over the first
    concept we had around container load balancing. The use of the overlay-based network
    not only provided name-based resolution of containers but also significantly reduced
    the complexity of the traffic path. Granted, the traffic took the same physical
    path in either case, but we didn't need to rely on so many different NATs for
    the traffic to work. It also made the entire solution far more dynamic. This type
    of design can be easily replicated to provide load balancing for many different
    backend overlay networks.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这种拓扑结构为我们提供了比我们最初在容器负载均衡方面的概念更多的优势。基于覆盖网络的使用不仅提供了基于名称的容器解析，还显著减少了流量路径的复杂性。当然，无论哪种情况，流量都会采用相同的物理路径，但我们不需要依赖那么多不同的NAT来使流量工作。这也使整个解决方案变得更加动态。这种设计可以很容易地复制，为许多不同的后端覆盖网络提供负载均衡。
