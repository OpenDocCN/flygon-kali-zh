- en: Chapter 2. Discover the Discovery Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。发现发现服务
- en: In [Chapter 1](ch01.html "Chapter 1. Welcome to Docker Swarm"), *Welcome to
    Docker Swarm* we created a simple yet well functioning local Docker Swarm cluster
    using the `nodes://` mechanism. This system is not very practical, except for
    learning the Swarm fundamentals.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html "第1章。欢迎来到Docker Swarm")*欢迎来到Docker Swarm*中，我们使用`nodes://`机制创建了一个简单但功能良好的本地Docker
    Swarm集群。这个系统对于学习Swarm的基本原理来说并不是很实用。
- en: In fact, it is just a flat model that does not contemplate any true master-slave
    architecture, not to mention the high-level services, such as nodes discovery
    and auto-configuration, resilience, leader elections, and failover (high availability).
    In practice, it's not suitable for a production environment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这只是一个扁平的模型，没有考虑任何真正的主从架构，更不用说高级服务，比如节点发现和自动配置、韧性、领导者选举和故障转移（高可用性）。实际上，它并不适合生产环境。
- en: 'Apart from `nodes://`, Swarm v1 officially supports four discovery services;
    however, one of them, Token, is a trivial non-production one. Basically, with
    Swarm v1 you need to integrate a discovery service manually, while with Swarm
    Mode (from Docker 1.12), a discovery service, Etcd, is already integrated. In
    this chapter we''re going to cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`nodes://`，Swarm v1正式支持四种发现服务；然而，其中一种Token，是一个微不足道的非生产级服务。基本上，使用Swarm v1，你需要手动集成一个发现服务，而使用Swarm
    Mode（从Docker 1.12开始），一个发现服务Etcd已经集成。在本章中，我们将涵盖：
- en: Discovery services
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现服务
- en: 'A test-grade discovery service: Token'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个测试级别的发现服务：Token
- en: Raft theory and Etcd
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raft理论和Etcd
- en: Zookeeper and Consul
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zookeeper和Consul
- en: Before exploring these services in depth, lets us discuss what is a discovery
    service?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨这些服务之前，让我们讨论一下什么是发现服务？
- en: A discovery service
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个发现服务
- en: 'Imagine you''re running a Swarm cluster on a static configuration, similar
    to the one in [Chapter 1](ch01.html "Chapter 1. Welcome to Docker Swarm"), *Welcome
    to Docker Swarm*, networking is flat and every container is assigned a specific
    task, for example a MySQL database. It''s easy to locate the MySQL container because
    you assigned it a defined IP address or you run some DNS server. It''s easy to
    notify whether this single container is working or not and it''s a known fact
    that it won''t change its port (`tcp/3336`). Moreover, it''s not necessary that
    our MySQL container announces its availability as a database container with its
    IP and port: We, of course, already know that.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在运行一个静态配置的Swarm集群，类似于[第1章](ch01.html "第1章。欢迎来到Docker Swarm")*欢迎来到Docker
    Swarm*中的配置，网络是扁平的，每个容器都被分配了一个特定的任务，例如一个MySQL数据库。很容易找到MySQL容器，因为你为它分配了一个定义的IP地址，或者你运行了一些DNS服务器。很容易通知这个单独的容器是否工作，而且我们知道它不会改变它的端口（`tcp/3336`）。此外，我们的MySQL容器并不需要宣布它的可用性作为一个带有IP和端口的数据库容器：我们当然已经知道了。
- en: This is the pet model, mocked-up manually by a system administrator. However,
    since we're more advanced operators, we want to drive a cattle instead.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个宠物模型，由系统管理员手动模拟。然而，由于我们是更高级的运营者，我们想要驱动一头牛。
- en: So, imagine you're running a Swarm made of hundreds of nodes, hosting several
    applications running a certain number of services (web servers, databases, key-value
    stores, caches, and queues). These applications run on a massive number of containers
    that may dynamically change their IP address, either because you restart them,
    you create new ones, you spin up replicas, or some high availability mechanism
    starts new ones for you.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，想象一下，你正在运行一个由数百个节点组成的Swarm，托管着运行一定数量服务的几个应用程序（Web服务器、数据库、键值存储、缓存和队列）。这些应用程序运行在大量的容器上，这些容器可能会动态地改变它们的IP地址，要么是因为你重新启动它们，要么是因为你创建了新的容器，要么是因为你复制了它们，或者是因为一些高可用性机制为你启动了新的容器。
- en: How can you find the MySQL services acting of your Acme app? How do you ensure
    that your load balancer knows the address of your 100 Nginx frontends so that
    their functionalities don't break? How do you notify if a service has moved away
    with a different configuration?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您如何找到您的Acme应用程序的MySQL服务？如何确保负载均衡器知道您的100个Nginx前端的地址，以便它们的功能不会中断？如果服务已经移动并具有不同的配置，您如何通知？
- en: '*You use a discovery service.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*您使用发现服务。*'
- en: A so called discovery service is a mechanism with many features. There are different
    services you can choose from, with more or less similar qualities, with their
    pros and their cons, but basically all discovery services target distributed systems,
    hence they must be distributed on all cluster nodes, be scalable, and fault-tolerant.
    The main goal of a discovery service is to help services to find and talk to one
    another. In order to do that, they need to save (register) information related
    to where each service is located, by announcing themselves, and they usually do
    that by acting as a key-value store. Discovery services existed way before of
    the rise of Docker, but the problem has become a lot more difficult with the advent
    of containers and container orchestration.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓的发现服务是一个具有许多特性的机制。有不同的服务可供选择，具有更多或更少相似的特性，有其优点和缺点，但基本上所有的发现服务都针对分布式系统，因此它们必须分布在所有集群节点上，具有可伸缩性和容错性。发现服务的主要目标是帮助服务找到并相互通信。为了做到这一点，它们需要保存（注册）与每个服务的位置相关的信息，通过宣布自己来做到这一点，它们通常通过充当键值存储来实现。发现服务在Docker兴起之前就已经存在，但随着容器和容器编排的出现，问题变得更加困难。
- en: 'Summarizing again, through a discovery service:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 再次总结，通过发现服务：
- en: You can locate single services in the infrastructure
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以定位基础设施中的单个服务
- en: You can notify a service configuration change
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通知服务配置更改
- en: Services register their availability
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务注册其可用性
- en: And more
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等
- en: 'Typically, a discovery service is made as a key-value store. Docker Swarm v1,
    officially, supports the following discovery services. However, you can integrate
    your own using `libkv` abstraction interface, you can integrate your own one as
    shown in the following site:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，发现服务是作为键值存储创建的。Docker Swarm v1官方支持以下发现服务。但是，您可以使用`libkv`抽象接口集成自己的发现服务，如下所示：
- en: '[https://github.com/docker/docker/tree/master/pkg/discovery](https://github.com/docker/docker/tree/master/pkg/discovery).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/docker/docker/tree/master/pkg/discovery](https://github.com/docker/docker/tree/master/pkg/discovery)。'
- en: Token
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Token
- en: Consul 0.5.1+
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul 0.5.1+
- en: Etcd 2.0+
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd 2.0+
- en: ZooKeeper 3.4.5+
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZooKeeper 3.4.5+
- en: However, the Etcd library has been integrated into the Swarm mode as its built-in
    discovery service.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Etcd库已经集成到Swarm模式中作为其内置的发现服务。
- en: Token
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Token
- en: Docker Swarm v1 includes an out-of-the-box discovery service, called Token.
    Token is integrated into the Docker Hub; hence, it requires all the Swarm nodes
    to be connected to the Internet and able to reach the Docker Hub. This is the
    main limitation of Token but, you will soon see, Token will allow us to make some
    practice in handling clusters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm v1包括一个开箱即用的发现服务，称为Token。Token已集成到Docker Hub中；因此，它要求所有Swarm节点连接到互联网并能够访问Docker
    Hub。这是Token的主要限制，但很快您将看到，Token将允许我们在处理集群时进行一些实践。
- en: In a nutshell, Token requires you to generate a UUID called, in fact, token.
    With this UUID, you can create a manager, act like a master, and join slaves to
    the cluster.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Token要求您生成一个称为token的UUID。有了这个UUID，您可以创建一个管理器，充当主节点，并将从节点加入集群。
- en: Re-architecting the example of Chapter 1 with token
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用token重新设计第1章的示例
- en: If we want to keep it practical, it's time to take a look at an example. We'll
    use token to re-architect the example of [Chapter 1](ch01.html "Chapter 1. Welcome
    to Docker Swarm"), *Welcome to Docker Swarm*. As a novelty, the cluster will be
    not flat anymore, but it will consist of 1 master and 3 slaves and each node will
    have security enabled by default.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想保持实用性，现在是时候看一个例子了。我们将使用令牌来重新设计[第1章](ch01.html "第1章。欢迎来到Docker Swarm")的示例，*欢迎来到Docker
    Swarm*。作为新功能，集群将不再是扁平的，而是由1个主节点和3个从节点组成，并且每个节点将默认启用安全性。
- en: The master node will be the node exposing Swarm port `3376`. We'll connect specifically
    to it in order to be able to drive the entire cluster.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点将是暴露Swarm端口`3376`的节点。我们将专门连接到它，以便能够驱动整个集群。
- en: '![Re-architecting the example of Chapter 1 with token](images/B05661_02_01-1.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/B05661_02_01-1.jpg)'
- en: 'We can create 4 nodes with the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令创建4个节点：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, we have four machines running the last version of the Engine, with the
    TLS enabled. This means, as you remember, that the Engine is exposing port `2376`
    and not `2375`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有四台运行最新版本引擎的机器，启用了TLS。这意味着，正如你记得的那样，引擎正在暴露端口`2376`而不是`2375`。
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_002.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/image_02_002.jpg)'
- en: 'We will now create the cluster, starting from the master. Pick up one of the
    nodes, for example `node0`, and source its variables:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建集群，从主节点开始。选择其中一个节点，例如`node0`，并获取其变量：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We now generate the cluster token and the unique ID. For this purpose, we use
    the `swarm create` command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们生成集群令牌和唯一ID。为此，我们使用`swarm create`命令：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_003.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/image_02_003.jpg)'
- en: 'As a result, the swarm container outputs the token, and the protocol that we''ll
    be using in this example will be invoked as shown: `token://3b905f46fef903800d51513d51acbbbe`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，集群容器输出了令牌，并且在本示例中将调用所示的协议：`token://3b905f46fef903800d51513d51acbbbe`。
- en: 'Take note of this token ID, for example assigning it to a shell variable:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个令牌ID，例如将其分配给一个shell变量：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now create a master and try to meet, at least, some of the basic standard
    security requirements, how is, we'll enable TLS encryption. As we'll see in a
    moment, the `swarm` command accepts TLS options as arguments. But how do we pass
    keys and certificates to a container? For this, we'll use the certificates generated
    by Docker Machine and placed in `/var/lib/boot2docker` on the host.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个主节点，并尝试满足至少一些基本的标准安全要求，也就是说，我们将启用TLS加密。正如我们将在一会儿看到的，`swarm`命令接受TLS选项作为参数。但是我们如何将密钥和证书传递给容器呢？为此，我们将使用Docker
    Machine生成的证书，并将其放置在主机上的`/var/lib/boot2docker`中。
- en: In practice, we mount a volume from the Docker host to the container on the
    Docker host. All remotely and controlled thanks to the environment variables.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们将从Docker主机挂载一个卷到Docker主机上的容器。所有远程控制都依赖于环境变量。
- en: 'With the `node0` variables already sourced, we start the Swarm master with
    the following command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 已经获取了`node0`变量，我们使用以下命令启动Swarm主节点：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To begin, we run the container in an interactive mode to observe the swarm output.
    Then, we mount the node `/var/lib/boot2docker` directory to the `/certs` directory
    inside the swarm container. We redirect the `3376` Swarm secure ports from node0
    to the swarm container. We execute the `swarm` command in the manage mode by binding
    it to `0.0.0.0:3376`. Then we specify some certificates options and file paths
    and finally describe that the discovery service in use is token, with our token.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们以交互模式运行容器以观察Swarm输出。然后，我们将节点`/var/lib/boot2docker`目录挂载到Swarm容器内部的`/certs`目录。我们将`3376`
    Swarm安全端口从node0重定向到Swarm容器。我们通过将其绑定到`0.0.0.0:3376`来以管理模式执行`swarm`命令。然后，我们指定一些证书选项和文件路径，并最后描述使用的发现服务是令牌，带有我们的令牌。
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_004.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/image_02_004.jpg)'
- en: 'With this node running, let''s open another terminal and join a node to this
    Swarm. Let''s start by sourcing the `node1` variables. Now, we need swarm to use
    the `join` command, in order to join the cluster whose master is `node0`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个节点运行，让我们打开另一个终端并加入一个节点到这个Swarm。让我们首先源`node1`变量。现在，我们需要让Swarm使用`join`命令，以加入其主节点为`node0`的集群：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here we specify the host (itself) at address `192.168.99.101` to join the cluster.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定主机（自身）的地址为`192.168.99.101`以加入集群。
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_005.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/image_02_005.jpg)'
- en: If we jump back to the first terminal, we'll see that the master has noticed
    that a node has joined the cluster. So, at this point of time we have a Swarm
    cluster composed of one master and one slave.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们跳回第一个终端，我们会看到主节点已经注意到一个节点已经加入了集群。因此，此时我们有一个由一个主节点和一个从节点组成的Swarm集群。
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_006.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/image_02_006.jpg)'
- en: 'Since we now understand the mechanism, we can stop both the `docker` commands
    in the terminals and rerun them with the `-d` option. So, to run containers in
    daemon mode:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在理解了机制，我们可以在终端中停止`docker`命令，并使用`-d`选项重新运行它们。因此，要在守护程序模式下运行容器：
- en: '**Master**:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Node**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 节点：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will now proceed by joining the other two nodes to the cluster, source their
    variables, and repeat the last command as shown:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将继续将其他两个节点加入集群，源其变量，并重复上述命令，如下所示：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For example, if we open a third terminal, source the `node0` variables, and
    specifically connect to port `3376` (Swarm) instead of `2376` (Docker Engine),
    we can see some fancy output coming from the `docker info` command. For example,
    there are three nodes in a cluster:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们打开第三个终端，源`node0`变量，并且特别连接到端口`3376`（Swarm）而不是`2376`（Docker Engine），我们可以看到来自`docker
    info`命令的一些花哨的输出。例如，集群中有三个节点：
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_007.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/image_02_007.jpg)'
- en: So, we have created a cluster with one master, three slaves, and with TLS enabled
    and ready to accept containers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经创建了一个具有一个主节点、三个从节点的集群，并启用了TLS，准备接受容器。
- en: 'We can ensure that from the master and list the nodes in the cluster. We will
    now use the `swarm list` command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从主节点确保并列出集群中的节点。我们现在将使用`swarm list`命令：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_008.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![使用令牌重新设计第1章的示例](images/image_02_008.jpg)'
- en: Token limitations
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 令牌限制
- en: 'Token is not deprecated yet, but probably it will be deprecated very soon.
    The standard requirement that every node in the Swarm should have internet connectivity
    is not very convenient. Moreover, the access to the Docker Hub makes this technique
    depend on Hub availability. In practice, it has the Hub as a single point of failure.
    However, using token, we were able to understand what''s behind the scenes a little
    bit better and we met the Swarm v1 commands: `create`, `manage`, `join`, and `list`.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Token尚未被弃用，但很可能很快就会被弃用。Swarm中的每个节点都需要具有互联网连接的标准要求并不是很方便。此外，对Docker Hub的访问使得这种技术依赖于Hub的可用性。实际上，它将Hub作为单点故障。然而，使用token，我们能够更好地了解幕后情况，并且我们遇到了Swarm
    v1命令：`create`、`manage`、`join`和`list`。
- en: Now it's time to proceed further and get acquainted with real discovery services
    and the consensus algorithm, a cardinal principle in fault-tolerant systems.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候继续前进，熟悉真正的发现服务和共识算法，这是容错系统中的一个基本原则。
- en: Raft
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Raft
- en: Consensus is an algorithm in distributed systems that forces agents in the system
    to agree on consistent values and elect a leader.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 共识是分布式系统中的一种算法，它强制系统中的代理就一致的值达成一致意见并选举领导者。
- en: Some well-known consensus algorithms are Paxos and Raft. Paxos and Raft deliver
    similar performances but Raft is less complex, easier to understand, and therefore
    becoming very popular in distributed store implementations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些著名的共识算法是Paxos和Raft。Paxos和Raft提供了类似的性能，但Raft更简单，更易于理解，因此在分布式存储实现中变得非常流行。
- en: As consensus algorithms, Consul and Etcd implement Raft while ZooKeeper implements
    Paxos. The CoreOS Etcd Go library, implementing Raft, is included into SwarmKit
    and Swarm Mode as a dependency (in `vendor/`), so in this book we'll focus more
    on it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 作为共识算法，Consul和Etcd实现了Raft，而ZooKeeper实现了Paxos。CoreOS Etcd Go库作为SwarmKit和Swarm
    Mode的依赖项（在`vendor/`中），因此在本书中我们将更多地关注它。
- en: Raft is described in detail in the Ongaro, Ousterhout paper, and it is available
    at [https://ramcloud.stanford.edu/raft.pdf](https://ramcloud.stanford.edu/raft.pdf).
    In the upcoming section we'll summarize its basic concepts.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Raft在Ongaro、Ousterhout的论文中有详细描述，可在[https://ramcloud.stanford.edu/raft.pdf](https://ramcloud.stanford.edu/raft.pdf)上找到。在接下来的部分中，我们将总结其基本概念。
- en: Raft theory
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Raft理论
- en: Raft was designed with simplicity in mind and compared to Paxos, it truly achieves
    this goal (there are even academic publications demonstrating this). For our purpose,
    the main difference between Raft and Paxos is that in Raft, messages and logs
    are sent only by the cluster leader to its peers, making the algorithm more understandable
    and easier to implement. The sample library that we'll use, in the theory section,
    is the Go one delivered by CoreOS Etcd, available at [https://github.com/coreos/etcd/tree/master/raft](https://github.com/coreos/etcd/tree/master/raft).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Raft的设计初衷是简单，与Paxos相比，它确实实现了这一目标（甚至有学术出版物证明了这一点）。对于我们的目的，Raft和Paxos的主要区别在于，在Raft中，消息和日志只由集群领导者发送给其同行，使得算法更易于理解和实现。我们将在理论部分使用的示例库是由CoreOS
    Etcd提供的Go库，可在[https://github.com/coreos/etcd/tree/master/raft](https://github.com/coreos/etcd/tree/master/raft)上找到。
- en: 'A Raft cluster is made of nodes that must maintain a replicated state machine
    in a consistent manner, no matter what: new nodes can join, old nodes can crash
    or become unavailable, but this state machine must be kept in sync.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Raft集群由节点组成，这些节点必须以一致的方式维护复制状态机，无论如何：新节点可以加入，旧节点可以崩溃或变得不可用，但这个状态机必须保持同步。
- en: To achieve this failure-aware goal, typically Raft clusters consist of an odd
    number of nodes, such as three or five to avoid split-brains. A split-brain occurs
    when the remaining node(s) split themselves in groups that can't agree on a leader
    election. If there is an odd number of nodes, they can finally agree on a leader
    with a majority. With an even number, the election can close with a 50%-50% result,
    which should not happen.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个具有容错能力的目标，通常Raft集群由奇数个节点组成，例如三个或五个，以避免分裂脑。当剩下的节点分裂成无法就领导者选举达成一致的组时，就会发生分裂脑。如果节点数是奇数，它们最终可以以多数同意的方式选出领导者。而如果节点数是偶数，选举可能以50%-50%的结果结束，这是不应该发生的。
- en: Back to Raft, a Raft cluster is defined as a type raft struct in `raft.go` and
    includes information such as the leader UUID, the current term, a pointer to the
    log, and utilities to check the status of quorum and elections. Let's illustrate
    all these concepts step-by-step by decomposing the definition of the cluster component,
    a Node. A Node is defined as an interface in `node.go`, which is implemented canonically
    in this library as a `type node struct`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 回到Raft，Raft集群被定义为`raft.go`中的一种类型raft结构，并包括领导者UUID、当前任期、指向日志的指针以及用于检查法定人数和选举状态的实用程序。让我们通过逐步分解集群组件Node的定义来阐明所有这些概念。Node在`node.go`中被定义为一个接口，在这个库中被规范地实现为`type
    node struct`。
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Every node keeps a tick (incremented by `Tick()`), denoting the term or period
    of time or epoch, of an arbitrary length, which is the current running instant.
    At every term, a node can be in one of the following StateType:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都保持一个滴答（通过`Tick()`递增），表示任意长度的当前运行时期或时间段或时代。在每个时期，一个节点可以处于以下StateType之一：
- en: Leader
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领导者
- en: Candidate
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选者
- en: Follower
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 追随者
- en: Under normal conditions, there is only one leader and all other nodes are followers.
    The leader, in order to make us respect its authority, sends heartbeat messages
    to its followers at regular intervals. When followers note that heartbeat messages
    are not arriving any longer, they understand that the leader is not available
    anymore and therefore they increment their values and become candidates and then
    attempt to become a leader by running `Campaign()`. They start with voting for
    themselves and trying to reach a quorum. When a node achieves this, a new leader
    is elected.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常情况下，只有一个领导者，所有其他节点都是跟随者。领导者为了让我们尊重其权威，定期向其追随者发送心跳消息。当追随者注意到心跳消息不再到达时，他们会意识到领导者不再可用，因此他们会增加自己的值，成为候选者，然后尝试通过运行`Campaign()`来成为领导者。他们从为自己投票开始，试图达成选举法定人数。当一个节点实现了这一点，就会选举出一个新的领导者。
- en: '`Propose()` is a method of proposal to append data to the log. A log is the
    data structure used in Raft to synchronize the cluster status and it''s another
    crucial concept in Etcd. It''s saved in a stable storage (memory), which has the
    ability to compact the log when it becomes huge to save space (snapshotting).
    The leader ensures that log is always in a consistent state and commits new data
    to append to its log (a master-log) only when it''s sure that this information
    has been replicated through the majority of its followers, so there is an agreement.
    There is a `Step()` method, which advances the state machine to the next step.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`Propose()`是一种向日志附加数据的提案方法。日志是Raft中用于同步集群状态的数据结构，也是Etcd中的另一个关键概念。它保存在稳定存储（内存）中，当日志变得庞大时具有压缩日志以节省空间（快照）的能力。领导者确保日志始终处于一致状态，并且只有在确定信息已经通过大多数追随者复制时，才会提交新数据以附加到其日志（主日志）上，因此存在一致性。有一个`Step()`方法，它将状态机推进到下一步。'
- en: '`ProposeConfChange()` is a method that allows us to change the cluster configuration
    at runtime. It is demonstrated to be safe under any condition, thanks to its two-phase
    mechanism that ensures that there is an agreement on this change from every possible
    majority. `ApplyConfChange()`applies this change to the current node.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProposeConfChange()`是一个允许我们在运行时更改集群配置的方法。由于其两阶段机制，它被证明在任何情况下都是安全的，确保每个可能的多数都同意这一变更。`ApplyConfChange()`将此变更应用到当前节点。'
- en: Then there is `Ready()`. In the Node interface, this function returns a read-only
    channel that returns encapsulated specification of messages that are ready to
    be read, saved to storage, and committed. Usually, after invoking Ready and applying
    its entries, a client must call `Advance()`, to notify that a progress in Ready
    has been made. In practice, `Ready()` and `Advance()` are parts of the method
    through which Raft keeps a high level of coherency, by avoiding inconsistencies
    in the log, its content, and status synchronization.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是`Ready()`。在Node接口中，此函数返回一个只读通道，返回准备好被读取、保存到存储并提交的消息的封装规范。通常，在调用Ready并应用其条目后，客户端必须调用`Advance()`，以通知Ready已经取得进展。在实践中，`Ready()`和`Advance()`是Raft保持高一致性水平的方法的一部分，通过避免日志、内容和状态同步中的不一致性。
- en: This is how Raft implementation looks like in CoreOS' Etcd.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是CoreOS' Etcd中Raft实现的样子。
- en: Raft in practice
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Raft的实践
- en: If you want to put your hands and practice Raft, a good idea is to use the `raftexample`
    from Etcd and start a three-member cluster.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要亲自尝试Raft，一个好主意是使用Etcd中的`raftexample`并启动一个三成员集群。
- en: 'Since Docker Compose YAML files are self-describing, the following example
    is of a compose file ready to be run:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Docker Compose YAML文件是自描述的，以下示例是一个准备运行的组合文件：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This template creates three Raft services (`raftexample1`, `raftexample2`, and
    `raftexample3`). Each runs an instance of raftexample, by exposing the APIs with
    `--port` and using a static cluster configuration with `--cluster`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此模板创建了三个Raft服务（`raftexample1`，`raftexample2`和`raftexample3`）。每个都运行一个raftexample实例，通过`--port`公开API，并使用`--cluster`进行静态集群配置。
- en: 'You can start this on a Docker host with:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Docker主机上启动它：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you can play, for example by killing the leader, observe new elections,
    set some values via API to one of the containers, remove the container, update
    the value, restart the container, retrieve this value, and note that it was correctly
    upgraded.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以玩了，例如杀死领导者，观察新的选举，通过API向一个容器设置一些值，移除容器，更新该值，重新启动容器，检索该值，并注意到它已经正确升级。
- en: 'Interactions with the APIs can be done via curl, as described at [https://github.com/coreos/etcd/tree/master/contrib/raftexample](https://github.com/coreos/etcd/tree/master/contrib/raftexample):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与API的交互可以通过curl完成，如[https://github.com/coreos/etcd/tree/master/contrib/raftexample](https://github.com/coreos/etcd/tree/master/contrib/raftexample)中所述：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We leave this exercise to the more enthusiastic readers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个练习留给更热心的读者。
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: When you're trying to adopt a Raft implementation, choose Etcd's Raft library
    for highest performance and choose Consul (from Serf library) for ready-to-use
    and easier implementation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当您尝试采用Raft实现时，选择Etcd的Raft库以获得最高性能，并选择Consul（来自Serf库）以获得即插即用和更容易的实现。
- en: Etcd
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Etcd
- en: Etcd is a highly available, distributed, and consistent key-value store that
    is used for shared configuration and service discovery. Some notable projects
    that use Etcd are SwarmKit, Kubernetes, and Fleet.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd是一个高可用、分布式和一致的键值存储，用于共享配置和服务发现。一些使用Etcd的知名项目包括SwarmKit、Kubernetes和Fleet。
- en: Etcd can gracefully manage master elections in case of network splits and can
    tolerate node failure, including the master. Applications, in our case Docker
    containers and Swarm nodes, can read and write data into Etcd's key-value storage,
    for example the location of services.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd可以在网络分裂的情况下优雅地管理主节点选举，并且可以容忍节点故障，包括主节点。应用程序，例如Docker容器和Swarm节点，可以读取和写入Etcd的键值存储中的数据，例如服务的位置。
- en: Re architecting the example of Chapter 1 with Etcd
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新设计第1章的示例，使用Etcd
- en: We once again create an example with one manager and three nodes, this time
    by illustrating Etcd.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次通过演示Etcd来创建一个管理器和三个节点的示例。
- en: 'This time, we''ll need a real discovery service. We can simulate a non-HA system
    by running the Etcd server inside Docker itself. We create a cluster made of four
    hosts, with the following names:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将需要一个真正的发现服务。我们可以通过在Docker内部运行Etcd服务器来模拟非HA系统。我们创建了一个由四个主机组成的集群，名称如下：
- en: '`etcd-m` will be the Swarm master and will host also the Etcd server'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd-m`将是Swarm主节点，也将托管Etcd服务器'
- en: '`etcd-1`: The first Swarm node'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd-1`：第一个Swarm节点'
- en: '`etcd-2`: The second Swarm node'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd-2`：第二个Swarm节点'
- en: '`etcd-3`: The third Swarm node'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd-3`：第三个Swarm节点'
- en: The operator, by connecting to `etcd-m:3376`, will operate Swarm on the three
    nodes, as usual.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 操作员通过连接到`etcd-m:3376`，将像往常一样在三个节点上操作Swarm。
- en: 'Let''s start by creating the hosts with Machine:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用Machine创建主机开始：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we will run the Etcd master on `etcd-m`. We use the `quay.io/coreos/etcd`
    official image from CoreOS, following the documentation available at [https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在`etcd-m`上运行Etcd主节点。我们使用来自CoreOS的`quay.io/coreos/etcd`官方镜像，遵循[https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md)上可用的文档。
- en: 'First, in a terminal, we source the `etcd-m` shell variables:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在终端中，我们设置`etcd-m` shell变量：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we run the Etcd master in a single-host mode (that is, no fault-tolerance,
    and so on):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们以单主机模式运行Etcd主节点（即，没有容错等）：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'What we do here is start the Etcd image in the daemon (`-d`) mode and expose
    ports `2379` (Etcd client communication), `2380` (Etcd server communication),
    `4001` (), and specify the following Etcd options:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的是以守护进程（`-d`）模式启动Etcd镜像，并暴露端口`2379`（Etcd客户端通信）、`2380`（Etcd服务器通信）、`4001`（），并指定以下Etcd选项：
- en: '`name`: The name of the node, in this case we select etcd-m, as the name of
    the node hosting this container'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：节点的名称，在这种情况下，我们选择etcd-m作为托管此容器的节点的名称'
- en: '`initial-advertise-peer-urls` in this static configuration is the address:port
    of the cluster'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个静态配置中，`initial-advertise-peer-urls`是集群的地址:端口
- en: '`listen-peer-urls`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`listen-peer-urls`'
- en: '`listen-client-urls`'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`listen-client-urls`'
- en: '`advertise-client-urls`'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`advertise-client-urls`'
- en: '`initial-cluster-token`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial-cluster-token`'
- en: '`initial-cluster`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial-cluster`'
- en: '`initial-cluster-state`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initial-cluster-state`'
- en: 'We can ensure that this one-node Etcd cluster is healthy, using the `etcdctl
    cluster-health` command-line utility:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`etcdctl cluster-health`命令行实用程序确保这个单节点Etcd集群是健康的：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Re architecting the example of Chapter 1 with Etcd](images/image_02_009.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![重新设计第1章的示例，使用Etcd](images/image_02_009.jpg)'
- en: This indicates that Etcd is at least up and running, so we can use it to set
    up a Swarm v1 cluster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明Etcd至少已经启动运行，因此我们可以使用它来设置Swarm v1集群。
- en: 'We create the Swarm manager on the same `etcd-m` host:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在同一台`etcd-m`主机上创建Swarm管理器：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This exposes the usual `3376` port from host to container, but this time starts
    the manager using the `etcd://` URL for the discovery service.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从主机到容器暴露通常的`3376`端口，但这次使用`etcd://` URL启动管理器以进行发现服务。
- en: We now join the nodes, `etcd-1`, `etcd-2`, and `etcd-3`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们加入节点，`etcd-1`，`etcd-2`和`etcd-3`。
- en: 'As usual, we can source and command machines, one, for each terminal:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们可以为每个终端提供源和命令机器：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With join `-advertise`, we order the local node to join the Swarm cluster, using
    the Etcd service running and exposed on `etcd-m`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`-advertise`加入本地节点到Swarm集群，使用运行并暴露在`etcd-m`上的Etcd服务。
- en: 'We now go to `etcd-m` and see the nodes of our cluster, by invoking the Etcd
    discovery service:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转到`etcd-m`并通过调用Etcd发现服务来查看我们集群的节点：
- en: '![Re architecting the example of Chapter 1 with Etcd](images/image_02_010.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![使用Etcd重新架构第1章的示例](images/image_02_010.jpg)'
- en: We have the three hosts already joined to the cluster as expected.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经如预期那样将三个主机加入了集群。
- en: ZooKeeper
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ZooKeeper
- en: ZooKeeper is another widely used and high-performance coordination service for
    distributed applications. Apache ZooKeeper was originally a subproject of Hadoop
    but is now a top-level project. It is a highly consistent, scalable, and reliable
    key-value store that can be used as a discovery service for a Docker Swarm v1
    cluster. As mentioned previously, ZooKeeper uses Paxos, rather than Raft.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ZooKeeper是另一个广泛使用且高性能的分布式应用协调服务。Apache ZooKeeper最初是Hadoop的一个子项目，但现在是一个顶级项目。它是一个高度一致、可扩展和可靠的键值存储，可用作Docker
    Swarm v1集群的发现服务。如前所述，ZooKeeper使用Paxos而不是Raft。
- en: Similar to Etcd, when ZooKeeper forms a nodes cluster with a quorum, it has
    one leader and the remaining nodes are followers. Internally, ZooKeeper uses its
    own ZAB, ZooKeeper Broadcasting Protocol, to maintain consistency and integrity.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与Etcd类似，当ZooKeeper与法定人数形成节点集群时，它有一个领导者和其余的节点是跟随者。在内部，ZooKeeper使用自己的ZAB，即ZooKeeper广播协议，来维护一致性和完整性。
- en: Consul
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Consul
- en: The last discovery service we're going to see here is Consul, a tool for discovering
    and configuring services. It provides an API that allows clients to register and
    discover services. Similar to Etcd and ZooKeeper, Consul is a key-value store
    with a REST API. It can perform health checks to determine service availability
    and uses the Raft consensus algorithm via the Serf library. Similar to Etcd and
    Zookeeper, of course, Consul can form a high availability quorum with leader election.
    Its member management system is based on `memberlist`, an efficient Gossip protocol
    implementation.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里看到的最后一个发现服务是Consul，这是一个用于发现和配置服务的工具。它提供了一个API，允许客户端注册和发现服务。与Etcd和ZooKeeper类似，Consul是一个带有REST
    API的键值存储。它可以执行健康检查以确定服务的可用性，并通过Serf库使用Raft一致性算法。当然，与Etcd和ZooKeeper类似，Consul可以形成具有领导者选举的高可用性法定人数。其成员管理系统基于`memberlist`，这是一个高效的Gossip协议实现。
- en: Re architecting the example of Chapter 1 with Consul
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Consul重新架构第1章的示例
- en: We will now create another Swarm v1, but in this section we create machines
    on a cloud provider, DigitalOcean. To do so, you need an access token. However,
    if you don't have a DigitalOcean account, you can replace `--driver digitalocean`
    with `--driver virtualbox` and run this example locally.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建另一个Swarm v1，但在本节中，我们将在云提供商DigitalOcean上创建机器。为此，您需要一个访问令牌。但是，如果您没有DigitalOcean帐户，可以将`--driver
    digitalocean`替换为`--driver virtualbox`并在本地运行此示例。
- en: 'Let''s start by creating the Consul master:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建Consul主节点开始：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We start the first agent here. Although we call it an agent, we are actually
    going to run it in the Server mode. We use the server mode (`-server`) and make
    it into the bootstrap node (`-bootstrap`). With these options, Consul will not
    perform the leader selection as it will force itself to be the leader.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里启动第一个代理。虽然我们称它为代理，但实际上我们将以服务器模式运行它。我们使用服务器模式（`-server`）并将其设置为引导节点（`-bootstrap`）。使用这些选项，Consul将不执行领导者选择，因为它将强制自己成为领导者。
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In case of a quorum for HA, the second and the third must be start with `-botstrap-expect
    3` to allow them to form a high availability cluster.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在HA的情况下，第二个和第三个节点必须以“-botstrap-expect 3”开头，以允许它们形成一个高可用性集群。
- en: Now, we can use `curl`  command  to test if our Consul quorum started successfully.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`curl`命令来测试我们的Consul quorum是否成功启动。
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If it's silent without showing any error, then Consul works correctly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有显示任何错误，那么Consul就正常工作了。
- en: Next, we're going to create another three nodes on DigitalOcean.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在DigitalOcean上创建另外三个节点。
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s start the master and use Consul as a discovery mechanism:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动主节点并使用Consul作为发现机制：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here''s what we get when running the `swarm list` command: All nodes joined
    the Swarm, so the example is running.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`swarm list`命令时，我们得到的结果是：所有节点都加入了Swarm，所以示例正在运行。
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Towards a decentralized discovery service
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 走向去中心化的发现服务
- en: The limitation of a Swarm v1 architecture is that it uses a centralized and
    external discovery service. This approach makes every agent to talk to the external
    discovery service and the discovery service servers may see their load growing
    exponentially. From our experiments, for a 500-node cluster, we recommend to form
    an HA discovery service with at least three machines with medium-high specification,
    say 8 cores with 8 GB of RAM.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm v1架构的局限性在于它使用了集中式和外部的发现服务。这种方法使每个代理都要与外部发现服务进行通信，而发现服务服务器可能会看到它们的负载呈指数级增长。根据我们的实验，对于一个500节点的集群，我们建议至少使用三台中高规格的机器来形成一个HA发现服务，比如8核8GB的内存。
- en: To properly address this problem, the discovery service used by SwarmKit and
    by Swarm Mode has been designed with decentralization in mind. Swarm mode uses
    the same discovery service codebase, Etcd, on all nodes, with no single point
    of failure.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确解决这个问题，SwarmKit和Swarm Mode使用的发现服务是以去中心化为设计理念的。Swarm Mode在所有节点上都使用相同的发现服务代码库Etcd，没有单点故障。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we got familiar with the concept of consensus and discovery
    service. We understood that they play an essential role in orchestration clusters,
    as they provide services such as fault-tolerance and safe configurations. We analyzed
    a consensus algorithm, such as Raft in detail, before looking to two concrete
    Raft discovery services implementations, Etcd and Consul, putting things in practice
    and re-architecting basic examples with them. In the next chapter we're now going
    to start exploring SwarmKit and Swarm that use the embedded Etcd library.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们熟悉了共识和发现服务的概念。我们了解到它们在编排集群中扮演着至关重要的角色，因为它们提供了容错和安全配置等服务。在详细分析了Raft等共识算法之后，我们看了两种具体的Raft发现服务实现，Etcd和Consul，并将它们应用到基本示例中进行了重新架构。在下一章中，我们将开始探索使用嵌入式Etcd库的SwarmKit和Swarm。
