- en: Topic Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: In the last chapter, we converted unstructured text data into a numerical format
    using the bag-of-words model. This model abstracts from word order and represents
    documents as word vectors, where each entry represents the relevance of a token
    to the document.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用词袋模型将非结构化文本数据转换为数值格式。该模型抽象出了词序，并将文档表示为单词向量，其中每个条目表示标记与文档的相关性。
- en: The resulting **document-term matrix** (**DTM**), (you may also come across
    the transposed term-document matrix) is useful to compare documents to each other
    or to a query vector based on their token content, and quickly find a needle in
    a haystack or classify documents accordingly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由此产生的**文档-术语矩阵**（**DTM**）（您可能还会遇到转置的术语-文档矩阵）可用于根据其标记内容将文档相互比较或与查询向量进行比较，并快速找到草堆中的针或相应地对文档进行分类。
- en: However, this document model is both high-dimensional and very sparse. As a
    result, it does little to summarize the content or get closer to understanding
    what it is about. In this chapter, we will use unsupervised machine learning in
    the form of topic modeling to extract hidden themes from documents. These themes
    can produce detailed insights into a large body of documents in an automated way.
    They are very useful to understand the haystack itself and permit the concise
    tagging of documents because using the degree of association of topics and documents.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种文档模型既高维又非常稀疏。因此，它很少总结内容或更接近理解其内容。在本章中，我们将使用无监督机器学习的主题建模形式来从文档中提取隐藏的主题。这些主题可以以自动化的方式产生对大量文档的详细见解。它们非常有用，可以帮助理解文档本身，并允许对文档进行简洁的标记，因为可以使用主题和文档的关联程度。
- en: Topic models permit the extraction of sophisticated, interpretable text features
    that can be used in various ways to extract trading signals from large collections
    of documents. They speed up the review of documents, help identify and cluster
    similar documents, and can be annotated as a basis for predictive modeling. Applications
    include the identification of key themes in company disclosures, or earnings call
    transcripts, customer reviews or contracts, annotated using, for example, sentiment
    analysis or direct labeling with subsequent asset returns.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型允许提取复杂的、可解释的文本特征，可以以各种方式用于从大量文档中提取交易信号。它们加快了文档的审阅，有助于识别和聚类相似的文档，并可以作为预测建模的基础进行注释。应用包括在公司披露、收益电话成绩单、客户评论或合同中识别关键主题，并使用情感分析或直接标记进行注释，随后进行资产回报。
- en: 'More specifically, in this chapter, we will cover these topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，在本章中，我们将涵盖以下主题：
- en: What topic modeling achieves, why it matters, and how it has evolved
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模的实现，其重要性以及其发展历程
- en: How **Latent Semantic Indexing** (**LSI**) reduces the dimensionality of the
    DTM
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在语义索引**（**LSI**）如何降低DTM的维度'
- en: How **probabilistic Latent Semantic Analysis** (**pLSA**) uses a generative
    model to extract topics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率潜在语义分析**（**pLSA**）如何使用生成模型提取主题'
- en: How **Latent Dirichlet Allocation** (**LDA**) refines pLSA and why it is the
    most popular topic model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）如何改进pLSA以及为什么它是最流行的主题模型。'
- en: How to visualize and evaluate topic modeling results
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化和评估主题建模结果
- en: How to implement LDA using sklearn and gensim
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用sklearn和gensim实现LDA
- en: How to apply topic modeling to collections of earnings calls and Yelp business
    reviews
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将主题建模应用于收益电话和Yelp商业评论的集合
- en: The code samples for the following sections are in the directory of the GitHub
    repository for this chapter, and references are listed in the main README file.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节的代码示例位于本章的GitHub存储库目录中，并在主README文件中列出了参考资料。
- en: 'Learning latent topics: goals and approaches'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习潜在主题：目标和方法
- en: Topic modeling aims to discover hidden topics or themes across documents that
    capture semantic information beyond individual words. It aims to address a key
    challenge in building a machine learning algorithm that learns from text data
    by going beyond the lexical level of what has been written to the semantic level
    of what was intended. The resulting topics can be used to annotate documents based
    on their association with various topics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模旨在发现跨文档的隐藏主题或主题，捕捉超越单个词的语义信息。它旨在解决从文本数据中学习的机器学习算法的一个关键挑战，即超越所写内容的词汇级别，到所想内容的语义级别。由此产生的主题可用于根据其与各种主题的关联来注释文档。
- en: In other words, topic modeling aims to automatically summarize large collections
    of documents to facilitate organization and management, as well as search and
    recommendations. At the same time, it can enable the understanding of documents
    to the extent that humans can interpret the descriptions of topics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，主题建模旨在自动总结大量文档，以便进行组织和管理，以及搜索和推荐。同时，它可以使人类能够理解文档的程度，以便解释主题的描述。
- en: Topic models aim to address the curse of dimensionality that can plague the
    bag-of-words model. The document representation based on high-dimensional sparse
    vectors can make similarity measures noisy, leading to inaccurate distance measurement
    and overfitting of text classification models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型旨在解决可能困扰词袋模型的维度诅咒。基于高维稀疏向量的文档表示可能会使相似性度量变得嘈杂，导致不准确的距离测量和文本分类模型的过度拟合。
- en: Moreover, the bag of words model ignores word order and loses context as well
    as semantic information because it is not able to capture synonymy (several words
    have the same meaning) and polysemy (one word has several meanings). As a result,
    document retrieval or similarity search may miss the point when the documents
    are not indexed by the terms used to search or compare.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，词袋模型忽略了词序并丢失了上下文以及语义信息，因为它无法捕捉同义词（几个词具有相同的含义）和多义词（一个词有几个含义）。因此，当文档未按用于搜索或比较的术语进行索引时，文档检索或相似性搜索可能会失去意义。
- en: 'These shortcoming prompt this question: how do we model and learn meaning topics
    that facilitate a more productive interaction with text data?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缺陷引发了这个问题：我们如何建模和学习有助于更有效地与文本数据交互的有意义的主题？
- en: From linear algebra to hierarchical probabilistic models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从线性代数到分层概率模型
- en: Initial attempts by topic models to improve on the vector space model (developed
    in the mid-1970s) applied linear algebra to reduce the dimensionality of the document-term
    matrix. This approach is similar to the algorithm we discussed as principal component
    analysis in [Chapter 12](c187906e-9fde-4f85-b709-df88dd0f7e88.xhtml), *Unsupervised
    Learning*, on unsupervised learning. While effective, it is difficult to evaluate
    the results of these models absent a benchmark model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 初始尝试通过主题模型改进向量空间模型（在20世纪70年代中期开发）应用线性代数来降低文档-术语矩阵的维度。这种方法类似于我们在《第12章》中讨论的主成分分析算法，即无监督学习。虽然有效，但在缺乏基准模型的情况下难以评估这些模型的结果。
- en: In response, probabilistic models emerged that assume an explicit document generation
    process and provide algorithms to reverse engineer this process and recover the
    underlying topics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回应，出现了概率模型，假设存在明确的文档生成过程，并提供算法来反向工程这个过程并恢复潜在的主题。
- en: 'This table highlights key milestones in the model evolution that we will address
    in more detail in the following sections:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这张表突出了模型演变中的关键里程碑，我们将在接下来的章节中更详细地讨论。
- en: '| **Model** | **Year** | **Description** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **年份** | **描述** |'
- en: '| **Latent Semantic Indexing** (**LSI**) | 1988 | Reduces the word space dimensionality
    to capture semantic document-term relationships by  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: 潜在语义索引（LSI）（1988）将词空间的维度降低，以捕捉语义文档-术语关系
- en: '| **Probabilistic Latent Semantic Analysis** (**pLSA**) | 1999 | Reverse-engineers
    a process that assumes words generate a topic and documents are a mix of topics
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **概率潜在语义分析**（pLSA）| 1999 | 反向工程一个假设词生成主题，文档是主题的混合的过程'
- en: '| **Latent Dirichlet Allocation** (**LDA**) | 2003 | Adds a generative process
    for documents: a three-level hierarchical Bayesian model |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **潜在狄利克雷分配**（LDA）| 2003 | 为文档添加一个生成过程：一个三级分层贝叶斯模型'
- en: Latent semantic indexing
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在语义索引
- en: Latent Semantic Indexing (LSI, also called Latent Semantic Analysis) sets out
    to improve the results of queries that omitted relevant documents containing synonyms
    of query terms. It aims to model the relationships between documents and terms
    to be able to predict that a term should be associated with a document, even though,
    because of variability in word use, no such association was observed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在语义索引（LSI，也称为潜在语义分析）旨在改善省略包含查询词的同义词的相关文档的查询结果。它旨在建模文档和术语之间的关系，以便能够预测术语应与文档相关联，即使由于词语使用的变化，没有观察到这样的关联。
- en: LSI uses linear algebra to find a given number, *k*, of latent topics by decomposing
    the DTM. More specifically, it uses **Singular Value Decomposition** (**SVD**)
    to find the best lower-rank DTM approximation using k singular values and vectors.
    In other words, LSI is an application of the unsupervised learning techniques
    of dimensionality reduction we encountered in [Chapter 12](c187906e-9fde-4f85-b709-df88dd0f7e88.xhtml), *Unsupervised
    Learning* to the text representation that we covered in [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml), *Working
    with Text Data*. The authors experimented with hierarchical clustering but found
    it too restrictive to explicitly model the document-topic and topic-term relationships,
    or capture associations of documents or terms with several topics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LSI使用线性代数来通过分解DTM找到给定数量*k*的潜在主题。更具体地说，它使用奇异值分解（SVD）来找到使用k个奇异值和向量的最佳低秩DTM近似。换句话说，LSI是我们在《第12章》中遇到的无监督学习技术的应用，用于我们在《第13章》中涵盖的文本表示。作者尝试了分层聚类，但发现它过于限制，无法明确地对文档-主题和主题-术语关系进行建模，或者捕捉文档或术语与多个主题的关联。
- en: In this context, SVD serves the purpose of identifying a set of uncorrelated
    indexing variables or factors that permit us to represent each term and document
    by its vector of factor values.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，SVD的作用是识别一组不相关的索引变量或因子，使我们能够通过其因子值向量来表示每个术语和文档。
- en: The following figure illustrates how SVD decomposes the DTM into three matrices,
    two containing orthogonal singular vectors and a diagonal matrix with singular
    values that serve as scaling factors. Assuming some correlation in the original
    data, singular values decay in value so that selecting only the largest *T* singular
    values produces a lower-dimensional approximation of the original DTM that loses
    relatively little information. Hence, in the reduced version the rows or columns
    that had *N* items only have *T<N* entries.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示了SVD如何将DTM分解为三个矩阵，其中两个包含正交奇异向量，一个包含奇异值的对角矩阵，这些奇异值作为缩放因子。假设原始数据中存在一些相关性，奇异值会衰减，因此仅选择最大的*T*奇异值会产生原始DTM的低维近似，且丢失相对较少的信息。因此，在降维版本中，原先有*N*个项目的行或列只有*T<N*个条目。
- en: 'This reduced decomposition can be interpreted as illustrated next, where the
    first *M x T* matrix represents the relationships between documents and topics,
    the diagonal matrix scales the topics by their corpus strength, and the third
    matrix models the term-topic relationship:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种降维分解可以解释为下图所示，其中第一个*M x T*矩阵表示文档和主题之间的关系，对角矩阵按其语料库强度缩放主题，第三个矩阵建模术语-主题关系。
- en: '![](img/4db67058-1bed-4104-8f63-b7d8e3b16fbe.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4db67058-1bed-4104-8f63-b7d8e3b16fbe.png)'
- en: The rows of the matrix that results from the product of the first two matrices, *U[T]Σ[T]*[,]corresponds
    to the locations of the original documents projected into the latent topic space.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由第一两个矩阵的乘积得到的矩阵的行，*U[T]Σ[T]*，对应于投影到潜在主题空间中的原始文档的位置。
- en: How to implement LSI using sklearn
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现LSI
- en: 'We will illustrate the application of LSI using the BBC article data that we
    introduced in the last chapter because it is small enough to permit quick training
    and allow us to compare topic assignments to category labels. See the `latent_semantic_indexing` notebook for
    additional implementation details:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一章介绍的BBC文章数据来说明LSI的应用，因为它足够小，可以快速训练，并且允许我们将主题分配与类别标签进行比较。有关其他实施细节，请参见`latent_semantic_indexing`笔记本：
- en: We begin by loading the documents and creating a train and (stratified) test
    set with 50 articles.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载文档，并创建一个包含50篇文章的训练和（分层）测试集。
- en: 'Then, we vectorize the data using `TfidfVectorizer` to obtain weighted DTM
    counts and filter out words that appear in less than 1% or more than 25% of the
    documents, as well as generic stopwords, to obtain a vocabulary of around 2,900
    words:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`TfidfVectorizer`对数据进行向量化，以获得加权的DTM计数，并过滤掉出现在不到1%或超过25%的文档中的单词，以及通用的停用词，以获得大约2900个单词的词汇表：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use `sklearn`'s `TruncatedSVD` class, which only computes the *k* largest
    singular values to reduce the dimensionality of the document-term matrix. The
    deterministic arpack algorithm delivers an exact solution, but the default randomized
    implementation is more efficient for large matrices.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`sklearn`的`TruncatedSVD`类，它只计算* k *个最大的奇异值，以减少文档-术语矩阵的维度。确定性arpack算法提供了一个精确的解决方案，但默认的随机实现对于大矩阵更有效。
- en: 'We compute five topics to match the five categories, which explain only 5.4%
    of the total DTM variance so higher values would be reasonable:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算五个主题以匹配五个类别，这仅解释了总DTM方差的5.4%，因此更高的值是合理的：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: LSI identifies a new orthogonal basis for the document-term matrix that reduces
    the rank to the number of desired topics.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSI识别文档-术语矩阵的新正交基，将秩降低到所需主题的数量。
- en: 'The `.transform()` method of the trained `svd` object projects the documents
    into the new topic space that is the result of reducing the dimensionality of
    the document vectors and corresponds to the *U[T]Σ[T]* transformation illustrated
    before:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后的`svd`对象的`.transform()`方法将文档投影到新的主题空间，这是通过减少文档向量的维度得到的结果，并对应于之前所示的*U[T]Σ[T]*变换：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can sample an article to view its location in the topic space. We draw a
    `Politics` article that is most (positively) associated with topics 1 and 2:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以对一篇文章进行抽样，以查看其在主题空间中的位置。我们选择了一个与主题1和2最相关（积极）的`Politics`文章：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The topic assignments for this sample align with the average topic weights for
    each category illustrated next (`Politics` is the leftmost). They illustrate how
    LSI expresses the k topics as directions in a k-dimensional space (the notebook
    includes a projection of the average topic assignments per category into two-dimensional
    space).
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此样本的主题分配与每个类别的平均主题权重一致，下面是示例（`Politics`是最左边的）。它们说明了LSI如何将k个主题表达为k维空间中的方向（笔记本包括每个类别的平均主题分配投影到二维空间）。
- en: 'Each category is clearly defined, and the test assignments match with train
    assignments. However, the weights are both positive and negative, making it more
    difficult to interpret the topics:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个类别都有明确定义，并且测试分配与训练分配相匹配。然而，权重既有正数又有负数，这使得主题更难以解释：
- en: '![](img/7cf3ba3c-4536-43d5-b1cd-c06c097c0fd3.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cf3ba3c-4536-43d5-b1cd-c06c097c0fd3.png)'
- en: 'We can also display the words that are most closely associated with each topic
    (in absolute terms). The topics appear to capture some semantic information but
    are not differentiated:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以显示与每个主题最相关的单词（绝对值）。主题似乎捕捉了一些语义信息，但没有区分：
- en: '![](img/e4d2bd25-5950-44fa-ba6c-6e3d6f7a45d0.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4d2bd25-5950-44fa-ba6c-6e3d6f7a45d0.png)'
- en: Pros and cons
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优缺点
- en: The benefits of LSI include the removal of noise and mitigation of the curse
    of dimensionality, while also capturing some semantics and  clustering both documents
    and terms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LSI的好处包括消除噪音和减轻维度灾难，同时捕捉一些语义并对文档和术语进行聚类。
- en: However, the results of LSI are difficult to interpret because topics are word
    vectors with both positive and negative entries. There is also no underlying model
    that would permit the evaluation of fit and provide guidance when selecting the
    number of dimensions or topics.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LSI的结果很难解释，因为主题是具有正负条目的词向量。还没有基础模型可以允许拟合的评估，并在选择维度或主题数量时提供指导。
- en: Probabilistic latent semantic analysis
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率潜在语义分析
- en: '**Probabilistic Latent Semantic Analysis (pLSA)** takes a statistical perspective
    on LSA and creates a generative model to address the lack of theoretical underpinnings
    of LSA.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率潜在语义分析（pLSA）**以统计角度看待LSA，并创建一个生成模型来解决LSA理论基础的缺乏。'
- en: pLSA explicitly models the probability each co-occurrence of documents *d* and
    words *w* described by the DTM as a mixture of conditionally independent multinomial
    distributions that involve topics *t*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: pLSA明确地对LSA中描述的文档* d *和单词* w *的每个共现的概率进行建模，作为涉及主题* t *的条件独立多项分布的混合物。
- en: 'The symmetric formulation of this generative process of word-document co-occurrences
    assumes both words and documents are generated by the latent topic class, whereas
    the asymmetric model assumes the topics are selected given the document, and words
    result from a second step given the topic:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个词-文档共现的生成过程的对称形式假设单词和文档都是由潜在主题类生成的，而不对称模型假设在给定文档的情况下选择主题，并且单词是在给定主题的情况下的第二步结果：
- en: '![](img/cfcdc229-8283-4358-b14a-6f50e483799c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfcdc229-8283-4358-b14a-6f50e483799c.png)'
- en: The number of topics is a hyperparameter chosen before training and is not learned
    from the data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 主题数量是在训练之前选择的超参数，并不是从数据中学习的。
- en: 'Probabilistic models often use the following plate notation to express dependencies.
    The following figure encodes the relationships just describe for the asymmetric
    model. Each rectangle represents multiple items, such as M Documents for the outer
    and N Words for each document for the inner block. We only observe the documents
    and their content, and the model infers the hidden or latent topic distribution:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 概率模型通常使用以下盘符表示来表达依赖关系。下图编码了刚刚描述的不对称模型的关系。每个矩形代表多个项目，例如外部的M个文档和内部块的每个文档的N个单词。我们只观察文档及其内容，模型推断隐藏或潜在的主题分布：
- en: '![](img/f1ce787d-33f0-4353-9732-b47427cca98e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1ce787d-33f0-4353-9732-b47427cca98e.png)'
- en: The benefit of using a probability model is that we can now compare models by
    evaluating the probability they assign to new documents given the parameters learned
    during training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用概率模型的好处是，我们现在可以通过评估它们在训练期间学习的参数给出的概率来比较模型对新文档的概率。
- en: How to implement pLSA using sklearn
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现pLSA
- en: pLSA is equivalent to non-negative matrix factorization using a Kullback-Leibler
    Divergence objective (see references on GitHub [https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading)).
    Hence, we can use the `sklearn.decomposition.NM` class to implement this model,
    following the LSA example.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: pLSA等价于使用Kullback-Leibler散度目标的非负矩阵分解（请参阅GitHub上的参考资料[https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading](https://github.com/PacktPublishing/Hands-On-Machine-Learning-for-Algorithmic-Trading)）。因此，我们可以使用`sklearn.decomposition.NM`类来实现这个模型，按照LSA示例。
- en: 'Using the same train-test split of the DTM produced by the `TfidfVectorizer`,
    we fit pLSA as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`TfidfVectorizer`生成的DTM的相同训练-测试拆分，我们按以下方式拟合pLSA：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get a measure of the reconstruction error, which is a substitute for the
    explained variance measure from before:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到重构误差的度量，这是之前解释方差度量的替代物：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Due to its probabilistic nature, pLSA produces only positive topic weights
    that result in more straightforward topic-category relationships for the test
    and training sets:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其概率性质，pLSA只产生正的主题权重，从而为测试和训练集产生更直接的主题-类别关系：
- en: '![](img/a505d985-d9a8-407c-be17-56937de08105.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a505d985-d9a8-407c-be17-56937de08105.png)'
- en: 'We can also see that the word lists that describe each topic begin to make
    more sense; for example, the Entertainment category is most directly associated
    with Topic 4, which includes the words film, start, and so on:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到描述每个主题的词列表开始变得更有意义；例如，娱乐类别最直接与主题4相关联，其中包括电影、开始等词：
- en: '![](img/e6c6fb32-72be-4dba-b9c1-522ad7f38eab.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6c6fb32-72be-4dba-b9c1-522ad7f38eab.png)'
- en: Latent Dirichlet allocation
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: '**Latent Dirichlet allocation** (**LDA**) extends pLSA by adding a generative
    process for topics.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（LDA）通过添加主题的生成过程扩展了pLSA。'
- en: It is the most popular topic model because it tends to produce meaningful topics
    that humans can relate to, can assign topics to new documents, and is extensible.
    Variants of LDA models can include metadata such as authors, or image data, or
    learn hierarchical topics.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最流行的主题模型，因为它倾向于产生人类可以相关联的有意义的主题，可以为新文档分配主题，并且是可扩展的。LDA模型的变体可以包括作者等元数据，或图像数据，或学习分层主题。
- en: How LDA works
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA的工作原理
- en: LDA is a hierarchical Bayesian model that assumes topics are probability distributions
    over words, and documents are distributions over topics. More specifically, the
    model assumes that topics follow a sparse Dirichlet distribution, which implies
    that documents cover only a small set of topics, and topics use only a small set
    of words frequently.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一个层次贝叶斯模型，假设主题是词的概率分布，文档是主题的分布。更具体地说，该模型假设主题遵循稀疏狄利克雷分布，这意味着文档只涵盖少量主题，主题只使用少量词频繁。
- en: The Dirichlet distribution
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 狄利克雷分布
- en: 'The Dirichlet distribution produces probability vectors that can be used with
    discrete distributions. That is, it randomly generates a given number of values
    that are positive and sum to one as expected for probabilities. It has a parameter
    of positive, real value that controls the concentration of the probabilities.
    Values closer to zero mean that only a few values will be positive and receive
    most probability mass. The following screenshot illustrates three draws of size
    10 for α *= 0.1* (the `dirichlet_distribution` notebook contains a simulation
    so you can experiment with different parameter values):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 狄利克雷分布产生可以与离散分布一起使用的概率向量。也就是说，它随机生成一定数量的值，这些值是正的并且总和为1，符合概率的预期。它有一个正实值参数，控制概率的集中度。接近零的值意味着只有少数值将是正的并且接收大部分概率质量。下面的屏幕截图展示了α
    *= 0.1*的大小为10的三次抽样（`dirichlet_distribution`笔记本包含一个模拟，您可以尝试不同的参数值）：
- en: '![](img/a3806199-b3dc-4b20-83a0-aac2e412bbf8.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3806199-b3dc-4b20-83a0-aac2e412bbf8.png)'
- en: Dirichlet allocation
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 狄利克雷分配
- en: The generative model
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型
- en: 'The Dirichlet distribution figures prominently in the LDA topic model, which
    assumes the following generative process when an author adds an article to a body
    of documents:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 狄利克雷分布在LDA主题模型中占据重要地位，它假设作者在向文档集添加文章时遵循以下生成过程：
- en: Randomly mix a small subset of shared topics *K* according to the topic probabilities
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据主题概率随机混合一小部分共享主题*K*
- en: For each word, select one of the topics according to the document-topic probabilities
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个词，根据文档-主题概率选择一个主题
- en: Select a word from the topic's word list according to the topic-word probabilities
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据主题-词概率从主题的词列表中选择一个词
- en: As a result, the article content depends on the weights of each topic and on
    the terms that make up each topic. The Dirichlet distribution governs the selection
    of topics for documents and words for topics and encodes the idea that a document
    only covers a few topics, while each topic uses only a small number of words frequently.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，文章内容取决于每个主题的权重以及构成每个主题的术语。Dirichlet分布控制了文档的主题选择和主题的单词，并编码了一个文档只涵盖少数主题，而每个主题只使用少量频繁的单词的想法。
- en: 'The plate notation for the LDA model summarizes these relationships:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: LDA模型的盘符符号总结了这些关系：
- en: '![](img/4e8db5eb-6a61-4ea6-9223-43cffa093e01.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e8db5eb-6a61-4ea6-9223-43cffa093e01.png)'
- en: Reverse-engineering the process
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向工程过程
- en: 'The generative process is fictional but turns out to be useful because it permits
    the recovery of the various distributions. The LDA algorithm reverse-engineers
    the work of the imaginary author and arrives at a summary of the document-topic-word
    relationships that concisely describes the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生成过程是虚构的，但事实证明是有用的，因为它允许恢复各种分布。LDA算法对虚构作者的工作进行了反向工程，并得出了对文档-主题-单词关系的摘要，简要描述了以下内容：
- en: The percentage contribution of each topic to a document
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主题对文档的百分比贡献
- en: The probabilistic association of each word with a topic
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词与主题的概率关联
- en: LDA solves the Bayesian inference problem of recovering the distributions from
    the body of documents and the words they contain by reverse-engineering the assumed
    content generation process. The original paper uses **variational Bayes** (**VB**)
    to approximate the posterior distribution. Alternatives include Gibbs sampling
    and expectation propagation. Later, we will illustrate implementations using the
    sklearn and gensim libraries.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LDA通过反向工程假定的内容生成过程来解决从文档集合和它们包含的单词中恢复分布的贝叶斯推断问题。原始论文使用**变分贝叶斯**（**VB**）来近似后验分布。替代方法包括吉布斯抽样和期望传播。稍后，我们将说明使用sklearn和gensim库的实现。
- en: How to evaluate LDA topics
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估LDA主题
- en: Unsupervised topic models do not provide a guarantee that the result will be
    meaningful or interpretable, and there is no objective metric to assess the result
    as in supervised learning. Human topic evaluation is considered the gold standard
    but is potentially expensive and not readily available at scale.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督主题模型不能保证结果是有意义或可解释的，并且没有客观的度量来评估结果，就像在监督学习中那样。人工主题评估被认为是金标准，但可能昂贵，并且不容易大规模获得。
- en: Two options to evaluate results more objectively include perplexity, which evaluates
    the model on unseen documents, and topic coherence metrics, which aim to evaluate
    the semantic quality of the uncovered patterns.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更客观地评估结果的两个选项包括困惑度，它评估未见文档上的模型，以及主题连贯性度量，它旨在评估发现的模式的语义质量。
- en: Perplexity
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 困惑度
- en: 'Perplexity, when applied to LDA, measures how well the topic-word probability
    distribution recovered by the model predicts a sample, for example, unseen text
    documents. It is based on the entropy *H*(*p*) of this distribution *p* and computed
    with respect to the set of tokens *w*:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度在应用于LDA时，衡量模型恢复的主题-单词概率分布对样本的预测能力，例如，未见的文本文档。它基于这个分布*p*的熵*H*(*p*)，并与标记集*w*相关计算：
- en: '![](img/16cbf178-55bd-47de-a8ac-120a74c5c3c0.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16cbf178-55bd-47de-a8ac-120a74c5c3c0.png)'
- en: Measures closer to zero imply the distribution is better at predicting the sample.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接近零的度量意味着分布在预测样本方面做得更好。
- en: Topic coherence
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题连贯性
- en: Topic coherence measures the semantic consistency of the topic model results,
    that is, whether humans would perceive the words and their probabilities associated
    with topics as meaningful.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 主题连贯性度量了主题模型结果的语义一致性，即人类是否会认为与主题相关的单词及其概率是有意义的。
- en: To this end, it scores each topic by measuring the degree of semantic similarity
    between the words most relevant to the topic. More specifically, coherence measures
    are based on the probability of observing the set of words *W* that define a topic
    together.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，它通过衡量与主题最相关的单词之间的语义相似度来对每个主题进行评分。更具体地说，连贯性度量是基于观察定义主题的单词集合*W*的概率来衡量的。
- en: We use two measures of coherence that have been designed for LDA and shown to
    align with human judgment of topic quality, namely the UMass and the UCI measures.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两种连贯性度量，这些度量是为LDA设计的，并且已经显示与人类对主题质量的判断一致，即UMass和UCI度量。
- en: 'The UCI metric defines a word pair''s score to be the sum of the **Pointwise
    Mutual Information** (**PMI**) between two distinct pairs of (top) topic words
    *w[i]*, *w[j]* ∈ *w* and a smoothing factor *ε*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: UCI度量将单词对的分数定义为两个不同的（顶部）主题单词*w[i]*，*w[j]* ∈ *w*之间的**点间互信息**（**PMI**）的总和和平滑因子*ε*：
- en: '![](img/f78a1a8b-6bca-421f-af56-c95b4da9ed1c.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f78a1a8b-6bca-421f-af56-c95b4da9ed1c.png)'
- en: The probabilities are computed from word co-occurrence frequencies in a sliding
    window over an external corpus such as Wikipedia, so that this metric can be thought
    of as an external comparison to a semantic ground truth.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概率是从在外部语料库（如维基百科）上的滑动窗口中的单词共现频率计算的，因此可以将这个度量看作是与语义基本事实的外部比较。
- en: 'In contrast, the UMass metric uses the co-occurrences in a number of documents
    *D* from the training corpus to compute a coherence score:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，UMass度量使用训练语料库中来自文档集合*D*的共现来计算连贯性分数：
- en: '![](img/4bedbe9e-42a9-4077-ade1-8045d9f952dd.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bedbe9e-42a9-4077-ade1-8045d9f952dd.png)'
- en: Rather than a comparison to an extrinsic ground truth, this measure reflects
    intrinsic coherence. Both measures have been evaluated to align well with human
    judgment. In both cases, values closer to zero imply that a topic is more coherent.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种度量反映了内在连贯性，而不是与外部基本事实的比较。在这两种情况下，值越接近零意味着主题更连贯。
- en: How to implement LDA using sklearn
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用sklearn实现LDA
- en: 'Using the BBC data as before, we use `sklearn.decomposition.LatentDirichletAllocation`
    to train an LDA model with five topics (see the sklearn documentation for detail
    on parameters, and the notebook `lda_with_sklearn` for implementation details):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们使用BBC数据，使用`sklearn.decomposition.LatentDirichletAllocation`训练了一个包含五个主题的LDA模型（有关参数的详细信息，请参阅sklearn文档，以及`lda_with_sklearn`笔记本中的实现细节）：
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The model tracks the in-sample perplexity during training and stops iterating
    once this measure stops improving. We can persist and load the result as usual
    with sklearn objects:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在训练期间跟踪样本困惑度，并在此度量停止改善时停止迭代。我们可以像通常一样持久化和加载结果与sklearn对象：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How to visualize LDA results using pyLDAvis
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用pyLDAvis可视化LDA结果
- en: Topic visualization facilitates the evaluation of topic quality using human
    judgment. pyLDAvis is a Python port of LDAvis, developed in R and `D3.js`. We
    will introduce the key concepts; each LDA implementation notebook contains examples.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 主题可视化有助于使用人类判断评估主题质量。pyLDAvis是LDAvis的Python版本，由R和`D3.js`开发。我们将介绍关键概念；每个LDA实现笔记本都包含示例。
- en: pyLDAvis displays the global relationships between topics while also facilitating
    their semantic evaluation by inspecting the terms most closely associated with
    each topic and, inversely, the topics associated with each term. It also addresses
    the challenge that terms that are frequent in a corpus tend to dominate the multinomial
    distribution over words that define a topic. LDAVis introduces the relevance *r*
    of the term *w* to topic *t*, to produce a flexible ranking of key terms using
    a weight parameter *0<=ƛ<=1*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: pyLDAvis显示了主题之间的全局关系，同时通过检查与每个主题最密切关联的术语以及与之相反的术语关联的主题，便于进行语义评估。它还解决了在语料库中频繁出现的术语往往会主导定义主题的词的多项式分布的挑战。LDAVis引入了术语*w*对主题*t*的相关性*r*，以使用权重参数*0<=ƛ<=1*生成关键术语的灵活排名。
- en: 'With ![](img/18742117-194f-4e46-b674-441e4a9fc60d.png) as the model''s probability
    estimate of observing the term w for topic *t*, and as the marginal probability
    of w in the corpus:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以![](img/18742117-194f-4e46-b674-441e4a9fc60d.png)作为观察主题*t*时观察到术语*w*的概率估计，以及作为语料库中术语*w*的边际概率：
- en: '![](img/81bc1770-6ae9-40a9-a3ca-e2b982fb5a6c.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81bc1770-6ae9-40a9-a3ca-e2b982fb5a6c.png)'
- en: The first term measures the degree of association of term *t* with topic *w*,
    and the second term measures the lift or saliency, that is, how much more likely
    the term is for the topic than in the corpus.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个术语衡量了术语*t*与主题*w*的关联程度，第二个术语衡量了提升或显著性，即术语在主题中比在语料库中更有可能的程度。
- en: '![](img/4be55e09-a229-4e9c-bb13-9fc5197a20ef.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4be55e09-a229-4e9c-bb13-9fc5197a20ef.png)'
- en: Topic 14
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 主题14
- en: The tool allows the user to interactively change *ƛ* to adjust the relevance,
    which updates the ranking of terms. User studies have found that *ƛ=0.6* produces
    the most plausible results.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具允许用户交互地改变*ƛ*以调整相关性，从而更新术语的排名。用户研究发现*ƛ=0.6*产生了最合理的结果。
- en: How to implement LDA using gensim
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用gensim实现LDA
- en: '`gensim` is a specialized NLP library with a fast LDA implementation and many
    additional features. We will also use it in the next chapter on word vectors (see
    the `latent_dirichlet_allocation_gensim` notebook for details).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`gensim`是一个专门的NLP库，具有快速的LDA实现和许多附加功能。我们还将在下一章关于词向量中使用它（有关详细信息，请参见`latent_dirichlet_allocation_gensim`笔记本）。'
- en: 'It facilitates the conversion of DTM produced by sklearn into gensim data structures
    as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它便于将sklearn生成的DTM转换为gensim数据结构，如下所示：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Gensim LDA algorithm includes numerous settings, which are as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim的LDA算法包括许多设置，如下所示：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Gensim also provides an `LdaMulticore` model for parallel training that may
    speed up training using Python's multiprocessing features for parallel computation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还提供了一个用于并行训练的`LdaMulticore`模型，可以利用Python的多进程功能加速训练。
- en: 'Model training just requires instantiating the `LdaModel` object as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练只需要实例化`LdaModel`对象，如下所示：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Topic coherence measures whether the words in a topic tend to co-occur together.
    It adds up a score for each distinct pair of top-ranked words. The score is the
    log of the probability that a document containing at least one instance of the
    higher-ranked word also contains at least one instance of the lower-ranked word.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 主题一致性衡量了主题中的词是否倾向于共同出现。它为每对排名最高的词的不同对得分相加。得分是包含至少一个较高排名词的文档也包含至少一个较低排名词的概率的对数。
- en: 'Large negative values indicate words that don''t co-occur often; values closer
    to zero indicate that words tend to co-occur more often. `gensim` permits topic
    coherence evaluation that produces the topic coherence and shows the most important
    words per topic:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 大的负值表示不经常共现的词；接近零的值表示词往往更经常共现。`gensim`允许进行主题一致性评估，产生主题一致性并显示每个主题的最重要词：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can display the results as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下显示结果：
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This shows the following top words for each topic:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了每个主题的顶级词：
- en: '| **Topic 1** |  | **Topic 2** |  | **Topic 3** |  | **Topic 4** |  | **Topic
    5** |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **主题1** |  | **主题2** |  | **主题3** |  | **主题4** |  | **主题5** |  |'
- en: '| Probability | Term | Probability | Term | Probability | Term | Probability
    | Term | Probability | Term |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 概率 | 术语 | 概率 | 术语 | 概率 | 术语 | 概率 | 术语 | 概率 | 术语 |'
- en: '| 0.55% | online | 0.90% | best | 1.04% | mobile | 0.64% | market | 0.94% |
    labour |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 0.55% | 在线 | 0.90% | 最佳 | 1.04% | 移动 | 0.64% | 市场 | 0.94% | 劳动 |'
- en: '| 0.51% | site | 0.87% | game | 0.98% | phone | 0.53% | growth | 0.72% | blair
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 0.51% | 网站 | 0.87% | 游戏 | 0.98% | 手机 | 0.53% | 增长 | 0.72% | 布莱尔 |'
- en: '| 0.46% | game | 0.62% | play | 0.51% | music | 0.52% | sales | 0.72% | brown
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 0.46% | 游戏 | 0.62% | 玩 | 0.51% | 音乐 | 0.52% | 销售 | 0.72% | 布朗 |'
- en: '| 0.45% | net | 0.61% | won | 0.48% | film | 0.49% | economy | 0.65% | election 
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 0.45% | 网络 | 0.61% | 赢 | 0.48% | 电影 | 0.49% | 经济 | 0.65% | 选举 |'
- en: '| 0.44% | used | 0.56% | win | 0.48% | use | 0.45% | prices | 0.57% | united
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 0.44% | 使用 | 0.56% | 赢 | 0.48% | 使用 | 0.45% | 价格 | 0.57% | 联合 |'
- en: 'And the corresponding coherence scores, which highlight the decay of topic
    quality (at least in part due to the relatively small dataset):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以及相应的一致性分数，突出了主题质量的衰减（至少部分原因是由于相对较小的数据集）：
- en: '![](img/0605996a-535f-454f-89f1-03720b976200.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0605996a-535f-454f-89f1-03720b976200.png)'
- en: Decay of topic quality
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 主题质量的衰减
- en: Topic modeling for earnings calls
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 盈利电话的主题建模
- en: In [Chapter 3](a7cec22f-095e-49c0-a2bb-e179f6e824a8.xhtml), *Alternative Data
    for Finance*, we learned how to scrape earnings call data from the SeekingAlpha
    site. In this section, we will illustrate topic modeling using this source. I'm
    using a sample of some 500 earnings call transcripts from the second half of 2018\.
    For a practical application, a larger dataset would be highly desirable. The `earnings_calls` directory contains
    several files, with examples mentioned later.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](a7cec22f-095e-49c0-a2bb-e179f6e824a8.xhtml)中，*金融的替代数据*，我们学习了如何从SeekingAlpha网站上爬取盈利电话数据。在本节中，我们将说明使用这个来源进行主题建模。我使用了2018年下半年的大约500个盈利电话转录的样本。对于实际应用，更大的数据集将是非常理想的。`earnings_calls`目录包含了几个文件，稍后会提到。
- en: See the `lda_earnings_calls` notebook for details on loading, exploring, and
    preprocessing the data, as well as training and evaluating individual models,
    and the `run_experiments.py` file for the experiments described here.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅`lda_earnings_calls`笔记本，了解有关加载、探索和预处理数据的详细信息，以及训练和评估单个模型，以及描述这里的实验的`run_experiments.py`文件。
- en: Data preprocessing
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'The transcripts consist of individual statements by a company representative,
    an operator, and usually a question and answer session with analysts. We will
    treat each of these statements as separate documents, ignoring operator statements,
    to obtain 22,766 items with mean and median word counts of 144 and 64, respectively:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 转录由公司代表、操作员和通常与分析师进行的问答环节的个别语句组成。我们将每个语句视为单独的文档，忽略操作员的语句，得到了22766个项目，平均和中位词数分别为144和64：
- en: '[PRE13]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We use `spaCy` to preprocess these documents as illustrated in [Chapter 13](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml), *Working
    with Text Data* (see the notebook) and store the cleaned and lemmatized text as
    a new text file.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`spaCy`对这些文档进行预处理，如[第13章](461c4d2b-9349-4b77-baf9-7b4e60926c93.xhtml)中所示，*处理文本数据*（请参阅笔记本），并将清理和词形还原后的文本存储为新的文本文件。
- en: Data exploration reveals domain-specific stopwords such as year and quarter
    that we remove in a second step, where we also filter out statements with fewer
    than ten words so that some 16,150 remain.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索显示出领域特定的停用词，如年和季度，我们在第二步中去除，同时过滤掉少于十个词的语句，剩下大约16150个。
- en: Model training and evaluation
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练和评估
- en: For illustration, we will create a document-term matrix containing terms appearing
    in between 0.5% and 50% of documents for around 1,560 features. Training a 15-topic
    model using 25 passes over the corpus takes a bit over two minutes on a four-core
    i7.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们将创建一个包含出现在0.5%到50%文档中的术语的文档-术语矩阵，大约包含1560个特征。在四核i7上对语料库进行25次训练15个主题模型需要两分钟多一点的时间。
- en: 'The top 10 words per topic identify several distinct themes that range from
    obvious financial information to clinical trials (topic 4) and supply chain issues
    (12):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主题的前10个词识别出了几个不同的主题，从明显的财务信息到临床试验（主题4）和供应链问题（12）。
- en: '![](img/150ee53e-4193-4353-904e-90552426dbaf.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/150ee53e-4193-4353-904e-90552426dbaf.png)'
- en: 'Using pyLDAvis'' relevance metric with a 0.6 weighting of unconditional frequency
    relative to lift, topic definitions become more intuitive, as illustrated for
    topic 14 about sales performance:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pyLDAvis的相关度指标，将无条件频率与提升的0.6加权，主题定义变得更加直观，如主题14关于销售业绩的示例所示：
- en: '![](img/464ea186-b942-464d-ab11-682ac5225cd4.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/464ea186-b942-464d-ab11-682ac5225cd4.png)'
- en: Sales performance for Topic 14
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 主题14的销售业绩
- en: The notebook also illustrates how to look up documents by their topic association.
    In this case, an analyst can review relevant statements for nuances, use sentiment
    analysis to further process the topic-specific text data, or assign labels derived
    from market prices.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本还说明了如何通过它们的主题关联查找文档。在这种情况下，分析师可以审查相关语句以获取细微差别，使用情感分析进一步处理特定主题的文本数据，或者分配从市场价格中得出的标签。
- en: Running experiments
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行实验
- en: To illustrate the impact of different parameter settings, we ran a few hundred
    experiments for different DTM constraints and model parameters. More specifically,
    we let the `min_df` and `max_df` parameters range from 50-500 words and 10% to
    100% of documents, respectively using alternatively binary and absolute counts.
    We then trained LDA models with 3 to 50 topics, using 1 and 25 passes over the
    corpus.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明不同参数设置的影响，我们对不同的DTM约束和模型参数进行了几百次实验。更具体地说，我们让`min_df`和`max_df`参数分别从50-500个词和10%到100%的文档范围内变化，交替使用二进制和绝对计数。然后我们对语料库进行了3到50个主题的LDA模型训练，使用1和25次训练。
- en: 'The following chart illustrates the results in terms of topic coherence (higher
    is better), and perplexity (lower is better). Coherence drops after 25-30 topics
    and perplexity similarly increases:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了主题连贯性（越高越好）和困惑度（越低越好）的结果。连贯性在25-30个主题后下降，困惑度同样增加：
- en: '![](img/87e28cac-ffb8-4e2e-8401-a0b466fe4841.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87e28cac-ffb8-4e2e-8401-a0b466fe4841.png)'
- en: The notebook includes regression results that quantify the relationships between
    parameters and outcomes. We generally get better results using absolute counts
    and a smaller vocabulary.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本包括量化参数和结果之间关系的回归结果。我们通常使用绝对计数和较小的词汇表获得更好的结果。
- en: Topic modeling for Yelp business reviews
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Yelp商业评论的主题建模
- en: The `lda_yelp_reviews` notebook contains an example of LDA applied to six million
    business review on Yelp. Reviews are more uniform in length than the statements
    extracted from the earnings call transcripts. After cleaning as before, the 10^(th)
    and 90^(th) percentiles range from 14 to 90 tokens.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`lda_yelp_reviews`笔记本包含了LDA应用于Yelp上六百万条商业评论的示例。评论的长度比从盈利电话转录中提取的语句更加统一。在清洗之后，第10和第90百分位数的标记范围从14到90。'
- en: 'We show results for one model using a vocabulary of 3,800 tokens based on *min_df=0.1%*
    and *max_df=25%* with a single pass to avoid a lengthy training time for 20 topics.
    We can use the `pyldavis topic_info` attribute to compute relevance values for
    *lambda=0.6* that produce the following word list (see the notebook for details):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了一个模型的结果，使用了一个包含3,800个标记的词汇表，基于*min_df=0.1%*和*max_df=25%*，通过一次遍历来避免为20个主题进行漫长的训练时间。我们可以使用`pyldavis
    topic_info`属性来计算*lambda=0.6*的相关性数值，产生以下单词列表（详细信息请参见笔记本）：
- en: '![](img/47ff32ac-676d-40bf-84e2-e429dff10d9b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47ff32ac-676d-40bf-84e2-e429dff10d9b.png)'
- en: Gensim provides a `LdaMultiCore` implementation that allows for parallel training
    using Python's multiprocessing module and improves performance by 50% when using
    four workers. More workers do not further reduce training time though, due to
    I/O bottlenecks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim提供了`LdaMultiCore`实现，允许使用Python的多进程模块进行并行训练，并且当使用四个工作进程时，性能提高了50%。然而，使用更多的工作进程并不能进一步减少训练时间，因为受到I/O瓶颈的限制。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the use of topic modeling to gain insights into
    the content of a large collection of documents. We covered Latent Semantic Analysis,
    which uses dimensionality reduction of the DTM to project documents into a latent
    topic space. While effective in addressing the curse of dimensionality caused
    by high-dimensional word vectors, it does not capture much semantic information.
    Probabilistic models make explicit assumptions about the interplay of documents,
    topics, and words that allow algorithms to reverse engineer the document generation
    process and evaluate the model fit on new documents. We saw that LDA is capable
    of extracting plausible topics that allow us to gain a high-level understanding
    of large amounts of text in an automated way, while also identifying relevant
    documents in a targeted way.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用主题建模来深入了解大量文档内容的方法。我们介绍了潜在语义分析，它使用DTM的降维来将文档投影到潜在主题空间中。虽然在解决高维词向量引起的维度灾难方面很有效，但它并没有捕捉到太多的语义信息。概率模型对文档、主题和词语的相互作用做出了明确的假设，这使得算法能够逆向工程文档生成过程，并在新文档上评估模型拟合度。我们看到LDA能够提取出合理的主题，使我们能够以自动化的方式对大量文本进行高层次理解，同时以有针对性的方式识别相关文档。
- en: In the next chapter, we will learn how to train neural networks that embed individual
    words in a high-dimensional vector space that captures important semantic information
    and allows us to use the resulting word vectors as high-quality text features.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何训练神经网络，将单词嵌入到一个捕捉重要语义信息的高维向量空间中，并且可以使用生成的单词向量作为高质量的文本特征。
