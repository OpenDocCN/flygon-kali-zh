- en: '*Chapter 8*: Pod Placement Controls'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：Pod放置控制'
- en: This chapter describes the various ways of controlling Pod placement in Kubernetes,
    as well as explaining why it may be a good idea to implement these controls in
    the first place. Pod placement means controlling which node a Pod is scheduled
    to in Kubernetes. We start with simple controls like node selectors, and then
    move on to more complex tools like taints and tolerations, and finish with two
    beta features, node affinity and inter-Pod affinity/anti-affinity.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了在Kubernetes中控制Pod放置的各种方式，以及解释为什么首先实施这些控制可能是一个好主意。Pod放置意味着控制Pod在Kubernetes中被调度到哪个节点。我们从简单的控制开始，比如节点选择器，然后转向更复杂的工具，比如污点和容忍度，最后介绍两个beta功能，节点亲和性和Pod间亲和性/反亲和性。
- en: In past chapters, we've learned how best to run application Pods on Kubernetes
    – from coordinating and scaling them using deployments, injecting configuration
    with ConfigMaps and Secrets, to adding storage with persistent volumes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的章节中，我们已经学习了如何在Kubernetes上最好地运行应用程序Pod - 从使用部署协调和扩展它们，使用ConfigMaps和Secrets注入配置，到使用持久卷添加存储。
- en: Throughout all of this, however, we have always relied on the Kubernetes scheduler
    to put Pods on the optimal node without giving the scheduler much information
    about the Pods in question. So far, we've added resource limits and requests to
    our Pods (`resource.requests` and `resource.limits` in the Pod spec). Resource
    requests specify a minimum level of free resources on a node that the Pod needs
    in order to be scheduled, while resource limits specify the maximum amount of
    resources a Pod is allowed to use. However, we have not put any specific requirements
    on which nodes or set of nodes a Pod must be run.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管如此，我们始终依赖Kubernetes调度程序将Pod放置在最佳节点上，而没有给调度程序提供有关所讨论的Pod的太多信息。到目前为止，我们已经在Pod中添加了资源限制和请求（Pod规范中的`resource.requests`和`resource.limits`）。资源请求指定Pod在调度时需要的节点上的最低空闲资源水平，而资源限制指定Pod允许使用的最大资源量。然而，我们并没有对Pod必须运行在哪些节点或节点集上提出任何具体要求。
- en: For many applications and clusters, this is fine. However, as we'll see in the
    first section, there are many cases where using more granular Pod placement controls
    is a useful strategy.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用程序和集群来说，这是可以的。然而，正如我们将在第一节中看到的，有许多情况下使用更精细的Pod放置控制是一种有用的策略。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Identifying use cases for Pod placement
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别Pod放置的用例
- en: Using node selectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用节点选择器
- en: Implementing taints and tolerations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施污点和容忍度
- en: Controlling Pods with node affinity
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用节点亲和性控制Pod
- en: Using inter-Pod affinity and anti-affinity
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Pod亲和性和反亲和性
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In order to run the commands detailed in this chapter, you will need a computer
    that supports the `kubectl` command-line tool along with a working Kubernetes
    cluster. See [*Chapter 1*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016), *Communicating
    with Kubernetes*, for several methods for getting up and running with Kubernetes
    quickly, and for instructions on how to install the `kubectl` tool.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本章中详细介绍的命令，您需要一台支持`kubectl`命令行工具的计算机，以及一个可用的Kubernetes集群。请参阅[*第1章*](B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016)，*与Kubernetes通信*，了解快速启动和运行Kubernetes的几种方法，以及如何安装`kubectl`工具的说明。
- en: The code used in this chapter can be found in the book's GitHub repository at
    [https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter8](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter8).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码可以在书的GitHub存储库中找到[https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter8](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter8)。
- en: Identifying use cases for Pod placement
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别Pod放置的用例
- en: Pod placement controls are tools that Kubernetes gives us to decide which node
    to schedule a Pod on, or when to completely prevent Pod scheduling due to a lack
    of the nodes we want. This can be used in several different patterns, but we'll
    review a few major ones. To start with, Kubernetes itself implements Pod placement
    controls completely by default – let's see how.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Pod放置控制是Kubernetes提供给我们的工具，用于决定将Pod调度到哪个节点，或者由于缺少我们想要的节点而完全阻止Pod的调度。这可以用于几种不同的模式，但我们将回顾一些主要的模式。首先，Kubernetes本身默认完全实现了Pod放置控制-让我们看看如何实现。
- en: Kubernetes node health placement controls
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes节点健康放置控制
- en: Kubernetes uses a few default placement controls to specify which nodes are
    unhealthy in some way. These are generally defined using taints and tolerations,
    which we will review in detail later in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用一些默认的放置控制来指定某种方式不健康的节点。这些通常是使用污点和容忍来定义的，我们将在本章后面详细讨论。
- en: 'Some default taints (which we''ll discuss in the next section) that Kubernetes
    uses are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用的一些默认污点（我们将在下一节中讨论）如下：
- en: '`memory-pressure`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory-pressure`'
- en: '`disk-pressure`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disk-pressure`'
- en: '`unreachable`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unreachable`'
- en: '`not-ready`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`not-ready`'
- en: '`out-of-disk`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out-of-disk`'
- en: '`network-unavailable`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network-unavailable`'
- en: '`unschedulable`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unschedulable`'
- en: '`uninitialized` (only for cloud-provider-created nodes)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`uninitialized`（仅适用于由云提供商创建的节点）'
- en: These conditions can mark nodes as unable to receive new Pods, though there
    is some flexibility in how these taints are handled by the scheduler, as we will
    see later. The purpose of these system-created placement controls is to prevent
    unhealthy nodes from receiving workloads that may not function properly.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些条件可以将节点标记为无法接收新的Pod，尽管调度器在处理这些污点的方式上有一定的灵活性，我们稍后会看到。这些系统创建的放置控制的目的是防止不健康的节点接收可能无法正常运行的工作负载。
- en: In addition to system-created placement controls for node health, there are
    several use cases where you, as a user, may want to implement fine-tuned scheduling,
    as we will see in the next section.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于节点健康的系统创建的放置控制之外，还有一些用例，您作为用户可能希望实现精细调度，我们将在下一节中看到。
- en: Applications requiring different node types
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要不同节点类型的应用程序
- en: In a heterogeneous Kubernetes cluster, every node is not created equal. You
    may have some more powerful VMs (or bare metal) and some less – or have different
    specialized sets of nodes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在异构的Kubernetes集群中，每个节点并不相同。您可能有一些更强大的虚拟机（或裸金属）和一些较弱的，或者有不同的专门的节点集。
- en: For instance, in a cluster that runs data science pipelines, you may have nodes
    with GPU acceleration capabilities to run deep learning algorithms, regular compute
    nodes to serve applications, nodes with high amounts of memory to do inference
    based on completed models, and more.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在运行数据科学流水线的集群中，您可能有具有GPU加速能力的节点来运行深度学习算法，常规计算节点来提供应用程序，具有大量内存的节点来基于已完成的模型进行推理，等等。
- en: Using Pod placement controls, you can ensure that the various pieces of your
    platform run on the hardware best suited for the task at hand.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pod放置控制，您可以确保平台的各个部分在最适合当前任务的硬件上运行。
- en: Applications requiring specific data compliance
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要特定数据合规性的应用程序
- en: Similar to the previous example, where application requirements may dictate
    the need for different types of compute, certain data compliance needs may require
    specific types of nodes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的例子类似，应用程序要求可能决定了对不同类型的计算需求，某些数据合规性需求可能需要特定类型的节点。
- en: For instance, cloud providers such as AWS and Azure often allow you to purchase
    VMs with dedicated tenancy – which means that no other applications run on the
    underlying hardware and hypervisor. This is different from other typical cloud-provider
    VMs, where multiple customers may share a single physical machine.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像AWS和Azure这样的云提供商通常允许您购买具有专用租户的VM - 这意味着没有其他应用程序在底层硬件和虚拟化程序上运行。这与其他典型的云提供商VM不同，其他客户可能共享单个物理机。
- en: For certain data regulations, this level of dedicated tenancy is required to
    maintain compliance. To fulfill this need, you could use Pod placement controls
    to ensure that the relevant applications only run on nodes with dedicated tenancy,
    while reducing costs by running the control plane on more typical VMs without
    it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些数据法规，需要这种专用租户级别来保持合规性。为了满足这种需求，您可以使用Pod放置控件来确保相关应用仅在具有专用租户的节点上运行，同时通过在更典型的VM上运行控制平面来降低成本。
- en: Multi-tenant clusters
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多租户集群
- en: If you are running a cluster with multiple tenants (separated by namespaces,
    for instance), you could use Pod placement controls to reserve certain nodes or
    groups of nodes for a tenant, to physically or otherwise separate them from other
    tenants in the cluster. This is similar to the concept of dedicated hardware in
    AWS or Azure.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行一个具有多个租户的集群（例如通过命名空间分隔），您可以使用Pod放置控件来为租户保留某些节点或节点组，以便将它们与集群中的其他租户物理或以其他方式分开。这类似于AWS或Azure中的专用硬件的概念。
- en: Multiple failure domains
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个故障域
- en: Though Kubernetes already provides high availability by allowing you to schedule
    workloads that run on multiple nodes, it is also possible to extend this pattern.
    We can create our own Pod scheduling strategies that account for failure domains
    that stretch across multiple nodes. A great way to handle this is via the Pod
    or node affinity or anti-affinity features, which we will discuss later in this
    chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Kubernetes已经通过允许您在多个节点上运行工作负载来提供高可用性，但也可以扩展这种模式。我们可以创建自己的Pod调度策略，考虑跨多个节点的故障域。处理这个问题的一个很好的方法是通过Pod或节点的亲和性或反亲和性特性，我们将在本章后面讨论。
- en: 'For now, let''s conceptualize a case where we have our cluster on bare metal
    with 20 nodes per physical rack. If each rack has its own dedicated power connection
    and backup, it can be thought of as a failure domain. When the power connections
    fail, all the machines on the rack fail. Thus, we may want to encourage Kubernetes
    to run two instances or Pods on separate racks/failure domains. The following
    figure shows how an application could run across failure domains:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构想一个情况，我们的集群在裸机上，每个物理机架有20个节点。如果每个机架都有自己的专用电源连接和备份，它可以被视为一个故障域。当电源连接失败时，机架上的所有机器都会失败。因此，我们可能希望鼓励Kubernetes在不同的机架/故障域上运行两个实例或Pod。以下图显示了应用程序如何跨故障域运行：
- en: '![Figure 8.1 – Failure domains](image/B14790_08_001.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 - 故障域](image/B14790_08_001.jpg)'
- en: Figure 8.1 – Failure domains
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 - 故障域
- en: As you can see in the figure, as the application pods are spread across multiple
    failure domains, not just multiple nodes in the same failure domain, we can maintain
    uptime even if *Failure Domain 1* goes down. *App A - Pod 1* and *App B - Pod
    1* are in the same (red) failure domain. However, if that failure domain (*Rack
    1*) goes down, we will still have a replica of each application on *Rack 2*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图中所看到的，由于应用程序Pod分布在多个故障域中，而不仅仅是在同一故障域中的多个节点，即使*故障域1*发生故障，我们也可以保持正常运行。*App
    A - Pod 1*和*App B - Pod 1*位于同一个（红色）故障域。但是，如果该故障域（*Rack 1*）发生故障，我们仍将在*Rack 2*上有每个应用的副本。
- en: We use the word "encourage" here because it is possible to configure some of
    this functionality as either a hard requirement or on a best effort basis in the
    Kubernetes scheduler.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用“鼓励”这个词，因为在Kubernetes调度程序中，可以将一些功能配置为硬性要求或尽力而为。
- en: These examples should give you a solid understanding of some potential use cases
    for advanced placement controls.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例应该让您对高级放置控件的一些潜在用例有一个扎实的理解。
- en: Let's discuss the actual implementation now, taking each placement toolset one
    by one. We'll start with the simplest, node selectors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论实际的实现，逐个使用每个放置工具集。我们将从最简单的节点选择器开始。
- en: Using node selectors and node name
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用节点选择器和节点名称
- en: Node selectors are a very simple type of placement control in Kubernetes. Each
    Kubernetes node can be labeled with one or more labels in the metadata block,
    and Pods can specify a node selector.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 节点选择器是Kubernetes中一种非常简单的放置控制类型。每个Kubernetes节点都可以在元数据块中带有一个或多个标签，并且Pod可以指定一个节点选择器。
- en: 'To label an existing node, you can use the `kubectl label` command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要为现有节点打标签，您可以使用`kubectl label`命令：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we're labeling our `node1` node with the label `cpu_speed`
    and the value `fast`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用标签`cpu_speed`和值`fast`来标记我们的`node1`节点。
- en: 'Now, let''s assume that we have an application that really needs fast CPU cycles
    to perform effectively. We can add a `nodeSelector` to our workload to ensure
    that it is only scheduled on nodes with our fast CPU speed label, as shown in
    the following code snippet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设我们有一个应用程序，它确实需要快速的CPU周期才能有效地执行。我们可以为我们的工作负载添加`nodeSelector`，以确保它只被调度到具有我们快速CPU速度标签的节点上，如下面的代码片段所示：
- en: pod-with-node-selector.yaml
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-node-selector.yaml
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When deployed, as part of a Deployment or by itself, our `speedy-app` Pod will
    only be scheduled on nodes with the `cpu_speed` label.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当部署时，作为部署的一部分或单独部署，我们的`speedy-app` Pod将只被调度到具有`cpu_speed`标签的节点上。
- en: Keep in mind that unlike some other more advanced Pod placement options that
    we will review shortly, there is no leeway in node selectors. If there are no
    nodes that have the required label, the application will not be scheduled at all.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，与我们即将审查的一些其他更高级的Pod放置选项不同，节点选择器中没有任何余地。如果没有具有所需标签的节点，应用程序将根本不会被调度。
- en: 'For an even simpler (but far more brittle) selector, you can use `nodeName`,
    which specifies the exact node that the Pod should be scheduled on. You can use
    it like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更简单（但更脆弱）的选择器，您可以使用`nodeName`，它指定Pod应该被调度到的确切节点。您可以像这样使用它：
- en: pod-with-node-name.yaml
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-node-name.yaml
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, this selector will only allow the Pod to be scheduled on `node1`,
    so if it isn't currently accepting Pods for any reason, the Pod will not be scheduled.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这个选择器只允许Pod被调度到`node1`，所以如果它当前由于任何原因不接受Pods，Pod将不会被调度。
- en: For slightly more nuanced placement control, let's move on to taints and tolerations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于稍微更加微妙的放置控制，让我们转向污点和容忍。
- en: Implementing taints and tolerations
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施污点和容忍
- en: Taints and tolerations in Kubernetes work like reverse node selectors. Rather
    than nodes attracting Pods due to having the proper labels, which are then consumed
    by a selector, we taint nodes, which repels all Pods from being scheduled on the
    node, and then mark our Pods with tolerations, which allow them to be scheduled
    on the tainted nodes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，污点和容忍的工作方式类似于反向节点选择器。与节点吸引Pods因具有适当的标签而被选择器消耗不同，我们对节点进行污点处理，这会排斥所有Pod被调度到该节点，然后标记我们的Pods具有容忍，这允许它们被调度到被污点处理的节点上。
- en: As mentioned at the beginning of the chapter, Kubernetes uses system-created
    taints to mark nodes as unhealthy and prevent new workloads from being scheduled
    on them. For instance, the `out-of-disk` taint will prevent any new pods from
    being scheduled to a node with that taint.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头提到的，Kubernetes使用系统创建的污点来标记节点为不健康，并阻止新的工作负载被调度到它们上面。例如，`out-of-disk`污点将阻止任何新的Pod被调度到具有该污点的节点上。
- en: 'Let''s take the same example use case that we had with node selectors and apply
    it using taints and tolerations. Since this is basically the reverse of our previous
    setup, let''s first give our node a taint using the `kubectl taint` command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用污点和容忍度来应用与节点选择器相同的示例用例。由于这基本上是我们先前设置的反向，让我们首先使用`kubectl taint`命令给我们的节点添加一个污点：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's pick apart this command. We are giving `node2` a taint called `cpu_speed`
    and a value, `slow`. We also mark this taint with an effect – in this case, `NoSchedule`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个命令。我们给`node2`添加了一个名为`cpu_speed`的污点和一个值`slow`。我们还用一个效果标记了这个污点 - 在这种情况下是`NoSchedule`。
- en: 'Once we''re done with our example (don''t do this quite yet if you''re following
    along with the commands), we can remove the `taint` using the minus operator:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了我们的示例（如果您正在跟随命令进行操作，请不要立即执行此操作），我们可以使用减号运算符删除`taint`：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `taint` effect lets us add in some granularity into how the scheduler handles
    the taints. There are three possible effect values:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`taint`效果让我们在调度器处理污点时增加了一些细粒度。有三种可能的效果值：'
- en: '`NoSchedule`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoSchedule`'
- en: '`NoExecute`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoExecute`'
- en: '`PreferNoSchedule`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PreferNoSchedule`'
- en: The first two effects, `NoSchedule` and `NoExecute`, provide hard effects –
    which is to say that, like node selectors, there are only two possibilities, either
    the toleration exists on the Pod (as we'll see momentarily) or the Pod is not
    scheduled. `NoExecute` adds to this base functionality by evicting all Pods on
    the node that do have the toleration, while `NoSchedule` lets existing pods stay
    put, while preventing any new Pods without the toleration from joining.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个效果，`NoSchedule`和`NoExecute`，提供了硬效果 - 也就是说，像节点选择器一样，只有两种可能性，要么Pod上存在容忍度（我们马上就会看到），要么Pod没有被调度。`NoExecute`通过驱逐所有具有容忍度的节点上的Pod来增加这个基本功能，而`NoSchedule`让现有的Pod保持原状，同时阻止任何没有容忍度的新Pod加入。
- en: '`PreferNoSchedule`, on the other hand, provides the Kubernetes scheduler with
    some leeway. It tells the scheduler to attempt to find a node for a Pod that doesn''t
    have an untolerated taint, but if none exist, to go ahead and schedule it anyway.
    It implements a soft effect.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`PreferNoSchedule`，另一方面，为Kubernetes调度器提供了一些余地。它告诉调度器尝试为没有不可容忍污点的Pod找到一个节点，但如果不存在，则继续安排它。它实现了软效果。'
- en: 'In our case, we have chosen `NoSchedule`, so no new Pods will be assigned to
    the node – unless, of course, we provide a toleration. Let''s do this now. Assume
    that we have a second application that doesn''t care about CPU clock speeds. It
    is happy to live on our slower node. This is the Pod manifest:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们选择了`NoSchedule`，因此不会将新的Pod分配给该节点 - 除非当然我们提供了一个容忍度。现在让我们这样做。假设我们有第二个应用程序，它不关心CPU时钟速度。它很乐意生活在我们较慢的节点上。这是Pod清单：
- en: pod-without-speed-requirement.yaml
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: pod-without-speed-requirement.yaml
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Right now, our `slow-app` Pod will not run on any node with a taint. We need
    to provide a toleration for this Pod in order for it to be scheduled on a node
    with a taint – which we can do like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的`slow-app` Pod将不会在任何具有污点的节点上运行。我们需要为这个Pod提供一个容忍度，以便它可以被调度到具有污点的节点上 - 我们可以这样做：
- en: pod-with-toleration.yaml
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-toleration.yaml
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s pick apart our `tolerations` entry, which is an array of values. Each
    value has a `key` – which is the same as our taint name. Then there is an `operator`
    value. This `operator` can be either `Equal` or `Exists`. For `Equal`, you can
    use the `value` key as in the preceding code to configure a value that the taint
    must equal in order to be tolerated by the Pod. For `Exists`, the taint name must
    be on the node, but it does not matter what the value is, as in this Pod spec:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解我们的`tolerations`条目，这是一个值数组。每个值都有一个`key`-与我们的污点名称相同。然后是一个`operator`值。这个`operator`可以是`Equal`或`Exists`。对于`Equal`，您可以使用`value`键，就像前面的代码中那样，配置污点必须等于的值，以便Pod容忍。对于`Exists`，污点名称必须在节点上，但不管值是什么都没有关系，就像这个Pod规范中一样：
- en: pod-with-toleration2.yaml
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-toleration2.yaml
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, we have used the `Exists` `operator` value to allow our Pod
    to tolerate any `cpu_speed` taint.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们已经使用了`Exists` `operator`值来允许我们的Pod容忍任何`cpu_speed`污点。
- en: Finally, we have our `effect`, which works the same way as the `effect` on the
    taint itself. It can contain the exact same values as the taint effect – `NoSchedule`,
    `NoExecute`, and `PreferNoSchedule`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有我们的`effect`，它的工作方式与污点本身的`effect`相同。它可以包含与污点效果完全相同的值- `NoSchedule`，`NoExecute`和`PreferNoSchedule`。
- en: 'A Pod with a `NoExecute` toleration will tolerate the taint associated with
    it indefinitely. However, you can add a field called `tolerationSeconds` in order
    to have the Pod leave the tainted node after a prescribed time has elapsed. This
    allows you to specify tolerations that take effect after a period of time. Let''s
    look at an example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 具有`NoExecute`容忍的Pod将无限期容忍与其关联的污点。但是，您可以添加一个名为`tolerationSeconds`的字段，以便在经过规定的时间后，Pod离开受污染的节点。这允许您指定在一段时间后生效的容忍。让我们看一个例子：
- en: pod-with-toleration3.yaml
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-toleration3.yaml
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case, the Pod already running on a node with the taint `slow` when the
    taint and toleration are executed will remain on the node for `60` seconds before
    being rescheduled to a different node.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当污点和容忍执行时，已经在具有`taint`的节点上运行的Pod将在重新调度到不同节点之前在节点上保留`60`秒。
- en: Multiple taints and tolerations
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个污点和容忍
- en: When there are multiple taints or tolerations on a Pod and node, the scheduler
    will check all of them. There is no `OR` logic operator here – if any of the taints
    on the node do not have a matching toleration on the Pod, it will not be scheduled
    on the node (with the exception of `PreferNoSchedule`, in which case, as before,
    the scheduler will try to not schedule on the node if possible). Even if out of
    six taints on the node, the Pod tolerates five of them, it will still not be scheduled
    for a `NoSchedule` taint, and it will still be evicted for a `NoExecute` taint.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod和节点上有多个污点或容忍时，调度程序将检查它们所有。这里没有`OR`逻辑运算符-如果节点上的任何污点在Pod上没有匹配的容忍，它将不会被调度到节点上（除了`PreferNoSchedule`之外，在这种情况下，与以前一样，调度程序将尽量不在节点上调度）。即使在节点上有六个污点中，Pod容忍了其中五个，它仍然不会被调度到`NoSchedule`污点，并且仍然会因为`NoExecute`污点而被驱逐。
- en: For a tool that gives us a much more subtle way of controlling placement, let's
    look at node affinity.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个可以更微妙地控制放置方式的工具，让我们看一下节点亲和力。
- en: Controlling Pods with node affinity
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用节点亲和力控制Pod
- en: As you can probably tell, taints and tolerations – while much more flexible
    than node selectors – still leave some use cases unaddressed and in general only
    allow a *filter* pattern where you can match on a specific taint using `Exists`
    or `Equals`. There may be more advanced use cases where you want more flexible
    methods of selecting nodes – and *affinities* are a feature of Kubernetes that
    addresses this.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，污点和容忍性 - 虽然比节点选择器灵活得多 - 仍然留下了一些用例未解决，并且通常只允许*过滤*模式，你可以使用`Exists`或`Equals`来匹配特定的污点。可能有更高级的用例，你想要更灵活的方法来选择节点
    - Kubernetes的*亲和性*就是解决这个问题的功能。
- en: 'There are two types of affinity:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种亲和性：
- en: '**Node affinity**'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点亲和性**'
- en: '**Inter-Pod affinity**'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨Pod的亲和性**'
- en: 'Node affinity is a similar concept to node selectors except that it allows
    for a much more robust set of selection characteristics. Let''s look at some example
    YAML and then pick apart the various pieces:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和性是节点选择器的类似概念，只是它允许更强大的选择特征集。让我们看一些示例YAML，然后分解各个部分：
- en: pod-with-node-affinity.yaml
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-node-affinity.yaml
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see, our `Pod` `spec` has an `affinity` key, and we''ve specified
    a `nodeAffinity` setting. There are two possible node affinity types:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们的`Pod` `spec`有一个`affinity`键，并且我们指定了一个`nodeAffinity`设置。有两种可能的节点亲和性类型：
- en: '`requiredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution`'
- en: '`preferredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preferredDuringSchedulingIgnoredDuringExecution`'
- en: The functionality of these two types maps directly to how `NoSchedule` and `PreferNoSchedule`
    work, respectively.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的功能直接映射到`NoSchedule`和`PreferNoSchedule`的工作方式。
- en: Using requiredDuringSchedulingIgnoredDuringExecution node affinities
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用requiredDuringSchedulingIgnoredDuringExecution节点亲和性
- en: For `requiredDuringSchedulingIgnoredDuringExecution`, Kubernetes will never
    schedule a Pod without a term matching to a node.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`requiredDuringSchedulingIgnoredDuringExecution`，Kubernetes永远不会调度一个没有与节点匹配的术语的Pod。
- en: For `preferredDuringSchedulingIgnoredDuringExecution`, it will attempt to fulfill
    the soft requirement but if it cannot, it will still schedule the Pod.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`preferredDuringSchedulingIgnoredDuringExecution`，它将尝试满足软性要求，但如果不能，它仍然会调度Pod。
- en: The real capability of node affinity over node selectors and taints and tolerations
    comes in the actual expressions and logic that you can implement when it comes
    to the selector.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和性相对于节点选择器和污点和容忍性的真正能力在于你可以在选择器方面实现的实际表达式和逻辑。
- en: The functionalities of the `requiredDuringSchedulingIgnoredDuringExecution`
    and `preferredDuringSchedulingIgnoredDuringExecution` affinities are quite different,
    so we will review each separately.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution`和`preferredDuringSchedulingIgnoredDuringExecution`亲和性的功能是非常不同的，因此我们将分别进行审查。'
- en: For our `required` affinity, we have the ability to specify `nodeSelectorTerms`
    – which can be one or more blocks containing `matchExpressions`. For each block
    of `matchExpressions`, there can be multiple expressions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`required`亲和性，我们有能力指定`nodeSelectorTerms` - 可以是一个或多个包含`matchExpressions`的块。对于每个`matchExpressions`块，可以有多个表达式。
- en: 'In the code block we saw in the previous section, we have one single node selector
    term, a `matchExpressions` block – which itself has only a single expression.
    This expression looks for `key`, which, just like with node selectors, represents
    a node label. Next, it has an `operator`, which gives us some flexibility on how
    we want to identify a match. Here are the possible values for the operator:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在上一节中看到的代码块中，我们有一个单一的节点选择器术语，一个`matchExpressions`块 - 它本身只有一个表达式。这个表达式寻找`key`，就像节点选择器一样，代表一个节点标签。接下来，它有一个`operator`，它给了我们一些灵活性，让我们决定如何识别匹配。以下是操作符的可能值：
- en: '`In`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`In`'
- en: '`NotIn`'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NotIn`'
- en: '`Exists`'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exists`'
- en: '`DoesNotExist`'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DoesNotExist`'
- en: '`Gt` (Note: greater than)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gt`（注意：大于）'
- en: '`Lt` (Note: less than)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Lt`（注意：小于）'
- en: In our case, we are using the `In` operator, which will check to see if the
    value is one of several that we specify. Finally, in our `values` section, we
    can list one or more values that must match, based on the operator, before the
    expression is true.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们使用了“In”运算符，它将检查值是否是我们指定的几个值之一。最后，在我们的“values”部分，我们可以列出一个或多个值，根据运算符，必须匹配才能使表达式为真。
- en: 'As you can see, this gives us significantly greater granularity in specifying
    our selector. Let''s look at our example of `cpu_speed` using a different operator:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这为我们在指定选择器时提供了更大的粒度。让我们看一个使用不同运算符的`cpu_speed`的例子：
- en: pod-with-node-affinity2.yaml
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-node-affinity2.yaml
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, we are using a very granular `matchExpressions` selector. This
    ability to use more advanced operator matching now allows us to ensure that our
    `speedy-app` is only scheduled on nodes that have a high enough clock speed (in
    this case, 5 GHz). Instead of classifying our nodes into broad groups like `slow`
    and `fast`, we can be much more granular in our specifications.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们正在使用非常精细的`matchExpressions`选择器。现在，使用更高级的运算符匹配的能力使我们能够确保我们的“speedy-app”只安排在具有足够高时钟速度（在本例中为5
    GHz）的节点上。我们可以更加精细地规定，而不是将我们的节点分类为“慢”和“快”这样的广泛组别。
- en: Next, let's look at the other node affinity type –`preferredDuringSchedulingIgnoredDuringExecution`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看另一种节点亲和性类型 - `preferredDuringSchedulingIgnoredDuringExecution`。
- en: Using preferredDuringSchedulingIgnoredDuringExecution node affinities
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用preferredDuringSchedulingIgnoredDuringExecution节点亲和性
- en: 'The syntax for this is slightly different and gives us even more granularity
    to affect this `soft` requirement. Let''s look at a Pod spec YAML that implements
    this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的语法略有不同，并且使我们能够更精细地影响这个“软”要求。让我们看一个实现这一点的 Pod spec YAML：
- en: pod-with-node-affinity3.yaml
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-node-affinity3.yaml
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This looks a bit different from our `required` syntax.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来与我们的“required”语法有些不同。
- en: For `preferredDuringSchedulingIgnoredDuringExecution`, we have the ability to
    assign a `weight` to each entry, with an associated preference, which can again
    be a `matchExpressions` block with multiple inner expressions that use the same
    `key-operator-values` syntax.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`preferredDuringSchedulingIgnoredDuringExecution`，我们有能力为每个条目分配一个“权重”，并附带一个偏好，这可以再次是一个包含多个内部表达式的`matchExpressions`块，这些表达式使用相同的“key-operator-values”语法。
- en: 'The `weight` value is the key difference here. Since `preferredDuringSchedulingIgnoredDuringExecution`
    is a **soft** requirement, we can list a few different preferences with associated
    weights, and let the scheduler try its best to satisfy them. The way this works
    under the hood is that the scheduler will go through all the preferences and compute
    a score for the node based on the weight of each preference and whether it was
    satisfied. Assuming all hard requirements are satisfied, the scheduler will select
    the node with the highest computed score. In the preceding case, we have a single
    preference with a weight of 1, but weight can be anywhere from 1 to 100 – so let''s
    look at a more complex setup for our `speedy-app` use case:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键区别是“权重”值。由于`preferredDuringSchedulingIgnoredDuringExecution`是一个**软**要求，我们可以列出几个不同的偏好，并附带权重，让调度器尽力满足它们。其工作原理是，调度器将遍历所有偏好，并根据每个偏好的权重和是否满足来计算节点的得分。假设所有硬性要求都得到满足，调度器将选择得分最高的节点。在前面的情况下，我们有一个权重为1的单个偏好，但权重可以从1到100不等
    - 所以让我们看一个更复杂的设置，用于我们的“speedy-app”用例：
- en: pod-with-node-affinity4.yaml
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-node-affinity4.yaml
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In our journey to ensure that our `speedy-app` runs on the best possible node,
    we have here decided to only implement `soft` requirements. If no fast nodes exist,
    we still want our app to be scheduled and run. To that end, we've specified two
    preferences – a node with a `cpu_speed` of over 3 (3 GHz) and a memory speed of
    over 4 (4 GHz).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保我们的`speedy-app`在最佳节点上运行的过程中，我们决定只实现`soft`要求。如果没有快速节点存在，我们仍希望我们的应用程序被调度和运行。为此，我们指定了两个偏好
    - 一个`cpu_speed`超过3（3 GHz）和一个内存速度超过4（4 GHz）的节点。
- en: Since our app is far more CPU-bound than memory-bound, we've decided to weight
    our preferences appropriately. In this case, `cpu_speed` carries a `weight` of
    `90`, while `memory_speed` carries a `weight` of `10`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的应用程序更多地受限于CPU而不是内存，我们决定适当地权衡我们的偏好。在这种情况下，`cpu_speed`具有`weight`为`90`，而`memory_speed`具有`weight`为`10`。
- en: Thus, any node that satisfies our `cpu_speed` requirement will have a much higher
    computed score than one that only satisfies the `memory_speed` requirement – but
    still less than one that satisfies both. When we're trying to schedule 10 or 100
    new Pods for this app, you can see how this calculation could be valuable.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，满足我们的`cpu_speed`要求的任何节点的计算得分都比仅满足`memory_speed`要求的节点高得多 - 但仍然比同时满足两者的节点低。当我们尝试为这个应用程序调度10或100个新的Pod时，您可以看到这种计算是如何有价值的。
- en: Multiple node affinities
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个节点亲和性
- en: When we're dealing with multiple node affinities, there are a few key pieces
    of logic to keep in mind. First off, even with a single node affinity, if it is
    combined with a node selector on the same Pod spec (which is indeed possible),
    the node selector must be satisfied before any of the node affinity logic will
    come into play. This is because node selectors only implement hard requirements,
    and there is no `OR` logical operator between the two. An `OR` logical operator
    would check both requirements and ensure that at least one of them is true – but
    node selectors do not let us do this.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理多个节点亲和性时，有一些关键的逻辑要记住。首先，即使只有一个节点亲和性，如果它与同一Pod规范下的节点选择器结合使用（这确实是可能的），则节点选择器必须在任何节点亲和性逻辑生效之前满足。这是因为节点选择器只实现硬性要求，并且两者之间没有`OR`逻辑运算符。`OR`逻辑运算符将检查两个要求，并确保它们中至少有一个为真
    - 但节点选择器不允许我们这样做。
- en: Secondly, for a `requiredDuringSchedulingIgnoredDuringExecution` node affinity,
    multiple entries under `nodeSelectorTerms` are handled in an `OR` logical operator.
    If one, but not all, is satisfied – the Pod will still be scheduled.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，对于`requiredDuringSchedulingIgnoredDuringExecution`节点亲和性，`nodeSelectorTerms`下的多个条目将在`OR`逻辑运算符中处理。如果满足一个但不是全部，则Pod仍将被调度。
- en: 'Finally, for any `nodeSelectorTerm` with multiple entries under `matchExpressions`,
    all must be satisfied – this is an `AND` logical operator. Let''s look at an example
    YAML of this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于`matchExpressions`下有多个条目的`nodeSelectorTerm`，所有条目都必须满足 - 这是一个`AND`逻辑运算符。让我们看一个这样的示例YAML：
- en: pod-with-node-affinity5.yaml
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-node-affinity5.yaml
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this case, if a node has a CPU speed of `5` but does not meet the memory
    speed requirement (or vice versa), the Pod will not be scheduled.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果一个节点的CPU速度为`5`，但不满足内存速度要求（或反之亦然），则Pod将不会被调度。
- en: One final thing to note about node affinity is that, as you've probably already
    noticed, neither of the two affinity types allows the same `NoExecute` functionality
    that was available to us in our taints and tolerations settings.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 关于节点亲和性的最后一件事要注意的是，正如您可能已经注意到的，这两种亲和性类型都不允许我们在我们的污点和容忍设置中可以使用的`NoExecute`功能。
- en: One additional node affinity type – `requiredDuringSchedulingRequiredDuring
    execution` – will add this functionality in a future version. As of Kubernetes
    1.19, this does not yet exist.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种节点亲和性类型 - `requiredDuringSchedulingRequiredDuring execution` - 将在将来的版本中添加此功能。截至Kubernetes
    1.19，这种类型尚不存在。
- en: Next, we will look at inter-pod affinity and anti-affinity, which provides affinity
    definitions between Pods, rather than defining rules for nodes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下Pod间亲和性和反亲和性，它提供了Pod之间的亲和性定义，而不是为节点定义规则。
- en: Using inter-Pod affinity and anti-affinity
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pod间亲和性和反亲和性
- en: Inter-Pod affinity and anti-affinity let you dictate how Pods should run based
    on which other Pods already exist on a node. Since the number of Pods in a cluster
    is typically much larger than the number of nodes, and some Pod affinity and anti-affinity
    rules can be somewhat complex, this feature can put quite a load on your cluster
    control plane if you are running many pods on many nodes. For this reason, the
    Kubernetes documentation does not recommend using these features with a large
    number of nodes in your cluster.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Pod间亲和性和反亲和性让您根据节点上已经存在的其他Pod来指定Pod应该如何运行。由于集群中的Pod数量通常比节点数量要大得多，并且一些Pod亲和性和反亲和性规则可能相当复杂，如果您在许多节点上运行许多Pod，这个功能可能会给您的集群控制平面带来相当大的负载。因此，Kubernetes文档不建议在集群中有大量节点时使用这些功能。
- en: Pod affinities and anti-affinities work fairly differently – let's look at each
    by itself before discussing how they can be combined.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Pod亲和性和反亲和性的工作方式有很大不同-让我们先单独看看每个，然后再讨论它们如何结合起来。
- en: Pod affinities
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod亲和性
- en: 'As with node affinities, let''s dive into the YAML in order to discuss the
    constituent parts of a Pod affinity spec:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点亲和性一样，让我们深入讨论YAML，以讨论Pod亲和性规范的组成部分：
- en: pod-with-pod-affinity.yaml
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-pod-affinity.yaml
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Just like with node affinity, Pod affinity lets us choose between two types:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 就像节点亲和性一样，Pod亲和性让我们在两种类型之间进行选择：
- en: '`preferredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preferredDuringSchedulingIgnoredDuringExecution`'
- en: '`requiredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution`'
- en: Again, similar to node affinity, we can have one or more selectors – which are
    called `labelSelector` since we are selecting Pods, not nodes. The `matchExpressions`
    functionality is the same as with node affinity, but Pod affinity adds a brand-new
    key called `topologyKey`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点亲和性类似，我们可以有一个或多个选择器-因为我们选择的是Pod而不是节点，所以它们被称为`labelSelector`。`matchExpressions`功能与节点亲和性相同，但是Pod亲和性添加了一个全新的关键字叫做`topologyKey`。
- en: '`topologyKey` is in essence a selector that limits the scope of where the scheduler
    should look to see whether other Pods of the same selector are running. That means
    that Pod affinity doesn''t only need to mean other Pods of the same type (selector)
    on the same node; it can mean groups of multiple nodes.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`topologyKey`本质上是一个选择器，限制了调度器应该查看的范围，以查看是否正在运行相同选择器的其他Pod。这意味着Pod亲和性不仅需要意味着同一节点上相同类型（选择器）的其他Pod；它可以意味着多个节点的组。'
- en: Let's go back to our failure domain example at the beginning of the chapter.
    In that example, each rack was its own failure domain with multiple nodes per
    rack. To extend this concept to `topologyKey`, we could label each node on a rack
    with `rack=1` or `rack=2`. Then we can use the `topologyKey` rack, as we have
    in our YAML, to designate that the scheduler should check all of the Pods running
    on nodes with the same `topologyKey` (which in this case means all of the Pods
    on `Node 1` and `Node 2` in the same rack) in order to apply Pod affinity or anti-affinity
    rules.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章开头的故障域示例。在那个例子中，每个机架都是自己的故障域，每个机架有多个节点。为了将这个概念扩展到`topologyKey`，我们可以使用`rack=1`或`rack=2`为每个机架上的节点打上标签。然后我们可以使用`topologyKey`机架，就像我们在YAML中所做的那样，指定调度器应该检查所有运行在具有相同`topologyKey`的节点上的Pod（在这种情况下，这意味着同一机架上的`Node
    1`和`Node 2`上的所有Pod）以应用Pod亲和性或反亲和性规则。
- en: 'So, adding this all up, what our example YAML tells the scheduler is this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将我们的示例YAML全部加起来，告诉调度器的是：
- en: This Pod *MUST* be scheduled on a node with the label `rack`, where the value
    of the label `rack` separates nodes into groups.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个Pod *必须*被调度到具有标签`rack`的节点上，其中标签`rack`的值将节点分成组。
- en: The Pod will then be scheduled in a group where there already exists a Pod running
    with the label `hunger` and a value of 1 or 2.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后Pod将被调度到一个组中，该组中已经存在一个带有标签`hunger`和值为1或2的Pod。
- en: Essentially, we are splitting our cluster into topology domains – in this case,
    racks – and prescribing to the scheduler to only schedule similar pods together
    on nodes that share the same topology domain. This is the opposite of our first
    failure domain example, where we wouldn't want pods to share the same domain if
    possible – but there are also reasons that you may want to keep like pods on the
    same domain. For example, in a multitenant setting where tenants want dedicated
    hardware tenancy over a domain, you could ensure that every Pod that belongs to
    a certain tenant is scheduled to the exact same topology domain.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们将我们的集群分成拓扑域 - 在这种情况下是机架 - 并指示调度器仅在共享相同拓扑域的节点上将相似的Pod一起调度。这与我们第一个故障域示例相反，如果可能的话，我们不希望Pod共享相同的域
    - 但也有理由希望相似的Pod在同一域上。例如，在多租户设置中，租户希望在域上拥有专用硬件租用权，您可以确保属于某个租户的每个Pod都被调度到完全相同的拓扑域。
- en: 'You can use `preferredDuringSchedulingIgnoredDuringExecution` in the same way.
    Before we get to anti-affinities, here''s an example with Pod affinities and the
    `preferred` type:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以相同的方式使用`preferredDuringSchedulingIgnoredDuringExecution`。在我们讨论反亲和性之前，这里有一个带有Pod亲和性和`preferred`类型的示例：
- en: pod-with-pod-affinity2.yaml
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-pod-affinity2.yaml
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As before, in this code block, we have our `weight` – in this case, `50` – and
    our expression match – in this case, using a less than (`Lt`) operator. This affinity
    will induce the scheduler to try its best to schedule the Pod on a node where
    it is or with another node on the same rack that has a Pod running with a `hunger`
    of less than 3\. The `weight` is used by the scheduler to compare nodes – as discussed
    in the section on node affinities – *Controlling Pods with Node Affinity* (see
    `pod-with-node-affinity4.yaml`). In this scenario specifically, the weight of
    `50` doesn't make any difference because there is only one entry in the affinity
    list.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，在这个代码块中，我们有我们的`weight` - 在这种情况下是`50` - 和我们的表达式匹配 - 在这种情况下，使用小于（`Lt`）运算符。这种亲和性将促使调度器尽力将Pod调度到一个节点上，该节点上已经运行着一个`hunger`小于3的Pod，或者与另一个在同一机架上运行着`hunger`小于3的Pod。调度器使用`weight`来比较节点
    - 正如在节点亲和性部分讨论的那样 - *使用节点亲和性控制Pod*（参见`pod-with-node-affinity4.yaml`）。在这种特定情况下，`50`的权重并没有任何区别，因为亲和性列表中只有一个条目。
- en: Pod anti-affinities extend this paradigm using the same selectors and topologies
    – let's take a look at them in detail.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Pod反亲和性使用相同的选择器和拓扑结构来扩展这种范例-让我们详细看一下它们。
- en: Pod anti-affinities
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod反亲和性
- en: 'Pod anti-affinities allow you to prevent Pods from running on the same topology
    domain as pods that match a selector. They implement the opposite logic to Pod
    affinities. Let''s dive into some YAML and explain how this works:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Pod反亲和性允许您阻止Pod在与匹配选择器的Pod相同的拓扑域上运行。它们实现了与Pod亲和性相反的逻辑。让我们深入了解一些YAML，并解释一下它是如何工作的：
- en: pod-with-pod-anti-affinity.yaml
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-pod-anti-affinity.yaml
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Similar to Pod affinity, we use the `affinity` key as the location to specify
    our anti-affinity under `podAntiAffinity`. Also, as with Pod affinity, we have
    the ability to use either `preferredDuringSchedulingIgnoredDuringExecution` or
    `requireDuringSchedulingIgnoredDuringExecution`. We even use all the same syntax
    for the selector as with Pod affinities.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与Pod亲和性类似，我们使用`affinity`键来指定`podAntiAffinity`下的反亲和性的位置。与Pod亲和性一样，我们可以使用`preferredDuringSchedulingIgnoredDuringExecution`或`requireDuringSchedulingIgnoredDuringExecution`。我们甚至可以使用与Pod亲和性相同的选择器语法。
- en: The only actual difference in syntax is the use of `podAntiAffinity` under the
    `affinity` key.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 语法上唯一的实际区别是在`affinity`键下使用`podAntiAffinity`。
- en: So, what does this YAML do? In this case, we are recommending to the scheduler
    (a `soft` requirement) that it should attempt to schedule this Pod on a node where
    it or any other node with the same value for the `rack` label does not have any
    Pods running with `hunger` label values of 4 or 5\. We're telling the scheduler
    *try not to colocate this Pod in a domain with any extra hungry Pods*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个YAML文件是做什么的呢？在这种情况下，我们建议调度器（一个`soft`要求）应该尝试将这个Pod调度到一个节点上，在这个节点或具有相同值的`rack`标签的任何其他节点上都没有运行带有`hunger`标签值为4或5的Pod。我们告诉调度器*尽量不要将这个Pod与任何额外饥饿的Pod放在一起*。
- en: This feature gives us a great way to separate pods by failure domain – we can
    specify each rack as a domain and give it an anti-affinity with a selector of
    its own kind. This will make the scheduler schedule clones of the Pod (or try
    to, in a preferred affinity) to nodes that are not in the same failure domain,
    giving the application greater availability in case of a domain failure.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能为我们提供了一个很好的方法来按故障域分隔Pod - 我们可以将每个机架指定为一个域，并给它一个与自己相同类型的反亲和性。这将使调度器将Pod的克隆（或尝试在首选亲和性中）调度到不在相同故障域的节点上，从而在发生域故障时提供更大的可用性。
- en: We even have the option to combine Pod affinities and anti-affinities. Let's
    look at how this could work.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以选择结合Pod的亲和性和反亲和性。让我们看看这样可以如何工作。
- en: Combined affinity and anti-affinity
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合亲和性和反亲和性
- en: This is one of those situations where you can really put undue load on your
    cluster control plane. Combining Pod affinities with anti-affinities can allow
    incredibly nuanced rules that can be passed to the Kubernetes scheduler, which
    has the Herculean task of working to fulfill them.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个情况，你可以真正给你的集群控制平面增加不必要的负载。结合Pod的亲和性和反亲和性可以允许传递给Kubernetes调度器的非常微妙的规则。
- en: 'Let''s look at some YAML for a Deployment spec that combines these two concepts.
    Remember, affinity and anti-affinity are concepts that are applied to Pods – but
    we normally do not specify Pods without a controller like a Deployment or a ReplicaSet.
    Therefore, these rules are applied at the Pod spec level in the Deployment YAML.
    We are only showing the Pod spec part of this deployment for conciseness, but
    you can find the full file on the GitHub repository:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些结合这两个概念的部署规范的YAML。请记住，亲和性和反亲和性是应用于Pod的概念 - 但我们通常不会指定没有像部署或副本集这样的控制器的Pod。因此，这些规则是在部署YAML中的Pod规范级别应用的。出于简洁起见，我们只显示了这个部署的Pod规范部分，但你可以在GitHub存储库中找到完整的文件。
- en: pod-with-both-antiaffinity-and-affinity.yaml
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-both-antiaffinity-and-affinity.yaml
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this code block, we are telling the scheduler to treat the Pods in our Deployment
    as such: the Pod must be scheduled onto a node with a `rack` label such that it
    or any other node with a `rack` label and the same value has a Pod with `app=hungry-label-cache`.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码块中，我们告诉调度器将我们的部署中的Pod视为这样：Pod必须被调度到具有`rack`标签的节点上，以便它或具有相同值的`rack`标签的任何其他节点都有一个带有`app=hungry-label-cache`的Pod。
- en: Secondly, the scheduler must attempt to schedule the Pod, if possible, to a
    node with the `rack` label such that it or any other node with the `rack` label
    and the same value does not have a Pod with the `app=other-hungry-app` label running.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，调度器必须尝试将Pod调度到具有`rack`标签的节点上，以便它或具有相同值的`rack`标签的任何其他节点都没有运行带有`app=other-hungry-app`标签的Pod。
- en: To boil this down, we want our Pods for `hungry-app` to run in the same topology
    as the `hungry-app-cache`, and we do not want them to be in the same topology
    as the `other-hungry-app` if at all possible.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们希望我们的`hungry-app`的Pod在与`hungry-app-cache`相同的拓扑结构中运行，并且如果可能的话，我们不希望它们与`other-hungry-app`在相同的拓扑结构中。
- en: Since with great power comes great responsibility, and our tools for Pod affinity
    and anti-affinity are equal parts powerful and performance-reducing, Kubernetes
    ensures that some limits are set on the possible ways you can use both of them
    in order to prevent strange behavior or significant performance issues.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强大的力量伴随着巨大的责任，而我们的Pod亲和性和反亲和性工具既强大又降低性能，Kubernetes确保对您可以使用它们的可能方式设置了一些限制，以防止奇怪的行为或重大性能问题。
- en: Pod affinity and anti-affinity limitations
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod亲和性和反亲和性限制
- en: The biggest restriction on affinity and anti-affinity is that you are not allowed
    to use a blank `topologyKey`. Without restricting what the scheduler treats as
    a single topology type, some very unintended behavior can happen.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和性和反亲和性的最大限制是，您不允许使用空的`topologyKey`。如果不限制调度器将作为单个拓扑类型处理的内容，可能会发生一些非常意外的行为。
- en: The second limitation is that, by default, if you're using the hard version
    of anti-affinity – `requiredOnSchedulingIgnoredDuringExecution`, you cannot just
    use any label as a `topologyKey`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个限制是，默认情况下，如果您使用反亲和性的硬版本-`requiredOnSchedulingIgnoredDuringExecution`，您不能只使用任何标签作为`topologyKey`。
- en: Kubernetes will only let you use the `kubernetes.io/hostname` label, which essentially
    means that you can only have one topology per node if you're using `required`
    anti-affinity. This limitation does not exist for either the `prefer` anti-affinity
    or either of the affinities, even the `required` one. It is possible to change
    this functionality, but it requires writing a custom admission controller – which
    we will discuss in [*Chapter 12*](B14790_12_Final_PG_ePub.xhtml#_idTextAnchor269),
    *Kubernetes Security and Compliance*, and [*Chapter 13*](B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289),
    *Extending Kubernetes with CRDs*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes只允许您使用`kubernetes.io/hostname`标签，这基本上意味着如果您使用`required`反亲和性，您只能在每个节点上有一个拓扑。这个限制对于`prefer`反亲和性或任何亲和性都不存在，甚至是`required`。可以更改此功能，但需要编写自定义准入控制器-我们将在[*第12章*](B14790_12_Final_PG_ePub.xhtml#_idTextAnchor269)中讨论，*Kubernetes安全性和合规性*，以及[*第13章*](B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289)，*使用CRD扩展Kubernetes*。
- en: So far, our work with placement controls has not discussed namespaces. However,
    with Pod affinities and anti-affinities, they do hold relevance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对放置控件的工作尚未讨论命名空间。但是，对于Pod亲和性和反亲和性，它们确实具有相关性。
- en: Pod affinity and anti-affinity namespaces
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod亲和性和反亲和性命名空间
- en: Since Pod affinities and anti-affinities cause changes in behavior based on
    the location of other Pods, namespaces are a relevant piece to decide which Pods
    count for or against an affinity or anti-affinity.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Pod亲和性和反亲和性会根据其他Pod的位置而改变行为，命名空间是决定哪些Pod计入或反对亲和性或反亲和性的相关因素。
- en: By default, the scheduler will only look to the namespace in which the Pod with
    the affinity or anti-affinity was created. For all our previous examples, we haven't
    specified a namespace so the default namespace will be used.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，调度器只会查看创建具有亲和性或反亲和性的Pod的命名空间。对于我们之前的所有示例，我们没有指定命名空间，因此将使用默认命名空间。
- en: 'If you want to add one or more namespaces in which Pods will affect the affinity
    or anti-affinity, you can do so using the following YAML:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要添加一个或多个命名空间，其中Pod将影响亲和性或反亲和性，可以使用以下YAML：
- en: pod-with-anti-affinity-namespace.yaml
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: pod-with-anti-affinity-namespace.yaml
- en: '[PRE18]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this code block, the scheduler will look to the frontend, backend, and logging
    namespaces when trying to match the anti-affinity (as you can see on the `namespaces`
    key in the `podAffinityTerm` block). This allows us to constrain which namespaces
    the scheduler operates on when validating its rules.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码块中，调度器将在尝试匹配反亲和性时查看前端、后端和日志命名空间（如您在`podAffinityTerm`块中的`namespaces`键中所见）。这允许我们限制调度器在验证其规则时操作的命名空间。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about a few different controls that Kubernetes provides
    in order to enforce certain Pod placement rules via the scheduler. We learned
    that there are both "hard" requirements and "soft" rules, the latter of which
    are given the scheduler's best effort but do not necessarily prevent Pods that
    break the rules from being placed. We also learned a few reasons why you may want
    to implement scheduling controls – such as real-life failure domains and multitenancy.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了Kubernetes提供的一些不同控件，以强制执行调度器通过规则来放置Pod。我们了解到有“硬”要求和“软”规则，后者是调度器尽最大努力但不一定阻止违反规则的Pod被放置。我们还了解了一些实施调度控件的原因，比如现实生活中的故障域和多租户。
- en: We learned that there are simple ways to influence Pod placement, such as node
    selectors and node names – in addition to more advanced methods like taints and
    tolerations, which Kubernetes itself also uses by default. Finally, we discovered
    that there are some advanced tools that Kubernetes provides for node and Pod affinities
    and anti-affinities, which allow us to create complex rulesets for the scheduler
    to follow.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到有一些简单的方法可以影响Pod的放置，比如节点选择器和节点名称，还有更高级的方法，比如污点和容忍，Kubernetes本身也默认使用这些方法。最后，我们发现Kubernetes提供了一些高级工具，用于节点和Pod的亲和性和反亲和性，这些工具允许我们创建复杂的调度规则。
- en: In the next chapter, we will discuss observability on Kubernetes. We'll learn
    how to view application logs and we'll also use some great tools to get a view
    of what is happening in our cluster in real time.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论Kubernetes上的可观察性。我们将学习如何查看应用程序日志，还将使用一些很棒的工具实时查看我们集群中正在发生的事情。
- en: Questions
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between node selectors and the Node name field?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点选择器和节点名称字段之间有什么区别？
- en: How does Kubernetes use system-provided taints and tolerations? For what reasons?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes如何使用系统提供的污点和容忍？出于什么原因？
- en: Why should you be careful when using multiple types of Pod affinities or anti-affinities?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用多种类型的Pod亲和性或反亲和性时，为什么要小心？
- en: How could you balance availability across multiple failure zones with colocation
    for performance reasons for a three-tier web application? Give an example using
    node or Pod affinities and anti-affinities.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在多个故障区域之间平衡可用性，并出于性能原因进行合作，为三层Web应用程序提供一个例子？使用节点或Pod的亲和性和反亲和性。
- en: Further reading
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a more in-depth explanation of the default system taints and tolerations,
    head to [https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解有关默认系统污点和容忍的更深入解释，请访问[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions)。
