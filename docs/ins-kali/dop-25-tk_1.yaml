- en: Autoscaling Deployments and StatefulSets Based on Resource Usage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据资源使用自动调整部署和有状态集
- en: Change is the essential process of all existence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 变化是所有存在的基本过程。
- en: '- *Spock*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- *斯波克*'
- en: By now, you probably understood that one of the critical aspects of a system
    based on Kubernetes is a high level of dynamism. Almost nothing is static. We
    define Deployments or StatefulSets, and Kubernetes distributes the Pods across
    the cluster. In most cases, those Pods are rarely sitting in one place for a long
    time. Rolling updates result in Pods being re-created and potentially moved to
    other nodes. Failure of any kind provokes rescheduling of the affected resources.
    Many other events cause the Pods to move around. A Kubernetes cluster is like
    a beehive. It's full of life, and it's always in motion.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经了解到，基于Kubernetes的系统的一个关键方面是高度的动态性。几乎没有什么是静态的。我们定义部署或有状态集，Kubernetes会在集群中分发Pods。在大多数情况下，这些Pods很少在一个地方停留很长时间。滚动更新会导致Pods被重新创建并可能移动到其他节点。任何类型的故障都会引发受影响资源的重新调度。许多其他事件也会导致Pods移动。Kubernetes集群就像一个蜂巢。它充满了生机，而且总是在运动中。
- en: Dynamic nature of a Kubernetes cluster is not only due to our (human) actions
    or rescheduling caused by failures. Autoscaling is to be blamed as well. We should
    fully embrace Kubernetes' dynamic nature and move towards autonomous and self-sufficient
    clusters capable of serving the needs of our applications without (much) human
    involvement. To accomplish that, we need to provide sufficient information that
    will allow Kubernetes' to scale the applications as well as the nodes that constitute
    the cluster. In this chapter, we'll focus on the former case. We'll explore commonly
    used and basic ways to auto-scale Pods based on memory and CPU consumption. We'll
    accomplish that using HorizontalPodAutoscaler.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群的动态性不仅是由我们（人类）的行为或由故障引起的重新调度所致。自动缩放也应该受到责备。我们应该充分接受Kubernetes的动态性，并朝着能够满足我们应用程序需求的自主和自给的集群发展。为了实现这一点，我们需要提供足够的信息，让Kubernetes能够调整应用程序以及构成集群的节点。在本章中，我们将重点关注前一种情况。我们将探讨基于内存和CPU消耗的自动缩放Pods的常用和基本方法。我们将使用HorizontalPodAutoscaler来实现这一点。
- en: HorizontalPodAutoscaler's only function is to automatically scale the number
    of Pods in a Deployment, a StatefulSet, or a few other types of resources. It
    accomplishes that by observing CPU and memory consumption of the Pods and acting
    when they reach pre-defined thresholds.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: HorizontalPodAutoscaler的唯一功能是自动调整部署、有状态集或其他一些类型资源中Pods的数量。它通过观察Pods的CPU和内存消耗，并在达到预定义阈值时采取行动来实现这一点。
- en: HorizontalPodAutoscaler is implemented as a Kubernetes API resource and a controller.
    The resource determines the behavior of the controller. The controller periodically
    adjusts the number of replicas in a StatefulSet or a Deployment to match the observed
    average CPU utilization to the target specified by a user.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: HorizontalPodAutoscaler被实现为Kubernetes API资源和控制器。资源决定了控制器的行为。控制器定期调整有状态集或部署中的副本数量，以匹配用户指定的目标平均CPU利用率。
- en: We'll see HorizontalPodAutoscaler in action soon and comment on its specific
    features through practical examples. But, before we get there, we need a Kubernetes
    cluster as well as a source of metrics.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会看到HorizontalPodAutoscaler的实际应用，并通过实际示例评论其特定功能。但在那之前，我们需要一个Kubernetes集群以及一个度量源。
- en: Creating a cluster
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集群
- en: Before we create a cluster (or start using one you already have available),
    we'll clone the `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository which contains most of the definitions we'll use in this book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群之前（或开始使用您已经可用的集群），我们将克隆 `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    存储库，其中包含本书中大部分我们将使用的定义。
- en: A note to Windows users Please execute all the commands from this book from
    Git Bash. That way, you'll be able to run them as they are instead of modifying
    their syntax to adapt them to Windows terminal or PowerShell.All the commands
    from this chapter are available in the `01-hpa.sh` ([https://gist.github.com/vfarcic/b46ca2eababb98d967e3e25748740d0d](https://gist.github.com/vfarcic/b46ca2eababb98d967e3e25748740d0d))
    Gist.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 给Windows用户的说明：请从Git Bash中执行本书中的所有命令。这样，您就可以直接运行它们，而不需要修改其语法以适应Windows终端或PowerShell。本章中的所有命令都可以在
    `01-hpa.sh` ([https://gist.github.com/vfarcic/b46ca2eababb98d967e3e25748740d0d](https://gist.github.com/vfarcic/b46ca2eababb98d967e3e25748740d0d))
    Gist 中找到。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you cloned the repository before, please make sure that you have the latest
    version by executing `git pull`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前克隆过该存储库，请确保通过执行 `git pull` 来获取最新版本。
- en: The gists and the specifications that follow are used to test the commands in
    this chapter. Please use them as inspiration when creating your own test cluster
    or to validate that the one you're planning to use for the exercises meets the
    minimum requirements.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的代码片段和规范用于测试本章中的命令。请在创建自己的测试集群时以此为灵感，或者验证您计划用于练习的集群是否满足最低要求。
- en: '`docker-scale.sh`: **Docker for Desktop** with 2 CPUs, 2 GB RAM and with **tiller**
    ([https://gist.github.com/vfarcic/ca52ff97fc80565af0c46c37449babac](https://gist.github.com/vfarcic/ca52ff97fc80565af0c46c37449babac)).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker-scale.sh`: **Docker for Desktop** with 2 CPUs, 2 GB RAM and with **tiller**
    ([https://gist.github.com/vfarcic/ca52ff97fc80565af0c46c37449babac](https://gist.github.com/vfarcic/ca52ff97fc80565af0c46c37449babac)).'
- en: '`minikube-scale.sh`: **minikube** with 2 CPUs, 2 GB RAM and with **tiller**
    ([https://gist.github.com/vfarcic/5bc07d822f8825263245829715261a68](https://gist.github.com/vfarcic/5bc07d822f8825263245829715261a68)).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minikube-scale.sh`: **minikube** with 2 CPUs, 2 GB RAM and with **tiller**
    ([https://gist.github.com/vfarcic/5bc07d822f8825263245829715261a68](https://gist.github.com/vfarcic/5bc07d822f8825263245829715261a68)).'
- en: '`gke-scale.sh`: **GKE** with 3 n1-standard-1 worker nodes and with **tiller**
    ([https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c](https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c)).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gke-scale.sh`: **GKE** with 3 n1-standard-1 worker nodes and with **tiller**
    ([https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c](https://gist.github.com/vfarcic/9c777487f7ebee6c09027d3a1df8663c)).'
- en: '`eks-scale.sh`: **EKS** with 3 t2.small worker nodes and with **tiller** ([https://gist.github.com/vfarcic/a94dffef7d6dc60f79570d351c92408d](https://gist.github.com/vfarcic/a94dffef7d6dc60f79570d351c92408d)).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eks-scale.sh`: **EKS** with 3 t2.small worker nodes and with **tiller** ([https://gist.github.com/vfarcic/a94dffef7d6dc60f79570d351c92408d](https://gist.github.com/vfarcic/a94dffef7d6dc60f79570d351c92408d)).'
- en: '`aks-scale.sh`: **AKS** with 3 Standard_B2s worker nodes and with **tiller**
    ([https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b](https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b)).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aks-scale.sh`: **AKS** with 3 Standard_B2s worker nodes and with **tiller**
    ([https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b](https://gist.github.com/vfarcic/f1b05d33cc8a98e4ceab3d3770c2fe0b)).'
- en: Please note that we will use Helm to install necessary applications, but we'll
    switch to "pure" Kubernetes YAML for experimenting with (probably new) resources
    used in this chapter and for deploying the demo application. In other words, we'll
    use Helm for one-time installations (for example, Metrics Server) and YAML for
    things we'll explore in more detail (for example, HorizontalPodAutoscaler).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将使用Helm来安装必要的应用程序，但我们将切换到“纯粹”的Kubernetes YAML来尝试（可能是新的）本章中使用的资源，并部署演示应用程序。换句话说，我们将使用Helm进行一次性安装（例如，Metrics
    Server），并使用YAML来更详细地探索我们将要使用的内容（例如，HorizontalPodAutoscaler）。
- en: Now, let's talk about Metrics Server.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来谈谈Metrics Server。
- en: Observing Metrics Server data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察Metrics Server数据
- en: The critical element in scaling Pods is the Kubernetes Metrics Server. You might
    consider yourself a Kubernetes ninja and yet never heard of the Metrics Server.
    Don't be ashamed if that's the case. You're not the only one.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展Pods的关键元素是Kubernetes Metrics Server。你可能认为自己是Kubernetes的高手，但从未听说过Metrics Server。如果是这种情况，不要感到羞愧。你并不是唯一一个。
- en: If you started observing Kubernetes metrics, you might have used Heapster. It's
    been around for a long time, and you likely have it running in your cluster, even
    if you don't know what it is. Both serve the same purpose, with one being deprecated
    for a while, so let's clarify things a bit.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开始观察Kubernetes的指标，你可能已经使用过Heapster。它已经存在很长时间了，你可能已经在你的集群中运行它，即使你不知道它是什么。两者都有相同的目的，其中一个已经被弃用了一段时间，所以让我们澄清一下事情。
- en: Early on, Kubernetes introduced Heapster as a tool that enables Container Cluster
    Monitoring and Performance Analysis for Kubernetes. It's been around since Kubernetes
    version 1.0.6\. You can say that Heapster has been part of Kubernetes' life since
    its toddler age. It collects and interprets various metrics like resource usage,
    events, and so on. Heapster has been an integral part of Kubernetes and enabled
    it to schedule Pods appropriately. Without it, Kubernetes would be blind. It would
    not know which node has available memory, which Pod is using too much CPU, and
    so on. But, just as with most other tools that become available early, its design
    was a "failed experiment".
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，Kubernetes引入了Heapster作为一种工具，用于为Kubernetes启用容器集群监控和性能分析。它从Kubernetes版本1.0.6开始存在。你可以说Heapster从Kubernetes的幼年时代就开始了。它收集和解释各种指标，如资源使用情况、事件等。Heapster一直是Kubernetes的一个重要组成部分，并使其能够适当地调度Pods。没有它，Kubernetes将是盲目的。它不会知道哪个节点有可用内存，哪个Pod使用了太多的CPU等等。但是，就像大多数其他早期可用的工具一样，它的设计是一个“失败的实验”。
- en: As Kubernetes continued growing, we (the community around Kubernetes) started
    realizing that a new, better, and, more importantly, a more extensible design
    is required. Hence, Metrics Server was born. Right now, even though Heapster is
    still in use, it is considered deprecated, even though today (September 2018)
    the Metrics Server is still in beta state.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Kubernetes的持续增长，我们（Kubernetes周围的社区）开始意识到需要一个新的、更好的、更重要的是更具可扩展性的设计。因此，Metrics
    Server诞生了。现在，尽管Heapster仍在使用中，但它被视为已弃用，即使在今天（2018年9月），Metrics Server仍处于测试阶段。
- en: So, what is Metrics Server? A simple explanation is that it collects information
    about used resources (memory and CPU) of nodes and Pods. It does not store metrics,
    so do not think that you can use it to retrieve historical values and predict
    tendencies. There are other tools for that, and we'll explore them later. Instead,
    Metrics Server's goal is to provide an API that can be used to retrieve current
    resource usage. We can use that API through `kubectl` or by sending direct requests
    with, let's say, `curl`. In other words, Metrics Server collects cluster-wide
    metrics and allows us to retrieve them through its API. That, by itself, is very
    powerful, but it is only the part of the story.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Metrics Server是什么？一个简单的解释是，它收集有关节点和Pod使用的资源（内存和CPU）的信息。它不存储指标，所以不要认为您可以使用它来检索历史值和预测趋势。有其他工具可以做到这一点，我们稍后会探讨它们。相反，Metrics
    Server的目标是提供一个API，可以用来检索当前的资源使用情况。我们可以通过`kubectl`或通过发送直接请求，比如`curl`来使用该API。换句话说，Metrics
    Server收集集群范围的指标，并允许我们通过其API检索这些指标。这本身就非常强大，但这只是故事的一部分。
- en: I already mentioned extensibility. We can extend Metrics Server to collect metrics
    from other sources. We'll get there in due time. For now, we'll explore what it
    provides out of the box and how it interacts with some other Kubernetes resources
    that will help us make our Pods scalable and more resilient.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经提到了可扩展性。我们可以扩展Metrics Server以从其他来源收集指标。我们会在适当的时候到达那里。现在，我们将探索它提供的开箱即用功能，以及它如何与一些其他Kubernetes资源交互，这些资源将帮助我们使我们的Pods可伸缩和更具弹性。
- en: If you read my other books, you know that I do not go into much theory and,
    instead, prefer demonstrating features and principles through practical examples.
    This book is no exception, and we'll dive straight into Metrics Server hands-on
    exercises. The first step is to install it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您读过我的其他书，您就会知道我不会过多涉及理论，而是更喜欢通过实际示例来演示功能和原则。这本书也不例外，我们将直接深入了解Metrics Server的实际练习。第一步是安装它。
- en: Helm makes installation of almost any publicly available software very easy
    if there is a Chart available. If there isn't, you might want to consider an alternative
    since that is a clear indication that the vendor or the community behind it does
    not believe in Kubernetes. Or, maybe they do not have the skills necessary to
    develop a Chart. Either way, the best course of action is to run away from it
    and adopt an alternative. If that's not an option, develop a Helm Chart yourself.
    In our case, there won't be a need for such measures. Metrics Server does have
    a Helm Chart, and all we need to do is to install it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Helm使安装几乎任何公开可用的软件变得非常容易，如果有Chart可用的话。如果没有，您可能需要考虑另一种选择，因为这清楚地表明供应商或社区不相信Kubernetes。或者，也许他们没有必要开发Chart的技能。无论哪种方式，最好的做法是远离它并采用另一种选择。如果这不是一个选择，那就自己开发一个Helm
    Chart。在我们的情况下，不需要这样的措施。Metrics Server确实有一个Helm Chart，我们需要做的就是安装它。
- en: A note to GKE and AKS users Google and Microsoft already ship Metrics Server
    as part of their managed Kubernetes clusters (GKE and AKS). There is no need to
    install it, so please skip the commands that follow.A note to minikube users Metrics
    Server is available as one of the plugins. Please execute `minikube addons enable
    metrics-server` and `kubectl -n kube-system rollout status deployment metrics-server`
    commands instead of those following.A note to Docker for Desktop users Recent
    updates to the Metrics Server do not work with self-signed certificates by default.
    Since Docker for Desktop uses such certificates, you'll need to allow insecure
    TLS. Please add `--set args={"--kubelet-insecure-tls=true"}` argument to the `helm
    install` command that follows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GKE和AKS用户请注意，Google和Microsoft已经将Metrics Server作为其托管的Kubernetes集群（GKE和AKS）的一部分进行了打包。无需安装它，请跳过接下来的命令。对于minikube用户，请注意，Metrics
    Server作为插件之一可用。请执行`minikube addons enable metrics-server`和`kubectl -n kube-system
    rollout status deployment metrics-server`命令，而不是接下来的命令。对于Docker for Desktop用户，请注意，Metrics
    Server的最新更新默认情况下不适用于自签名证书。由于Docker for Desktop使用这样的证书，您需要允许不安全的TLS。请在接下来的`helm
    install`命令中添加`--set args={"--kubelet-insecure-tls=true"}`参数。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We used Helm to install Metrics Server, and we waited until it rolled out.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Helm安装了Metrics Server，并等待直到它部署完成。
- en: Metrics Server will periodically fetch metrics from Kubeletes running on the
    nodes. Those metrics, for now, contain memory and CPU utilization of the Pods
    and the nodes. Other entities can request data from the Metrics Server through
    the API Server which has the Master Metrics API. An example of those entities
    is the Scheduler that, once Metrics Server is installed, uses its data to make
    decisions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Metrics Server将定期从运行在节点上的Kubeletes中获取指标。目前，这些指标包括Pod和节点的内存和CPU利用率。其他实体可以通过具有Master
    Metrics API的API服务器从Metrics Server请求数据。这些实体的一个例子是调度程序，一旦安装了Metrics Server，就会使用其数据来做出决策。
- en: As you will see soon, the usage of the Metrics Server goes beyond the Scheduler
    but, for now, the explanation should provide an image of the basic flow of data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 很快您将会看到，Metrics Server的使用超出了调度程序，但是目前，这个解释应该提供了一个基本数据流的图像。
- en: '![](assets/b3d2c07d-2505-4373-ae4c-181a83afb8c0.png)Figure 1-1: The basic flow
    of the data to and from the Metrics Server (arrows show directions of data flow)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b3d2c07d-2505-4373-ae4c-181a83afb8c0.png)图1-1：数据流向和从Metrics Server获取数据的基本流程（箭头显示数据流向）'
- en: Now we can explore one of the ways we can retrieve the metrics. We'll start
    with those related to nodes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以探索一种检索指标的方式。我们将从与节点相关的指标开始。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you were fast, the output should state that `metrics are not available yet`.
    That's normal. It takes a few minutes before the first iteration of metrics retrieval
    is executed. The exception is GKE and AKS that already come with the Metrics Server
    baked in.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您很快，输出应该会声明“尚未提供指标”。这是正常的。在执行第一次迭代的指标检索之前需要几分钟时间。例外情况是GKE和AKS，它们已经预先安装了Metrics
    Server。
- en: Fetch some coffee before we repeat the command.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在重复命令之前先去冲杯咖啡。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This time, the output is different.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，输出是不同的。
- en: In this chapter, I'll show the outputs from Docker for Desktop. Depending on
    the Kubernetes flavor you're using, your outputs will be different. Still, the
    logic is the same and you should not have a problem to follow along.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将展示来自Docker for Desktop的输出。根据您使用的Kubernetes版本不同，您的输出也会有所不同。但是，逻辑是相同的，您不应该有问题跟随操作。
- en: My output is as follows.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我的输出如下。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can see that I have one node called `docker-for-desktop`. It is using 248
    CPU milliseconds. Since the node has two cores, that's 12% of the total available
    CPU. Similarly, 1.2 GB of RAM is used, which is 63% of the total available memory
    of 2 GB.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我有一个名为`docker-for-desktop`的节点。它正在使用248 CPU毫秒。由于节点有两个核心，这占总可用CPU的12%。同样，使用了1.2GB的RAM，这占总可用内存2GB的63%。
- en: Resource usage of the nodes is useful but is not what we're looking for. In
    this chapter, we're focused on auto-scaling Pods. But, before we get there, we
    should observe how much memory each of our Pods is using. We'll start with those
    running in the `kube-system` Namespace.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的资源使用情况很有用，但不是我们要寻找的内容。在本章中，我们专注于Pod的自动扩展。但是，在我们开始之前，我们应该观察一下我们的每个Pod使用了多少内存。我们将从在`kube-system`命名空间中运行的Pod开始。
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output (on Docker for Desktop) is as follows.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（在Docker for Desktop上）如下。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see resource usage (CPU and memory) for each of the Pods currently running
    in `kube-system`. If we do not find better tools, we could use that information
    to adjust `requests` of those Pods to be more accurate. However, there are better
    ways to get that info, so we'll skip adjustments for now. Instead, let's try to
    get current resource usage of all the Pods, no matter the Namespace.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`kube-system`中当前运行的每个Pod的资源使用情况（CPU和内存）。如果我们找不到更好的工具，我们可以使用该信息来调整这些Pod的`requests`以使其更准确。但是，有更好的方法来获取这些信息，所以我们将暂时跳过调整。相反，让我们尝试获取所有Pod的当前资源使用情况，无论命名空间如何。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output (on Docker for Desktop) is as follows.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（在Docker for Desktop上）如下。
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That output shows the same information as the previous one, only extended to
    all Namespaces. There should be no need to comment it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出显示与上一个输出相同的信息，只是扩展到所有命名空间。不需要对其进行评论。
- en: Often, metrics of a Pod are not granular enough, and we need to observe the
    resources of each of the containers that constitute a Pod. All we need to do to
    get container metrics is to add `--containers` argument.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Pod的度量不够精细，我们需要观察构成Pod的每个容器的资源。要获取容器度量，我们只需要添加`--containers`参数。
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output (on Docker for Desktop) is as follows.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（在Docker for Desktop上）如下。
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that, this time, the output shows each container separately. We can,
    for example, observe metrics of the `kube-dns-*` Pod separated into three containers
    (`kubedns`, `dnsmasq`, `sidecar`).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这次输出显示了每个容器。例如，我们可以观察到`kube-dns-*` Pod的度量分为三个容器（`kubedns`，`dnsmasq`，`sidecar`）。
- en: When we request metrics through `kubectl top`, the flow of data is almost the
    same as when the scheduler makes requests. A request is sent to the API Server
    (Master Metrics API), which gets data from the Metrics Server which, in turn,
    was collecting information from Kubeletes running on the nodes of the cluster.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过`kubectl top`请求指标时，数据流几乎与调度程序发出请求时的流程相同。请求被发送到API服务器（主度量API），该服务器从度量服务器获取数据，而度量服务器又从集群节点上运行的Kubeletes收集信息。
- en: '![](assets/e5faabe0-8a82-4b4a-b2b3-7e8298f6dc0a.png)Figure 1-2: The flow of
    the data to and from the Metrics Server (arrows show directions of data flow)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e5faabe0-8a82-4b4a-b2b3-7e8298f6dc0a.png)图1-2：数据流向和从度量服务器流向的方向（箭头显示数据流向）'
- en: While `kubectl top` command is useful to observe current metrics, it is pretty
    useless if we'd like to access them from other tools. After all, the goal is not
    for us to sit in front of a terminal with `watch "kubectl top pods"` command.
    That would be a waste of our (human) talent. Instead, our goal should be to scrape
    those metrics from other tools and create alerts and (maybe) dashboards based
    on both real-time and historical data. For that, we need output in JSON or some
    other machine-parsable format. Luckily, `kubectl` allows us to invoke its API
    directly in raw format and retrieve the same result as if a tool would query it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `kubectl top` 命令对观察当前指标很有用，但如果我们想从其他工具访问它们，它就没什么用了。毕竟，我们的目标不是坐在终端前用 `watch
    "kubectl top pods"` 命令。那将是浪费我们（人类）的才能。相反，我们的目标应该是从其他工具中抓取这些指标，并根据实时和历史数据创建警报和（也许）仪表板。为此，我们需要以
    JSON 或其他机器可解析的格式输出。幸运的是，`kubectl` 允许我们以原始格式直接调用其 API，并检索与工具查询相同的结果。
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output is as follows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see that the `/apis/metrics.k8s.io/v1beta1` endpoint is an index API
    that has two resources (`nodes` and `pods`).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `/apis/metrics.k8s.io/v1beta1` 端点是一个索引 API，有两个资源（`nodes` 和 `pods`）。
- en: Let's take a closer look at the `pods` resource of the metrics API.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下度量 API 的 `pods` 资源。
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output is too big to be presented in a book, so I'll leave it up to you
    to explore it. You'll notice that the output is JSON equivalent of what we observed
    through the `kubectl top pods --all-namespaces --containers` command.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出太大，无法在一本书中呈现，所以我会留给你去探索。你会注意到输出是通过 `kubectl top pods --all-namespaces --containers`
    命令观察到的 JSON 等效物。
- en: That was a rapid overview of the Metrics Server. There are two important things
    to note. First of all, it provides current (or short-term) memory and CPU utilization
    of the containers running inside a cluster. The second and the more important
    note is that we will not use it directly. Metrics Server was not designed for
    humans but for machines. We'll get there later. For now, remember that there is
    a thing called Metrics Server and that you should not use it directly (once you
    adopt a tool that will scrape its metrics).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是度量服务器的快速概述。有两件重要的事情需要注意。首先，它提供了集群内运行的容器的当前（或短期）内存和 CPU 利用率。第二个更重要的注意事项是我们不会直接使用它。度量服务器不是为人类设计的，而是为机器设计的。我们以后会到那里。现在，记住有一个叫做度量服务器的东西，你不应该直接使用它（一旦你采用了一个会抓取其度量的工具）。
- en: Now that we explored Metrics Server, we'll try to put it to good use and learn
    how to auto-scale our Pods based on resource utilization.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了度量服务器，我们将尝试充分利用它，并学习如何根据资源利用率自动扩展我们的 Pods。
- en: Auto-scaling Pods based on resource utilization
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据资源利用率自动扩展 Pods
- en: Our goal is to deploy an application that will be automatically scaled (or de-scaled)
    depending on its use of resources. We'll start by deploying an app first, and
    discuss how to accomplish auto-scaling later.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是部署一个应用程序，根据其资源使用情况自动扩展（或缩小）。我们将首先部署一个应用程序，然后讨论如何实现自动扩展。
- en: 'I already warned you that I assume that you are familiar with Kubernetes and
    that in this book we''ll explore a particular topic of monitoring, alerting, scaling,
    and a few other things. We will not discuss Pods, StatefulSets, Deployments, Services,
    Ingress, and other "basic" Kubernetes resources. This is your last chance to admit
    that you do NOT understand Kubernetes'' fundamentals, to take a step back, and
    to read *The DevOps 2.3 Toolkit: Kubernetes* ([https://www.devopstoolkitseries.com/posts/devops-23/](https://www.devopstoolkitseries.com/posts/devops-23/))
    and *The DevOps 2.4 Toolkit: Continuous Deployment To Kubernetes* ([https://www.devopstoolkitseries.com/posts/devops-24/](https://www.devopstoolkitseries.com/posts/devops-24/)*)*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '我已经警告过您，我假设您熟悉Kubernetes，并且在本书中我们将探讨监控，警报，扩展和其他一些特定主题。我们不会讨论Pods，StatefulSets，Deployments，Services，Ingress和其他“基本”Kubernetes资源。这是您承认您不了解Kubernetes基础知识的最后机会，退一步，并阅读*The
    DevOps 2.3 Toolkit: Kubernetes* ([https://www.devopstoolkitseries.com/posts/devops-23/](https://www.devopstoolkitseries.com/posts/devops-23/))和*The
    DevOps 2.4 Toolkit: Continuous Deployment To Kubernetes* ([https://www.devopstoolkitseries.com/posts/devops-24/](https://www.devopstoolkitseries.com/posts/devops-24/)*)*。'
- en: Let's take a look at a definition of the application we'll use in our examples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们示例中将使用的应用程序的定义。
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If you are familiar with Kubernetes, the YAML definition should be self-explanatory.
    We'll comment only the parts that are relevant for auto-scaling.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉Kubernetes，YAML定义应该是不言自明的。我们只会评论与自动扩展相关的部分。
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下。
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have two Pods that form an application. The `api` Deployment is a backend
    API that uses `db` StatefulSet for its state.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个形成应用程序的Pod。 “api”部署是一个后端API，使用“db” StatefulSet来保存其状态。
- en: The essential parts of the definition are `resources`. Both the `api` and the
    `db` have `requests` and `limits` defined for memory and CPU. The database uses
    a sidecar container that will join MongoDB replicas into a replica set. Please
    note that, unlike other containers, the sidecar does not have `resources`. The
    importance behind that will be revealed later. For now, just remember that two
    containers have the `requests` and the `limits` defined, and that one doesn't.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的基本部分是“资源”。 “api”和“db”都为内存和CPU定义了“请求”和“限制”。数据库使用一个sidecar容器，将MongoDB副本加入到副本集中。请注意，与其他容器不同，sidecar没有“资源”。这背后的重要性将在稍后揭示。现在，只需记住两个容器有定义的“请求”和“限制”，而另一个没有。
- en: Now, let's create those resources.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建这些资源。
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The output should show that quite a few resources were created and our next
    action is to wait until the `api` Deployment is rolled out thus confirming that
    the application is up-and-running.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该显示已创建了相当多的资源，我们的下一步是等待“api”部署推出，从而确认应用程序正在运行。
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: After a few moments, you should see the message stating that `deployment "api"
    was successfully rolled out`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，您应该会看到消息，指出“api”部署成功推出。
- en: To be on the safe side, we'll list the Pods in the `go-demo-5` Namespace and
    confirm that one replica of each is running.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，我们将列出“go-demo-5”命名空间中的Pod，并确认每个Pod都在运行一个副本。
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output is as follows.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So far, we did not yet do anything beyond the ordinary creation of the StatefulSet
    and the Deployment.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有做任何超出StatefulSet和Deployment的普通创建。
- en: They, in turn, created ReplicaSets, which resulted in the creation of the Pods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 他们又创建了ReplicaSets，这导致了Pod的创建。
- en: '![](assets/ed5f3b30-98d3-4735-b3f4-7a4a79755f72.png)Figure 1-3: Creation of
    the StatefulSet and the Deployment'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/ed5f3b30-98d3-4735-b3f4-7a4a79755f72.png)图1-3：StatefulSet和Deployment的创建'
- en: As you hopefully know, we should aim at having at least two replicas of each
    Pod, as long as they are scalable. Still, neither of the two had `replicas` defined.
    That is intentional. The fact that we can specify the number of replicas of a
    Deployment or a StatefulSet does not mean that we should. At least, not always.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你知道，我们应该至少有每个Pod的两个副本，只要它们是可扩展的。然而，这两者都没有定义`replicas`。这是有意的。我们可以指定部署或StatefulSet的副本数量，并不意味着我们应该这样做。至少，不总是。
- en: If the number of replicas is static and you have no intention to scale (or de-scale)
    your application over time, set `replicas` as part of your Deployment or StatefulSet
    definition. If, on the other hand, you plan to change the number of replicas based
    on memory, CPU, or other metrics, use HorizontalPodAutoscaler resource instead.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果副本数量是静态的，并且你没有打算随时间扩展（或缩减）你的应用程序，那么将`replicas`作为部署或StatefulSet定义的一部分。另一方面，如果你计划根据内存、CPU或其他指标更改副本数量，请改用HorizontalPodAutoscaler资源。
- en: Let's take a look at a simple example of a HorizontalPodAutoscaler.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个HorizontalPodAutoscaler的简单示例。
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output is as follows.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The definition uses `HorizontalPodAutoscaler` targeting the `api` Deployment.
    Its boundaries are the minimum of two and the maximum of five replicas. Those
    limits are fundamental. Without them, we'd run a risk of scaling up into infinity
    or scaling down to zero replicas. The `minReplicas` and `maxReplicas` fields are
    a safety net.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 定义使用`HorizontalPodAutoscaler`来定位`api`部署。它的边界是最少两个和最多五个副本。这些限制是基本的。没有这些限制，我们会面临无限扩展或缩减到零副本的风险。`minReplicas`和`maxReplicas`字段是一个安全网。
- en: The key section of the definition is `metrics`. It provides formulas Kubernetes
    should use to decide whether it should scale (or de-scale) a resource. In our
    case, we're using the `Resource` type entries. They are targeting average utilization
    of eighty percent for memory and CPU. If the actual usage of the either of the
    two deviates, Kubernetes will scale (or de-scale) the resource.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的关键部分是`metrics`。它提供了Kubernetes应该使用的公式来决定是否应该扩展（或缩减）资源。在我们的例子中，我们使用`Resource`类型的条目。它们针对内存和CPU的平均利用率为80％。如果两者中的任何一个实际使用情况偏离，Kubernetes将扩展（或缩减）资源。
- en: Please note that we used `v2beta1` version of the API and you might be wondering
    why we chose that one instead of the stable and production ready `v1`. After all,
    `beta1` releases are still far from being polished enough for general usage. The
    reason is simple. HorizontalPodAutoscaler `v1` is too basic. It only allows scaling
    based on CPU. Even our simple example goes beyond that by adding memory to the
    mix. Later on, we'll extend it even more. So, while `v1` is considered stable,
    it does not provide much value, and we can either wait until `v2` is released
    or start experimenting with `v2beta` releases right away. We're opting for the
    latter option. By the time you read this, more stable releases are likely to exist
    and to be supported in your Kubernetes cluster. If that's the case, feel free
    to change `apiVersion` before applying the definition.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了API的`v2beta1`版本，你可能想知道为什么我们选择了这个版本，而不是稳定且适用于生产的`v1`。毕竟，`beta1`版本仍远未经过充分打磨以供一般使用。原因很简单。HorizontalPodAutoscaler
    `v1`太基础了。它只允许基于CPU进行扩展。即使我们的简单示例也超越了这一点，通过将内存加入其中。以后，我们将进一步扩展它。因此，虽然`v1`被认为是稳定的，但它并没有提供太多价值，我们可以等待`v2`发布，或者立即开始尝试`v2beta`版本。我们选择了后者。当你阅读这篇文章时，更稳定的版本可能已经存在并且在你的Kubernetes集群中得到支持。如果是这种情况，请随时在应用定义之前更改`apiVersion`。
- en: Now let's apply it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用它。
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We applied the definition that created the **HorizontalPodAutoscaler** (**HPA**).
    Next, we'll take a look at the information we'll get by retrieving the HPA resources.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了创建**HorizontalPodAutoscaler**（**HPA**）的定义。接下来，我们将查看检索HPA资源时获得的信息。
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you were quick, the output should be similar to the one that follows.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你很快，输出应该类似于以下内容。
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can see that Kubernetes does not yet have the actual CPU and memory utilization
    and that it output `<unknown>` instead. We need to give it a bit more time until
    the next iteration of data gathering from the Metrics Server. Get yourself some
    coffee before we repeat the same query.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Kubernetes尚未具有实际的CPU和内存利用率，而是输出了`<unknown>`。在从Metrics Server收集下一次数据之前，我们需要再给它一些时间。在我们重复相同的查询之前，先喝杯咖啡。
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This time, the output is without unknowns.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，输出中没有未知项。
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can see that both CPU and memory utilization are way below the expected utilization
    of `80%`. Still, Kubernetes increased the number of replicas from one to two because
    that's the minimum we defined. We made the contract stating that the `api` Deployment
    should never have less than two replicas, and Kubernetes complied with that by
    scaling up even if the resource utilization is way below the expected average
    utilization. We can confirm that behavior through the events of the HorizontalPodAutoscaler.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，CPU和内存利用率远低于预期的`80%`利用率。尽管如此，Kubernetes将副本数从一个增加到两个，因为这是我们定义的最小值。我们签订了合同，规定`api`
    Deployment的副本数永远不得少于两个，即使资源利用率远低于预期的平均利用率，Kubernetes也会遵守这一点进行扩展。我们可以通过HorizontalPodAutoscaler的事件来确认这种行为。
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output, limited to the event messages, is as follows.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于事件消息，如下所示。
- en: '[PRE28]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The message of the event should be self-explanatory. The HorizontalPodAutoscaler
    changed the number of replicas to `2` because the current number (1) was below
    the `MinReplicas` value.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 事件的消息应该是不言自明的。HorizontalPodAutoscaler将副本数更改为`2`，因为当前数量（1）低于`MinReplicas`值。
- en: Finally, we'll list the Pods to confirm that the desired number of replicas
    is indeed running.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将列出Pods，以确认所需数量的副本确实正在运行。
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output is as follows.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So far, the HPA did not yet perform auto-scaling based on resource usage. Instead,
    it only increased the number of Pod to meet the specified minimum. It did that
    by manipulating the Deployment.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，HPA尚未根据资源使用情况执行自动缩放。相反，它只增加了Pod的数量以满足指定的最小值。它通过操纵Deployment来实现这一点。
- en: '![](assets/c9a6db6e-d3a9-485e-a687-b92b0629e19e.png)Figure 1-4: Scaling of
    the Deployment based on minimum number of replicas specified in the HPA'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/c9a6db6e-d3a9-485e-a687-b92b0629e19e.png)图1-4：根据HPA中指定的最小副本数进行部署的扩展'
- en: Next, we'll try to create another HorizontalPodAutoscaler but, this time, we'll
    target the StatefulSet that runs our MongoDB. So, let's take a look at yet another
    YAML definition.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试创建另一个HorizontalPodAutoscaler，但这次，我们将以运行我们的MongoDB的StatefulSet为目标。因此，让我们再看一下另一个YAML定义。
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output is as follows.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That definition is almost the same as the one we used before. The only difference
    is that this time we're targeting `StatefulSet` called `db` and that the minimum
    number of replicas should be `3`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该定义几乎与我们之前使用的定义相同。唯一的区别是，这次我们的目标是名为`db`的`StatefulSet`，并且最小副本数应为`3`。
- en: Let's apply it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用它。
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Let's take another look at the HorizontalPodAutoscaler resources.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一下HorizontalPodAutoscaler资源。
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The output is as follows.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE35]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can see that the second HPA was created and that the current utilization
    is `unknown`. That must be a similar situation as before. Should we give it some
    time for data to start flowing in? Wait for a few moments and retrieve HPAs again.
    Are the targets still `unknown`?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到第二个HPA已经创建，并且当前利用率为“未知”。这一定是之前的类似情况。我们应该给它一些时间让数据开始流动吗？等待片刻，然后再次检索HPA。目标仍然是“未知”吗？
- en: There might be something wrong since the resource utilization continued being
    unknown. Let's describe the newly created HPA and see whether we'll be able to
    find the cause behind the issue.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 资源利用持续未知可能有问题。让我们描述新创建的HPA，看看是否能找到问题的原因。
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The output, limited to the event messages, is as follows.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于事件消息，如下所示。
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Please note that your output could have only one event, or even none of those.
    If that's the case, please wait for a few minutes and repeat the previous command.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您的输出可能只有一个事件，甚至没有这些事件。如果是这种情况，请等待几分钟，然后重复上一个命令。
- en: If we focus on the first message, we can see that it started well. HPA detected
    that the current number of replicas is below the limit and increased them to three.
    That is the expected behavior, so let's move to the other two messages.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们关注第一条消息，我们可以看到它开始得很好。HPA检测到当前副本数低于限制，并将它们增加到了三个。这是预期的行为，所以让我们转向其他两条消息。
- en: HPA could not calculate the percentage because we did not specify how much memory
    we are requesting for the `db-sidecar` container. Without `requests`, HPA cannot
    calculate the percentage of the actual memory usage. In other words, we missed
    specifying resources for the `db-sidecar` container and HPA could not do its work.
    We'll fix that by applying `go-demo-5-no-hpa.yml`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: HPA无法计算百分比，因为我们没有指定“db-sidecar”容器请求多少内存。没有“requests”，HPA无法计算实际内存使用的百分比。换句话说，我们忽略了为“db-sidecar”容器指定资源，HPA无法完成其工作。我们将通过应用“go-demo-5-no-hpa.yml”来解决这个问题。
- en: Let's take a quick look at the new definition.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下新定义。
- en: '[PRE38]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE39]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The only noticeable difference, when compared with the initial definition, is
    that this time we defined the resources for the `db-sidecar` container. Let's
    apply it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与初始定义相比，唯一显着的区别是这次我们为“db-sidecar”容器定义了资源。让我们应用它。
- en: '[PRE40]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Next, we'll wait for a few moments for the changes to take effect, before we
    retrieve the HPAs again.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将等待片刻以使更改生效，然后再次检索HPA。
- en: '[PRE41]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This time, the output is more promising.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，输出更有希望。
- en: '[PRE42]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Both HPAs are showing the current and the target resource usage. Neither reached
    the target values, so HPA is maintaining the minimum number of replicas. We can
    confirm that by listing all the Pods in the `go-demo-5` Namespace.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 两个HPA都显示了当前和目标资源使用情况。都没有达到目标值，所以HPA保持了最小副本数。我们可以通过列出“go-demo-5”命名空间中的所有Pod来确认这一点。
- en: '[PRE43]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The output is as follows.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE44]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can see that there are two Pods for the `api` Deployment and three replicas
    of the `db` StatefulSet. Those numbers are equivalent to the `spec.minReplicas`
    entries in the HPA definitions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到“api”部署有两个Pod，而“db” StatefulSet有三个副本。这些数字等同于HPA定义中的“spec.minReplicas”条目。
- en: Let's see what happens when the actual memory usage is above the target value.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当实际内存使用量高于目标值时会发生什么。
- en: We'll modify the definition of one of the HPAs by lowering one of the targets
    as a way to reproduce the situation in which our Pods are consuming more resources
    than desired.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过降低其中一个HPA的目标来修改其定义，以重现我们的Pod消耗资源超出预期的情况。
- en: Let's take a look at a modified HPA definition.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下修改后的HPA定义。
- en: '[PRE45]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE46]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We decreased `targetAverageUtilization` to `10`. That will surely be below the
    current memory utilization, and we'll be able to witness HPA in action. Let's
    apply the new definition.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`targetAverageUtilization`减少到`10`。这肯定低于当前的内存利用率，我们将能够见证HPA的工作。让我们应用新的定义。
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Please wait a few moments for the next iteration of data gathering to occur,
    and retrieve the HPAs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请等待一段时间，以便进行下一次数据收集迭代，并检索HPAs。
- en: '[PRE48]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The output is as follows.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE49]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We can see that the actual memory of the `api` HPA (`49%`) is way above the
    threshold (`10%`). However, the number of replicas is still the same (`2`). We'll
    have to wait for a few more minutes before we retrieve HPAs again.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`api` HPA的实际内存（`49%`）远远超过了阈值（`10%`）。然而，副本的数量仍然是相同的（`2`）。我们需要等待几分钟，然后再次检索HPAs。
- en: '[PRE50]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This time, the output is slightly different.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，输出略有不同。
- en: '[PRE51]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We can see that the number of replicas increased to `4`. HPA changed the Deployment,
    and that produced the cascading effect that resulted in the increased number of
    Pods.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到副本数量增加到`4`。HPA改变了部署，导致了级联效应，从而增加了Pod的数量。
- en: Let's describe the `api` HPA.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下`api` HPA。
- en: '[PRE52]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The output, limited to the messages of the events, is as follows.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于事件消息，如下所示。
- en: '[PRE53]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We can see that the HPA changed the size to `4` because `memory resource utilization
    (percentage of request)` was `above target`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到HPA将大小更改为`4`，因为`内存资源利用率（请求百分比）`高于目标。
- en: Since, in this case, increasing the number of replicas did not reduce memory
    consumption below the HPA target, we should expect that the HPA will continue
    scaling up the Deployment until it reaches the limit of `5`. We'll confirm that
    assumption by waiting for a few minutes and describing the HPA one more time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在这种情况下，增加副本数量并没有将内存消耗降低到HPA目标以下，我们应该期望HPA将继续扩展部署，直到达到`5`的限制。我们将通过等待几分钟并再次描述HPA来确认这一假设。
- en: '[PRE54]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The output, limited to the messages of the events, is as follows.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于事件消息，如下所示。
- en: '[PRE55]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We got the message stating that the new size is now `5`, thus proving that the
    HPA will continue scaling up until the resources are below the target or, as in
    our case, it reaches the maximum number of replicas.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到了消息，说明新的大小现在是`5`，从而证明HPA将继续扩展，直到资源低于目标，或者在我们的情况下，达到最大副本数量。
- en: We can confirm that scaling indeed worked by listing all the Pods in the `go-demo-5`
    Namespace.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过列出`go-demo-5`命名空间中的所有Pod来确认扩展确实起作用。
- en: '[PRE56]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The output is as follows.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE57]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: As we can see, there are indeed five replicas of the `api` Deployment.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`api`部署确实有五个副本。
- en: HPA retrieved data from the Metrics Server, concluded that the actual resource
    usage is higher than the threshold, and manipulated the Deployment with the new
    number of replicas.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: HPA从Metrics Server中检索数据，得出实际资源使用量高于阈值，并使用新的副本数量操纵了部署。
- en: '![](assets/1176f620-4e31-44e3-8eb0-5c8a4ef9b2e6.png)Figure 1-5: HPA scaling
    through manipulation of the Deployment'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/1176f620-4e31-44e3-8eb0-5c8a4ef9b2e6.png)图1-5：HPA通过操纵部署进行扩展'
- en: Next, we'll validate that de-scaling works as well. We'll do that by re-applying
    the initial definition that has both the memory and the CPU set to eighty percent.
    Since the actual memory usage is below that, the HPA should start scaling down
    until it reaches the minimum number of replicas.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将验证缩减副本数量也能正常工作。我们将重新应用初始定义，其中内存和CPU都设置为百分之八十。由于实际内存使用量低于该值，HPA应该开始缩减，直到达到最小副本数量。
- en: '[PRE58]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Just as before, we'll wait for a few minutes before we describe the HPA.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们将等待几分钟，然后再描述HPA。
- en: '[PRE59]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The output, limited to the events messages, is as follows.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于事件消息，如下所示。
- en: '[PRE60]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As we can see, it changed the size to `3` since all the `metrics` are `below
    target`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，它将大小更改为`3`，因为所有的`metrics`都`below target`。
- en: A while later, it will de-scale again to two replicas and stop since that's
    the limit we set in the HPA definition.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，它会再次缩减到两个副本，并停止，因为这是我们在HPA定义中设置的限制。
- en: To replicas or not to replicas in Deployments and StatefulSets?
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在部署和有状态集中使用副本还是不使用副本？
- en: Knowing that HorizontalPodAutoscaler (HPA) manages auto-scaling of our applications,
    the question might arise regarding replicas. Should we define them in our Deployments
    and StatefulSets, or should we rely solely on HPA to manage them? Instead of answering
    that question directly, we'll explore different combinations and, based on results,
    define the strategy.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 知道HorizontalPodAutoscaler（HPA）管理我们应用程序的自动扩展，可能会产生关于副本的问题。我们应该在我们的部署和有状态集中定义它们，还是应该完全依赖HPA来管理它们？我们不直接回答这个问题，而是探讨不同的组合，并根据结果定义策略。
- en: First, let's see how many Pods we have in our cluster right now.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看我们集群中有多少个Pods。
- en: '[PRE61]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The output is as follows.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE62]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We can see that there are two replicas of the `api` Deployment, and three replicas
    of the `db` StatefulSets.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`api`部署有两个副本，`db`有三个有状态集的副本。
- en: Let's say that we want to roll out a new release of our `go-demo-5` application.
    The definition we'll use is as follows.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要发布一个新版本的`go-demo-5`应用程序。我们将使用的定义如下。
- en: '[PRE63]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE64]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The important thing to note is that our `api` Deployment has `10` replicas and
    that we have the HPA. Everything else is the same as it was before.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的重要事情是我们的`api`部署有`10`个副本，并且我们有HPA。其他一切都和以前一样。
- en: What will happen if we apply that definition?
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们应用了那个定义会发生什么？
- en: '[PRE65]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We applied the new definition and retrieved all the Pods from the `go-demo-5`
    Namespace. The output of the latter command is as follows.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了新的定义，并从`go-demo-5`命名空间中检索了所有的Pods。后一条命令的输出如下。
- en: '[PRE66]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Kubernetes complied with our desire to have ten replicas of the `api` and created
    eight Pods (we had two before). At the first look, it seems that HPA does not
    have any effect. Let's retrieve the Pods one more time.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes遵循我们希望有十个`api`副本的要求，并创建了八个Pods（之前我们有两个）。乍一看，HPA似乎没有任何效果。让我们再次检索Pods。
- en: '[PRE67]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The output is as follows.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE68]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Our Deployment de-scaled from ten to five replicas. HPA detected that there
    are more replicas then the maximum threshold and acted accordingly. But what did
    it do? Did it simply remove five replicas? That could not be the case since that
    would only have a temporary effect. If HPA removes or adds Pods, Deployment would
    also remove or add Pods, and the two would be fighting with each other. The number
    of Pods would be fluctuating indefinitely. Instead, HPA modified the Deployment.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的部署从十个缩减到了五个副本。HPA检测到副本超过了最大阈值，并相应地采取了行动。但它做了什么？它只是简单地移除了五个副本吗？那不可能，因为那只会有暂时的效果。如果HPA移除或添加Pods，部署也会移除或添加Pods，两者将互相对抗。Pods的数量将无限波动。相反，HPA修改了部署。
- en: Let's describe the `api`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下`api`。
- en: '[PRE69]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE70]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The number of replicas is set to `5 desired`. HPA modified our Deployment. We
    can observe that better through the event messages. The second to last states
    that the number of replicas was scaled up to `10`, while the last message indicates
    that it scaled down to `5`. The former is the result of us executing rolling update
    by applying the new Deployment, while the latter was produced by HPA modifying
    the Deployment by changing its number of replicas.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 副本的数量设置为`5 desired`。HPA修改了我们的部署。我们可以通过事件消息更好地观察到这一点。倒数第二条消息表明副本的数量被扩展到`10`，而最后一条消息表明它被缩减到`5`。前者是我们通过应用新的部署来执行滚动更新的结果，而后者是由HPA修改部署并改变其副本数量产生的。
- en: So far, we observed that HPA modifies our Deployments. No matter how many replicas
    we defined in a Deployment (or a StatefulSets), HPA will change it to fit its
    own thresholds and calculations. In other words, when we update a Deployment,
    the number of replicas will be temporarily changed to whatever we have defined,
    only to be modified again by HPA a few moments later. That behavior is unacceptable.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们观察到HPA修改了我们的部署。无论我们在部署（或StatefulSets）中定义了多少副本，HPA都会更改它以适应自己的阈值和计算。换句话说，当我们更新部署时，副本的数量将暂时更改为我们定义的任何内容，然后在几分钟后再次被HPA修改。这种行为是不可接受的。
- en: If HPA changed the number of replicas, there is usually a good reason for that.
    Resetting that number to whatever is set in a Deployment (or a StatetefulSet)
    can produce serious side-effect.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果HPA更改了副本的数量，通常会有很好的理由。将该数字重置为部署（或StatetifulSet）中设置的任何数字可能会产生严重的副作用。
- en: Let's say that we have three replicas defined in a Deployment and that HPA scaled
    it to thirty because there is an increased load on that application. If we `apply`
    the Deployment because we want to roll out a new release, for a brief period,
    there will be three replicas, instead of thirty.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在部署中定义了三个副本，并且HPA将其扩展到三十个，因为该应用程序的负载增加了。如果我们`apply`部署，因为我们想要推出一个新的版本，那么在短暂的时间内，将会有三个副本，而不是三十个。
- en: As a result, our users would experience slow response times from our application,
    or some other effect caused by too few replicas serving too much traffic. We must
    try to avoid such a situation. The number of replicas should be controlled by
    HPA at all times. That means we'll need to change our strategy.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的用户可能会在我们的应用程序中经历较慢的响应时间，或者由于太少的副本提供了太多的流量而导致其他影响。我们必须努力避免这种情况。副本的数量应始终由HPA控制。这意味着我们需要改变我们的策略。
- en: If specifying the number of replicas in a Deployment does not produce the effect
    we want, we might just as well remove them altogether. Let's see what happens
    in that case.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在部署中指定副本的数量没有产生我们想要的效果，我们可能会干脆将它们全部删除。让我们看看在这种情况下会发生什么。
- en: We'll use `go-demo-5.yml` definition, so let's see how it differs from `go-demo-5-replicas-10.yml`
    that we used previously.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`go-demo-5.yml`的定义，让我们看看它与我们之前使用的`go-demo-5-replicas-10.yml`有何不同。
- en: '[PRE71]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The output shows that the only difference is that, this time, we are not specifying
    the number of replicas.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示的唯一区别是，这一次，我们没有指定副本的数量。
- en: Let's apply the change and see what happens.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个变化，看看会发生什么。
- en: '[PRE72]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The output of the latter command, limited to the relevant parts, is as follows.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 后一条命令的输出，仅限于相关部分，如下所示。
- en: '[PRE73]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Applying the Deployment without `replicas` resulted in `1 desired`. Sure, HPA
    will scale it up to `2` (its minimum) soon enough, but we still failed in our
    mission to maintain the number of replicas defined by HPA at all times.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 应用部署而没有`副本`导致`1 desired`。当然，HPA很快会将其扩展到`2`（其最小值），但我们仍然未能实现我们的使命，即始终保持HPA定义的副本数量。
- en: What else can we do? No matter whether we define our Deployment with or without
    `replicas`, the result is the same. Applying the Deployment always cancels the
    effect of the HPA, even when we do NOT specify `replicas`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还能做什么？无论我们是使用`副本`定义还是不使用`副本`定义我们的部署，结果都是一样的。应用部署总是会取消 HPA 的效果，即使我们没有指定`副本`。
- en: Actually, that statement is incorrect. We can accomplish the desired behavior
    without `replicas` if we know how the whole process works.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个说法是不正确的。如果我们知道整个过程是如何工作的，我们可以实现期望的行为而不需要`副本`。
- en: If `replicas` is defined for a Deployment, it will be used every time we `apply`
    a definition. If we change the definition by removing `replicas`, the Deployment
    will think that we want to have one, instead of the number of replicas we had
    before. But, if we never specify the number of `replicas`, they will be entirely
    controlled by HPA.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为部署定义了`副本`，那么每次我们`应用`一个定义时都会使用它。如果我们通过删除`副本`来更改定义，部署将认为我们想要一个副本，而不是之前的副本数量。但是，如果我们从未指定`副本`的数量，它们将完全由
    HPA 控制。
- en: Let's test it out.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下。
- en: '[PRE74]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: We deleted everything related to the `go-demo-5` application. Now, let's test
    how the Deployment behaves if `replicas` is not defined from the start.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们删除了与`go-demo-5`应用程序相关的所有内容。现在，让我们测试一下，如果从一开始就没有定义`副本`，部署会如何行为。
- en: '[PRE75]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The output of the latter command, limited to the relevant parts, is as follows.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 后一条命令的输出，仅限于相关部分，如下所示。
- en: '[PRE76]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Seems that we failed. The Deployment did set the number of replicas to `1`.
    But, what you cannot see, is that replicas are not defined internally.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们失败了。部署确实将副本的数量设置为`1`。但是，您看不到的是副本在内部没有定义。
- en: Nevertheless, a few moments later, our Deployment will be scaled up by HPA to
    two replicas. That is the expected behavior, but we'll confirm it anyway.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，几分钟后，我们的部署将被 HPA 扩展到两个副本。这是预期的行为，但我们将确认一下。
- en: '[PRE77]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: You should see from the output that the number of replicas was changed (by HPA)
    to `2`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该从输出中看到副本的数量已经被（由 HPA）更改为`2`。
- en: Now comes the final test. If we make a new release of the Deployment, will it
    scale down to `1` replica, or will it stay on `2`?
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是最终测试。如果我们发布一个新版本的部署，它会缩减到`1`个副本，还是会保持在`2`个副本？
- en: We'll apply a new definition. The only difference, when compared with the one
    currently running, is in the tag of the image. That way we'll guarantee that the
    Deployment will be indeed updated.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用一个新的定义。与当前运行的定义相比，唯一的区别在于镜像的标签。这样我们将确保部署确实被更新。
- en: '[PRE78]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The output of the latter command, limited to the relevant parts, is as follows.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 后一条命令的输出，仅限于相关部分，如下所示。
- en: '[PRE79]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: We can see that the number of replicas, set by the HPA, is preserved.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，由 HPA 设置的副本数量得到了保留。
- en: Don't be alarmed if you see in the `events` that the number of replicas was
    scaled to `1`. That's the second ReplicaSet spin up by the Deployment. You can
    see that by observing the name of the ReplicaSet. The Deployment is doing rolling
    updates by joggling two ReplicaSets in the attempt to roll out the new release
    without downtime. That is unrelated to auto-scaling, and I assume that you already
    know how rolling updates work. If you don't, you know where to learn it.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在“事件”中看到副本的数量被缩减为`1`，不要惊慌。那是部署启动的第二个 ReplicaSet。您可以通过观察 ReplicaSet 的名称来看到这一点。部署正在通过搅动两个
    ReplicaSet 来进行滚动更新，以尝试在没有停机时间的情况下推出新版本。这与自动扩展无关，我假设您已经知道滚动更新是如何工作的。如果您不知道，您知道在哪里学习它。
- en: Now comes the critical question. How should we define replicas in Deployments
    and StatefulSets?
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现了关键问题。在部署和有状态集中，我们应该如何定义副本？
- en: If you plan to use HPA with a Deployment or a StatefulSet, do NOT declare replicas.
    If you do, each rolling update will cancel the effect of the HPA for a while.
    Define replicas only for the resources that are NOT used in conjunction with HPA.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划在部署或StatefulSet中使用HPA，请不要声明副本。如果这样做，每次滚动更新都会暂时取消HPA的效果。仅为不与HPA一起使用的资源定义副本。
- en: What now?
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在呢？
- en: We explored the simplest way to scale our Deployments and StatefulSets. It's
    simple because the mechanism is baked into Kubernetes. All we had to do is define
    a HorizontalPodAutoscaler with target memory and CPU. While this method for auto-scaling
    is commonly used, it is often not sufficient. Not all applications increase memory
    or CPU usage when under stress. Even when they do, those two metrics might not
    be enough.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了扩展部署和StatefulSets的最简单方法。这很简单，因为这个机制已经内置在Kubernetes中。我们所要做的就是定义一个具有目标内存和CPU的HorizontalPodAutoscaler。虽然这种自动缩放的方法通常被使用，但通常是不够的。并非所有应用程序在压力下都会增加内存或CPU使用率。即使它们这样做了，这两个指标可能还不够。
- en: In one of the following chapters, we'll explore how to extend HorizontalPodAutoscaler
    to use a custom source of metrics. For now, we'll destroy what we created, and
    we'll start the next chapter fresh.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨如何扩展HorizontalPodAutoscaler以使用自定义的指标来源。现在，我们将销毁我们创建的内容，并开始下一章。
- en: If you are planning to keep the cluster running, please execute the commands
    that follow to remove the resources we created.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划保持集群运行，请执行以下命令以删除我们创建的资源。
- en: '[PRE80]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Otherwise, please delete the whole cluster if you created it only for the purpose
    of this book and you're not planning to dive into the next chapter right away.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，请删除整个集群，如果您只是为了本书的目的而创建它，并且不打算立即深入下一章。
- en: Before you leave, you might want to go over the main points of this chapter.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在您离开之前，您可能希望复习本章的要点。
- en: HorizontalPodAutoscaler's only function is to automatically scale the number
    of Pods in a Deployment, a StatefulSet, or a few other types of resources. It
    accomplishes that by observing CPU and memory consumption of the Pods and acting
    when they reach pre-defined thresholds.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平Pod自动缩放器的唯一功能是自动调整部署、StatefulSet或其他一些类型的资源中Pod的数量。它通过观察Pod的CPU和内存消耗，并在它们达到预定义的阈值时采取行动来实现这一点。
- en: Metrics Server collects information about used resources (memory and CPU) of
    nodes and Pods.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metrics Server收集有关节点和Pod使用的资源（内存和CPU）的信息。
- en: Metrics Server periodically fetches metrics from Kubeletes running on the nodes.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metrics Server定期从运行在节点上的Kubeletes获取指标。
- en: If the number of replicas is static and you have no intention to scale (or de-scale)
    your application over time, set `replicas` as part of your Deployment or StatefulSet
    definition. If, on the other hand, you plan to change the number of replicas based
    on memory, CPU, or other metrics, use HorizontalPodAutoscaler resource instead.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果副本的数量是静态的，并且您没有打算随时间缩放（或反向缩放）您的应用程序，请将`replicas`作为部署或StatefulSet定义的一部分。另一方面，如果您计划根据内存、CPU或其他指标更改副本的数量，请改用HorizontalPodAutoscaler资源。
- en: If `replicas` is defined for a Deployment, it will be used every time we `apply`
    a definition. If we change the definition by removing `replicas`, the Deployment
    will think that we want to have one, instead of the number of replicas we had
    before. But, if we never specify the number of `replicas`, they will be entirely
    controlled by HPA.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为部署定义了`replicas`，那么每次我们`apply`一个定义时都会使用它。如果我们通过删除`replicas`来更改定义，部署将认为我们想要一个，而不是我们之前拥有的副本数量。但是，如果我们从未指定`replicas`的数量，它们将完全由HPA控制。
- en: If you plan to use HPA with a Deployment or a StatefulSet, do NOT declare `replicas`.
    If you do, each rolling update will cancel the effect of the HPA for a while.
    Define `replicas` only for the resources that are NOT used in conjunction with
    HPA.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您计划在部署或StatefulSet中使用HPA，请不要声明`replicas`。如果这样做，每次滚动更新都会暂时取消HPA的效果。仅为不与HPA一起使用的资源定义`replicas`。
