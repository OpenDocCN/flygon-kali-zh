- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: CNNs for Financial Time Series and Satellite Images
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于金融时间序列和卫星图像的CNN
- en: In this chapter, we introduce the first of several specialized deep learning
    architectures that we will cover in *Part 4*. Deep **convolutional neural networks**
    (**CNNs**) have enabled superhuman performance in various computer vision tasks
    such as classifying images and video and detecting and recognizing objects in
    images. CNNs can also extract signals from time-series data that shares certain
    characteristics with image data and have been successfully applied to speech recognition
    (Abdel-Hamid et al. 2014). Moreover, they have been shown to deliver state-of-the-art
    performance on time-series classification across various domains (Ismail Fawaz
    et al. 2019).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍*第4部分*中将涵盖的几种专门的深度学习架构中的第一种。深度**卷积神经网络**（**CNNs**）在各种计算机视觉任务中实现了超人类的性能，如对图像和视频进行分类以及在图像中检测和识别对象。CNN还可以从与图像数据具有某些特征相似的时间序列数据中提取信号，并已成功应用于语音识别（Abdel-Hamid等人2014年）。此外，它们已被证明在各个领域的时间序列分类中提供了最先进的性能（Ismail
    Fawaz等人2019年）。
- en: CNNs are named after a linear algebra operation called a **convolution** that
    replaces the general matrix multiplication typical of feedforward networks (discussed
    in the last chapter) in at least one of their layers. We will show how convolutions
    work and why they are particularly well suited to data with a certain regular
    structure typically found in images but also present in time series.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: CNN以一种称为**卷积**的线性代数操作命名，该操作在它们的至少一个层中取代了前馈网络（在上一章中讨论过）中的一般矩阵乘法。我们将展示卷积的工作原理以及为什么它们特别适合具有某种规则结构的数据，通常在图像中发现，但也存在于时间序列中。
- en: Research into **CNN architectures** has proceeded very rapidly, and new architectures
    that improve benchmark performance continue to emerge. We will describe a set
    of building blocks consistently used by successful applications. We will also
    demonstrate how **transfer learning** can speed up learning by using pretrained
    weights for CNN layers closer to the input while fine-tuning the final layers
    to a specific task. We will also illustrate how to use CNNs for the specific computer
    vision task of **object detection**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对CNN架构的研究进展非常迅速，不断出现提高基准性能的新架构。我们将描述一组成功应用中一直使用的构建模块。我们还将演示如何使用预训练权重来加速学习，通过使用预训练权重来进行CNN层的微调，以适应特定任务。我们还将说明如何将CNN用于特定的计算机视觉任务，如目标检测。
- en: 'CNNs can help build a **trading strategy** by generating signals from images
    or (multiple) time-series data:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: CNN可以通过从图像或（多个）时间序列数据生成信号来构建**交易策略**：
- en: '**Satellite data** may signal future commodity trends, including the supply
    of certain crops or raw materials via aerial images of agricultural areas, mines,
    or transport networks like oil tankers. **Surveillance camera** footage, for example,
    from shopping malls, could be used to track and predict consumer activity.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卫星数据**可能预示着未来商品趋势，包括通过农业区域、矿山或油轮等空中图像来预测某些作物或原材料的供应。例如，来自购物中心的**监控摄像头**视频可以用于跟踪和预测消费者活动。'
- en: '**Time-series data** encompasses a very broad range of data sources and CNNs
    have been shown to deliver high-quality classification results by exploiting their
    structural similarity with images.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列数据**涵盖了非常广泛的数据来源，CNN已被证明通过利用它们与图像的结构相似性来提供高质量的分类结果。'
- en: We will create a trading strategy based on predictions of a CNN that uses time-series
    data that's been deliberately formatted like images and demonstrate how to build
    a CNN to classify satellite images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将基于CNN的预测创建一个交易策略，该CNN使用了被故意格式化为图像的时间序列数据，并演示如何构建CNN来对卫星图像进行分类。
- en: 'More specifically, in this chapter, you will learn about the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，您将了解以下内容：
- en: How CNNs employ several building blocks to efficiently model grid-like data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN如何使用多个构建模块有效地对网格数据进行建模
- en: Training, tuning, and regularizing CNNs for images and time-series data using
    TensorFlow
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow对图像和时间序列数据进行CNN的训练、调整和正则化
- en: Using transfer learning to streamline CNNs, even with less data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习来简化CNN，即使数据较少
- en: Designing a trading strategy using return predictions by a CNN trained on time-series
    data formatted like images
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在时间序列数据上训练的CNN生成的回报预测来设计交易策略
- en: How to classify satellite images
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何对卫星图像进行分类
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub存储库的相应目录中找到本章的代码示例和其他资源的链接。笔记本包括图像的彩色版本。
- en: How CNNs learn to model grid-like data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN如何学习对网格数据进行建模
- en: 'CNNs are conceptually similar to feedforward **neural networks** (**NNs**):
    they consist of units with parameters called weights and biases, and the training
    process adjusts these parameters to optimize the network''s output for a given
    input according to a loss function. They are most commonly used for classification.
    Each unit uses its parameters to apply a linear operation to the input data or
    activations received from other units, typically followed by a nonlinear transformation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在概念上类似于前馈**神经网络**（**NNs**）：它们由具有称为权重和偏差的参数的单元组成，训练过程调整这些参数，以优化网络对给定输入的输出，根据损失函数。它们最常用于分类。每个单元使用其参数对输入数据或来自其他单元的激活应用线性操作，通常后跟非线性转换。
- en: The overall network models a **differentiable function** that maps raw data,
    such as image pixels, to class probabilities using an output activation function
    like softmax. CNNs use an objective function such as cross-entropy loss to measure
    the quality of the output with a single metric. They also rely on the gradients
    of the loss with respect to the network parameter to learn via backpropagation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 整个网络模型了一个可微函数，将原始数据（如图像像素）映射到使用类似softmax的输出激活函数的类概率。CNN使用诸如交叉熵损失之类的目标函数来衡量输出的质量，并依赖于损失相对于网络参数的梯度，通过反向传播来学习。
- en: Feedforward NNs with fully connected layers do not scale well to high-dimensional
    image data with a large number of pixel values. Even the low-resolution images
    included in the CIFAR-10 dataset that we'll use in the next section contain 32×32
    pixels with up to 256 different color values represented by 8 bits each. With
    three channels, for example, for the red, green, and blue channels of the RGB
    color model, a single unit in a fully connected input layer implies 32 × 32 ×
    3=3,072 weights. A more standard resolution of 640×480 pixels already yields closer
    to 1 million weights for a single input unit. Deep architectures with several
    layers of meaningful width quickly lead to an exploding number of parameters that
    make overfitting during training all but certain.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 具有完全连接层的前馈NN在具有大量像素值的高维图像数据上无法很好地扩展。即使是在我们将在下一节中使用的CIFAR-10数据集中包含的低分辨率图像也包含32×32像素，每个像素最多由8位表示的256种不同的颜色值。例如，对于RGB颜色模型的红色、绿色和蓝色通道，一个完全连接的输入层中的单个单元意味着32×32×3=3,072个权重。一个更标准的分辨率，比如640×480像素，已经产生了接近100万个权重，用于单个输入单元。具有几层有意义宽度的深度架构很快导致参数数量激增，使得在训练过程中过拟合几乎是肯定的。
- en: 'A fully connected feedforward NN makes no assumptions about the local structure
    of the input data so that arbitrarily reordering the features has no impact on
    the training result. By contrast, CNNs make the **key assumption** that the **data
    has a grid-like topology** and that the **local structure matters**. In other
    words, they encode the assumption that the input has a structure typically found
    in image data: pixels form a two-dimensional grid, possibly with several channels
    to represent the components of the color signal. Furthermore, the values of nearby
    pixels are likely more relevant to detect key features such as edges and corners
    than faraway data points. Naturally, initial CNN applications such as handwriting
    recognition focused on image data.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接的前馈NN对输入数据的局部结构没有任何假设，因此对特征进行任意重新排序对训练结果没有影响。相比之下，CNN做出了关键假设，即数据具有类似网格的拓扑结构，并且局部结构很重要。换句话说，它们编码了这样一个假设，即输入具有图像数据中通常发现的结构：像素形成一个二维网格，可能有几个通道来表示颜色信号的组成部分。此外，附近像素的值可能更相关，以便检测边缘和角落等关键特征，而远离的数据点可能不那么相关。自然地，最初的CNN应用，如手写识别，侧重于图像数据。
- en: Over time, however, researchers recognized **similar characteristics in time-series
    data**, broadening the scope for the productive use of CNNs. Time-series data
    consists of measurements at regular intervals that create a one-dimensional grid
    along the time axis, such as the lagged returns for a given ticker. There can
    also be a second dimension with additional features for this ticker and the same
    time periods. Finally, we could represent additional tickers using the third dimension.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着时间的推移，研究人员认识到时间序列数据中的类似特征，扩大了CNN的有效使用范围。时间序列数据由定期间隔的测量组成，沿时间轴创建一个一维网格，例如给定股票的滞后收益。还可以有第二维，包含该股票和相同时间段的其他特征。最后，我们可以使用第三维来表示其他股票。
- en: A common CNN use case beyond images includes audio data, either in a one-dimensional
    waveform in the time domain or, after a Fourier transform, as a two-dimensional
    spectrum in the frequency domain. CNNs also play a key role in AlphaGo, the first
    algorithm to win a game of Go against humans, where they evaluated different positions
    on the grid-like board.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像之外，常见的CNN用例还包括音频数据，可以是时间域中的一维波形，也可以是傅立叶变换后的频率域中的二维频谱。CNN还在AlphaGo中发挥了关键作用，这是第一个击败人类的围棋算法，它评估了棋盘上不同位置。
- en: The most important element to encode the **assumption of a grid-like topology**
    is the **convolution** operation that gives CNNs their name, combined with **pooling**.
    We will see that the specific assumptions about the functional relationship between
    input and output data imply that CNNs need far fewer parameters and compute more
    efficiently.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 编码“类似网格拓扑结构”的最重要元素是给CNN命名的卷积操作，结合池化。我们将看到，关于输入和输出数据之间的功能关系的具体假设意味着CNN需要更少的参数，并且计算更有效。
- en: In this section, we will explain how convolution and pooling layers learn filters
    that extract local features and why these operations are particularly suitable
    for data with the structure just described. State-of-the-art CNNs combine many
    of these basic building blocks to achieve the layered representation learning
    described in the previous chapter. We conclude by describing key architectural
    innovations over the last decade that saw enormous performance improvements.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释卷积和池化层如何学习提取局部特征的过滤器，以及为什么这些操作特别适合具有刚才描述的结构的数据。最先进的CNN将许多这些基本构建块组合起来，以实现上一章描述的分层表示学习。最后，我们将描述过去十年中看到的巨大性能改进的关键架构创新。
- en: From hand-coding to learning filters from data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从手工编码到从数据中学习过滤器
- en: For image data, this local structure has traditionally motivated the development
    of hand-coded filters that extract such patterns for the use as features in **machine
    learning** (**ML**) models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像数据，这种局部结构传统上促使开发手工编码的过滤器，以提取这些模式，用作机器学习（ML）模型中的特征。
- en: '*Figure 18.1* displays the effect of simple filters designed to detect certain
    edges. The notebook `filter_example.ipynb` illustrates how to use hand-coded filters
    in a convolutional network and visualizes the resulting transformation of the
    image. The filters are simple [-1, 1] patterns arranged in a ![](img/B15439_18_001.png)
    matrix, shown in the upper right of the figure. Below each filter, its effects
    are shown; they are a bit subtle and will be easier to spot in the accompanying
    notebook.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.1*显示了设计用于检测特定边缘的简单滤波器的效果。笔记本`filter_example.ipynb`说明了如何在卷积网络中使用手工编码的滤波器，并可视化图像的结果转换。滤波器是一个简单的[-1,
    1]模式排列在一个![](img/B15439_18_001.png)矩阵中，显示在图的右上方。在每个滤波器下面，显示了它的效果；它们有点微妙，将更容易在附带的笔记本中发现。'
- en: '![](img/B15439_18_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_01.png)'
- en: 'Figure 18.1: The result of basic edge filters applied to an image'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '图18.1: 应用于图像的基本边缘滤波器的结果'
- en: Convolutional layers, by contrast, are designed to learn such local feature
    representations from the data. A key insight is to restrict their input, called
    the **receptive field**, to a small area of the input so it captures basic pixel
    constellations that reflect common patterns like edges or corners. Such patterns
    may occur anywhere in an image, though, so CNNs also need to recognize similar
    patterns in different locations and possibly with small variations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层相反，是设计用来从数据中学习这样的局部特征表示的。一个关键的见解是限制它们的输入，称为**感受野**，到输入的一个小区域，这样它就可以捕捉基本的像素组合，反映出像边缘或角落这样的常见模式。这样的模式可能出现在图像的任何地方，所以CNN也需要在不同的位置和可能有小的变化的情况下识别相似的模式。
- en: Subsequent layers then learn to synthesize these local features to detect **higher-order
    features**. The linked resources on GitHub include examples of how to visualize
    the filters learned by a deep CNN using some of the deep architectures that we
    present in the next section on reference architectures.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的层学习合成这些局部特征以检测**更高阶的特征**。GitHub上的链接资源包括如何使用我们在参考架构的下一节中介绍的一些深度架构来可视化深度CNN学习的滤波器的示例。
- en: How the elements of a convolutional layer operate
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层中的元素是如何运作的
- en: 'Convolutional layers integrate **three architectural ideas** that enable the
    learning of feature representations that are to some degree invariant to shifts,
    changes in scale, and distortion:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层整合了**三种架构思想**，使得学习到的特征表示在某种程度上对于移位、尺度变化和扭曲是不变的：
- en: Sparse rather than dense connectivity
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏而不是密集的连接
- en: Weight sharing
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重共享
- en: Spatial or temporal downsampling
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空间或时间下采样
- en: Moreover, convolutional layers allow for inputs of variable size. We will walk
    through a typical convolutional layer and describe each of these ideas in turn.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，卷积层允许可变大小的输入。我们将逐步介绍典型的卷积层，并依次描述这些想法。
- en: '*Figure 18.2* outlines the set of operations that typically takes place in
    a three-dimensional convolutional layer, assuming image data is input with the
    three dimensions of height, width, and depth, or the number of channels. The range
    of pixel values depends on the bit representation, for example, [0, 255] for 8
    bits. Alternatively, the width axis could represent time, the height different
    features, and the channels could capture observations on distinct objects such
    as tickers.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.2*概述了在三维卷积层中通常发生的一系列操作，假设图像数据输入具有高度、宽度和深度，或通道数三个维度。像素值的范围取决于位表示，例如，8位的范围是[0,
    255]。另外，宽度轴可以表示时间，高度可以表示不同的特征，通道可以捕捉对不同对象的观察，比如股票。'
- en: '![](img/B15439_18_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_02.png)'
- en: 'Figure 18.2: Typical operations in a two-dimensional convolutional layer'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '图18.2: 二维卷积层中的典型操作'
- en: Successive computations process the input through the convolutional, detector,
    and pooling stages that we describe in the next three sections. In the example
    depicted in *Figure 18.2*, the convolutional layer receives three-dimensional
    input and produces an output of the same dimensionality.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 连续的计算通过卷积、检测器和池化阶段处理输入，我们将在接下来的三个部分中描述这些阶段。在*图18.2*中描述的例子中，卷积层接收三维输入，并产生相同维度的输出。
- en: State-of-the-art CNNs are composed of several such layers of varying sizes that
    are either stacked on top of each other or operate in parallel on different branches.
    With each layer, the network can detect higher-level, more abstract features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的CNN由几个这样大小不同的层组成，这些层要么堆叠在一起，要么在不同的分支上并行操作。通过每一层，网络可以检测到更高级别、更抽象的特征。
- en: The convolution stage – extracting local features
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积阶段 - 提取局部特征
- en: The first stage applies a filter, also called the **kernel**, to overlapping
    patches of the input image. The filter is a matrix of a much smaller size than
    the input so that its receptive field is limited to a few contiguous values such
    as pixels or time-series values. As a result, it focuses on local patterns and
    dramatically reduces the number of parameters and computations relative to a fully
    connected layer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段将一个滤波器，也称为**核**，应用于输入图像的重叠补丁。滤波器是一个比输入要小得多的矩阵，因此它的感受野被限制在一些连续的值，比如像素或时间序列值。因此，它专注于局部模式，并且相对于完全连接的层，大大减少了参数和计算的数量。
- en: A complete convolutional layer has several **feature maps** organized as depth
    slices (depicted in *Figure 18.2*) so that each layer can extract multiple features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的卷积层有几个**特征图**组织成深度切片（在*图18.2*中描述），这样每一层可以提取多个特征。
- en: From filters to feature maps
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从滤波器到特征图
- en: While scanning the input, the kernel is convolved with each input segment covered
    by its receptive field. The convolution operation is simply the dot product between
    the filter weights and the values of the matching input area after both have been
    reshaped to vectors. Each convolution thus produces a single number, and the entire
    scan yields a feature map. Since the dot product is maximized for identical vectors,
    the feature map indicates the degree of activation for each input region.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在扫描输入时，核与其感受野覆盖的每个输入段进行卷积。卷积操作简单地是滤波器权重和匹配输入区域的值的点积，两者都被重塑为向量。每个卷积因此产生一个数字，整个扫描产生一个特征图。由于对于相同的向量，点积被最大化，特征图指示了每个输入区域的激活程度。
- en: '*Figure 18.3* illustrates the result of the scan of a ![](img/B15439_18_002.png)
    input using a ![](img/B15439_18_003.png) filter with given values, and how the
    activation in the upper-right corner of the feature map results from the dot product
    of the flattened input region and the kernel:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.3*说明了使用给定值的![](img/B15439_18_003.png)滤波器扫描![](img/B15439_18_002.png)输入的结果，以及特征图右上角的激活是通过扁平化输入区域和核的点积得到的：'
- en: '![](img/B15439_18_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_03.png)'
- en: 'Figure 18.3: From convolutions to a feature map'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3：从卷积到特征图
- en: The most important aspect is that the **filter values are the parameters** of
    the convolutional layers, **learned from the data** during training to minimize
    the chosen loss function. In other words, CNNs learn useful feature representations
    by finding kernel values that activate input patterns that are most useful for
    the task at hand.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的方面是**滤波器数值是卷积层的参数**，在训练过程中从数据中学习，以最小化所选择的损失函数。换句话说，CNN通过找到激活输入模式的核数值来学习有用的特征表示，这对于当前任务非常有用。
- en: How to scan the input – strides and padding
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何扫描输入-步幅和填充
- en: 'The **stride** defines the step size used for scanning the input, that is,
    the number of pixels to shift horizontally and vertically. Smaller strides scan
    more (overlapping) areas but are computationally more expensive. Four options
    are commonly used when the filter does not fit the input perfectly and partially
    crosses the image boundary during the scan:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**步幅**定义了用于扫描输入的步长，即水平和垂直移动的像素数。较小的步幅扫描更多（重叠）区域，但计算成本更高。当滤波器不完全适合输入并且在扫描过程中部分穿过图像边界时，通常有四个选项：'
- en: '**Valid convolution**: Discards scans where the image and filter do not perfectly
    match'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效卷积**：丢弃图像和滤波器不完全匹配的扫描'
- en: '**Same convolution**: Zero-pads the input to produce a feature map of equal
    size'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相同卷积**：对输入进行零填充，以产生相同大小的特征图'
- en: '**Full convolution**: Zero-pads the input so that each pixel is scanned an
    equal number of times, including pixels at the border (to avoid oversampling pixels
    closer to the center)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全卷积**：对输入进行零填充，以便每个像素被扫描相同次数，包括边界上的像素（以避免过采样靠近中心的像素）'
- en: '**Causal**: Zero-pads the input only on the left so that the output does not
    depend on an input from a later period; maintains the temporal order for time-series
    data'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因果关系**：仅在左侧对输入进行零填充，以使输出不依赖于稍后时期的输入；保持时间序列数据的时间顺序'
- en: The choices depend on the nature of the data and where useful features are most
    likely located. In combination with the number of depth slices, they determine
    the output size of the convolution stage. The Stanford lecture notes by Andrew
    Karpathy (see GitHub) contain helpful examples using NumPy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 选择取决于数据的性质以及有用特征最可能位于的位置。与深度切片的数量结合使用，它们确定了卷积阶段的输出大小。Andrew Karpathy的斯坦福讲义（见GitHub）包含使用NumPy的有用示例。
- en: Parameter sharing for robust features and fast computation
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参数共享用于稳健特征和快速计算
- en: The location of salient features may vary due to distortion or shifts. Furthermore,
    elementary feature detectors are likely useful across the entire image. CNNs encode
    these assumptions by sharing or tying the weights for the filter in a given depth
    slice.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于扭曲或移位，显著特征的位置可能会有所不同。此外，基本特征检测器可能在整个图像上都很有用。CNN通过在给定深度切片中共享或绑定滤波器的权重来编码这些假设。
- en: As a result, each depth slice specializes in a certain pattern and the number
    of parameters is further reduced. Weight sharing works less well, however, when
    images are spatially centered and key patterns are less likely to be uniformly
    distributed across the input area.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个深度切片专门针对某种模式，并且参数的数量进一步减少。然而，当图像在空间上居中且关键模式不太可能均匀分布在输入区域时，权重共享效果就不太好了。
- en: The detector stage – adding nonlinearity
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测器阶段-添加非线性
- en: The feature maps are usually passed through a nonlinear transformation. The
    **rectified linear unit** (**ReLU**) that we encountered in the last chapter is
    a common function for this purpose. ReLUs replace negative activations element-wise
    by zero and mitigate the risk of vanishing gradients found in other activation
    functions such as tanh (see *Chapter 17*, *Deep Learning for Trading*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 特征图通常通过非线性变换。我们在上一章中遇到的**修正线性单元**（**ReLU**）是一个常用的函数。ReLU通过将负激活逐元素替换为零，减轻了其他激活函数（如tanh）中出现的梯度消失风险（见*第17章*，*交易的深度学习*）。
- en: 'A popular alternative is the **softplus function**:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的替代方案是**softplus函数**：
- en: '![](img/B15439_18_004.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_004.png)'
- en: In contrast to ReLU, it has a derivative everywhere, namely the sigmoid function
    that we used for logistic regression (see *Chapter 7*, *Linear Models – From Risk
    Factors to Return Forecasts*).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与ReLU相比，它在任何地方都有导数，即我们用于逻辑回归的sigmoid函数（见*第7章*，*线性模型-从风险因素到收益预测*）。
- en: The pooling stage – downsampling the feature maps
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化阶段-降采样特征图
- en: 'The last stage of the convolutional layer may downsample the feature map''s
    input representation to do the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的最后阶段可能会将特征图的输入表示降采样，以执行以下操作：
- en: Reduce its dimensionality and prevent overfitting
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低其维度并防止过拟合
- en: Lower the computational cost
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低计算成本
- en: Enable basic translation invariance
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用基本的翻译不变性
- en: This assumes that the precise location of the features is not only less important
    for identifying a pattern but can even be harmful because it will likely vary
    for different instances of the target. Pooling lowers the spatial resolution of
    the feature map as a simple way to render the location information less precise.
    However, this step is optional and many architectures use pooling only for some
    layers or not at all.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设特征的精确位置不仅对于识别模式不那么重要，甚至可能是有害的，因为目标的不同实例的位置可能会有所不同。池化降低了特征图的空间分辨率，是一种简单的方法，使位置信息不那么精确。然而，这一步是可选的，许多架构只对一些层使用池化，或者根本不使用。
- en: A common pooling operation is **max pooling**, which uses only the maximum activation
    value from (typically) non-overlapping subregions. For a small ![](img/B15439_18_005.png)
    feature map, for instance, ![](img/B15439_18_001.png) max pooling outputs the
    maximum for each of the four non-overlapping ![](img/B15439_18_001.png) areas.
    Less common pooling operators use the average or the median. Pooling does not
    add or learn new parameters but the size of the input window and possibly the
    stride are additional hyperparameters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的池化操作是**最大池化**，它仅使用（通常）不重叠的子区域中的最大激活值。例如，对于一个小的![](img/B15439_18_005.png)特征图，![](img/B15439_18_001.png)最大池化输出每个四个不重叠的![](img/B15439_18_001.png)区域的最大值。较少见的池化操作使用平均值或中位数。池化不会增加或学习新的参数，但输入窗口的大小和可能的步幅是额外的超参数。
- en: The evolution of CNN architectures – key innovations
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN架构的演变-关键创新
- en: Several CNN architectures have pushed performance boundaries over the past two
    decades by introducing important innovations. Predictive performance growth accelerated
    dramatically with the arrival of big data in the form of ImageNet (Fei-Fei 2015)
    with 14 million images assigned to 20,000 classes by humans via Amazon's Mechanical
    Turk. The **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**) became
    the focal point of CNN progress around a slightly smaller set of 1.2 million images
    from 1,000 classes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的20年中，一些CNN架构通过引入重要的创新推动了性能的边界。随着ImageNet（Fei-Fei 2015）的到来，大数据以1400万张图像的形式分配给了人类通过亚马逊的Mechanical
    Turk的20000个类别。**ImageNet大规模视觉识别挑战**（ILSVRC）成为了围绕1000个类别的120万张图像的CNN进展的焦点。
- en: It is useful to be familiar with the **reference architectures** dominating
    these competitions for practical reasons. As we will see in the next section on
    working with CNNs for image data, they offer a good starting point for standard
    tasks. Moreover, **transfer learning** allows us to address many computer vision
    tasks by building on a successful architecture with pretrained weights. Transfer
    learning not only speeds up architecture selection and training but also enables
    successful applications on much smaller datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉这些比赛中占主导地位的**参考架构**是有用的，出于实际原因。正如我们将在下一节关于处理图像数据的CNN中看到的那样，它们为标准任务提供了一个很好的起点。此外，**迁移学习**使我们能够通过利用具有预训练权重的成功架构来解决许多计算机视觉任务。迁移学习不仅加快了架构选择和训练，还使我们能够在更小的数据集上成功应用。
- en: In addition, many publications refer to these architectures, and they often
    serve as a basis for networks tailored to segmentation or localization tasks.
    We will further describe some landmark architectures in the section on image classification
    and transfer learning.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多出版物都提到了这些架构，它们经常作为针对分割或定位任务的网络的基础。我们将在图像分类和迁移学习部分进一步描述一些具有里程碑意义的架构。
- en: Performance breakthroughs and network size
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能突破和网络规模
- en: The left side of *Figure 18.4* plots the top-1 accuracy against the computational
    cost of a variety of network architectures. It suggests a positive relationship
    between the number of parameters and performance, but also shows that the marginal
    benefit of more parameters declines and that architectural design and innovation
    also matter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4的左侧绘制了各种网络架构的top-1准确度与计算成本之间的关系。它表明参数数量与性能之间存在正向关系，但也显示出更多参数的边际收益下降，架构设计和创新也很重要。
- en: The right side plots the top-1 accuracy per parameter for all networks. Several
    new architectures target use cases on less powerful devices such as mobile phones.
    While they do not achieve state-of-the-art performance, they have found much more
    efficient implementations. See the resources on GitHub for more details on these
    architectures and the analysis behind these charts.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧绘制了所有网络的每个参数的top-1准确度。一些新的架构针对移动电话等性能较低的设备的使用情况。虽然它们没有达到最先进的性能，但它们找到了更有效的实现。有关这些架构和这些图表背后的分析的更多详细信息，请参见GitHub上的资源。
- en: '![](img/B15439_18_04.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_04.png)'
- en: 'Figure 18.4: Predictive performance and computational complexity'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4：预测性能和计算复杂性
- en: Lessons learned
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所学到的教训
- en: 'Some of the lessons learned from 20 years of CNN architecture developments,
    especially since 2012, include the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从20年的CNN架构发展中学到的一些教训，特别是自2012年以来，包括以下内容：
- en: '**Smaller convolutional** filters perform better (possibly except at the first
    layer) because several small filters can substitute for a larger filter at a lower
    computational cost.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**较小的卷积**滤波器表现更好（可能除了在第一层），因为几个小滤波器可以以较低的计算成本替代一个较大的滤波器。'
- en: '**1 × 1 convolutions** reduce the dimensionality of feature maps so that the
    network can learn a larger number overall.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1×1卷积**降低了特征图的维度，使网络可以总体学习更多。'
- en: '**Skip connections** are able to create multiple paths through the network
    and enable the training of much higher-capacity CNNs.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跳跃连接**能够在网络中创建多条路径，并实现训练更高容量的CNN。'
- en: CNNs for satellite images and object detection
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卫星图像和目标检测的CNN
- en: 'In this section, we demonstrate how to solve key computer vision tasks such
    as image classification and object detection. As mentioned in the introduction
    and in *Chapter 3*, *Alternative Data for Finance – Categories and Use Cases*,
    image data can inform a trading strategy by providing clues about future trends,
    changing fundamentals, or specific events relevant to a target asset class or
    investment universe. Popular examples include exploiting satellite images for
    clues about the supply of agricultural commodities, consumer and economic activity,
    or the status of manufacturing or raw material supply chains. Specific tasks might
    include the following, for example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们演示了如何解决关键的计算机视觉任务，如图像分类和目标检测。正如在介绍和第3章“金融的替代数据 – 类别和用例”中提到的，图像数据可以通过提供关于未来趋势、变化的基本面或特定事件的线索，来为交易策略提供信息，这些信息与目标资产类别或投资领域相关。流行的例子包括利用卫星图像来获取有关农产品供应、消费者和经济活动，或者制造业或原材料供应链状况的线索。具体的任务可能包括以下内容，例如：
- en: '**Image classification**: Identifying whether cultivated land for certain crops
    is expanding, or predicting harvest quality and quantities'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类**：识别特定作物的耕地是否在扩大，或者预测收获的质量和数量'
- en: '**Object detection**: Counting the number of oil tankers on a certain transport
    route or the number of cars in a parking lot, or identifying the locations of
    shoppers in a mall'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测**：计算特定运输路线上油轮的数量，停车场中汽车的数量，或者识别商场中顾客的位置'
- en: In this section, we'll demonstrate how to design CNNs to automate the extraction
    of such information, both from scratch using popular architectures and via transfer
    learning that fine-tunes pretrained weights to a given task. We'll also demonstrate
    how to detect objects in a given scene.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何设计CNN来自动提取这些信息，既可以从流行的架构开始，也可以通过迁移学习微调预训练权重来完成给定的任务。我们还将演示如何在给定场景中检测物体。
- en: We will introduce key CNN architectures for these tasks, explain why they work
    well, and show how to train them using TensorFlow 2\. We will also demonstrate
    how to source pretrained weights and fine-tune time. Unfortunately, satellite
    images with information directly relevant for a trading strategy are very costly
    to obtain and are not readily available. We will, however, demonstrate how to
    work with the EuroSat dataset to build a classifier that identifies different
    land uses. This brief introduction to CNNs for computer vision aims to demonstrate
    how to approach common tasks that you will likely need to tackle when aiming to
    design a trading strategy based on images relevant to the investment universe
    of your choice.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍用于这些任务的关键CNN架构，解释它们为什么效果好，并展示如何使用TensorFlow 2来训练它们。我们还将演示如何获取预训练权重并进行微调。不幸的是，直接与交易策略相关的卫星图像非常昂贵且不容易获取。然而，我们将演示如何使用EuroSat数据集来构建一个识别不同土地用途的分类器。这篇关于计算机视觉CNN的简要介绍旨在演示在设计基于图像的交易策略时，您可能需要处理的常见任务的方法。
- en: All the libraries we introduced in the last chapter provide support for convolutional
    layers; we'll focus on the Keras interface of TensorFlow 2\. We are first going
    to illustrate the LeNet5 architecture using the MNIST handwritten digit dataset.
    Next, we'll demonstrate the use of data augmentation with AlexNet on CIFAR-10,
    a simplified version of the original ImageNet. Then we'll continue with transfer
    learning based on state-of-the-art architectures before we apply what we've learned
    to actual satellite images. We conclude with an example of object detection in
    real-life scenes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章介绍的所有库都支持卷积层；我们将专注于TensorFlow 2的Keras接口。我们首先将使用MNIST手写数字数据集来说明LeNet5的架构。接下来，我们将演示在CIFAR-10上使用AlexNet进行数据增强的用法，这是原始ImageNet的简化版本。然后，我们将继续使用最先进的架构进行迁移学习，然后将所学到的应用于实际的卫星图像。最后，我们将以实际场景中的物体检测为例结束。
- en: LeNet5 – The first CNN with industrial applications
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LeNet5 – 第一个具有工业应用的CNN
- en: Yann LeCun, now the Director of AI Research at Facebook, was a leading pioneer
    in CNN development. In 1998, after several iterations starting in the 1980s, LeNet5
    became the first modern CNN used in real-world applications that introduced several
    architectural elements still relevant today.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun，现任Facebook AI研究总监，是CNN发展的领先先驱。在经历了几次迭代后，LeNet5于1998年成为第一个现代CNN在实际应用中使用的模型，引入了一些至今仍然相关的架构元素。
- en: LeNet5 was published in a very instructive paper, *Gradient-Based Learning Applied
    to Document Recognition* (LeCun et al. 1989), that laid out many of the central
    concepts. Most importantly, it promoted the insight that convolutions with learnable
    filters are effective at extracting related features at multiple locations with
    few parameters. Given the limited computational resources at the time, efficiency
    was of paramount importance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet5发表在一篇非常有启发性的论文《基于梯度的学习应用于文档识别》（LeCun等人，1989年）中，其中阐述了许多核心概念。最重要的是，它提出了可学习滤波器的卷积在多个位置提取相关特征时具有很高的效率。鉴于当时的有限计算资源，效率至关重要。
- en: LeNet5 was designed to recognize the handwriting on checks and was used by several
    banks. It established a new benchmark for classification accuracy, with a result
    of 99.2 percent on the MNIST handwritten digit dataset. It consists of three convolutional
    layers, each containing a nonlinear tanh transformation, a pooling operation,
    and a fully connected output layer. Throughout the convolutional layers, the number
    of feature maps increases while their dimensions decrease. It has a total of 60,850
    trainable parameters (Lecun et al. 1998).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet5是为了识别支票上的手写字而设计的，并被几家银行使用。它在MNIST手写数字数据集上取得了99.2%的分类准确率，建立了一个新的基准。它包括三个卷积层，每个层包含非线性的tanh变换，一个池化操作和一个全连接的输出层。在卷积层中，特征图的数量增加，而它们的维度减小。它总共有60,850个可训练参数（Lecun等人，1998年）。
- en: '"Hello World" for CNNs – handwritten digit classification'
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN的“Hello World” – 手写数字分类
- en: In this section, we'll implement a slightly simplified version of LeNet5 to
    demonstrate how to build a CNN using a TensorFlow implementation. The original
    MNIST dataset contains 60,000 grayscale images in ![](img/B15439_18_008.png) pixel
    resolution, each containing a single handwritten digit from 0 to 9\. A good alternative
    is the more challenging but structurally similar Fashion MNIST dataset that we
    encountered in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation with
    Unsupervised Learning*. See the `digit_classification_with_lenet5` notebook for
    implementation details.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个稍微简化的LeNet5版本，以演示如何使用TensorFlow实现构建CNN。原始的MNIST数据集包含60,000个灰度图像，分辨率为![](img/B15439_18_008.png)，每个图像包含一个从0到9的单个手写数字。一个很好的替代方案是更具挑战性但结构相似的时尚MNIST数据集，我们在*第13章*中遇到过，*使用无监督学习进行数据驱动的风险因素和资产配置*。有关实现细节，请参阅`digit_classification_with_lenet5`笔记本。
- en: 'We can load it in Keras out of the box:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Keras中直接加载它：
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure 18.5* shows the first ten images in the dataset and highlights significant
    variation among instances of the same digit. On the right, it shows how the pixel
    values for an individual image range from 0 to 255:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.5*显示了数据集中的前十个图像，并突出显示了相同数字的实例之间的显着变化。右侧显示了单个图像的像素值范围从0到255：'
- en: '![](img/B15439_18_05.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_05.png)'
- en: 'Figure 18.5: MNIST sample images'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.5：MNIST样本图像
- en: 'We rescale the pixel values to the range [0, 1] to normalize the training data
    and facilitate the backpropagation process and convert the data to 32-bit floats,
    which reduce memory requirements and computational cost while providing sufficient
    precision for our use case:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像素值重新缩放到范围[0,1]，以规范化训练数据并促进反向传播过程，并将数据转换为32位浮点数，这样可以减少内存需求和计算成本，同时为我们的用例提供足够的精度。
- en: '[PRE1]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Defining the LeNet5 architecture
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义LeNet5架构
- en: 'We can define a simplified version of LeNet5 that omits the original final
    layer containing radial basis functions as follows, using the default "valid"
    padding and single-step strides unless defined otherwise:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个简化版本的LeNet5，省略了包含径向基函数的原始最终层，如下所示，使用默认的“valid”填充和单步长，除非另有定义：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The summary indicates that the model thus defined has over 300,000 parameters:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要表明，所定义的模型具有超过300,000个参数：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We compile with `sparse_crossentropy_loss`, which accepts integers rather than
    one-hot-encoded labels and the original stochastic gradient optimizer:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`sparse_crossentropy_loss`进行编译，它接受整数而不是独热编码的标签和原始的随机梯度优化器：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training and evaluating the model
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: 'Now we are ready to train the model. The model expects four-dimensional input,
    so we reshape accordingly. We use the standard batch size of 32 and an 80:20 train-validation
    split. Furthermore, we leverage checkpointing to store the model weights if the
    validation error improves, and make sure the dataset is randomly shuffled. We
    also define an `early_stopping` callback to interrupt training once the validation
    accuracy no longer improves for 20 iterations:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练模型。模型期望四维输入，因此我们相应地进行了重塑。我们使用标准的批量大小为32和80:20的训练-验证分割。此外，我们利用检查点来存储模型权重，如果验证错误改进，确保数据集被随机洗牌。我们还定义了一个`early_stopping`回调，一旦验证准确性不再提高20次迭代，就会中断训练：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The training history records the last improvement after 81 epochs that take
    around 4 minutes on a single GPU. The test accuracy of this sample run is 99.09
    percent, almost exactly the same result as for the original LeNet5:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 训练历史记录了在大约4分钟内单个GPU上进行81个时期后的最后改进。这个样本运行的测试准确率为99.09％，几乎与原始LeNet5的结果完全相同：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For comparison, a simple two-layer feedforward network achieves "only" 97.04
    percent test accuracy (see the notebook). The LeNet5 improvement on MNIST is,
    in fact, modest. Non-neural methods have also achieved classification accuracies
    greater than or equal to 99 percent, including K-nearest neighbors and support
    vector machines. CNNs really shine with more challenging datasets as we will see
    next.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，一个简单的两层前馈网络只能达到“仅有”97.04％的测试准确率（见笔记本）。实际上，LeNet5在MNIST上的改进是适度的。非神经方法也已经实现了大于或等于99％的分类准确率，包括K最近邻和支持向量机。正如我们将在接下来看到的那样，CNN在更具挑战性的数据集上表现出色。
- en: AlexNet – reigniting deep learning research
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet - 重新点燃深度学习研究
- en: AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton at the
    University of Toronto, dramatically reduced the error rate and significantly outperformed
    the runner-up at the 2012 ILSVRC, achieving a top-5 error of 16 percent versus
    26 percent (Krizhevsky, Sutskever, and Hinton 2012). This breakthrough triggered
    a renaissance in ML research and put deep learning for computer vision firmly
    on the global technology map.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton在多伦多大学开发，大大降低了错误率，并在2012年ILSVRC的亚军中显着表现出色，实现了16％的前5错误率，而亚军为26％（Krizhevsky，Sutskever和Hinton
    2012）。这一突破引发了机器学习研究的复兴，并将计算机视觉的深度学习牢牢地放在了全球技术地图上。
- en: The AlexNet architecture is similar to LeNet, but much deeper and wider. It
    is often credited with discovering **the importance of depth** with around 60
    million parameters, exceeding LeNet5 by a factor of 1,000, a testament to increased
    computing power, especially the use of GPUs, and much larger datasets.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet的架构类似于LeNet，但更深更宽。它通常被认为是发现**深度的重要性**，具有约6000万个参数，比LeNet5高出1000倍，这证明了计算能力的增加，特别是GPU的使用，以及更大的数据集。
- en: It included convolutions stacked on top of each other rather than combining
    each convolution with a pooling stage, and successfully used dropout for regularization
    and ReLU for efficient nonlinear transformations. It also employed data augmentation
    to increase the number of training samples, added weight decay, and used a more
    efficient implementation of convolutions. It also accelerated training by distributing
    the network over two GPUs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括了卷积层堆叠在一起，而不是将每个卷积与池化阶段相结合，并成功地使用了dropout进行正则化和ReLU进行高效的非线性变换。它还使用了数据增强来增加训练样本的数量，增加了权重衰减，并使用了更高效的卷积实现。它还通过在两个GPU上分布网络来加速训练。
- en: The notebook `image_classification_with_alexnet.ipynb` has a slightly simplified
    version of AlexNet tailored to the CIFAR-10 dataset that contains 60,000 images
    from 10 of the original 1,000 classes. It has been compressed to a ![](img/B15439_18_009.png)
    pixel resolution from the original ![](img/B15439_18_010.png), but still has three
    color channels.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`image_classification_with_alexnet.ipynb`中有一个针对CIFAR-10数据集的稍简化版本的AlexNet，其中包含来自原始1000个类别中的10个类别的60000张图像。它已经被压缩到了![](img/B15439_18_009.png)像素分辨率，但仍然有三个颜色通道。
- en: See the notebook `image_classification_with_alexnet` for implementation details;
    we will skip over some repetitive steps here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有关实现细节，请参阅笔记本`image_classification_with_alexnet`；我们将在这里跳过一些重复的步骤。
- en: Preprocessing CIFAR-10 data using image augmentation
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用图像增强预处理CIFAR-10数据
- en: CIFAR-10 can also be downloaded using TensorFlow's Keras interface, and we rescale
    the pixel values and one-hot encode the ten class labels as we did with MNIST
    in the previous section.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10也可以使用TensorFlow的Keras接口下载，并且我们对像素值进行了重新缩放，并对十个类别标签进行了独热编码，就像我们在上一节中对MNIST数据集所做的那样。
- en: We first train a two-layer feedforward network on 50,000 training samples for
    45 epochs to achieve a test accuracy of 45.78 percent. We also experiment with
    a three-layer convolutional net with over 528,000 parameters that achieves 74.51
    percent test accuracy (see the notebook).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对50000个训练样本训练了一个两层前馈网络，经过45个epochs达到了45.78%的测试准确率。我们还尝试了一个具有超过528000个参数的三层卷积网络，达到了74.51%的测试准确率（请参阅笔记本）。
- en: 'A common trick to enhance performance is to artificially increase the size
    of the training set by creating synthetic data. This involves randomly shifting
    or horizontally flipping the image or introducing noise into the image. TensorFlow
    includes an `ImageDataGenerator` class for this purpose. We can configure it and
    fit the training data as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 增强性能的常见技巧是通过创建合成数据来人为增加训练集的大小。这包括随机移动或水平翻转图像，或者向图像引入噪音。TensorFlow包括一个`ImageDataGenerator`类来实现这个目的。我们可以按照以下方式配置它并拟合训练数据：
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result shows how the augmented images (in low 32×32 resolution) have been
    altered in various ways as expected:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了增强图像（低32×32分辨率）如预期地以各种方式被改变：
- en: '![](img/B15439_18_06.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_06.png)'
- en: 'Figure 18.6: Original and augmented samples'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.6：原始和增强样本
- en: The test accuracy for the three-layer CNN improves modestly to 76.71 percent
    after training on the larger, augmented data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 三层CNN的测试准确率在训练更大的增强数据后略有提高，达到了76.71%。
- en: Defining the model architecture
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型架构
- en: We need to adapt the AlexNet architecture to the lower dimensionality of CIFAR-10
    images relative to the ImageNet samples used in the competition. To this end,
    we use the original number of filters but make them smaller (see the notebook
    for implementation details).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要根据CIFAR-10图像的较低维度来调整AlexNet的架构，相对于比赛中使用的ImageNet样本。为此，我们使用了原始数量的滤波器，但使它们更小（有关实现细节，请参阅笔记本）。
- en: The summary (see the notebook) shows the five convolutional layers followed
    by two fully connected layers with frequent use of batch normalization, for a
    total of 21.5 million parameters.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 总结（请参阅笔记本）显示了五个卷积层，后面跟着两个全连接层，并频繁使用批量归一化，总共有2150万个参数。
- en: Comparing AlexNet performance
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较AlexNet的性能
- en: 'In addition to AlexNet, we trained a 2-layer feedforward NN and a 3-layer CNN,
    the latter with and without image augmentation. After 100 epochs (with early stopping
    if the validation accuracy does not improve for 20 rounds), we obtain the cross-validation
    trajectories and test accuracy for the four models, as displayed in *Figure 18.7*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除了AlexNet之外，我们还训练了一个2层前馈NN和一个3层CNN，后者有图像增强和没有图像增强。在100个epochs之后（如果验证准确率在20轮内没有提高，则提前停止），我们得到了四个模型的交叉验证轨迹和测试准确率，如*图18.7*所示：
- en: '![](img/B15439_18_07.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_07.png)'
- en: 'Figure 18.7: Validation performance and test accuracy on CIFAR-10'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7：CIFAR-10的验证性能和测试准确率
- en: AlexNet achieves the highest test accuracy with 79.33 percent after some 35
    epochs, closely followed by the shallower CNN with augmented images at 78.29 percent
    that trains for longer due to the larger dataset. The feedforward NN performs
    much worse than on MNIST on this more complex dataset, with a test accuracy of
    43.05 percent.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet在大约35个epochs后达到了79.33%的最高测试准确率，紧随其后的是使用增强图像的较浅的CNN，测试准确率为78.29%，由于数据集更大，训练时间更长。与在MNIST上表现相比，前馈NN在这个更复杂的数据集上表现得更差，测试准确率为43.05%。
- en: Transfer learning – faster training with less data
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习-更快的训练，更少的数据
- en: In practice, sometimes we do not have enough data to train a CNN from scratch
    with random initialization. **Transfer learning** is an ML technique that repurposes
    a model trained on one set of data for another task. Naturally, it works if the
    learning from the first task carries over to the task of interest. If successful,
    it can lead to better performance and faster training that requires less labeled
    data than training a neural network from scratch on the target task.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，有时我们没有足够的数据来从头开始训练一个CNN。**迁移学习**是一种机器学习技术，它重新利用在一个数据集上训练的模型来处理另一个任务。如果成功，它可以带来更好的性能和更快的训练，需要的标记数据比在目标任务上从头开始训练神经网络要少。
- en: Alternative approaches to transfer learning
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习的替代方法
- en: The transfer learning approach to CNN relies on pretraining on a very large
    dataset like ImageNet. The goal is for the convolutional filters to extract a
    feature representation that generalizes to new images. In a second step, it leverages
    the result to either initialize and retrain a new CNN or use it as input to a
    new network that tackles the task of interest.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的迁移学习方法依赖于在像ImageNet这样的非常大的数据集上进行预训练。目标是让卷积滤波器提取一个对新图像泛化的特征表示。在第二步中，它利用结果来初始化和重新训练一个新的CNN，或者将其作为输入到解决感兴趣任务的新网络。
- en: As discussed, CNN architectures typically use a sequence of convolutional layers
    to detect hierarchical patterns, adding one or more fully connected layers to
    map the convolutional activations to the outcome classes or values. The output
    of the last convolutional layer that feeds into the fully connected part is called
    the bottleneck features. We can use the **bottleneck features** of a pretrained
    network as inputs into a new fully connected network, usually after applying a
    ReLU activation function.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论的那样，CNN架构通常使用一系列卷积层来检测分层模式，并添加一个或多个全连接层将卷积激活映射到结果类别或值。进入全连接部分的最后一个卷积层的输出被称为瓶颈特征。我们可以使用预训练网络的**瓶颈特征**作为新的全连接网络的输入，通常在应用ReLU激活函数之后。
- en: In other words, we freeze the convolutional layers and **replace the dense part
    of the network**. An additional benefit is that we can then use inputs of different
    sizes because it is the dense layers that constrain the input size.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们冻结卷积层并**替换网络的密集部分**。另一个好处是我们可以使用不同大小的输入，因为是密集层限制了输入大小。
- en: Alternatively, we can use the bottleneck features as **inputs into a different
    machine learning algorithm**. In the AlexNet architecture, for instance, the bottleneck
    layer computes a vector with 4,096 entries for each ![](img/B15439_18_010.png)
    input image. We then use this vector as features for a new model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以将瓶颈特征作为**不同机器学习算法的输入**。例如，在AlexNet架构中，瓶颈层为每个输入图像计算一个包含4,096个条目的向量。然后我们可以将这个向量作为新模型的特征。
- en: We also can go a step further and not only replace and retrain the final layers
    using new data but also **fine-tune the weights of the pretrained CNN**. To achieve
    this, we continue training, either only for later layers while freezing the weights
    of some earlier layers, or for all layers. The motivation is presumably to preserve
    more generic patterns learned by lower layers, such as edge or color blob detectors,
    while allowing later layers of the CNN to adapt to the details of a new task.
    ImageNet, for example, contains a wide variety of dog breeds, which may lead to
    feature representations specifically useful for differentiating between these
    classes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更进一步，不仅使用新数据替换和重新训练最终层，而且还可以**微调预训练的CNN的权重**。为了实现这一点，我们继续训练，可以只针对后面的层进行训练，同时冻结一些较早的层的权重，或者对所有层进行训练。动机可能是保留较低层学习到的更通用的模式，比如边缘或颜色斑块检测器，同时允许CNN的后续层适应新任务的细节。例如，ImageNet包含各种各样的狗品种，这可能导致对于区分这些类别特别有用的特征表示。
- en: Building on state-of-the-art architectures
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建立在最先进的架构之上
- en: Transfer learning permits us to leverage top-performing architectures without
    incurring the potentially fairly GPU- and data-intensive training. We briefly
    outline the key characteristics of a few additional popular architectures that
    are popular starting points.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习使我们能够利用表现最佳的架构，而不需要进行潜在的大规模GPU和数据密集型的训练。我们简要概述了一些其他流行的架构的关键特征，这些架构是流行的起点。
- en: VGGNet – more depth and smaller filters
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VGGNet - 更深和更小的滤波器
- en: The runner-up in ILSVRC 2014 was developed by Oxford University's Visual Geometry
    Group (VGG, Simonyan 2015). It demonstrated the effectiveness of **much smaller**
    ![](img/B15439_18_012.png) **convolutional filters** combined in sequence and
    reinforced the importance of depth for strong performance. VGG16 contains 16 convolutional
    and fully connected layers that only perform ![](img/B15439_18_012.png) convolutions
    and ![](img/B15439_18_014.png) pooling (see *Figure 18.5*).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年ILSVRC的亚军是由牛津大学的视觉几何组（VGG，Simonyan 2015）开发的。它展示了**更小的**卷积滤波器的有效性，这些滤波器按顺序组合，并强调了深度对于强大性能的重要性。VGG16包含16个卷积和全连接层，只执行卷积和池化操作（见*图18.5*）。
- en: VGG16 has **140 million parameters** that increase the computational costs of
    training and inference as well as the memory requirements. However, most parameters
    are in the fully connected layers that were since discovered not to be essential
    so that removing them greatly reduces the number of parameters without negatively
    impacting performance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16有**1.4亿个参数**，这增加了训练和推断的计算成本以及内存需求。然而，大多数参数都在全连接层中，后来发现这些参数并非必不可少，因此删除它们可以大大减少参数数量，而不会对性能产生负面影响。
- en: GoogLeNet – fewer parameters through Inception
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GoogLeNet - 通过Inception减少参数
- en: Christian Szegedy at Google reduced the computational costs using more efficient
    CNN implementations to facilitate practical applications at scale. The resulting
    GoogLeNet (Szegedy et al. 2015) won the ILSVRC 2014 with only 4 million parameters
    due to the **Inception module**, compared to AlexNet's 60 million and VGG16's
    140 million.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Google的Christian Szegedy通过更高效的CNN实现降低了计算成本，以便在规模上实现实际应用。由于**Inception模块**，GoogLeNet（Szegedy等人，2015）只有400万个参数，而AlexNet有6000万个，VGG16有1.4亿个。
- en: The Inception module builds on the **network-in-network concept** that uses
    ![](img/B15439_18_015.png) convolutions to compress a deep stack of convolutional
    filters and thus reduce the cost of computation. The module uses parallel ![](img/B15439_18_015.png),
    ![](img/B15439_18_017.png), and ![](img/B15439_18_018.png) filters, combining
    the latter two with ![](img/B15439_18_015.png) convolutions to reduce the dimensionality
    of the filters passed in by the previous layer.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Inception模块建立在**网络内网络概念**的基础上，该概念利用![](img/B15439_18_015.png)卷积来压缩深度卷积滤波器堆栈，从而降低计算成本。该模块使用并行的![](img/B15439_18_015.png)、![](img/B15439_18_017.png)和![](img/B15439_18_018.png)滤波器，将后两者与![](img/B15439_18_015.png)卷积结合起来，以降低上一层传入的滤波器的维度。
- en: In addition, it uses average pooling instead of fully connected layers on top
    of the convolutional layers to eliminate many of the less impactful parameters.
    There have been several enhanced versions, most recently Inception-v4.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它使用平均池化代替卷积层之上的全连接层，以消除许多影响较小的参数。最近还出现了几个增强版本，最近的是Inception-v4。
- en: ResNet – shortcut connections beyond human performance
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ResNet-超越人类表现的快捷连接
- en: The **residual network** **(ResNet)** architecture was developed at Microsoft
    and won the ILSVRC 2015\. It pushed the top-5 error to 3.7 percent, below the
    level of human performance on this task of around 5 percent (He et al. 2015).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差网络（ResNet）**架构是在微软开发的，并赢得了ILSVRC 2015。它将前5个错误推到了3.7％以下，低于人类在该任务上的表现水平，约为5％（He等人，2015）。'
- en: It introduces identity shortcut connections that skip several layers and overcome
    some of the challenges of training deep networks, enabling the use of hundreds
    or even over a thousand layers. It also heavily uses batch normalization, which
    was shown to allow higher learning rates and be more forgiving about weight initialization.
    The architecture also omits the fully connected final layers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 它引入了身份快捷连接，跳过了几层，并克服了训练深度网络的一些挑战，使得可以使用数百甚至一千多层。它还大量使用批量归一化，据显示可以允许更高的学习速率，并且对权重初始化更宽容。该架构还省略了最终的全连接层。
- en: 'As mentioned in the last chapter, the training of deep networks faces the notorious
    vanishing gradient challenge: as the gradient propagates to earlier layers, repeated
    multiplication of small weights risks shrinking the gradient toward zero. Hence,
    increasing depth may limit learning.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一章所述，深度网络的训练面临着臭名昭著的梯度消失挑战：随着梯度传播到较早的层，重复小权重的乘法会使梯度向零收缩的风险增加。因此，增加深度可能会限制学习。
- en: The shortcut connection that skips two or more layers has become one of the
    most popular developments in CNN architectures and triggered numerous research
    efforts to refine and explain its performance. See the references on GitHub for
    additional information.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过两个或更多层的快捷连接已成为CNN架构中最受欢迎的发展之一，并引发了许多研究努力来改进和解释其性能。有关更多信息，请参阅GitHub上的参考资料。
- en: Transfer learning with VGG16 in practice
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在实践中使用VGG16进行迁移学习
- en: Modern CNNs can take weeks to train on multiple GPUs on ImageNet, but fortunately,
    many researchers share their final weights. TensorFlow 2, for example, contains
    pretrained models for several of the reference architectures discussed previously,
    namely VGG16 and its larger version, VGG19, ResNet50, InceptionV3, and InceptionResNetV2,
    as well as MobileNet, DenseNet, NASNet, and MobileNetV2.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现代CNN在ImageNet上可能需要数周的时间进行训练，但幸运的是，许多研究人员分享他们的最终权重。例如，TensorFlow 2包含了几种先前讨论的参考架构的预训练模型，即VGG16及其更大的版本VGG19，ResNet50，InceptionV3和InceptionResNetV2，以及MobileNet，DenseNet，NASNet和MobileNetV2。
- en: How to extract bottleneck features
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何提取瓶颈特征
- en: 'The notebook `bottleneck_features.ipynb` illustrates how to download the pretrained
    VGG16 model, either with the final layers to generate predictions or without the
    final layers, as illustrated in *Figure 18.8*, to extract the outputs produced
    by the bottleneck features:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`bottleneck_features.ipynb`说明了如何下载预训练的VGG16模型，可以选择是否包含最终层以生成预测，或者不包含最终层，如*图18.8*所示，以提取瓶颈特征产生的输出：
- en: '![](img/B15439_18_08.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_08.png)'
- en: 'Figure 18.8: The VGG16 architecture'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.8：VGG16架构
- en: 'TensorFlow 2 makes it very straightforward to download and use pretrained models:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2使得下载和使用预训练模型变得非常简单：
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can use this model for predictions like any other Keras model: we pass
    in seven sample images and obtain class probabilities for each of the 1,000 ImageNet
    categories:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像使用其他Keras模型一样使用该模型进行预测：我们传入七张样本图像，并为1,000个ImageNet类别中的每个类别获取类别概率：
- en: '[PRE9]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To exclude the fully connected layers, just add the keyword `include_top=False`.
    Predictions are now output by the final convolutional layer `block5_pool` and
    match this layer''s shape:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要排除全连接层，只需添加关键字`include_top=False`。现在，预测由最终卷积层`block5_pool`输出，并与该层的形状匹配：
- en: '[PRE10]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By omitting the fully connected layers and keeping only the convolutional modules,
    we are no longer forced to use a fixed input size for the model such as the original
    ![](img/B15439_18_010.png) ImageNet format. Instead, we can adapt the model to
    arbitrary input sizes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过省略全连接层并仅保留卷积模块，我们不再被迫使用固定的输入尺寸，比如原始的![](img/B15439_18_010.png) ImageNet格式。相反，我们可以将模型调整为任意输入尺寸。
- en: How to fine-tune a pretrained model
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何微调预训练模型
- en: We will demonstrate how to freeze some or all of the layers of a pretrained
    model and continue training using a new fully-connected set of layers and data
    with a different format (see the notebook `transfer_learning.ipynb` for code examples,
    adapted from a TensorFlow 2 tutorial).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示如何冻结预训练模型的一些或所有层，并继续使用新的完全连接层和不同格式的数据进行训练（请参阅笔记本`transfer_learning.ipynb`中的代码示例，该示例改编自TensorFlow
    2教程）。
- en: We use the VGG16 weights, pretrained on ImageNet with TensorFlow's built-in
    cats versus dogs images (see the notebook on how to source the dataset).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用在ImageNet上预训练的VGG16权重，该数据集包含TensorFlow内置的猫与狗的图像（请参阅有关如何获取数据集的笔记本）。
- en: 'Preprocessing resizes all images to ![](img/B15439_18_021.png) pixels. We indicate
    the new input size as we instantiate the pretrained VGG16 instance and then freeze
    all weights:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理将所有图像调整大小为![](img/B15439_18_021.png)像素。当我们实例化预训练的VGG16实例并冻结所有权重时，我们指示新的输入大小：
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The shape of the model output for 32 sample images now matches that of the
    last convolutional layer in the headless model:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在32个样本图像的模型输出形状与无头模型中的最后一个卷积层的形状相匹配：
- en: '[PRE12]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can append new layers to the headless model using either the Sequential
    or the Functional API. For the Sequential API, adding `GlobalAveragePooling2D`,
    `Dense`, and `Dropout` layers works as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用顺序API或功能API将新层附加到无头模型。对于顺序API，添加`GlobalAveragePooling2D`、`Dense`和`Dropout`层的工作如下：
- en: '[PRE13]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We set `from_logits=True` for the `BinaryCrossentropy` loss because the model
    provides a linear output. The summary shows how the new model combines the pretrained
    VGG16 convolutional layers and the new final layers:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`BinaryCrossentropy`损失设置了`from_logits=True`，因为模型提供了线性输出。摘要显示了新模型如何结合预训练的VGG16卷积层和新的最终层：
- en: '[PRE14]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: See the notebook for the Functional API version.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有关功能API版本，请参阅笔记本。
- en: 'Prior to training the new final layer, the pretrained VGG16 delivers a validation
    accuracy of 48.75 percent. Now we proceed to train the model for 10 epochs as
    follows, adjusting only the final layer weights:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练新的最终层之前，预训练的VGG16提供了48.75％的验证准确度。现在我们继续训练模型10个时期，只调整最终层的权重：
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '10 epochs boost validation accuracy above 94 percent. To fine-tune the model,
    we can unfreeze the VGG16 models and continue training. Note that you should only
    do so after training the new final layers: randomly initialized classification
    layers will likely produce large gradient updates that can eliminate the pretraining
    results.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 10个时期将验证准确度提高到94％以上。要微调模型，我们可以解冻VGG16模型并继续训练。请注意，只有在训练新的最终层之后才应该这样做：随机初始化的分类层可能会产生大的梯度更新，这可能会消除预训练结果。
- en: 'To unfreeze parts of the model, we select a layer, after which we set the weights
    to `trainable`; in this case, layer 12 of the total 19 layers in the VGG16 architecture:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要解冻模型的部分，我们选择一层，然后将权重设置为`trainable`；在这种情况下，VGG16架构中的19层中的第12层：
- en: '[PRE16]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now just recompile the model and continue training for up to 50 epochs using
    early stopping, starting in epoch 10 as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只需重新编译模型，并使用提前停止继续训练，最多进行50个时期，从第10个时期开始，如下所示：
- en: '[PRE17]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Figure 18.9* shows how the validation accuracy increases substantially, reaching
    97.89 percent after another 22 epochs:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.9*显示验证准确度显着增加，经过另外22个时期后达到97.89％：'
- en: '![](img/B15439_18_09.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_09.png)'
- en: 'Figure 18.9: Cross-validation performance: accuracy and cross-entropy loss'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.9：交叉验证性能：准确度和交叉熵损失
- en: Transfer learning is an important technique when training data is limited as
    is very often the case in practice. While cats and dogs are unlikely to produce
    tradeable signals, transfer learning could certainly help improve the accuracy
    of predictions on a relevant alternative dataset, such as the satellite images
    that we'll tackle next.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据有限时，迁移学习是一种重要的技术，这在实践中经常发生。虽然猫和狗不太可能产生可交易的信号，但迁移学习肯定可以帮助提高对相关替代数据集的预测准确性，例如我们接下来将处理的卫星图像。
- en: Classifying satellite images with transfer learning
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用迁移学习对卫星图像进行分类
- en: Satellite images figure prominently among alternative data (see *Chapter 3*,
    *Alternative Data for Finance – Categories and Use Cases*). For instance, commodity
    traders may rely on satellite images to predict the supply of certain crops or
    resources by monitoring, activity on farms, at mining sites, or oil tanker traffic.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 卫星图像在替代数据中占据重要地位（请参阅*第3章*，*金融替代数据-类别和用例*）。例如，大宗商品交易商可能依赖卫星图像来预测某些作物或资源的供应，方法是通过监视农场、矿山或油轮交通的活动。
- en: The EuroSat dataset
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EuroSat数据集
- en: 'To illustrate working with this type of data, we load the EuroSat dataset included
    in the TensorFlow 2 datasets (Helber et al. 2019). The EuroSat dataset includes
    around 27,000 images in ![](img/B15439_18_022.png) format that represent 10 different
    types of land uses. *Figure 18.10* displays an example for each label:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明使用这种类型的数据，我们加载了包含在TensorFlow 2数据集中的EuroSat数据集（Helber等人，2019）。EuroSat数据集包括大约27,000张图像，格式为![](img/B15439_18_022.png)，代表10种不同类型的土地利用。*图18.10*显示了每个标签的一个示例：
- en: '![](img/B15439_18_10.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_10.png)'
- en: 'Figure 18.10: Ten types of land use contained in the dataset'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.10：数据集中包含的十种土地利用类型
- en: A time series of similar data could be used to track the relative sizes of cultivated,
    industrial, and residential areas or the status of specific crops to predict harvest
    quantities or quality, for example, for wine.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 类似数据的时间序列可以用于跟踪耕地、工业区和住宅区的相对大小，或者特定作物的状态以预测收获数量或质量，例如葡萄酒。
- en: Fine-tuning a very deep CNN – DenseNet201
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调非常深的CNN – DenseNet201
- en: Huang et al. (2018) developed a new architecture dubbed **densely connected**
    based on the insight that CNNs can be deeper, more accurate, and more efficient
    to train if they contain shorter connections between layers close to the input
    and those close to the output.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Huang等人（2018）开发了一种名为**密集连接**的新架构，其基础是CNN可以更深、更准确且更容易训练，如果它们包含接近输入和接近输出的层之间的短连接。
- en: One architecture, labeled **DenseNet201**, connects each layer to every other
    layer in a feedforward fashion. It uses the feature maps of all preceding layers
    as inputs, while each layer's own feature maps become inputs into all subsequent
    layers.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一种架构，标记为**DenseNet201**，以前馈方式将每一层连接到每一层。它使用所有前面层的特征图作为输入，而每一层自己的特征图成为所有后续层的输入。
- en: 'We download the DenseNet201 architecture from `tensorflow.keras.applications`
    and replace its final layers with the following dense layers interspersed with
    batch normalization to mitigate exploding or vanishing gradients in this very
    deep network with over 700 layers:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`tensorflow.keras.applications`下载DenseNet201架构，并用以下密集层替换其最终层，这些密集层与批量归一化交替使用，以减轻这个非常深的网络中的梯度爆炸或消失问题，该网络有700多层：
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Model training and results evaluation
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型训练和结果评估
- en: We use 10 percent of the training images for validation purposes and achieve
    the best out-of-sample classification accuracy of 97.96 percent after 10 epochs.
    This exceeds the performance cited in the original paper for the best-performing
    ResNet-50 architecture with a 90-10 split.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用10％的训练图像进行验证，并在10个时期后实现97.96％的最佳样本外分类准确度。这超过了原始论文中引用的最佳ResNet-50架构的性能，该架构使用90-10分割的准确度。
- en: '![](img/B15439_18_11.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_11.png)'
- en: 'Figure 18.11: Cross-validation performance'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.11：交叉验证性能
- en: There would likely be additional performance gains from augmenting the relatively
    small training set.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能会从扩充相对较小的训练集中获得额外的性能提升。
- en: Object detection and segmentation
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测和分割
- en: 'Image classification is a fundamental computer vision task that requires labeling
    an image based on certain objects it contains. Many practical applications, including
    investment and trading strategies, require additional information:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是一个基本的计算机视觉任务，它要求根据图像中包含的某些对象对图像进行标记。许多实际应用，包括投资和交易策略，需要额外的信息：
- en: The **object detection** task requires not only the identification but also
    the spatial location of all objects of interest, typically using bounding boxes.
    Several algorithms have been developed to overcome the inefficiency of brute-force
    sliding-window approaches, including region proposal methods (R-CNN; see for example
    Ren et al. 2015) and the **You Only Look Once** (**YOLO**) real-time object detection
    algorithm (Redmon 2016).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测**任务不仅需要识别，还需要空间位置的所有感兴趣对象，通常使用边界框。已经开发了几种算法来克服暴力滑动窗口方法的低效率，包括区域提议方法（R-CNN；例如参见Ren等人2015年）和**You
    Only Look Once**（**YOLO**）实时目标检测算法（Redmon 2016）。'
- en: The **object segmentation** task goes a step further and requires a class label
    and an outline of every object in the input image. This may be useful to count
    objects such as oil tankers, individuals, or cars in an image and evaluate a level
    of activity.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象分割**任务更进一步，需要输入图像中每个对象的类别标签和轮廓。这可能有助于计算图像中的对象数量，如油轮、个人或汽车，并评估活动水平。'
- en: '**Semantic segmentation**, also called scene parsing, makes dense predictions
    to assign a class label to each pixel in the image. As a result, the image is
    divided into semantic regions and each pixel is assigned to its enclosing object
    or region.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**，也称为场景解析，对图像中的每个像素进行密集预测，为其分配一个类别标签。因此，图像被划分为语义区域，并且每个像素被分配到其封闭对象或区域。'
- en: Object detection requires the ability to distinguish between several classes
    of objects and to decide how many and which of these objects are present in an
    image.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测需要区分多个对象类别的能力，并决定图像中存在多少个对象以及其中的哪些对象。
- en: Object detection in practice
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际中的目标检测
- en: 'A prominent example is Ian Goodfellow''s identification of house numbers from
    Google''s **Street View House Numbers** (**SVHN**) dataset (Goodfellow 2014).
    It requires the model to identify the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的例子是Ian Goodfellow在Google的**街景房屋数字**（**SVHN**）数据集（Goodfellow 2014）中识别房屋数字。它要求模型识别以下内容：
- en: How many of up to five digits make up the house number
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 房屋号码由多少个最多五位数字组成
- en: The correct digit for each component
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个组件的正确数字
- en: The proper order of the constituent digits
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组成数字的正确顺序
- en: We will show how to preprocess the irregularly shaped source images, adapt the
    VGG16 architecture to produce multiple outputs, and train the final layer, before
    fine-tuning the pretrained weights to address the task.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何预处理不规则形状的源图像，调整VGG16架构以产生多个输出，并训练最终层，然后微调预训练权重以解决任务。
- en: Preprocessing the source images
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理源图像
- en: The notebook `svhn_preprocessing.ipynb` contains code to produce a simplified,
    cropped dataset that uses bounding box information to create regularly shaped
    ![](img/B15439_18_023.png) images containing the digits; the original images are
    of arbitrary shape (Netzer 2011).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`svhn_preprocessing.ipynb`包含代码，用于生成一个简化的裁剪数据集，该数据集使用边界框信息创建规则形状的图像，其中包含数字；原始图像的形状是任意的（Netzer
    2011）。
- en: '![](img/B15439_18_12.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_12.png)'
- en: 'Figure 18.12: Cropped sample images of the SVHN dataset'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.12：SVHN数据集的裁剪样本图像
- en: The SVHN dataset contains house numbers with up to five digits and uses the
    class 10 if a digit is not present. However, since there are very few examples
    with five digits, we limit the images to those including up to four digits only.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: SVHN数据集包含多达五位数字的房屋号码，并且如果数字不存在则使用类别10。然而，由于包含五位数字的示例非常少，我们将图像限制为仅包含最多四位数字的图像。
- en: Transfer learning with a custom final layer
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自定义最终层的迁移学习
- en: The notebook `svhn_object_detection.ipynb` illustrates how to apply transfer
    learning to a deep CNN based on the VGG16 architecture, as outlined in the previous
    section. We will describe how to create new final layers that produce several
    outputs to meet the three SVHN task objectives, including one prediction of how
    many digits are present, and one for the value of each digit in the order they
    appear.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本`svhn_object_detection.ipynb`说明了如何将迁移学习应用于基于VGG16架构的深度CNN，如前一节所述。我们将描述如何创建新的最终层，以产生多个输出，以满足三个SVHN任务目标，包括一个预测有多少位数字，以及按照它们出现的顺序每个数字的值。
- en: 'The best-performing architecture on the original dataset has eight convolutional
    layers and two final fully connected layers. We will use **transfer learning**,
    departing from the VGG16 architecture. As before, we import the VGG16 network
    pretrained on ImageNet weights, remove the layers after the convolutional blocks,
    freeze the weights, and create new dense and predictive layers as follows using
    the Functional API:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集上表现最佳的架构具有八个卷积层和两个最终全连接层。我们将使用**迁移学习**，离开VGG16架构。与之前一样，我们导入在ImageNet权重上预训练的VGG16网络，删除卷积块后的层，冻结权重，并使用Functional
    API创建新的密集和预测层，如下所示：
- en: '[PRE19]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The prediction layer combines the four-class output for the number of digits
    `n_digits` with four outputs that predict which digit is present at that position.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 预测层将数字的四类输出`n_digits`与预测在该位置出现的数字的四个输出相结合。
- en: Creating a custom loss function and evaluation metrics
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义损失函数和评估指标
- en: The custom output requires us to define a loss function that captures how well
    the model is meeting its objective. We would also like to measure accuracy in
    a way that reflects predictive accuracy tailored to the specific labels.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义输出要求我们定义一个捕捉模型是否达到其目标的损失函数。我们还希望以反映特定标签的预测准确度的方式来衡量准确性。
- en: 'For the custom loss, we average the cross-entropy over the five categorical
    outputs, namely the number of digits and their respective values:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自定义损失，我们将五个分类输出上的交叉熵进行平均，即数字的数量和它们各自的值：
- en: '[PRE20]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To measure predictive accuracy, we compare the five predictions with the corresponding
    label values and average the share of correct matches over the batch of samples:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量预测准确度，我们将五个预测与相应的标签值进行比较，并在样本批次上平均正确匹配的份额：
- en: '[PRE21]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we integrate the base and final layers and compile the model with
    the custom loss and accuracy metric as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们集成基础和最终层，并使用自定义损失和准确度指标编译模型，如下所示：
- en: '[PRE22]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Fine-tuning the VGG16 weights and final layer
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调VGG16的权重和最终层
- en: We train the new final layers for 14 periods and continue fine-tuning all VGG16
    weights, as in the previous section, for another 23 epochs (using early stopping
    in both cases).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对新的最终层进行了14个周期的训练，并继续微调所有VGG16的权重，就像前一节一样，再进行了23个周期的训练（在这两种情况下都使用了提前停止）。
- en: 'The following charts show the training and validation accuracy and the loss
    over the entire training period. As we unfreeze the VGG16 weights after the initial
    training period, the accuracy drops and then improves, achieving a validation
    performance of 94.52 percent:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了整个训练期间的训练和验证准确度以及损失。当我们在初始训练期之后解冻VGG16的权重时，准确度会下降然后提高，达到94.52%的验证性能：
- en: '![](img/B15439_18_13.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_13.png)'
- en: 'Figure 18.13: Cross-validation performance'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.13：交叉验证性能
- en: See the notebook for additional implementation details and an evaluation of
    the results.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其他实施细节和结果评估，请参阅笔记本。
- en: Lessons learned
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所学到的教训
- en: We can achieve decent levels of accuracy using only the small training set.
    However, state-of-the-art performance achieves an error rate of only 1.02 percent
    ([https://benchmarks.ai/svhn](https://benchmarks.ai/svhn)). To get closer, the
    most important step is to increase the amount of training data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以仅使用小训练集就能达到相当高的准确度。然而，最先进的性能只有1.02%的错误率（[https://benchmarks.ai/svhn](https://benchmarks.ai/svhn)）。要更接近这个水平，最重要的一步是增加训练数据的数量。
- en: 'There are two easy ways to accomplish this: we can include the larger number
    of samples included in the **extra** dataset, and we can use image augmentation
    (see the *AlexNet: reigniting deep learning research* section). The currently
    best-performing approach relies heavily on augmentation learned from data (Cubuk
    2019).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种简单的方法可以实现这一点：我们可以包括“额外”数据集中包含的更多样本，也可以使用图像增强（请参阅*AlexNet：重新点燃深度学习研究*部分）。目前表现最佳的方法主要依赖于从数据中学到的增强（Cubuk
    2019）。
- en: CNNs for time-series data – predicting returns
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于时间序列数据的CNN - 预测回报
- en: CNNs were originally developed to process image data and have achieved superhuman
    performance on various computer vision tasks. As discussed in the first section,
    time-series data has a grid-like structure similar to that of images, and CNNs
    have been successfully applied to one-, two- and three-dimensional representations
    of temporal data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: CNN最初是为处理图像数据而开发的，并在各种计算机视觉任务上取得了超人的表现。正如第一节所讨论的，时间序列数据具有类似图像的网格结构，CNN已成功应用于一维、二维和三维时间数据的表示。
- en: The application of CNNs to time series will most likely bear fruit if the data
    meets the model's key assumption that local patterns or relationships help predict
    the outcome. In the time-series context, local patterns could be autocorrelation
    or similar non-linear relationships at relevant intervals. Along the second and
    third dimensions, local patterns imply systematic relationships among different
    components of a multivariate series or among these series for different tickers.
    Since locality matters, it is important that the data is organized accordingly,
    in contrast to feed-forward networks where shuffling the elements of any dimension
    does not negatively affect the learning process.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据符合模型的关键假设，即局部模式或关系有助于预测结果，那么将CNN应用于时间序列很可能会取得成功。在时间序列的背景下，局部模式可能是自相关或相关间隔的类似非线性关系。在第二和第三维度上，局部模式意味着多元系列的不同分量之间或不同股票的这些系列之间存在系统关系。由于局部性很重要，因此重要的是数据相应地组织，与前馈网络相反，在前馈网络中，任何维度的元素洗牌都不会对学习过程产生负面影响。
- en: In this section, we provide a relatively simple example using a one-dimensional
    convolution to model an autoregressive process (see *Chapter 9*, *Time-Series
    Models for Volatility Forecasts and Statistical Arbitrage*) that predicts future
    returns based on lagged returns. Then we replicate a recent research paper that
    achieved good results by formatting multivariate time-series data like images
    to predict returns. We will also develop and test a trading strategy based on
    the signals contained in the predictions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一个相对简单的示例，使用一维卷积来建模自回归过程（参见*第9章*，*波动率预测和统计套利的时间序列模型*），该模型基于滞后收益预测未来收益。然后我们复制了一篇最近的研究论文，该论文通过将多变量时间序列数据格式化为图像来预测收益，并将基于预测中包含的信号开发和测试交易策略。
- en: An autoregressive CNN with 1D convolutions
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有一维卷积的自回归CNN
- en: We will introduce the time series use case for CNN using a univariate autoregressive
    asset return model. More specifically, the model receives the most recent 12 months
    of returns and uses a single layer of one-dimensional convolutions to predict
    the subsequent month.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍使用一维卷积来建模单变量自回归资产收益模型的时间序列用例。更具体地说，该模型接收最近12个月的收益，并使用一层一维卷积来预测随后的一个月。
- en: 'The requisite steps are as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 所需步骤如下：
- en: Creating the rolling 12 months of lagged returns and corresponding outcomes
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建滚动的12个月滞后收益和相应结果
- en: Defining the model architecture
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构
- en: Training the model and evaluating the results
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型和评估结果
- en: In the following sections, we'll describe each step in turn; the notebook `time_series_prediction`
    contains the code samples for this section.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将依次描述每个步骤；笔记本`time_series_prediction`包含了本节的代码示例。
- en: Preprocessing the data
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'First, we''ll select the adjusted close price for all Quandl Wiki stocks since
    2000 as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将选择自2000年以来所有Quandl Wiki股票的调整收盘价如下：
- en: '[PRE23]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we resample the price data to month-end frequency, compute returns, and
    set monthly returns over 100 percent to missing as they likely represent data
    errors. Then we drop tickers with missing observations, retaining 1,511 stocks
    with 215 observations each:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将价格数据重新采样为月末频率，计算收益，并将超过100%的月收益设置为缺失，因为它们可能代表数据错误。然后我们删除具有缺失观察值的股票，保留了每个215个观察值的1,511只股票：
- en: '[PRE24]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To create the rolling series of 12 lagged monthly returns with their corresponding
    outcome, we iterate over rolling 13-month slices and append the transpose of each
    slice to a list after assigning the outcome date to the index. After completing
    the loop, we concatenate the DataFrames in the list as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建12个滞后月收益及其相应结果的滚动系列，我们遍历滚动的13个月切片，并在将每个切片的转置分配给索引后，将其附加到列表中。完成循环后，我们按以下方式连接列表中的数据框：
- en: '[PRE25]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We end up with over 305,000 pairs of outcomes and lagged returns for the 2001-2017
    period:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到了2001-2017年间超过305,000对结果和滞后收益：
- en: '[PRE26]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When we compute the information coefficient for each lagged return and the
    outcome, we find that only lag 5 is not statistically significant:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们计算每个滞后收益和结果的信息系数时，我们发现只有滞后5不具有统计学显著性：
- en: '![](img/B15439_18_14.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_14.png)'
- en: 'Figure 18.14: Information coefficient with respect to forward return by lag'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.14：滞后收益与前瞻收益的信息系数
- en: Defining the model architecture
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型架构
- en: 'Now we''ll define the model architecture using TensorFlow''s Keras interface.
    We combine a one-dimensional convolutional layer with max pooling and batch normalization
    to produce a real-valued scalar output:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用TensorFlow的Keras接口定义模型架构。我们将一维卷积层与最大池化和批量归一化结合起来，产生一个实值标量输出：
- en: '[PRE27]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The one-dimensional convolution computes the sliding dot product of a (regularized)
    vector of length 4 with each input sequence of length 12, using causal padding
    to maintain the temporal order (see the *How to scan the input: strides and padding*
    section). The resulting 32 feature maps have the same length, 12, as the input
    that max pooling in groups of size 4 reduces to 32 vectors of length 3.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一维卷积计算长度为4的（正则化）向量与长度为12的每个输入序列的滑动点积，使用因果填充以保持时间顺序（参见*如何扫描输入：步幅和填充*部分）。得到的32个特征图具有与输入相同的长度12，最大池化以4个为一组减少为长度为3的32个向量。
- en: 'The model outputs the weighted average plus the bias of the flattened and normalized
    single vector of length 96, and has 449 trainable parameters:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型输出加权平均值加上长度为96的扁平化和归一化单一向量的偏差，并具有449个可训练参数：
- en: '[PRE28]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The notebook wraps the model generation and subsequent compilation into a `get_model()`
    function that parametrizes the model configuration to facilitate experimentation.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本将模型生成和后续编译包装成一个`get_model()`函数，该函数对模型配置进行参数化以便进行实验。
- en: Model training and performance evaluation
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练和性能评估
- en: We train the model on five years of data for each ticker to predict the first
    month after this period and repeat this procedure 36 times using the `MultipleTimeSeriesCV`
    we developed in *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*.
    See the notebook for the training loop that follows the pattern demonstrated in
    the previous chapter.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个股票的五年数据进行模型训练，以预测此期后的第一个月，并使用我们在*第7章*，*线性模型-从风险因子到收益预测*中开发的`MultipleTimeSeriesCV`重复此过程36次。请参阅笔记本中的训练循环，该循环遵循了上一章中演示的模式。
- en: 'We use early stopping after five epochs to simplify the exposition, resulting
    in a positive bias so that the results have only illustrative character. Training
    length varies from 1 to 27 epochs, with a median of 5 epochs, which demonstrates
    that the model can often only learn very limited amounts of systematic information
    from the past returns. Thus cherry-picking the results yields a cumulative average
    information coefficient of around 4, as shown in *Figure 18.15*:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在五个时期后使用提前停止来简化表述，从而产生积极的偏差，使得结果仅具有说明性质。训练长度从1到27个时期不等，中位数为5个时期，这表明模型通常只能从过去的收益中学到非常有限的系统信息。因此，结果的挑选产生了大约4的累积平均信息系数，如*图18.15*所示：
- en: '![](img/B15439_18_15.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_15.png)'
- en: 'Figure 18.15: (Biased) out-of-sample information coefficients for best epochs'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.15：（有偏）最佳时期的样本外信息系数
- en: We'll now proceed to a more complex example of using CNNs for multiple time-series
    data.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将进行更复杂的示例，使用CNN处理多个时间序列数据。
- en: CNN-TA – clustering time series in 2D format
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN-TA-在2D格式中对时间序列进行聚类
- en: To exploit the grid-like structure of time-series data, we can use CNN architectures
    for univariate and multivariate time series. In the latter case, we consider different
    time series as channels, similar to the different color signals.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用时间序列数据的网格结构，我们可以使用CNN架构处理单变量和多变量时间序列。在后一种情况下，我们将不同的时间序列视为通道，类似于不同的颜色信号。
- en: An alternative approach converts a time series of alpha factors into a two-dimensional
    format to leverage the ability of CNNs to detect local patterns. Sezer and Ozbayoglu
    (2018) propose **CNN-TA**, which computes 15 technical indicators for different
    intervals and uses hierarchical clustering (see *Chapter 13*, *Data-Driven Risk
    Factors and Asset Allocation with Unsupervised Learning*) to locate indicators
    that behave similarly close to each other in a two-dimensional grid.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将Alpha因子的时间序列转换为二维格式，以利用CNN检测局部模式的能力。Sezer和Ozbayoglu（2018）提出了**CNN-TA**，它为不同间隔计算15个技术指标，并使用分层聚类（请参见*第13章*，*使用无监督学习进行数据驱动的风险因素和资产配置*）来定位在二维网格中行为相似的指标。
- en: The authors train a CNN similar to the CIFAR-10 example we used earlier to predict
    whether to buy, hold, or sell an asset on a given day. They compare the CNN performance
    to "buy-and-hold" and other models and find that it outperforms all alternatives
    using daily price series for Dow 30 stocks and the nine most-traded ETFs over
    the 2007-2017 time period.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 作者训练了一个类似于我们之前使用的CIFAR-10示例的CNN，以预测在某一天是否买入、持有或卖出资产。他们将CNN的表现与“买入并持有”和其他模型进行比较，并发现在2007-2017年的时间段内，使用道琼斯30只股票和九只交易量最大的ETF的每日价格序列时，CNN的表现优于所有其他替代方案。
- en: In this section, we experiment with this approach using daily US equity price
    data and demonstrate how to compute and convert a similar set of indicators into
    image format. Then we train a CNN to predict daily returns and evaluate a simple
    long-short strategy based on the resulting signals.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用美国股票每日价格数据进行实验，并演示如何计算并转换类似的指标集为图像格式。然后，我们训练CNN来预测每日收益，并评估基于结果信号的简单多空策略。
- en: Creating technical indicators at different intervals
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在不同间隔创建技术指标
- en: We first select a universe of the 500 most-traded US stocks from the Quandl
    Wiki dataset by dollar volume for rolling five-year periods for 2007-2017\. See
    the notebook `engineer_cnn_features.ipynb` for the code examples in this section
    and some additional implementation details.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从Quandl Wiki数据集中选择了美国交易量最大的500只股票，按照滚动五年期间的美元交易量进行选择，时间为2007-2017年。有关本节中的代码示例和一些额外的实现细节，请参见笔记本`engineer_cnn_features.ipynb`。
- en: 'Our features consist of 15 technical indicators and risk factors that we compute
    for 15 different intervals and then arrange them in a ![](img/B15439_18_024.png)
    grid. The following table lists some of the technical indicators; in addition,
    we follow the authors in using the following metrics (see the *Appendix* for additional
    information):'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征包括15个技术指标和风险因素，我们计算了15个不同的间隔，然后将它们排列在一个网格中。以下表格列出了一些技术指标；此外，我们遵循作者在使用以下指标方面的做法（有关更多信息，请参见*附录*）：
- en: '**Weighted and exponential moving averages** (**WMA** and **EMA**) of the close
    price'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权和指数移动平均线**（WMA和EMA）的收盘价'
- en: '**Rate of change** (**ROC**) of the close price'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收盘价的**变化率**（ROC）
- en: '**Chande Momentum Oscillator** (**CMO**)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chande动量振荡器**（CMO）'
- en: '**Chaikin A/D Oscillators** (**ADOSC**)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chaikin A/D振荡器**（ADOSC）'
- en: '**Average Directional Movement Index** (**ADX**)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均趋向指数**（ADX）'
- en: '![](img/B15439_18_16.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_16.png)'
- en: 'Figure 8.16: Technical indicators'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16：技术指标
- en: 'For each indicator, we vary the time period from 6 to 20 to obtain 15 distinct
    measurements. For example, the following code example computes the **relative
    strength index** (**RSI**):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个指标，我们将时间段从6到20进行变化，以获得15个不同的测量。例如，以下代码示例计算**相对强度指数**（RSI）：
- en: '[PRE29]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For the **Normalized Average True Range** (**NATR**) that requires several
    inputs, the computation works as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要多个输入的**标准化平均真实范围**（NATR），计算如下：
- en: '[PRE30]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: See the TA-Lib documentation for further details.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅TA-Lib文档以获取更多详细信息。
- en: Computing rolling factor betas for different horizons
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算不同视野的滚动因子贝塔
- en: 'We also use **five Fama-French risk factors** (**Fama** and French, 2015; see
    *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*).
    They reflect the sensitivity of a stock''s returns to factors consistently demonstrated
    to impact equity returns. We capture these factors by computing the coefficients
    of a rolling OLS regression of a stock''s daily returns on the returns of portfolios
    designed to reflect the underlying drivers:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用**五个法玛-法国风险因素**（Fama和French，2015年；请参见*第4章*，*金融特征工程-如何研究Alpha因子*）。它们反映了股票收益对已被证明影响股票收益的因素的敏感性。我们通过计算股票每日收益与旨在反映基础驱动因素的投资组合的收益之间的滚动OLS回归系数来捕捉这些因素：
- en: '**Equity risk premium**: Value-weighted returns of US stocks minus the 1-month
    US Treasury bill rate'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**股权风险溢价**：美国股票的市值加权回报减去1个月期美国国库券利率'
- en: '**Size** (**SMB**): Returns of stocks categorized as **Small** (by market cap)
    **Minus** those of **Big equities**'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规模**（**SMB**）：被归类为**小**（按市值）的股票的回报**减去**大型股票的回报'
- en: '**Value** (**HML**): Returns of stocks with **High** book-to-market value **Minus**
    those with a **Low value**'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值**（**HML**）：具有**高**账面市值比的股票的回报**减去**具有**低**价值的股票的回报'
- en: '**Investment** (**CMA**): Returns differences for companies with **Conservative**
    investment expenditures **Minus** those with **Aggressive spending**'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资**（**CMA**）：具有**保守**投资支出的公司的回报差异**减去**具有**激进**支出的公司的回报'
- en: '**Profitability** (**RMW**): Similarly, return differences for stocks with
    **Robust** profitability **Minus** that with a **Weak** metric.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**盈利能力**（**RMW**）：类似地，股票的**强劲**盈利能力与**弱劲**指标的回报差异。'
- en: 'We source the data from Kenneth French''s data library using `pandas_datareader`
    (see *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`pandas_datareader`从Kenneth French的数据库中获取数据（见*第4章*，*金融特征工程-如何研究Alpha因子*）：
- en: '[PRE31]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we apply statsmodels'' `RollingOLS()` to run regressions over windowed
    periods of different lengths, ranging from 15 to 90 days. We set the `params_only`
    parameter on the `.fit()` method to speed up computation and capture the coefficients
    using the `.params` attribute of the fitted `factor_model`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用statsmodels的`RollingOLS()`来在不同长度的窗口期内运行回归，范围从15天到90天不等。我们在`.fit()`方法上设置`params_only`参数以加快计算速度，并使用拟合的`factor_model`的`.params`属性来捕获系数：
- en: '[PRE32]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Features selecting based on mutual information
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于互信息的特征选择
- en: The next step is to select the 15 most relevant features from the 20 candidates
    to fill the 15×15 input grid. The code examples for the following steps are in
    the notebook `convert_cnn_features_to_image_format`.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从20个候选项中选择15个最相关的特征，以填充15×15的输入网格。以下步骤的代码示例在笔记本`convert_cnn_features_to_image_format`中。
- en: 'To this end, we estimate the mutual information for each indicator and the
    15 intervals with respect to our target, the one-day forward returns. As discussed
    in *Chapter 4*, *Financial Feature Engineering – How to Research Alpha Factors*,
    scikit-learn provides the `mutual_info_regression()` function that makes this
    straightforward, albeit time-consuming and memory-intensive. To accelerate the
    process, we randomly sample 100,000 observations:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们估计了每个指标和15个间隔相对于我们的目标-一天的前瞻性回报的互信息。正如在*第4章*，*金融特征工程-如何研究Alpha因子*中讨论的那样，scikit-learn提供了`mutual_info_regression()`函数，使这一过程变得简单，尽管耗时且占用内存。为了加快这一过程，我们随机抽样了10万个观察值：
- en: '[PRE33]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The left panel in *Figure 18.16* shows the mutual information, averaged across
    the 15 intervals for each indicator. NATR, PPO, and Bollinger Bands are most important
    from this metric''s perspective:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.16*的左侧面板显示了每个指标的15个间隔的互信息的平均值。从这个度量的角度来看，NATR、PPO和Bollinger Bands是最重要的：'
- en: '![](img/B15439_18_17.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_17.png)'
- en: 'Figure 18.17: Mutual information and two-dimensional grid layout for time series'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.17：时间序列的互信息和二维网格布局
- en: Hierarchical feature clustering
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层特征聚类
- en: The right panel in *Figure 18.16* sketches the 15 X 15 two-dimensional feature
    grid that we will feed into our CNN. As discussed in the first section of this
    chapter, CNNs rely on the locality of relevant patterns that is typically found
    in images where nearby pixels are closely related and changes from one pixel to
    the next are often gradual.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.16*的右侧面板描绘了我们将输入到CNN中的15×15二维特征网格。正如本章的第一节中所讨论的，CNN依赖于通常在图像中找到的相关模式的局部性，其中附近的像素密切相关，并且从一个像素到下一个的变化通常是渐进的。'
- en: To organize our indicators in a similar fashion, we will follow Sezer and Ozbayoglu's
    approach of applying hierarchical clustering. The goal is to identify features
    that behave similarly and order the columns and the rows of the grid accordingly.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以类似的方式组织我们的指标，我们将遵循Sezer和Ozbayoglu的方法，应用分层聚类。目标是识别行为相似的特征，并相应地对网格的列和行进行排序。
- en: 'We can build on SciPy''s `pairwise_distance()`, `linkage()`, and `dendrogram()`
    functions that we introduced in *Chapter 13*, *Data-Driven Risk Factors and Asset
    Allocation with Unsupervised Learning* alongside other forms of clustering. We
    create a helper function that standardizes the input column-wise to avoid distorting
    distances among features due to differences in scale, and use the Ward criterion
    that merges clusters to minimize variance. The function returns the order of the
    leaf nodes in the dendrogram that in turn displays the successive formation of
    larger clusters:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在SciPy的`pairwise_distance()`、`linkage()`和`dendrogram()`函数的基础上构建，这些函数是我们在*第13章*中介绍的，*使用无监督学习进行数据驱动的风险因素和资产配置*，以及其他形式的聚类。我们创建一个辅助函数，对输入进行逐列标准化，以避免由于规模差异而扭曲特征之间的距离，并使用沃德准则来合并集群以最小化方差。该函数返回树状图中叶节点的顺序，进而显示较大集群的逐步形成：
- en: '[PRE34]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To obtain the optimized order of technical indicators in the columns and the
    different intervals in the rows, we use NumPy''s `.reshape()` method to ensure
    that the dimension we would like to cluster appears in the columns of the two-dimensional
    array we pass to `cluster_features()`:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得列中技术指标和行中不同间隔的优化顺序，我们使用NumPy的`.reshape()`方法来确保我们想要聚类的维度出现在我们传递给`cluster_features()`的二维数组的列中：
- en: '[PRE35]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '*Figure 18.18* shows the dendrograms for both the row and column features:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.18*显示了行和列特征的树状图：'
- en: '![](img/B15439_18_18.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_18.png)'
- en: 'Figure 18.18: Dendrograms for row and column features'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.18：行和列特征的树状图
- en: We reorder the features accordingly and store the result as inputs for the CNN
    that we will create in the next step.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相应地重新排列特征，并将结果存储为CNN的输入，我们将在下一步中创建。
- en: Creating and training a convolutional neural network
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建和训练卷积神经网络
- en: Now we are ready to design, train, and evaluate a CNN following the steps outlined
    in the previous section. The notebook `cnn_for_trading.ipynb` contains the relevant
    code examples.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备按照前一节中概述的步骤设计、训练和评估CNN。笔记本`cnn_for_trading.ipynb`包含相关的代码示例。
- en: 'We again closely follow the authors in creating a CNN with 2 convolutional
    layers with kernel size 3 and 16 and 32 filters, respectively, followed by a max
    pooling layer of size 2\. We flatten the output of the last stack of filters and
    connect the resulting 1,568 outputs to a dense layer of size 32, applying 25 and
    50 percent dropout probability to the incoming and outcoming connections to mitigate
    overfitting. The following table summarizes the CNN structure that contains 55,041
    trainable parameters:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次紧密地按照作者的方法创建一个CNN，其中包括2个卷积层，卷积核大小为3，分别为16和32个滤波器，然后是一个大小为2的最大池化层。我们展平最后一组滤波器的输出，并将结果的1568个输出连接到大小为32的密集层，对传入和传出连接应用25％和50％的丢失概率，以减少过拟合。以下表总结了包含55,041个可训练参数的CNN结构：
- en: '[PRE36]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We cross-validate the model with the `MutipleTimeSeriesCV` train and validation
    set index generator introduced in *Chapter 7*, *Linear Models – From Risk Factors
    to Return Forecasts*. We provide 5 years of trading days during the training period
    in batches of 64 random samples and validate using the subsequent 3 months, covering
    the years 2014-2017\.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用在*第7章*中介绍的`MutipleTimeSeriesCV`训练和验证集索引生成器对模型进行交叉验证，*线性模型-从风险因素到回报预测*。我们在训练期间提供了5年的交易日，每批64个随机样本，并使用接下来的3个月进行验证，涵盖2014-2017年。
- en: 'We scale the features to the range [-1, 1] and again use NumPy''s `.reshape()`
    method to create the requisite ![](img/Image78145.png) format:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将特征缩放到范围[-1,1]，并再次使用NumPy的`.reshape()`方法创建所需的![](img/Image78145.png)格式：
- en: '[PRE37]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Training and validation follow the process laid out in *Chapter 17*, *Deep Learning
    for Trading*, relying on checkpointing to store weights after each epoch and generate
    predictions for the best-performing iterations without the need for costly retraining.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证遵循*第17章*中概述的过程，*交易的深度学习*，依靠检查点在每个epoch后存储权重，并在无需昂贵的重新训练的情况下生成最佳表现迭代的预测。
- en: 'To evaluate the model''s predictive accuracy, we compute the daily **information
    coefficient** (**IC**) for the validation set like so:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的预测准确性，我们计算验证集的每日**信息系数**（**IC**）：
- en: '[PRE38]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We train the model for up to 10 epochs using **stochastic gradient descent**
    with **Nesterov** momentum (see *Chapter 17*, *Deep Learning for Trading*) and
    find that the best performing epochs, 8 and 9, achieve a (low) daily average IC
    of around 0.009.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**随机梯度下降**和**Nesterov**动量（参见*第17章*，*交易的深度学习*）对模型进行最多10个epochs的训练，并发现表现最佳的第8和第9个epochs的每日平均IC约为0.009（低）。
- en: Assembling the best models to generate tradeable signals
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组装最佳模型以生成可交易的信号
- en: To reduce the variance of the test-period forecasts, we generate and average
    the predictions for the 3 models that perform best during cross-validation, which
    here correspond to training for 4, 8, and 9 epochs. As in the previous time-series
    example, the relatively short training period underscores that the amount of signals
    in financial time series is low compared to the systematic information contained
    in, for example, image data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少测试期预测的方差，我们生成并平均了在交叉验证中表现最佳的3个模型的预测结果，这里对应于训练4、8和9个epochs。与之前的时间序列示例一样，相对较短的训练期强调了金融时间序列中信号的数量较少，与例如图像数据中包含的系统信息相比。
- en: 'The `generate_predictions()` function reloads the model weights and returns
    the forecasts for the target period:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_predictions()`函数重新加载模型权重并返回目标期间的预测：'
- en: '[PRE39]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We store the predictions and proceed to backtest a trading strategy based on
    these daily return forecasts.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们存储预测结果，并继续进行基于这些每日回报预测的交易策略的回测。
- en: Backtesting a long-short trading strategy
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进行多空交易策略的回测
- en: To get a sense of the signal quality, we compute the spread between equally
    weighted portfolios invested in stocks selected according to the signal quintiles
    using Alphalens (see *Chapter 4*, *Financial Feature Engineering – How to Research
    Alpha Factors*).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解信号质量，我们使用Alphalens计算在信号分位数下选定的股票的等权组合之间的差距（参见*第4章*，*金融特征工程-如何研究Alpha因子*）。
- en: '*Figure 18.19* shows that for a one-day investment horizon, this naive strategy
    would have earned a bit over four basis points per day during the 2013-2017 period:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.19*显示，在一天的投资周期内，这种简单的策略在2013-2017年期间每天可以赚取略高于四个基点：'
- en: '![](img/B15439_18_19.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_19.png)'
- en: 'Figure 18.19: Alphalens signal quality evaluation'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.19：Alphalens信号质量评估
- en: We translate this slightly encouraging result into a simple strategy that enters
    long (short) positions for the 25 stocks with the highest (lowest) return forecasts,
    trading on a daily basis. *Figure 18.20* shows that this strategy is competitive
    with the S&P 500 benchmark over much of the backtesting period (left panel), resulting
    in a 35.6 percent cumulative return and a Sharpe ratio of 0.53 (before transaction
    costs; right panel)
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个略微令人鼓舞的结果转化为一个简单的策略，该策略每天进行交易，对预测回报最高（最低）的25只股票进行多头（空头）操作。*图18.20*显示，这种策略在大部分回测期内与标准普尔500指数基准竞争（左侧面板），在没有交易成本的情况下，累计回报率为35.6％，夏普比率为0.53（右侧面板）
- en: '![](img/B15439_18_20.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_18_20.png)'
- en: 'Figure 18.20: Backtest performance in- and out-of-sample'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.20：样本内外回测表现
- en: Summary and lessons learned
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结和经验教训
- en: It appears that the CNN is able to extract meaningful information from the time
    series of alpha factors converted into a two-dimensional grid. Experimentation
    with different architectures and training parameters shows that the result is
    not very robust and slight modifications can yield significantly worse performance.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来CNN能够从转换为二维网格的alpha因子的时间序列中提取有意义的信息。通过尝试不同的架构和训练参数，我们发现结果并不是非常稳健，轻微的修改可能会导致显著更差的表现。
- en: 'Tuning attempts also surface the notorious difficulties in successfully training
    a deep NN, especially when the signal-to-noise ratio is low: too complex a network
    or the wrong optimizer can lead the CNN to a local optimum where it always predicts
    a constant value.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 调整尝试也凸显了成功训练深度NN的困难，特别是当信噪比较低时：过于复杂的网络或错误的优化器可能导致CNN陷入始终预测恒定值的局部最优解。
- en: The most important step to improve the results and obtain a performance closer
    to that achieved by the authors (using different outcomes) would be to revisit
    the features. There are many alternatives to different intervals of a limited
    set of technical indicators. Any appropriate number of time-series features could
    be arranged in a rectangular *n*×*m* format and benefit from the CNN's ability
    to learn local patterns. The choice of *n* indicators and *m* intervals just makes
    it easier to organize the rows and the columns of the two-dimensional grid. Give
    it a shot!
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 改善结果并获得接近作者所取得的性能的最重要步骤（使用不同的结果）将是重新审视特征。有许多替代方案可用于有限一组技术指标的不同间隔。任何适当数量的时间序列特征都可以以*
    n *×* m *格式排列，并受益于CNN学习局部模式的能力。选择* n *指标和* m *间隔只是为了更容易地组织二维网格的行和列。试一试吧！
- en: Furthermore, the authors take a classification approach to the algorithmically
    labeled buy, hold, and sell outcomes (see the paper for an outline of the computation),
    whereas our experiment applied regression to the daily returns. The Alphalens
    chart in *Figure 18.18* suggests that longer holding periods (especially 10 days)
    might work better, so there is also scope for adjusting the strategy accordingly
    or switching to a classification approach.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者们对算法标记的买入、持有和卖出结果采取了分类方法（有关计算概述，请参阅论文），而我们的实验则将回归应用于每日收益。*图18.18*中的Alphalens图表表明，较长的持有期（尤其是10天）可能效果更好，因此也可以调整策略或切换到分类方法。
- en: Summary
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced CNNs, a specialized NN architecture that has
    taken cues from our (limited) understanding of human vision and performs particularly
    well on grid-like data. We covered the central operation of convolution or cross-correlation
    that drives the discovery of filters that in turn detect features useful to solve
    the task at hand.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了CNN，这是一种专门针对网格数据表现特别出色的NN架构，它从我们（有限的）对人类视觉的理解中汲取了灵感。我们介绍了卷积或交叉相关的中心操作，它推动了发现滤波器，进而检测出有助于解决手头任务的特征。
- en: We reviewed several state-of-the-art architectures that are good starting points,
    especially because transfer learning enables us to reuse pretrained weights and
    reduce the otherwise rather computationally and data-intensive training effort.
    We also saw that Keras makes it relatively straightforward to implement and train
    a diverse set of deep CNN architectures.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了几种最先进的架构，它们是良好的起点，特别是因为迁移学习使我们能够重用预训练的权重，并减少原本相当计算和数据密集型的训练工作。我们还看到，Keras使实现和训练多样化的深度CNN架构相对简单。
- en: In the next chapter, we turn our attention to recurrent neural networks that
    are designed specifically for sequential data, such as time-series data, which
    is central to investment and trading.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把注意力转向专门设计用于顺序数据（如时间序列数据）的递归神经网络，这对于投资和交易至关重要。
