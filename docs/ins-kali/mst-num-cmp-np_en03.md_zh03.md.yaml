- en: Chapter 3. Exploratory Data Analysis of Boston Housing Data with NumPy Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章：使用 NumPy 统计函数对波士顿住房数据进行探索性数据分析
- en: '**Exploratory data analysis** (**EDA**) is a crucial component of a data science
    project (as shown in Figure *Data Science Process*). Even though it is a very
    important step before applying any statistical model or machine learning algorithm
    to your data, it is often skipped or underestimated by many practitioners:'
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析**（**EDA**）是数据科学项目的重要组成部分（如图*数据科学过程*所示）。 尽管在将任何统计模型或机器学习算法应用于数据之前这是非常重要的一步，但许多从业人员经常忽略或低估了它：'
- en: '![](Images/2af48e08-afe9-4f52-8278-5b73b62ee3f8.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2af48e08-afe9-4f52-8278-5b73b62ee3f8.png)'
- en: Data Science Process (https://en.wikipedia.org/wiki/Data_analysis)
  prefs: []
  type: TYPE_NORMAL
  zh: '数据科学流程（[`en.wikipedia.org/wiki/Data_analysis`](https://en.wikipedia.org/wiki/Data_analysis)）'
- en: 'John Wilder Tukey promoted exploratory data analysis in 1977 with his book *Exploratory
    Data Analysis*. In his book, he guides statisticians to analyze their datasets
    statistically by using several different visuals, which will help them to formulate
    their hypotheses. In addition, EDA is also used to prepare your analysis for advance
    modeling after you identify the key data characteristics and learn which questions
    you should ask about your data. At a high level, EDA is an assumption-free exploration
    of your data that takes advantage of quantitative methods, which allows you to
    visualize your results so that you can identify patterns, anomalies, and data
    characteristics. In this chapter, you will be using NumPy''s built-in statistics
    methods to perform exploratory data analysis.'
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰·怀尔德·图基（John Wilder Tukey）于 1977 年通过他的著作《探索性数据分析》促进了探索性数据分析。 在他的书中，他指导统计学家通过使用几种不同的视觉手段对统计数据集进行统计分析，这将帮助他们制定假设。
    此外，在确定关键数据特征并了解有关数据的哪些问题后，EDA 还可以用于为高级建模准备分析。 在较高的层次上，EDA 是对您的数据的无假设探索，它利用了定量方法，使您可以可视化结果，从而可以识别模式，异常和数据特征。
    在本章中，您将使用 NumPy 的内置统计方法执行探索性数据分析。
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Loading and saving files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和保存文件
- en: Exploring the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据集
- en: Looking at the basic statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看基本统计量
- en: Computing averages and variances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算平均值和方差
- en: Computing correlations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算相关性
- en: Computing histograms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算直方图
- en: Loading and saving files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和保存文件
- en: In this section, you will learn how to load/import your data and save it. There
    are many different ways of loading data, and the right way depends on your file
    type. You can load/import text files, SAS/Stata files, HDF5 files, and many others.
    **HDF** (**Hierarchical Data Format**) is one of the popular data formats which
    is used to store and organize large amounts of data and it is very useful while
    working with a multidimensional homogeneous arrays. For example, Pandas library
    has a very handy class named as `HDFStore` where you can easily work with HDF5
    files. While working on data science projects, you will most likely see many of
    these types of files, but in this book, we will cover the most popular ones, such
    as **NumPy binary files,** **text files** (`.txt`), and **comma-separated values** (`.csv`)
    files.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何加载/导入数据并保存。 加载数据的方式有很多，正确的方式取决于您的文件类型。 您可以加载/导入文本文件，SAS/Stata 文件，HDF5
    文件以及许多其他文件。 **HDF**（**分层数据格式**）是一种流行的数据格式，用于存储和组织大量数据，并且在处理多维同构数组时非常有用。 例如，Pandas
    库有一个非常方便的类，名为`HDFStore`，您可以在其中轻松使用 HDF5 文件。 在从事数据科学项目时，您很可能会看到许多这类文件，但是在本书中，我们将介绍最受欢迎的文件，例如
    **NumPy 二进制文件**，**文本文件**（`.txt`）和**逗号分隔值**（`.csv`）文件。
- en: If you have a large dataset in memory and on disk to manage, you can use a library
    called `bcolz`. It provides columnar and compressed data containers. bcolz objects
    are called `chunks` where you compressed whole data as bits then partially decompressed
    when you query. As data is compressed, it uses the storage very efficiently. `bcolz` objects
    improve data fetching performance as well. If you are interested more about the
    performance of this library. You can check query and speed comparison on their
    official GitHub repository [https://github.com/Blosc/bcolz/wiki/Query-Speed-and-Compression](https://github.com/Blosc/bcolz/wiki/Query-Speed-and-Compression).
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内存和磁盘上有大量数据要管理，则可以使用`bcolz`库。 它提供了列式和压缩数据容器。 `bcolz`对象称为`chunks`，您在其中将整个数据压缩为位，然后在查询时部分解压缩。
    压缩数据后，它会非常高效地使用存储。`bcolz`对象也提高了数据获取性能。 如果您对该库的性能感兴趣。 您可以在其官方 GitHub 仓库上检查查询和速度比较：[`github.com/Blosc/bcolz/wiki/Query-Speed-and-Compression`](https://github.com/Blosc/bcolz/wiki/Query-Speed-and-Compression)。
- en: 'When you are working with arrays, you will usually save them as NumPy binary
    files after you have finished working with them. The reason for this is that you
    need to store your array shape and data type as well. When you reload your array,
    you expect NumPy to remember it, and you can continue working from where you left
    off. Moreover, NumPy binary files can store information about an array, even when
    you open the file on another machine with a different architecture. In NumPy,
    the `load()`, `save()`, `savez()`, and `savez_compressed()` methods help you to
    load and save NumPy binary files as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数组时，通常在完成使用后将它们另存为 NumPy 二进制文件。 原因是您还需要存储数组形状和数据类型。 重新加载数组时，您希望 NumPy 记住它，并且您可以从上次中断的地方继续工作。
    此外，即使您在具有不同架构的另一台计算机上打开文件， NumPy 二进制文件也可以存储有关数组的信息。 在 NumPy 中，`load()`，`save()`，`savez()`和`savez_compressed()`方法可帮助您加载和保存
    NumPy 二进制文件，如下所示：
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding code, we execute the following steps to a practice saving
    array as a binary file and how to load it back without affecting its shape:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们执行以下步骤来练习将数组保存为二进制文件，以及如何在不影响其形状的情况下将其加载回：
- en: Create an array with a shape *(3,4)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建形状为`(3, 4)`的数组
- en: Save the array as a binary file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数组另存为二进制文件
- en: Load back the array
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载回数组
- en: Check whether the shape is still the same
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查形状是否仍然相同
- en: 'Similarly, you can use the `savez()` function to save several arrays into a
    single file. If you want to save your files as compressed NumPy binary files,
    you can use `savez_compressed()` as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以使用`savez()`函数将多个数组保存到单个文件中。 如果要将文件另存为压缩的 NumPy 二进制文件，则可以按以下方式使用`savez_compressed()`：
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you save several arrays in a single file, if you give a keyword argument
    such as `first_array=x`, your array will be saved with this name. Otherwise, by
    default, your first array will be given a variable name, such as `arr_0`. NumPy
    has a built-in function called `loadtxt()` for loading data from a text file.
    Let''s load a `.txt` file that consists of some integers and a header, which is
    a string at the top of the file:'
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将多个数组保存在单个文件中时，如果您提供关键字参数如`first_array=x`，则将使用该名称保存数组。 否则，默认情况下，您的第一个数组将被赋予一个变量名，例如`arr_0`。
    NumPy 具有称为`loadtxt()`的内置函数，用于从文本文件加载数据。 让我们加载一个`.txt`文件，该文件由一些整数和一个标头组成，该标头是文件顶部的字符串：
- en: '![](Images/a3c1ba5d-a61a-48e6-8f96-417910d8473c.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3c1ba5d-a61a-48e6-8f96-417910d8473c.png)'
- en: 'In the preceding code, you will get an error as you cannot convert a string
    to a float, which actually means that you are reading non-numeric values. The
    reason for this is that the file contains the string on top as a header as well
    as the numerical values. If you know how many lines your header has, you can skip
    these rows by using the `skiprows` parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，由于无法将字符串转换为浮点数，因此会出现误差，这实际上意味着您正在读取非数字值。 这样做的原因是该文件在顶部包含作为标题的字符串以及数值。
    如果知道标题有多少行，则可以使用`skiprows`参数跳过这些行，如下所示：
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Alternatively, you can use `genfromtxt()` and set the conversion of your header,
    by default, to `nan`. Then, you can detect how many lines are occupied by your
    header and use the `skip_header` parameter to skip them, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，您可以使用`genfromtxt()`并将标头的转换默认设置为`nan`。 然后，您可以检测到标题占用了多少行，并使用`skip_header`参数跳过它们，如下所示：
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, you can use the `loadtxt()`, `genfromtxt()`, and `savetxt()` functions
    to load and save `.csv` files. The only thing you have to remember is to use a
    comma as a delimiter, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以使用`loadtxt()`，`genfromtxt()`和`savetxt()`函数加载和保存`.csv`文件。 您唯一需要记住的是使用逗号作为定界符，如下所示：
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When you load the `.txt` files, they return a `numpy` array with the values
    by default, as you can see in the following code:'
  prefs: []
  type: TYPE_NORMAL
  zh: 加载`.txt`文件时，它们会默认返回带有值的`numpy`数组，如后面的代码中所示：
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can change your data from an array to a list with the `tolist()` method
    and save it as a new file with `savetxt()` by using different delimiters, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`tolist()`方法将数据从数组更改为列表，然后使用`savetxt()`使用不同的分隔符将其保存为新文件，如下所示：
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After you save the list in `My_List.txt`, you can load this file with `loadtxt()`
    and it will return as a `numpy` array once more. If you want to return a string
    representation of your array, you can use `array_str()`, `array_repr()`, or `array2string()`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表保存到`My_List.txt`后，可以使用`loadtxt()`加载此文件，它将再次作为`numpy`数组返回。 如果要返回数组的字符串表示形式，可以使用`array_str()`，`array_repr()`或`array2string()`方法，如下所示：
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Even though `array_str()` and `array_repr()` look the same, `array_str()` returns
    a string representation of the data inside the array, while `array_repr()` actually
    returns the string representation of an array. Therefore, `array_repr()` returns
    information about the type of array and its data type. Both of these functions
    use `array2string()` internally, which is actually the most flexible version of
    the string formatting functions as it has more optional parameters. The following
    code block loads the Boston housing data set with `load_boston()` method:'
  prefs: []
  type: TYPE_NORMAL
  zh: 即使`array_str()`和`array_repr()`看起来一样，`array_str()`返回数组内部数据的字符串表示形式，而`array_repr()`实际上返回数组的字符串表示形式。
    因此，`array_repr()`返回有关数组类型及其数据类型的信息。 这两个函数在内部都使用`array2string()`，这实际上是字符串格式化函数最灵活的版本，因为它具有更多可选参数。
    以下代码块使用`load_boston()`方法加载波士顿房屋数据集：
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this chapter, you will practice exploratory data analysis on a sample dataset
    from the `sklearn.datasets` package. The dataset is about Boston house prices.
    In the preceding code, the `load_boston()` function is imported from the `sklearn.datasets`
    package, and as you can see, it returns a dictionary-like object with the attributes `DESCR`,
    `data`, `feature_names`, and `target`. The details of these attributes are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将对`sklearn.datasets`包中的示例数据集进行探索性数据分析。 该数据集是关于波士顿房价的。 在前面的代码中，`load_boston()`函数是从`sklearn.datasets`包中导入的，如您所见，它返回具有属性`DESCR`，`data`，`feature_names`和`target`的类似字典的对象
    ]。 这些属性的详细信息如下：
- en: '| **Attribute ** | **Explanation** |'
  prefs: []
  type: TYPE_TB
  zh: '| **属性** | **解释** |'
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `DESCR` | Full description of the dataset |'
  prefs: []
  type: TYPE_TB
  zh: '| `DESCR` | 数据集的完整描述 |'
- en: '| `data` | Feature columns |'
  prefs: []
  type: TYPE_TB
  zh: '| `data` | 特征列 |'
- en: '| `feature_names` | Feature names |'
  prefs: []
  type: TYPE_TB
  zh: '| `feature_names` | 特征名称 |'
- en: '| `target` | Label data |'
  prefs: []
  type: TYPE_TB
  zh: '| `target` | 标签数据 |'
- en: '![](Images/8f460997-dad6-4c49-8675-c1d3ef24c21a.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f460997-dad6-4c49-8675-c1d3ef24c21a.png)'
- en: In this section, you learned how to load and save files with `numpy` functions.
    In the next section, you will explore the Boston housing dataset.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何使用`numpy`函数加载和保存文件。 在下一部分中，您将探索波士顿住房数据集。
- en: Exploring our dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索我们的数据集
- en: 'In this section, you will explore and perform quality checks on the dataset.
    You will check what your data shape is, as well as its data types, any missing/NaN
    values, how many feature columns you have, and what each column represents. Let''s
    start by loading the data and exploring it:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将探索数据集并对其进行质量检查。 您将检查数据形状是什么，以及它的数据类型，所有缺失值或 NaN，拥有多少个特征列以及每个列代表什么。 让我们开始加载数据并进行探索：
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding code, you load the dataset and parse the attributes of your
    dataset. This shows us that we have `506` samples with `13` features and that
    we have `506` labels (regression targets). If you want to read the dataset''s
    description, you can use `print(dataset.DESCR)`. As the output of this code is
    too long to put here, you can check out the various features and their descriptions
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，您加载了数据集并解析了数据集的属性。 这表明我们拥有具有`13`函数的`506`样本，并且具有`506`标签（回归目标）。 如果要阅读数据集的描述，可以使用`print(dataset.DESCR)`。
    由于此代码的输出太长，无法放置在此处，因此您可以在以下屏幕截图中查看各种特征及其描述：
- en: '![](Images/15b8e96b-8011-429c-8f01-1ce51f2cc647.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15b8e96b-8011-429c-8f01-1ce51f2cc647.png)'
- en: 'As shown in the first chapter, you can use `dtype` to check the data type of
    the array. As seen in the following code, we have a numeric `float64` data type
    in each column of the sample and a label. Checking the data type is a very important
    step—you may realize that there''s some inconsistency between the types and the
    column description, or you may want to decrease the memory size of your array
    by changing its data type if you think that you can still achieve your goal with
    less precise values:'
  prefs: []
  type: TYPE_NORMAL
  zh: 如第一章所示，您可以使用`dtype`检查数组的数据类型。 如下面的代码所示，示例的每一列中都有一个数字`float64`数据类型和一个标签。 检查数据类型是非常重要的一步，您可能会意识到类型和列描述之间存在一些不一致，或者，如果您认为使用不太精确的目标值仍然可以实现，则可以通过更改其数据类型来减少数组的内存大小：
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Missing value handling is slightly different in Python packages. The `numpy` library
    doesn''t have a missing value. If you have missing values in your dataset, they
    will convert to `NaN` after you import them. A very common method in NumPy is
    to use masked arrays in order to disregard `NaN` values, which was shown to you
    in the first chapter:'
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值处理在 Python 软件包中略有不同。`numpy`库没有缺失值。 如果您的数据集中缺少值，则导入后它们将转换为`NaN`。 NumPy 中非常常见的方法是使用掩码数组以忽略`NaN`值，这在第一章中已向您展示：
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To test element-wise for `NaN` values in your data, you can use the `isnan()`
    method. This method will return a Boolean array. It can be cumbersome for large
    arrays to detect whether it will return true or not. In such cases, you can use
    the `np.sum()` of your array as a parameter for `isnan()` so that it will return
    a single Boolean value for the result.
  prefs: []
  type: TYPE_NORMAL
  zh: 要针对数据中的`NaN`值逐元素进行测试，可以使用`isnan()`方法。 此方法将返回一个布尔数组。 对于大型数组而言，检测它是否返回`true`可能很麻烦。
    在这种情况下，您可以将数组的`np.sum()`用作`isnan()`的参数，这样它将为结果返回单个布尔值。
- en: In this section, you explored data at a high level and performed a generic quality
    check. In the next section, we will continue with basic statistics.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将深入研究数据并进行常规质量检查。 在下一节中，我们将继续进行基本统计。
- en: Looking at basic statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看基本统计量
- en: In this section, you will start with the first step in statistical analysis
    by calculating the basic statistics of your dataset. Even though NumPy has limited
    built-in statistical functions, we can leverage its usage with SciPy. Before we
    start, let's describe how our analysis will flow. All of the feature columns and
    label columns are numerical, but you may have noticed that the **Charles River
    dummy variable** (**CHAS**) column has binary values *(0,1)*, which means that
    it's actually encoded from categorical data. When you analyze your dataset, you
    can separate your columns into `Categorical` and `Numerical`. In order to analyze
    them all together, one type should be converted to another. If you have a categorical
    value and you want to convert it into a numeric value, you can do so by converting
    each category to a numerical value. This process is called **encoding**. On the
    other hand, you can perform binning by transforming your numerical values into
    category counterparts, which you create by splitting your data into intervals.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将从统计分析的第一步开始，计算数据集的基本统计信息。 即使 NumPy 的内置统计函数有限，我们也可以将其与 SciPy 结合使用。 在开始之前，让我们描述一下我们的分析将如何进行。
    所有特征列和标签列都是数字，但您可能已经注意到 **Charles River 虚拟变量**（**CHAS**）列具有二进制值`(0, 1)`，这意味着它实际上是根据分类数据编码的。
    分析数据集时，可以将列分为`Categorical`和`Numerical`。 为了一起分析它们，应将一种类型转换为另一种类型。 如果您具有分类值，并且想要将其转换为数字值，则可以通过将每个类别转换为数字值来实现。
    该过程称为**编码**。 另一方面，可以通过将数值转换为类别对应物来执行装箱，类别对应物是通过将数据分成间隔来创建的。
- en: 'We will start our analysis by exploring its features one by one. In statistics,
    this method is known as univariate analysis. The purpose of univariate analysis
    mainly centered around description. We will calculate the minimum, maximum, range,
    percentiles, mean, and variance, and then we will plot some histograms and analyze
    the distribution of each feature. We will touch upon the concept of skewness and
    kurtosis and then look at the importance of trimming. After finishing our univariate
    analysis, we will continue with bivariate analysis, which means simultaneously
    analyzing two features. To do this, we will explore the relationship between two
    sets of features:'
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过逐一探讨其特征来开始分析。 在统计中，此方法称为单变量分析。 单变量分析的目的主要围绕描述。 我们将计算最小值，最大值，范围，百分位数，均值和方差，然后绘制一些直方图并分析每个特征的分布。
    我们将介绍偏度和峰度的概念，然后看一下修剪的重要性。 完成单变量分析后，我们将继续进行双变量分析，这意味着同时分析两个特征。 为此，我们将探索两组特征之间的关系：
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](Images/680e068b-e478-4396-8bdd-89a6f7eca960.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/680e068b-e478-4396-8bdd-89a6f7eca960.png)'
- en: 'In the preceding code, we begin by setting our print options with the `set_printoptions()`
    method in order to see rounded decimals and have a line width big enough to fit
    all of the columns. To calculate basic statistics, we use `numpy` functions, such
    as `amin()`, `amax()` , `mean()`, `median()`, `var()` , `percentile()`, and `ptp()`.
    All of the calculations are done column-wise as each column represents a feature.
    The final array looks a bit sloppy and uninformative as you can''t see which row
    shows which statistics:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先使用`set_printoptions()`方法设置打印选项，以便查看四舍五入的小数位数，并且其线宽足以容纳所有列。 要计算基本统计信息，我们使用`numpy`函数，例如`amin()`，`amax()`，`mean()`，`median()`，`var()`，`percentile()`和`ptp()`。
    所有计算都是按列进行的，因为每一列都代表一个要素。 最终的数组看起来有点草率且无用，因为您看不到哪行显示了哪些统计信息：
- en: '![](Images/5ea3d6ea-1e66-4cb2-baf1-caaca38963d6.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ea3d6ea-1e66-4cb2-baf1-caaca38963d6.png)'
- en: 'In order to print an aligned numpy array, you can use the `zip()` function
    to add your row labels and print the column labels before your array. In SciPy,
    you can use many more statistical functions to calculate basic statistics. SciPy
    supplies the `describe()` function, which returns several descriptive statistics
    of the given array. You can calculate nobs, minimum, maximum, mean, variance,
    skewness, and kurtosis with a single function and parse them separately, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 为了打印对齐的 NumPy 数组，可以使用`zip()`函数添加行标签并在数组之前打印列标签。 在 SciPy 中，您可以使用更多统计函数来计算基本统计信息。
    SciPy 提供`describe()`函数，该函数返回给定数组的一些描述性统计信息。 您可以使用一个函数来计算点，最小值，最大值，均值，方差，偏度和峰度，然后分别解析它们，如下所示：
- en: '![](Images/fdb5e4a5-6f20-4232-95bb-91f566f6c65b.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdb5e4a5-6f20-4232-95bb-91f566f6c65b.png)'
- en: 'The following code block calculates the basic statistics separately and stacks
    them into a final array:'
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块分别计算基本统计信息并将其堆叠到最终数组中：
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](Images/d48d1fa4-bb7a-43f0-a188-07c0ce25eeeb.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d48d1fa4-bb7a-43f0-a188-07c0ce25eeeb.png)'
- en: In contrast to NumPy, we can use the `iqr()` function to calculate the range.
    This function calculates the interquartile range of the data along the specified
    axis and **range** (`rng` parameter). By default, *rng = (25, 75),* which means
    that the function will calculate the difference between the 75th and 25th percentile
    values of the data. In order to return the same output as the `numpy.ptp()` function,
    you can use rng =(0, 100)*,* as this will calculate the range of all given data.
    We used `stat.scoreatpercentile()` as an equivalent to the `numpy.percentile()`
    method in order to calculate the 10th and 90th percentile values of the features.
    If you take a glance at the results, you will notice that we have very high variance
    in almost every feature. You can see that the range values decreases considerably
    as we limit the range calculation by passing the parameters as `(20,80)`. This also shows
    you that we have many extreme values on both sides of the distributed features.
    We can conclude from our results that, for most of the features, the mean is higher
    than the median, which shows us that the distribution of these features is skewed
    to the right. In the next section, you will see this clearly when we plot the
    histograms and then deeply analyze the skewness and kurtosis of these features.
  prefs: []
  type: TYPE_NORMAL
  zh: 与 NumPy 相比，我们可以使用`iqr()`函数来计算范围。 此函数计算沿指定轴和范围（`rng`参数）的数据的四分位数**范围**。 默认情况下，`rng
    = (25, 75)`，这意味着该函数将计算数据的第 75 和第 25 个百分位数之间的差。 为了返回与`numpy.ptp()`函数相同的输出，可以使用`rng
    = (0, 100)`，因为这将计算所有给定数据的范围。 我们使用`stat.scoreatpercentile()`作为与`numpy.percentile()`方法的等效物，来计算特征的第
    10 个和第 90 个百分位值。 如果您看一眼结果，您会发现几乎每个特征的差异都很大。 您可以看到，由于我们通过将参数传递为`(20,80)`来限制范围计算，因此范围值显着减小。
    它也向您展示了我们在分布式要素的两侧都有许多极值。 从我们的结果可以得出结论，对于大多数特征，平均值均高于中位数，这表明我们这些特征的分布偏向右侧。 在下一节中，当我们绘制直方图然后深入分析这些特征的偏度和峰度时，您将清楚地看到这一点。
- en: Computing histograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算直方图
- en: 'A histogram is a visual representation of the distribution of numerical data.
    Karl Pearson first introduced this concept more than a century ago. A histogram
    is a kind of bar chart that is used for continuous data, while a bar chart visually
    represents categorical variables. As a first step, you need to divide your entire
    range of values into a series of intervals (bins). Each bin has to be adjacent
    and none of them can overlap. In general, bin sizes are equal, and the rule of
    thumb for the number of bins to include is 5–20\. This means that if you have
    more than 20 bins, your graph will be hard to read. On the contrary, if you have
    fewer than 5 bins, then your graph will give very little insight into the distribution
    of your data:'
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图是数字数据分布的直观表示。 一百多年前，卡尔·皮尔森（Karl Pearson）首次提出了这一概念。 直方图是一种用于连续数据的条形图，而条形图直观地表示类别变量。
    第一步，您需要将整个值范围划分为一系列间隔（仓位）。 每个桶必须相邻，并且它们不能重叠。 通常，桶大小相等，要包含的桶数量的经验法则是 5–20。 这意味着，如果您有
    20 个以上的容器，则图形将很难阅读。 相反，如果您的箱数少于 5 个，则您的图形将很少了解数据的分布：
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In preceding code, we plot the histogram for the feature `NOX`. Bins calculations
    done automatically as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们绘制了特征`NOX`的直方图。 箱柜计算自动完成，如下所示：
- en: '![](Images/fca9f227-5f9d-4e2b-b0af-6eef9a02da0a.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fca9f227-5f9d-4e2b-b0af-6eef9a02da0a.png)'
- en: 'You can plot your histogram with `pyplot.hist()` by giving it your sliced array
    as the first argument. The *y*-axis shows the count of how many values fall into
    each interval (bin) in your dataset and the *x*-axis represents their values.
    By setting `normed=True`, you can see the percentage of your bins as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将切片数组作为第一个参数，使用`pyplot.hist()`绘制直方图。 *y* 轴显示数据集中每个间隔（桶）中有多少值的计数， *x* 轴表示其值。
    通过设置`normed=True`，您可以看到箱柜的百分比，如下所示：
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](Images/00e82a04-57d0-46e8-a8f5-af8047bf104d.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00e82a04-57d0-46e8-a8f5-af8047bf104d.png)'
- en: 'When you look at the histogram, it can be hard to calculate the size of each
    bin and its edges. As `pyplot.hist()` returns a tuple that includes this information,
    you can easily get these values, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 当您查看直方图时，可能很难计算每个桶及其边界的大小。 当`pyplot.hist()`返回包含此信息的元组时，您可以轻松地获得这些值，如下所示：
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding code, we print how many numbers included in each bin and what
    are the edges of bins as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们打印出每个桶中包含多少个数字，以及桶的边界是什么，如下所示：
- en: '![](Images/a42d4fc0-11fd-4acf-9757-a8847db2eaf4.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a42d4fc0-11fd-4acf-9757-a8847db2eaf4.png)'
- en: 'Let''s interpret the preceding output. The first bin''s size is `1`, which
    means that there is only a single value drop in this bin. The interval of the
    first bin is between `3.561` and `3.74096552`. In order to make them tidier, you
    can use the following code block, which stacks these two arrays meaningfully (`bin_interval`,
    `bin_size`):'
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下前面的输出。 第一个桶的大小为`1`，这意味着此桶中仅存在一个值下降。 第一仓的间隔在`3.561`和`3.74096552`之间。 为了使它们更整洁，可以使用以下代码块，该代码块有意义地堆叠了这两个数组（`bin_interval`，`bin_size`）：
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](Images/8361235f-cc63-4982-bc61-6b6028f94a87.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8361235f-cc63-4982-bc61-6b6028f94a87.png)'
- en: 'Deciding on the number of bins and their width is very important. Some theoreticians
    experiment with this to determine the best fit. The following table shows you
    the most popular estimators. You can set the estimator to the `bins` parameter
    in `numpy.histogram()` to change the bin calculation accordingly. These methods
    are implicitly supported by the `pyplot.hist()` function, as its arguments are
    passed to `numpy.histogram()`:'
  prefs: []
  type: TYPE_NORMAL
  zh: 确定桶的数量及其宽度非常重要。 一些理论家对此进行试验以确定最佳拟合。 下表显示了最受欢迎的估计器。 您可以将估计器设置为`numpy.histogram()`中的`bins`参数，以相应地更改桶计算。
    这些方法由`pyplot.hist()`函数隐式支持，因为其参数传递给`numpy.histogram()`：
- en: '| **Estimator** | **Formula** |'
  prefs: []
  type: TYPE_TB
  zh: '| **估计器** | **公式** |'
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Freedman–Diaconis estimator | ![](Images/63323518-986a-4f81-88de-405fe865727b.png)
    |'
  prefs: []
  type: TYPE_TB
  zh: '| 弗里德曼-迪阿科尼斯（Freedman-Diaconis）估计 | ![](img/63323518-986a-4f81-88de-405fe865727b.png)
    |'
- en: '| Doane''s formula | ![](Images/053a3769-7ce2-4c5f-bbe9-99cb7803f84b.png) |'
  prefs: []
  type: TYPE_TB
  zh: '| 多恩（Doane）公式 | ![](img/053a3769-7ce2-4c5f-bbe9-99cb7803f84b.png) |'
- en: '| Rice rule | ![](Images/b3ba595c-b794-43d8-ba7f-9d085535135a.png) |'
  prefs: []
  type: TYPE_TB
  zh: '| 莱斯（Rice）法则 | ![](img/b3ba595c-b794-43d8-ba7f-9d085535135a.png) |'
- en: '| Scott''s normal reference rule | ![](Images/afb60450-fb70-464a-aa7b-187e16798810.png)
    |'
  prefs: []
  type: TYPE_TB
  zh: '| 斯科特（Scott）的常规参考法则 | ![](img/afb60450-fb70-464a-aa7b-187e16798810.png) |'
- en: '| Sturges'' formula | ![](Images/8c699cc7-abf4-4135-b160-8b075bc3d5b1.png)
    |'
  prefs: []
  type: TYPE_TB
  zh: '| 斯特格斯（Sturges）公式 | ![](img/8c699cc7-abf4-4135-b160-8b075bc3d5b1.png) |'
- en: '| Square-root choice | ![](Images/3ce08cdf-0f4b-4382-8b37-4e0a37817825.png)
    |'
  prefs: []
  type: TYPE_TB
  zh: '| 平方根选项 | ![](img/3ce08cdf-0f4b-4382-8b37-4e0a37817825.png) |'
- en: '*IQR = Interquartile range*'
  prefs: []
  type: TYPE_NORMAL
  zh: '`IQR = `四分位间距'
- en: '*g[1 ]= Estimated third-moment skewness of the distribution*'
  prefs: []
  type: TYPE_NORMAL
  zh: '`g[1] = `分布的估计三阶偏度'
- en: 'All of these methods have different strengths. For example, Sturges'' formula
    is optimal for Gaussian data. Rice''s rule is a simplified version of Sturges''s
    formula and assumes an approximately normal distribution, so it may not perform
    well if the data is not normally distributed. Doane''s formula is an improved
    version of Sturges''s formula, especially with nonnormal distributions. The Freedman–Diaconis
    Estimator is the modified version of Scott''s rule, where he replaced the 3.5
    standard deviations with 2 *IQR*, which makes the formula less sensitive to the
    outliers. The square-root choice is a very common method, which is used by many
    tools for its speed and simplicity. In `numpy.histogram()`, there is another option
    called `''auto''`, which gives us the maximum of the Sturges and Freedman–Diaconis
    estimator methods. Let''s see how these methods will affect our histogram:'
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都有不同的优势。 例如，斯特格斯公式对于高斯数据是最佳的。 莱斯规则是斯特格斯公式的简化版本，并假定近似正态分布，因此，如果数据不是正态分布，则可能效果不佳。
    多恩的公式是斯特格斯公式的改进版本，尤其是在具有非正态分布的情况下。 弗里德曼-迪亚科尼斯估计器是斯科特规则的修改版本，他用`IQR=2`代替了
    3.5 个标准差，这使公式对异常值的敏感性降低。 平方根选择是一种非常常见的方法，许多工具都以其快速简便的方式使用它。 在`numpy.histogram()`中，还有一个名为`'auto'`的选项，它为我们提供了最大的斯特格斯和弗里德曼-迪亚科尼斯估计器方法。 让我们看看这些方法将如何影响直方图：
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, we compile six histograms and all of them share the
    same *x* axis as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们编译了六个直方图，它们全部共享相同的 *x* 轴，如下所示：
- en: '![](Images/48e983f3-08ab-485b-beb1-f7679db4ac47.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48e983f3-08ab-485b-beb1-f7679db4ac47.png)'
- en: 'All of the histograms represent the same feature but with different binning
    methods. For example, the `fd` method shows that the data looks close to its normal
    distribution, while on the other hand, the `doane` method looks more like the
    student''s *t*-distribution. In addition, the `sturges` method creates two little
    bins, so it''s very hard to analyze the data. When we were looking at the basic
    statistics, we stated that most of the features had a mean that is higher than
    their median, so they skewed their data to the right. However, if you look at
    the `sturges`, `rice`, and `sqrt` methods, it''s very hard to say that. Nevertheless,
    this is obvious when we plot the histogram with auto bins:'
  prefs: []
  type: TYPE_NORMAL
  zh: 所有直方图都表示相同的特征，但具有不同的合并方法。 例如，`fd`方法显示数据看起来接近其正态分布，而另一方面，`doane`方法看起来更像学生的 *T*
    分布。 另外，`sturges`方法会创建两个小容器，因此很难分析数据。 当我们查看基本统计数据时，我们指出大多数特征的均值均高于其中位数，因此他们将数据向右倾斜。
    但是，如果您查看`sturges`，`rice`和`sqrt`方法，则很难这么说。 但是，当我们使用自动箱绘制直方图时，这是显而易见的：
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code, we compile all the feature histograms in a single layout.
    This will help us to compare them easily. The output of the preceding code is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们在单个布局中编译所有特征直方图。 这将帮助我们轻松比较它们。 前面代码的输出如下：
- en: '![](Images/e566d79c-dfe0-4ea5-b50b-8263abe30f57.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e566d79c-dfe0-4ea5-b50b-8263abe30f57.png)'
- en: In the preceding code, we delete **CHAS** as it contains a binary value, which
    means that the histogram will not help us gain more insight about this feature.
    We also took out the feature's name from the feature list in order to plot the
    rest of the features correctly.
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们删除了`CHAS`，因为它包含二进制值，这意味着直方图将无助于我们进一步了解此特征。 我们还从特征列表中取出了特征名称，以便正确绘制其余特征。
- en: We can conclude from these graphs that, per capita, crime is very low in most
    towns, but that there are towns where this ratio is extremely high. Generally,
    residential land zones are lower than 25,000 ft. In many cases, non-retail business
    acres cover less than 10% of the total town. On the other hand, we can see that
    many towns have around 20% non-retail business acres. Nitric oxide concentration
    is very right-skewed, although there are some outliers here, which are very far
    from the mean. The average number of rooms per dwelling is between 5–7 rooms.
    Of the buildings that were built before 1940, more than 50% of them were occupied
    by their owners. Most of them don't live very far away from Boston's employment
    centers. More than 10% don't have good accessibility to radial highways. Property
    taxes are very high for a considerable number of people. Generally, classroom
    sizes are between 15–20 children. Most of the towns have a very similar proportion
    of black people living in them. Most of the townspeople have a lower economic
    status. You can draw many other conclusions by looking at these histograms. As
    you can see, histograms are very useful when you are describing how your data
    is distributed, and you can see the average, variance, and outliers. In the next
    section, we will focus on skewness and kurtosis.
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些图表我们可以得出结论，在大多数城镇中，人均犯罪率非常低，但是在某些城镇中，这一比例非常高。 通常，住宅用地面积低于 25,000 英尺。在许多情况下，非零售业务用地所占土地不到整个城镇的
    10% 。 另一方面，我们可以看到许多城镇的非零售业务用地约为 20% 。 一氧化氮的浓度非常偏斜，尽管此处存在一些离平均值很远的异常值。 每个住宅的平均房间数在
    5 至 7 间之间。 在 1940 年之前建造的架构物中，超过 50% 的架构物被其所有者占用。 他们中的大多数人离波士顿的就业中心都不远。 超过 10%
    的人无法轻松到达径向公路。 对于相当多的人来说，财产税很高。 通常，教室规模为 15 至 20 个孩子。 大多数城镇中居住的黑人比例非常相似。 大多数城镇居民的经济地位较低。
    通过查看这些直方图，您可以得出许多其他结论。 如您所见，直方图在描述数据的分布方式时非常有用，并且可以看到平均值，方差和离群值。 在下一节中，我们将重点介绍偏度和峰度。
- en: Explaining skewness and kurtosis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释偏度和峰度
- en: 'In statistical analysis, a moment is a quantitative measure that describes
    the expected distance from a reference point. If the reference point is expected,
    then it''s called a central moment. In statistics, the central moments are the
    moments that are related with the mean. The first and second moments are the mean
    and the variance, respectively. The mean is the average of your data points. The
    variance is the total deviation of each data point from the mean. In other words,
    the variance shows how your data is dispersed from the mean. The third central
    moment is skewness, which measures the asymmetry of the distribution of the mean.
    In standard normal distribution, skewness equals zero as it''s symmetrical. On
    the other hand, if mean < median < mode, then there is negative skew, or left
    skew; likewise, if mode < median < mean, there is positive skew or right skew.
    You may be confused when you hear the terms right skewed or left skewed, and start
    imagining a distribution that leans to the right or left side. Technically, it
    is the other way around, so when you are talking about left skew, you should think
    about the tails (outliers) of the distribution on the left side, since your distribution
    actually looks like it''s leaning to the right:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计分析中，力矩是一种定量度量，用于描述距参考点的预期距离。 如果期望参考点，则将其称为中心矩。 在统计中，中心矩是与均值相关的矩。 第一个和第二个矩分别是平均值和方差。
    平均值是您的数据点的平均值。 方差是每个数据点与平均值的总偏差。 换句话说，方差表明您的数据如何与均值分散。 第三个中心矩是偏度，它测量均值分布的不对称性。
    在标准正态分布中，偏度是对称的，因此等于零。 另一方面，如果均值
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](Images/8f10c12d-1ebd-44f2-b319-5f7f71e0193f.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f10c12d-1ebd-44f2-b319-5f7f71e0193f.png)'
- en: You can easily create a normal distribution with skewness by using the `skewnorm()`
    function. As you can see in the preceding code, we generate a percent point function
    (an inverse of the cumulative distribution function percentiles) with 100 values
    and then create lines with different skewness levels. You cannot directly conclude
    that the left skew is better than right skew or vice versa. When you analyze skewness
    of your data, you need to consider what these tails can cause. As an example,
    if you are analyzing a time series of stock returns, and after plotting it you
    see a right skew, you don't need to be concerned about getting higher returns
    than you expect since these outliers will not cause any risk to your trading strategy.
    A similar example can be when you're analyzing your server's response time. When
    you plot the probability density function of your response time, you aren't really
    interested in left tails as they represent quick response times.
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`skewnorm()`函数轻松创建具有偏斜度的正态分布。 如您在前面的代码中看到的，我们生成一个具有 100 个值的百分点函数（与累积分布函数百分位数相反），然后创建具有不同偏斜度的线。
    您不能直接得出结论，左偏斜比右偏斜好，反之亦然。 当分析数据的偏斜度时，您需要考虑这些尾巴会导致什么。 例如，如果您正在分析股票收益的时间序列，并将其绘制后看到一个正确的偏斜，则不必担心获得比预期更高的收益，因为这些离群值不会给您的股票带来任何风险。
    交易策略。 当您分析服务器的响应时间时，可能会出现类似的示例。 当您绘制响应时间的概率密度函数时，您对左尾巴并不真正感兴趣，因为它们代表了快速的响应时间。
- en: 'The fourth central moment concerning the mean is kurtosis. Kurtosis describes
    the tails (outliers) in relation to how flat or thin the curve of the distribution
    is. In a normal distribution, kurtosis equals 3\. There are three main types of
    kurtosis: `Leptokurtic` (thin), `Mesokurtic`, and `Platykurtic` (flat):'
  prefs: []
  type: TYPE_NORMAL
  zh: 关于均值的第四个中心矩是峰度。 峰度描述了与分布曲线的平坦程度或变薄程度有关的尾部（离群值）。 在正态分布中，峰度等于 3。峰度主要有三种类型：`Leptokurtic`（薄），`Mesokurtic`和`Platykurtic`（平坦）：
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](Images/d1dc366d-f241-40b0-adad-eef119933a77.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1dc366d-f241-40b0-adad-eef119933a77.png)'
- en: 'In the preceding code, you can see the differences in the shape of the distribution
    more clearly. All of the kurtosis values are normalized with the `stats.describe()`
    method, so the actual kurtosis value of the normal distribution is around 3, whereas
    0.02 is the normalized version. Let''s quickly check what the skewness and kurtosis
    values for our features are:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，您可以更清楚地看到分布形状的差异。 所有峰度值均使用`stats.describe()`方法进行归一化，因此正态分布的实际峰度值约为 3，而归一化版本为
    0.02。 让我们快速检查一下我们特征的偏度和峰度值是什么：
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we can see from the results, all of the features have a positive skewness,
    which indicates that they are skewed to the right. In respect of kurtosis, they
    all have positive values, especially NOX and RM, which have very high kurtoses.
    We can conclude that they all have a leptokurtic shape type, which indicates that
    the data is more concentrated on the mean. In the next section, we will focus
    on data trimming and calculate statistics with trimmed data.
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中可以看出，所有特征都具有正偏度，这表明它们向右偏斜。 就峰度而言，它们都具有正值，尤其是 NOX 和 RM，它们具有非常高的色氨酸。 我们可以得出结论，它们全都是瘦腿型的，这表明数据更加集中在均值上。
    在下一节中，我们将着重于数据修整并使用修整后的数据计算统计数据。
- en: Trimmed statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整理统计量
- en: 'As you will have noticed in the previous section, the distributions of our
    features are very dispersed. Handling the outliers in your model is a very important
    part of your analysis. It is also very crucial when you look at descriptive statistics.
    You can be easily confused and misinterpret the distribution because of these
    extreme values. SciPy has very extensive statistical functions for calculating
    your descriptive statistics in regards to trimming your data. The main idea of
    using the trimmed statistics is to remove the outliers (tails) in order to reduce
    their effect on statistical calculations. Let''s see how we can use these functions
    and how they will affect our feature distribution:'
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在上一节中已经注意到的那样，我们特征的分布非常分散。 处理模型中的异常值是分析中非常重要的部分。 当您查看描述性统计数据时，它也非常重要。 由于这些极端值，您很容易混淆并曲解分布。
    SciPy 具有非常广泛的统计函数，可以计算有关修剪数据的描述性统计数据。 使用调整后的统计数据的主要思想是删除异常值（尾部），以减少其对统计计算的影响。
    让我们看看如何使用这些函数，以及它们如何影响我们的特征分布：
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To calculate the trimmed statistics, we used `tmin()`, `tmax()`, `tvar()`,
    and `tmean()`, as shown in the preceding code. All of these functions take limit
    values as a second parameter. In the `CRIM` feature, we can see many tails on
    the right side, so we limit the data to `(1, 40)` and then calculate the statistics.
    You can observe the difference by comparing the calculated statistics both before
    and after we have trimmed the values. As an alternative for `tmean()`, the `trim_mean()`
    function can be used if you want to slice your data proportionally from both sides.
    You can see how our mean and variance significantly changes after trimming the
    data. The variance is significantly decreased as we removed many extreme outliers
    between 40 and 89\. The same trimming has a different effect on the mean, where
    the mean afterwards is more than doubled. The reason for this is that there were
    many zeros in the previous distribution, and by limiting the calculation between
    the values of 1 and 40, we removed all of these zeros, which resulted in a higher
    mean. Be advised that all of the preceding functions just trim your data on the
    fly while calculating these values, so the `CRIM` array is not trimmed. If you
    want to trim your data from both sides, you can use `trimboth()` and `trim1()`
    for one side. In both methods, instead of using limit values, you should use proportions
    as parameters. As an example, if you pass `proportiontocut =0.2`, it will slice
    your leftmost and rightmost values by 20%:'
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算调整后的统计信息，我们使用了`tmin()`，`tmax()`，`tvar()`和`tmean()`，如前面的代码所示。 所有这些函数都将极限值作为第二个参数。
    在`CRIM`函数中，我们可以在右侧看到很多尾巴，因此我们将数据限制为`(1, 40)`，然后计算统计信息。 在修整值之前和之后，您可以通过比较计算的统计信息来观察差异。
    作为`tmean()`的替代方法，如果要从两侧按比例分割数据，可以使用`trim_mean()`函数。 您可以看到我们的均值和方差在修剪数据后如何显着变化。
    当我们删除了 40 至 89 之间的许多极端离群值时，方差显着减小。相同的修整对均值有不同的影响，此后均值增加了一倍以上。 原因是先前的分布中有很多零，并且通过将计算限制在
    1 和 40 的值之间，我们删除了所有这些零，这导致了更高的平均值。 请注意，上述所有函数只是在计算这些值时即时修剪数据，因此`CRIM`数组不会被修剪。
    如果要从两侧修剪数据，则可以在一侧使用`trimboth()`和`trim1()`。 在这两种方法中，应该使用比例作为参数，而不是使用极限值。 例如，如果您传递`proportiontocut
    =0.2`，它将把您最左边和最右边的值切成 20% ：
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](Images/3061ab59-53fa-4277-9bf8-577d661a1590.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3061ab59-53fa-4277-9bf8-577d661a1590.png)'
- en: After trimming 20% of the values from both sides, we can interpret the distribution
    better and actually see that most of the values are between 0 and 1.5\. It's hard
    to get this insight just by looking at the left histogram, as extreme values are
    dominating the histogram, and we can only see a single line around 0\. As a result,
    `trimmed` functions are very useful in exploratory data analysis. In the next
    section, we will focus on box plots, which are very useful and popular graphical
    visuals for the descriptive analysis of data and outlier detection.
  prefs: []
  type: TYPE_NORMAL
  zh: 从两侧修剪掉 20% 的值后，我们可以更好地解释分布，实际上可以看到大多数值在 0 到 1.5 之间。 仅查看左方的直方图就很难获得这种见解，因为极值主导了直方图，并且我们只能看到
    0 周围的一条线。因此，`trimmed`函数在探索性数据分析中非常有用。 在下一部分中，我们将重点讨论箱形图，它们对于数据的描述性分析和异常值检测非常有用且非常流行。
- en: Box plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 箱形图
- en: 'Another important visual in exploratory data analysis is the box plot, also
    known as the box-and-whisker plot. It''s built based on the five-number summary,
    which is the minimum, first quartile, median, third quartile, and maximum values.
    In a standard box plot, these values are represented as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析中的另一个重要视觉效果是箱形图，也称为箱须图。 它基于五位数摘要建立，该摘要是最小值，第一四分位数，中位数，第三四分位数和最大值。 在标准箱形图中，这些值表示如下：
- en: '![](Images/6ffe37a0-87de-4b43-817e-5232b4b7f4c0.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ffe37a0-87de-4b43-817e-5232b4b7f4c0.png)'
- en: 'It''s a very convenient way of comparing several distributions. In general,
    the whiskers of the plot generally extend to the extreme points. Alternatively,
    you can cut them with the 1.5 interquartile range. Let''s check our `CRIM` and
    `RM` features:'
  prefs: []
  type: TYPE_NORMAL
  zh: 这是比较几种分布的一种非常方便的方法。 通常，情节的胡须通常延伸到极限点。 或者，您可以将其切入 1.5 个四分位范围。 让我们检查一下`CRIM`和`RM`函数：
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](Images/8740fb8d-05c7-4f22-944c-82015cb1b9fd.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8740fb8d-05c7-4f22-944c-82015cb1b9fd.png)'
- en: As you can see in the preceding code, the RM values have been continuously dispersed
    along the minimum and maximum so that the whiskers look like a line. You can easily
    detect that the median is almost in the middle between the first and third quartile.
    In `CRIM`, the extreme values do not continue along the minimum; they are more
    like individual outlying data points, so the representation is like a circle.
    You can see that these outliers are mostly after the third quartile and that the
    median is pretty close to the first quartile. From this, you can also conclude
    that the distribution is right-skewed. As a result, the box plot is a very useful
    alternative to the histogram, as you can easily analyze your distributions and
    compare several at once. In the next section, we will continue with bivariate
    analysis, and we will look at the correlation of label data with features.
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的代码中看到的那样，RM 值一直沿最小值和最大值连续分散，以便胡须看起来像一条线。 您可以轻松地检测到中位数几乎位于第一和第三四分位数之间的中间位置。
    在`CRIM`中，极值不会沿着最小值继续； 它们更像是单个外围数据点，因此表示就像一个圆圈。 您可以看到这些离群值大多位于第三个四分位数之后，并且中位数非常接近第一个四分位数。
    由此，您还可以得出结论，分布是右偏的。 因此，箱形图是直方图的非常有用的替代方法，因为您可以轻松分析分布并一次比较多个分布。 在下一节中，我们将继续进行双变量分析，并研究标签数据与要素的相关性。
- en: Computing correlations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算相关性
- en: This section is dedicated to bivariate analysis, where you analyze two columns.
    In such cases, we generally investigate the association between these two variables,
    which is called **correlation**. Correlation shows the relationship between two
    variables and answers questions such as what will happen to variable A if variable
    B increases by 10%? In this section, we will explain how to calculate the correlation
    of our data and represent it in a two-dimensional scatter plot.
  prefs: []
  type: TYPE_NORMAL
  zh: 本节专门用于双变量分析，您可以在其中分析两列。 在这种情况下，我们通常研究这两个变量之间的关联，这称为**相关性**。 相关性显示两个变量之间的关系，并回答一些问题，例如，如果变量
    B 增加 10% ，变量 A 将会发生什么？ 在本节中，我们将说明如何计算数据的相关性并以二维散点图表示它。
- en: 'In general, correlation refers to any statistical dependency. A correlation
    coefficient is a quantitative value that calculates the measure of correlation.
    You can think of the relationship between correlation and a correlation coefficient
    as being of a similar relationship between a hygrometer and humidity. One of the
    most popular types of correlation coefficient is the Pearson product-moment correlation
    coefficient. The value of the correlation coefficient is between +1 and -1, where
    -1 indicates a strong negative linear relationship between two variables and +1
    indicates a strong positive linear relationship between two variables. In NumPy,
    we can calculate the coefficient correlation matrix by using the `corrcoef()`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，相关是指任何统计依赖性。 相关系数是计算相关度量的定量值。 您可以将相关性和相关系数之间的关系视为湿度计和湿度之间的相似关系。 相关系数最流行的类型之一是皮尔逊积矩相关系数。
    相关系数的值在 +1 和 -1 之间，其中 -1 表示两个变量之间的强负线性关系，而 +1 表示两个变量之间的强正线性关系。 在 NumPy 中，我们可以使用`corrcoef()`方法来计算系数相关矩阵，如下所示：
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](Images/2b192386-db02-47bf-a8c9-3d399218f917.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b192386-db02-47bf-a8c9-3d399218f917.png)'
- en: 'Seaborn is a statistical data visualization library based on matplotlib which
    you can use to create very attractive and aesthetic statistical graphics. It`s
    very popular library with beautiful visualization with a perfect compatibility
    with popular packages, especially with pandas. You can use the heat map from the `seaborn` package
    to visualize the correlation coefficient matrix. It''s very useful for detecting
    high-correlation coefficients when you have hundreds of features:'
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn 是一个基于 Matplotlib 的统计数据可视化库，您可以使用它创建非常吸引人的美观统计图形。 它是一个非常受欢迎的库，具有精美的可视化效果，并且与流行的软件包（尤其是
    Pandas）具有完美的兼容性。 您可以使用`seaborn`程序包中的热图来形象化相关系数矩阵。 当您具有数百种特征时，它对于检测高相关系数非常有用：
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](Images/bc05cabb-b50c-4b9b-869a-624d4737f662.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc05cabb-b50c-4b9b-869a-624d4737f662.png)'
- en: 'Here, we have the correlation coefficients of features of the label column.
    You can add an additional set of variables in the `corrcoef()` function as a second
    argument, as we did for the label column, which is shown in the following screenshot.
    As long as the shapes are the same, the function will stack this column at the
    end and calculate the correlation coefficient matrix:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们具有标签列特征的相关系数。 您可以在`corrcoef()`函数中添加其他变量集作为第二个参数，就像我们对`label`列所做的那样，如下面的屏幕快照所示。
    只要形状相同，该函数将在最后堆叠此列并计算相关系数矩阵：
- en: '![](Images/0498b2e5-0d37-4d6a-9581-b3367b644d18.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0498b2e5-0d37-4d6a-9581-b3367b644d18.png)'
- en: 'As you can see, most features have a weak or moderate negative linear relationship,
    apart from `F13`. On the other hand, `F6` has a strong positive linear relationship.
    Let''s plot this feature and look at this relationship with a scatter plot. The
    following code block shows three different scatter plots of the features (''RM'',
    ''DIS'' and ''LSTAT'') and a label column with the help of `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，除了`F13`之外，大多数特征都具有弱或中等的负线性关系。 另一方面，`F6`具有很强的正线性关系。 让我们绘制此特征并用散点图查看这种关系。
    以下代码块借助`matplotlib`显示了三个不同的特征散点图（`RM`，`DIS`和`LSTAT`）和一个标签列：
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](Images/b261aaa8-4f9b-4392-93dd-4ba18fd814d6.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b261aaa8-4f9b-4392-93dd-4ba18fd814d6.png)'
- en: 'In the preceding code, the RM and label values are dispersed as a positive
    linear line, which is consistent with the correlation coefficient value of 0.7,
    as shown in the preceding screenshot. The scatter plot indicates that the higher
    the RM values of a sample, the higher the label''s value. In the middle scatter
    plot, you can see where the coefficient correlation equals 0.25\. This shows a
    very weak positive linear correlation, which is also visible from the scatter
    plot. We can conclude that there is no clear relation as the values are dispersed
    everywhere. The third scatter plot shows a strong linear relation with a correlation
    coefficient of -0.7\. As the `LSTAT` goes down, the label values start increasing.
    All of these correlation matrices and scatter plots calculated this with untrimmed
    data. Let''s see how trimming the data by 10% from both sides for each feature
    and label will change the linear relation results of our dataset:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，RM 和标签值分散为一条正线性线，这与相关系数值为 0.7 一致，如前面的屏幕快照所示。 散点图表明样品的 RM 值越高，标签的值越高。
    在中间的散点图中，您可以看到系数相关性等于 0.25 的位置。 这显示出非常弱的正线性相关性，这在散点图中也可以看到。 我们可以得出结论，由于值分散在各处，因此没有明确的关系。
    第三个散点图显示了强大的线性关系，相关系数为 -0.7。 随着`LSTAT`下降，标签值开始增加。 所有这些相关矩阵和散点图均使用未修剪的数据对此进行了计算。
    让我们看看如何针对每个要素和标签从两侧修剪数据 10% 来改变数据集的线性关系结果：
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](Images/ab8f7318-a30a-4356-ae3c-cec809e7c41a.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab8f7318-a30a-4356-ae3c-cec809e7c41a.png)'
- en: After trimming the data, you can see that all of the features have a strong
    positive linear correlation with the label, especially the correlation of `DIS` and
    `LSTAT`, where the label has changed tremendously. This shows the power of trimming.
    You can easily misinterpret your data if you don't know how to deal with outliers.
    Outliers can change the shape of the distribution and the correlation between
    other variables, and in the end they can affect your model's performance.
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪数据后，您可以看到所有特征都与标签具有很强的正线性相关性，尤其是`DIS`和`LSTAT`的相关性，其中标签发生了巨大变化。 这显示了修剪的力量。
    如果您不知道如何处理异常值，则很容易误解您的数据。 离群值可以更改分布的形状以及其他变量之间的相关性，最终它们可以影响模型的性能。
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered exploratory data analysis by using the NumPy, SciPy,
    matplotlib, and Seaborn packages. At the start, we learned how to load and save
    files and explore our dataset. Then, we explained and calculated important statistical
    central moments, such as the mean, variance, skewness, and kurtosis. Four important visualizations
    were used for the graphical representation of univariate and variate analysis,
    respectively; these were the histogram, box plot, scatter plot, and heatmap. The
    importance of data trimming was also emphasized using examples.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了使用 NumPy，SciPy，Matplotlib 和 Seaborn 软件包进行探索性数据分析的过程。 首先，我们学习了如何加载和保存文件以及探索数据集。
    然后，我们解释并计算了重要的统计中心矩，例如均值，方差，偏度和峰度。 四个重要的可视化分别用于单变量和变量分析的图形表示; 这些是直方图，箱形图，散点图和热图。
    还通过示例强调了数据修整的重要性。
- en: In the next chapter, we will go one step further and start predicting housing
    prices using linear regression.
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更进一步，并开始使用线性回归预测房价。
