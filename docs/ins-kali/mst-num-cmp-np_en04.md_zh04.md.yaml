- en: Chapter 4. Predicting Housing Prices Using Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：使用线性回归预测房价
- en: In this chapter, we will introduce supervised learning and predictive modeling
    by implementing linear regression. In the previous chapter, you learned about
    exploratory analysis, but haven't looked at modeling yet. In this chapter, we
    will create a linear regression model to predict housing market prices. Broadly
    speaking, we are going to predict target variable with the help of its relationship
    with other variables. Linear regression is very widely used and is a simple model
    for supervised machine learning algorithms. It's essentially about fitting a line
    for the observed data. We will start our journey with explaining supervised learning
    and linear regression. Then, we will analyze the crucial concepts of linear regression
    such as independent and dependent variables, hyperparameters, loss and error functions,
    and stochastic gradient descent. For modeling, we will use the same dataset that
    we used in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过实现线性回归来介绍监督学习和预测建模。 在上一章中，您了解了探索性分析的知识，但尚未研究建模。 在本章中，我们将创建一个线性回归模型来预测房地产市场价格。
    广义上讲，我们将借助目标变量与其他变量的关系来预测目标变量。 线性回归被广泛使用，并且是监督机器学习算法的简单模型。 本质上是关于为观察到的数据拟合一条线。
    我们将从解释监督学习和线性回归开始我们的旅程。 然后，我们将分析线性回归的关键概念，例如自变量和因变量，超参数，损失和误差函数以及随机梯度下降。 对于建模，我们将使用与上一章相同的数据集。
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下主题：
- en: Supervised learning and linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习和线性回归
- en: Independent and dependent variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自变量和因变量
- en: Hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数
- en: Loss and error functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失和误差函数
- en: Implementing our algorithm for a single variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为单个变量实现我们的算法
- en: Computing stochastic gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算随机梯度下降
- en: Using linear regression to model housing prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用线性回归建模房价
- en: Supervised learning and linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习和线性回归
- en: Machine learning gives computer systems an ability to learn without explicit
    programming. One of the most common types of machine learning is supervised learning.
    Supervised learning consists of a set of different algorithms which formulates
    a learning problem and solves them by mapping inputs and outputs using historical
    data. The algorithms analyze the input and a corresponding output, then link them
    together to find a relationship (learning). Finally, for the new given dataset,
    it will predict the output by using this learning.
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习使计算机系统无需显式编程即可学习。 监督学习是最常见的机器学习类型之一。 监督学习由一组不同的算法组成，这些算法提出学习问题并通过使用历史数据映射输入和输出来解决它们。
    该算法分析输入和相应的输出，然后将它们链接在一起以找到关系（学习）。 最后，对于新的给定数据集，它将使用此学习来预测输出。
- en: In order to differentiate between supervised and unsupervised learning, we can
    think about input/output-based modeling. In supervised learning, the computer
    system will be supervised with labels for every set of input data. In unsupervised
    learning, the computer system will only use input data without any labels.
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分监督学习和非监督学习，我们可以考虑基于输入/输出的建模。 在监督学习中，将对计算机系统的每组输入数据使用标签进行监督。 在无监督学习中，计算机系统将仅使用没有任何标签的输入数据。
- en: As an example, let's assume that we have 1 million photos of cats and dogs.
    In supervised learning, we label the input data and state whether a given photo
    is of a cat or a dog. Let's say we have 20 features for each photo (input data).
    The computer system will know whether the photo is a cat or a dog as it's labeled
    (output data). When we show the computer system a new photo, it will decide whether
    it's a cat or a dog by analyzing 20 features of the new photo and make a prediction
    based on its previous learning. In unsupervised learning, we will just have 1
    million cat and dog photos without any labeling stating whether the photo's of
    a cat or a dog, so the algorithm will cluster the data by analyzing its features
    without our supervision. After clustering is finished, a new photo will be fed
    into the unsupervised learning algorithm and the system will tell us which cluster
    the photo belongs to.
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有 100 万张猫和狗的照片。 在监督学习中，我们标记输入数据并声明给定的照片是猫还是狗。 假设每张照片（输入数据）有 20 个特征。 标有照片的计算机系统将知道照片是猫还是狗（输出数据）。
    当我们向计算机系统显示一张新照片时，它将通过分析新照片的 20 个特征来确定它是猫还是狗，并根据其先前的学习进行预测。 在无监督学习中，我们将只拥有 100
    万张猫和狗的照片，而没有任何标签说明是猫还是狗的照片，因此该算法将在没有我们监督的情况下通过分析其特征来对数据进行聚类。 聚类完成后，会将新照片输入到无监督学习算法中，系统会告诉我们该照片属于哪个聚类。
- en: 'In both scenarios, the system will have a simple or complex decision algorithm.
    The only difference is whether there is any initial supervision or not. An overview
    scheme of supervised learning methods is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，系统都将具有简单或复杂的决策算法。 唯一的区别是是否有任何初始监督。 监督学习方法的概述方案如下：
- en: '![](img/de58c2db-6998-45ad-a4a8-29665d077600.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de58c2db-6998-45ad-a4a8-29665d077600.png)'
- en: Supervised learning can be split into two types as Classification and Regression,
    which is shown in the preceding chart. Classification models predict labels. For
    example, preceding example can be considered a supervised classification problem.
    In order to perform classification, we need to train classification algorithms
    such as **Support Vector Classifier** (**SVC**), Random Forests, **k-nearest **neighbors (**KNN**),
    and so on. Basically, classifiers refer to algorithms that are used to classify
    (categorize) the data.
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以分为分类和回归两种类型，如上图所示。 分类模型预测标签。 例如，可以将前面的示例视为监督分类问题。 为了执行分类，我们需要训练分类算法，例如**支持向量分类器**（**SVC**），随机森林，
    **K 近邻**（**KNN**），依此类推。 基本上，分类器是指用于对数据进行分类（分类）的算法。
- en: Classification methods are used when our target variable is categorical but
    when our target variable is continuous, regression models are applied, as the
    aim is predicting numerical value rather than category.
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的目标变量是分类变量时使用分类方法，而当我们的目标变量是连续变量时，则应用回归模型，因为目标是预测数值而不是类别。
- en: 'Think about the dataset that we used in the previous chapter: the Boston housing
    prices dataset. In that dataset, our aim was to analyze feature values statistically
    because we needed to know how they are distributed, what their basic stats are,
    and the correlations between each other. In the end, we would like to know how
    each feature contributes to a housing price. Does it affect positively, negatively,
    or not at all? If there is a potential effect (relationship) between *feature
    x* and *housing price A*, how strong or weak is this relation?'
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下我们在上一章中使用的数据集：波士顿房屋价格数据集。 在该数据集中，我们的目的是对特征值进行统计分析，因为我们需要了解特征值的分布方式，其基本统计量以及之间的相互关系。
    最后，我们想知道每个特征如何导致房价上涨。 它对正面，负面还是根本没有影响？ 如果*特征 x* 与*房屋价格 A* 之间存在潜在的影响（关系），则该关系的强弱是多少？
- en: 'We try to answer these questions by building a model for predicting a house
    price with given features. As a result, after we feed our model with the new features,
    we expect our model to produce output variables as continuous values (150k, 120,
    $154, and so on). Let''s see a very basic workflow:'
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图通过建立模型来预测具有给定特征的房价来回答这些问题。 结果，在为模型提供新特征后，我们期望模型将输出变量生成为连续值（150k，120，154
    美元等）。 让我们看一个非常基本的工作流程：
- en: '![](img/0730e3e0-034e-4b16-ba7d-8e9799eaf82c.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0730e3e0-034e-4b16-ba7d-8e9799eaf82c.png)'
- en: As in the preceding chart, the analysis starts with preparing our data for our
    model. In this phase, we should clean the data, handle the missing values, and
    extract the features that will be used. After our data is clean, we should divide
    it into two parts, training and test data, in order to test the performance of
    our model.
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，分析从为模型准备数据开始。 在此阶段，我们应该清理数据，处理缺失的值，并提取将要使用的特征。 数据干净后，我们应该将其分为训练数据和测试数据两部分，以测试模型的性能。
- en: Important part in model validation is a concept called **overfitting**. In layman's
    terms, overfitting means learning too much from the training dataset, so our model
    overfits and produces nearly perfect results for the training dataset. However,
    it is not flexible enough to produce good results when it comes to data it has
    never seen, hence it's not able to **generalize** well.
  prefs: []
  type: TYPE_NORMAL
  zh: 模型验证中的重要部分是**过拟合**的概念。 用外行的术语来说，过度拟合意味着从训练数据集中学到太多，因此我们的模型过度拟合并为训练数据集产生近乎完美的结果。
    但是，对于从未见过的数据，它的灵活性不足以产生良好的结果，因此**无法很好地泛化**。
- en: Splitting dataset into training, validation (highly recommended) and test datasets
    is done to overcome this issue. Training data is where the algorithm initially
    learns the parameters (weights) of the algorithm and builds a model where errors
    are minimized. Validation dataset is very useful when you have several algorithms
    and you need to tune the hyperparameters, or when you have many parameters in
    your algorithm and need to tune your parameters. The test dataset is used for
    performance assessment.
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将数据集分为训练，验证（强烈推荐）和测试数据集来解决此问题。 训练数据是算法最初学习算法参数（权重）并在其中将误差最小化的地方建立模型的地方。 当您有几种算法并且需要调整超参数时，或者当算法中有许多参数并且需要调整参数时，平移数据集非常有用。
    测试数据集用于性能评估。
- en: In a nutshell, you train your algorithm with training data, then you fine-tune
    the parameter or weights of your algorithm in the validation dataset and in the
    last stage, you test your tuned algorithm's performance in the test dataset.
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，您可以使用训练数据来训练算法，然后在验证数据集中微调算法的参数或权重，最后一步，在测试数据集中测试调整后的算法的性能。
- en: 'The opposite case to overfitting is underfitting, which means that the algorithm
    learns less from the data and our algorithm does not fit well with our observations.
    Let''s see graphically what overfitting, underfitting, and best-fitting look like:'
  prefs: []
  type: TYPE_NORMAL
  zh: 与过度拟合相反的情况是欠拟合，这意味着该算法从数据中学习得更少，并且我们的算法与我们的观察结果不太吻合。 让我们以图形方式查看过度拟合，欠拟合和最佳拟合的样子：
- en: '![](img/ba96d5f8-786a-490f-a306-6f9bce0ff103.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba96d5f8-786a-490f-a306-6f9bce0ff103.png)'
- en: In the preceding graph, as you may have noticed, even though overfitting seems
    like it fits very well, it generates a regression line which is very unique for
    this dataset and does not correctly capture the characteristics. The second graph,
    the underfitting graph, actually couldn't capture the shape of the data; it didn't
    learn from it and produced a linear regression line when our data is non-linear.
    The third graph, our best fit graph, fitted very well, grasped the distribution
    characteristics and produced a curvilinear line. We can expect that it will not
    have disappointing performance measures for continuation of this dataset.
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，在上图中，即使过度拟合看起来非常合适，它也会生成一个回归线，该回归线对于该数据集非常独特，并且无法正确捕获特征。 第二个图，即欠拟合图，实际上无法捕获数据的形状。
    当我们的数据是非线性的时，它没有从中学习并产生了线性回归线。 第三个图是我们的最佳拟合图，拟合得很好，掌握了分布特征并产生了一条曲线。 我们可以预期，对于此数据集的延续，它不会有令人失望的性能指标。
- en: 'In this chapter, we will use linear regression as a supervised learning method.
    We will start by explaining very important concepts such as independent and dependent
    variables, hyperparameters, and loss and error functions. We will also go through
    practical examples for linear regression. In the next chapter, we will cover the
    most important components of the linear regression model: independent and dependent
    variables.'
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用线性回归作为有监督的学习方法。 我们将首先解释非常重要的概念，例如自变量和因变量，超参数以及损失和误差函数。 我们还将介绍线性回归的实际示例。
    在下一章中，我们将介绍线性回归模型的最重要组成部分：独立变量和因变量。
- en: Independent and dependent variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自变量和因变量
- en: As we mentioned in the previous subsection, linear regression is used to predict
    a value of a variable based on other variables. We are investigating the relationship
    between input variables, *X* and the output variable, *Y*.
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的小节中提到的，线性回归用于基于其他变量来预测变量的值。 我们正在研究输入变量`X`与输出变量`Y`之间的关系。
- en: In linear regression, **dependent variable** is the variable that we want to
    predict. The reason that we call it the dependent variable is because of the assumption
    behind linear regression. The model assumes that these variables depend on the
    variables that are on the other side of the equation, which are called **independent
    variables**.
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，**因变量**是我们要预测的变量。 之所以称其为因变量，是因为线性回归背后的假设。 该模型假设这些变量取决于方程另一侧的变量，这些变量称为**独立变量**。
- en: In simple regression model, model will explain how the dependent variable changes
    based on independent variable.
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单回归模型中，模型将解释因变量如何基于自变量而变化。
- en: 'As an example, let''s imagine that we want to analyze how the sales values
    are effected based on changes in prices for a given product. If you read this
    sentence carefully, you can easily detect what our dependent and independent variables
    are. In our example, we assume that the sales value is affected by price changes,
    in other words, the sales value depends on the price of the product. As a result,
    the sales value is a dependent value and the price is an independent value. It
    does not necessarily mean that the price of a given product is not dependent on
    anything. Of course, it depends on many factors (variables), but in our model,
    we assume that the price is given, and a given price will change the sales value.
    The formula for a linear regression line is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们要分析基于给定产品的价格变化如何影响销售值。 如果您仔细阅读此句子，则可以轻松检测出我们的因变量和自变量。 在我们的示例中，我们假设销售值受价格变化的影响，换句话说，销售值取决于产品的价格。
    结果，销售值是从属值，价格是独立的值。 这不一定意味着给定产品的价格不依赖于任何东西。 当然，它取决于许多因素（变量），但是在我们的模型中，我们假设价格是给定的，并且给定的价格会改变销售值。
    线性回归线的公式如下：
- en: '![](img/aee30221-904b-4eb6-8f4f-9a80085dadc2.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aee30221-904b-4eb6-8f4f-9a80085dadc2.png)'
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
  zh: 哪里：
- en: '*Y*[*i* ]= Estimated value or dependent variable'
  prefs: []
  type: TYPE_NORMAL
  zh: '`Yi = `估计值或因变量'
- en: '*B[0]*= Intercept'
  prefs: []
  type: TYPE_NORMAL
  zh: '`B0 = `截距'
- en: '*B[1]* = Slope'
  prefs: []
  type: TYPE_NORMAL
  zh: '`B1 = `斜率'
- en: '*X*[*i* ]= Independent or exploratory variable:'
  prefs: []
  type: TYPE_NORMAL
  zh: '`Xi = `自变量或探索变量：'
- en: '![](img/b3d04ce5-8bd6-49b0-b400-79a1a2f48a55.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3d04ce5-8bd6-49b0-b400-79a1a2f48a55.png)'
- en: 'The slope (*B1*) of the linear regression line actually shows the relationship
    between these two variables. Let''s say we calculate the slope as 0.8\. This means
    that a one unit increase in the independent variable is likely to effect a 0.8
    unit increase for the estimated value. The preceding linear regression line only
    generates estimations, which means that they are just predictions of *Y* for a
    given *X*. As you see in the following graph, there is a distance between each
    observation and the linear line. That distance is called an **error**, which is
    expected and also very important in regression line fitting and model evaluation:'
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归线的斜率（`B1`）实际上显示了这两个变量之间的关系。 假设我们将斜率计算为 0.8。 这意味着自变量增加 1 个单位可能会使估计值增加 0.8
    个单位。 前面的线性回归线仅生成估计，这意味着它们仅是给定`X`的`Y`的预测。 如下图所示，每个观测值和线性线之间有一段距离。 该距离称为**误差**，这是预期的，在回归线拟合和模型评估中也非常重要：
- en: '![](img/0cde89de-9bc8-410f-8c49-8e93e30630e1.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cde89de-9bc8-410f-8c49-8e93e30630e1.png)'
- en: 'The most common way of fitting a linear regression line is by using the **least
    squares** method. This method fits the regression line by minimizing the sum of
    the squares of these errors. The formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合线性回归线的最常见方法是使用**最小二乘**方法。 该方法通过最小化这些误差的平方和来拟合回归线。 计算公式如下：
- en: '![](img/d909067d-337b-4e00-9625-9c4b5e4a024d.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d909067d-337b-4e00-9625-9c4b5e4a024d.png)'
- en: The reason that these errors are squared is that we don't want negative and
    positive errors to cancel each other out. In the model evaluation, R-squared,
    F-test, and **Root Mean Square Error** (**RMSE**) are used. They all use **Sum
    of Squares Total** (**SST**) and **Sum of Squares Error **(**SSE**) as base measures.
    As you can see, in SSE, we once again calculate the difference between predicted
    values and actual values, take the square, and sum them up in order to evaluate
    how well the regression line fits the data.
  prefs: []
  type: TYPE_NORMAL
  zh: 这些误差均方根的原因是我们不希望负误差和正误差彼此抵消。 在模型评估中，使用 R 平方，F 检验和**均方根误差**（**RMSE**）。 他们都使用**平方和**（**SST**）和**平方和误差**（**SSE**）作为基本度量。
    如您所见，在 SSE 中，我们再次计算预测值和实际值之间的差，取平方并求和，以评估回归线对数​​据的拟合程度。
- en: 'As we mentioned previously, the least squares method aims to minimize squared
    errors (residuals) and find the slope and intercept value which will fit best
    to the data points. As this is a closed-form solution, you can easily calculate
    it by hand in order to see what this method does in the background. Let''s use
    an example with a small dataset:'
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，最小二乘方法旨在最小化平方误差（残差），并找到最适合数据点的斜率和截距值。 由于这是一种封闭形式的解决方案，因此您可以轻松地手动计算它，以便在后台查看此方法的作用。
    让我们使用一个包含少量数据集的示例：
- en: '| **Milk Consumption (liter per week)** | **Height ** |'
  prefs: []
  type: TYPE_TB
  zh: '| **牛奶消耗量（每周升）** | **身高** |'
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 14 | 175 |'
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 175 |'
- en: '| 20 | 182 |'
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 182 |'
- en: '| 10 | 170 |'
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 170 |'
- en: '| 15 | 185 |'
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 185 |'
- en: '| 12 | 164 |'
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 164 |'
- en: '| 15 | 173 |'
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 173 |'
- en: '| 22 | 181 |'
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 181 |'
- en: '| 25 | 193 |'
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 193 |'
- en: '| 12 | 160 |'
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 160 |'
- en: '| 13 | 165 |'
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 165 |'
- en: 'Let''s assume that we have 10 observations for weekly milk consumption and
    the height values for the people who consume the milk, as per the preceding table.
    If we plot this data, we can see that there is a positive correlation between
    daily milk consumption and height:'
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 10 个观察值，如上表所示，用于每周食用牛奶和食用牛奶的人的身高值。 如果我们绘制这些数据，我们可以看到每日牛奶消耗量与身高之间存在正相关：
- en: '![](img/84c84101-f75a-4fc9-a0c6-862018e8d70d.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84c84101-f75a-4fc9-a0c6-862018e8d70d.png)'
- en: 'Now, we want to fit the linear regression line by using the least squares method,
    which is done by using the following formula for the slope and intercept:'
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要使用最小二乘法拟合线性回归线，该方法通过对斜率和截距使用以下公式来完成：
- en: '![](img/661ea0e3-392d-440c-b5c9-c776beb754ae.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/661ea0e3-392d-440c-b5c9-c776beb754ae.png)'
- en: '![](img/062f6ba5-bbaa-41dd-923e-9cc421340337.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/062f6ba5-bbaa-41dd-923e-9cc421340337.png)'
- en: 'Then, we need to create a table to help us work out calculations such as for
    the sum of x, y, xy, x², and y²:'
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建一个表格来帮助我们进行计算，例如`x`，`y`，`xy`，`x^2`和`y^2`的总和：
- en: '|  | **x (Milk Consumption)** | **y (Height)** | **xy** | **x²** | **y²** |'
  prefs: []
  type: TYPE_TB
  zh: '|  | **`x`（牛奶消耗量）** | **`y`（高度）** | **`xy`** | **`x^2`** | **`y^2`** |'
- en: 
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 14 | 175 | 2,450 | 196 | 30,625 |'
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 14 | 175 | 2,450 | 196 | 30,625 |'
- en: '| 2 | 20 | 182 | 3,640 | 400 | 33,124 |'
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 20 | 182 | 3,640 | 400 | 33,124 |'
- en: '| 3 | 10 | 170 | 1,700 | 100 | 28,900 |'
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10 | 170 | 1,700 | 100 | 28,900 |'
- en: '| 4 | 15 | 185 | 2,775 | 225 | 34,225 |'
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 15 | 185 | 2,775 | 225 | 34,225 |'
- en: '| 5 | 12 | 164 | 1,968 | 144 | 26,896 |'
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 12 | 164 | 1,968 | 144 | 26,896 |'
- en: '| 6 | 15 | 173 | 2,595 | 225 | 29,929 |'
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 15 | 173 | 2,595 | 225 | 29,929 |'
- en: '| 7 | 22 | 181 | 3,982 | 484 | 32,761 |'
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 22 | 181 | 3,982 | 484 | 32,761 |'
- en: '| 8 | 25 | 193 | 4,800 | 625 | 37,249 |'
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 25 | 193 | 4,800 | 625 | 37,249 |'
- en: '| 9 | 12 | 160 | 1,920 | 144 | 25,600 |'
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 12 | 160 | 1,920 | 144 | 25,600 |'
- en: '| 10 | 13 | 165 | 2,145 | 169 | 27,225 |'
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 13 | 165 | 2,145 | 169 | 27,225 |'
- en: '| ∑ | 158 | 1,748 | 27,975
    | 2,712 | 306,534 |'
  prefs: []
  type: TYPE_TB
  zh: '| ∑ | 158 | 1,748 | 27,975 | 2,712 | 306,534 |'
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we can formulate the regression line as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将回归线公式如下：
- en: '![](img/84802b9d-f919-408a-8b63-b78bb331be77.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84802b9d-f919-408a-8b63-b78bb331be77.png)'
- en: In this subsection, we mentioned independent and dependent variables and introduced
    the linear regression line and fitting methods. In the next section, we will cover
    hyperparameters, which are very useful in regression model tuning.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们提到了自变量和因变量，并介绍了线性回归线和拟合方法。 在下一节中，我们将介绍超参数，这些参数在回归模型调整中非常有用。
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: Before we start, maybe it's better to explain why we call them hyperparameters
    and not parameters. In machine learning, model parameters can be learned from
    the data, which means that while you train your model, you fit the model's parameters.
    On the other hand, we usually set hyperparameters before we start training the
    model. In order to give an example, you can think of coefficients in regression
    models as model parameters. A hyperparameter example, we can say the learning
    rate in many different models or the number of clusters (k) in k-means clustering.
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，也许最好解释一下为什么我们称它们为超参数而不是参数。 在机器学习中，可以从数据中学习模型参数，这意味着在训练模型时，您可以拟合模型的参数。
    另一方面，我们通常在开始训练模型之前先设置超参数。 为了举例说明，您可以将回归模型中的系数视为模型参数。 以超参数为例，我们可以说许多不同模型中的学习率或
    K 均值聚类中的簇数（K）。
- en: Another important thing is the relationship between model parameters and hyperparameters,
    and how they shape our machine learning model, in other words, the hypothesis
    of our model. In machine learning, parameters are used for configuring the model,
    and this configuration will tailor the algorithm specifically for our dataset.
    What we need to handle is how to optimize the hyperparameters. In addition, this
    optimization may be performed during validation as previously mentioned. Optimizing
    hyperparameters will bring performance improvement in many cases.
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的事情是模型参数和超参数之间的关系，以及它们如何塑造我们的机器学习模型，换句话说，就是我们模型的假设。 在机器学习中，参数用于配置模型，此配置将为我们的数据集专门定制算法。
    我们需要处理的是如何优化超参数。 另外，如上所述，可以在验证期间执行该优化。 在许多情况下，优化超参数将带来性能提升。
- en: You can also think of hyperparameters as high-level parameters on top of model
    parameters. Imagine a case where you use k-means clustering which is unsupervised
    learning. If you use the wrong cluster number (K) as a hyperparameter, it's guaranteed
    that you cannot have the suitable fit for your data.
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将超参数视为模型参数之上的高级参数。 想象一下您使用无监督学习的 K 均值聚类的情况。 如果您使用误差的簇号（K）作为超参数，则可以确保您的数据不适合。
- en: By now, you should be asking how we can tune the hyperparameters if we set them
    manually prior to training the model. There are several ways to tune hyperparameters.
    The bottom line for this optimization is to test the algorithm with a set of different
    hyperparameters and calculate the error function or loss function for each scenario
    before picking the hyperparameter set which has better performance.
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该问的是，如果在训练模型之前手动设置超参数，我们该如何调整它们。 有几种方法可以调整超参数。 此优化的底线是使用一组不同的超参数测试算法，并在选择性能更好的超参数集之前针对每种情况计算误差函数或损失函数。
- en: In this section, we briefly covered parameters, hyperparameters, and their differences.
    In the next section, we will touch on loss and error functions, which are very
    important for hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍了参数，超参数及其差异。 在下一节中，我们将介绍损失和误差函数，这对于超参数优化非常重要。
- en: Loss and error functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失和误差函数
- en: In the previous subsections, we explain supervised and unsupervised learning.
    Regardless of which machine learning algorithm is used, our main challenge is
    regarding issues with optimization. In optimization functions, we are actually
    trying to minimize the loss function. Imagine a case where you are trying to optimize
    your monthly savings. In a closed state, what you will do is minimize your spending,
    in other words, minimize your loss function.
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的小节中，我们解释了有监督和无监督的学习。 无论使用哪种机器学习算法，我们的主要挑战都是关于优化的问题。 在优化函数中，我们实际上是在尝试使损失函数最小化。
    设想一下您试图优化每月储蓄的情况。 在关闭状态下，您要做的就是最小化支出，换句话说，将损失函数最小化。
- en: 'A very common way to build a loss function is starting with the difference
    between the predicted value and the actual value. In general, we try to estimate
    the parameters of our model, and then prediction is made. The main measurement
    that we can use to evaluate how good our prediction is involves calculating the
    difference between the actual values:'
  prefs: []
  type: TYPE_NORMAL
  zh: 建立损失函数的一种非常常见的方法是从预测值与实际值之差开始。 通常，我们尝试估计模型的参数，然后进行预测。 我们可以用来评估我们的预测水平的主要度量包括计算实际值之间的差：
- en: '![](img/89c3fa77-bdb9-4483-adaa-d1848eb13841.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89c3fa77-bdb9-4483-adaa-d1848eb13841.png)'
- en: 'In different models, different loss functions are used. For example, you can
    use a mean squared error in your regression model but it''s probably not a good
    idea to use it as a loss function for your classification model. As an example,
    you can calculate the mean squared error as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的模型中，使用了不同的损失函数。 例如，您可以在回归模型中使用均方误差，但是将其用作分类模型的损失函数可能不是一个好主意。 例如，您可以按以下方式计算均方误差：
- en: '![](img/35191209-55d6-48df-9649-03972e32cb34.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35191209-55d6-48df-9649-03972e32cb34.png)'
- en: 'Where the regression model is as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 其中回归模型如下：
- en: '![](img/cbf08fa7-665d-426a-941c-2a2ed7b50df5.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbf08fa7-665d-426a-941c-2a2ed7b50df5.png)'
- en: 'There are many different loss functions that can be used in different machine
    learning models. Here are some important ones with a brief explanation and their
    usage:'
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的损失函数可用于不同的机器学习模型。 以下是一些重要的说明，并简要说明了它们的用法：
- en: '| **Loss Function** | **Explanation** |'
  prefs: []
  type: TYPE_TB
  zh: '| **损失函数** | **解释** |'
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Cross-Entropy | This is used for classification models where the output of
    the model is a probability between 0-1\. It''s a logarithmic loss function, also
    known as log loss. When the predicted probability approaches 1.0, which is a perfect
    model, cross-entropy loss decreases. |'
  prefs: []
  type: TYPE_TB
  zh: '| 交叉熵 | 这用于分类模型，其中模型的输出为 0-1 之间的概率。 这是对数损失函数，也称为对数损失。 当理想的模型的概率接近 1.0 时， 交叉熵损失降低。
    |'
- en: '| MAE(L1) | Calculates the average errors. As it just uses the absolute value,
    it does not amplify weights to large errors. This is useful when the large errors
    are tolerable compared to small ones. |'
  prefs: []
  type: TYPE_TB
  zh: '| MAE（L1） | 计算误差绝对值的均值。 由于它仅使用绝对值，因此不会将较大误差的权重放大。 当较大误差与较小误差相比可以容忍时，这很有用。 |'
- en: '| MSE(L2) | Takes the square root of errors. Amplifies the weights of large
    errors. This is useful when the large errors are undesirable. |'
  prefs: []
  type: TYPE_TB
  zh: '| MSE（L2） | 计算误差的平方根的均值。 放大较大误差的权重。 当不希望出现较大误差时，这很明智。 |'
- en: '| Hinge | This is a loss function used for linear classifier models such as Support
    Vector Machines. |'
  prefs: []
  type: TYPE_TB
  zh: '| Hinge | 这是用于线性分类器模型（例如支持向量机）的损失函数。 |'
- en: '| Huber | This is a loss function for regression models. It is very similar
    to MSE but less sensitive to outliers. |'
  prefs: []
  type: TYPE_TB
  zh: '| Huber | 这是回归模型的损失函数。 它与 MSE 非常相似，但对异常值不敏感。 |'
- en: '| Kullback-Leibler | Kullback-Leibler divergence measures the difference between
    two probability distributions. The KL loss function used a lot in t-distributed
    stochastic neighbor embedding algorithms. |'
  prefs: []
  type: TYPE_TB
  zh: '| 库尔贝克-莱布勒（Kullback-Leibler） | 库尔贝克-莱布勒散度衡量两个概率分布之间的差异 。 KL 损失函数在 T 分布随机邻居嵌入算法中大量使用。
    |'
- en: In machine learning algorithms, loss functions are crucial while updating the
    weights of variables. Let's say you use backpropagation to train neural networks.
    In each iteration, the total error is calculated. Then, the weights are updated
    in order to minimize the total error. Therefore, using the correct loss function
    directly affects the performance of your machine learning algorithm as it has
    a direct effect on model parameters. In the next chapter, we will start simple
    linear regression with a single variable in housing data.
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习算法中，损失函数在更新变量权重时至关重要。 假设您使用反向传播训练神经网络。 在每次迭代中，都会计算出总误差。 然后，权重被更新以便最小化总误差。
    因此，使用正确的损失函数会直接影响您的机器学习算法的性能，因为它直接影响模型参数。 在下一章中，我们将从住房数据中的单个变量开始简单的线性回归。
- en: Univariate linear regression with gradient descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降的单变量线性回归
- en: 'In this subsection, we will implement univariate linear regression for the
    Boston housing dataset, which we used for exploratory data analysis in the previous
    chapter. Before we fit the regression line, let''s import the necessary libraries
    and load the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将为波士顿住房数据集实现单变量线性回归，该数据已在上一章中用于探索性数据分析。 在拟合回归线之前，让我们导入必要的库并按以下方式加载数据集：
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Compared to the previous chapter, we are using the Pandas DataFrame instead
    of the numpy array in order to show you the usage of dataframe as it`s a very
    convenient data structure as well. Technically, it doesn''t make any difference
    whether you store the data in a numpy array or a Pandas DataFrame in most cases
    if you have only numerical values. Let''s add the target value to our data frame
    and see the relationship between the `RM` feature and the target value with a
    scatter plot:'
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章相比，我们使用 Pandas 数据帧代替了 NumPy 数组，以便向您展示数据帧的用法，因为它也是一个非常方便的数据结构。 从技术上讲，在大多数情况下，如果仅将数值存储在
    numpy 数组或 Pandas 数据帧中，则没有任何区别。 让我们将目标值添加到数据框中，并使用散点图查看`RM`函数与目标值之间的关系：
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/0fabf008-a74b-44b0-ae96-400e6039a165.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fabf008-a74b-44b0-ae96-400e6039a165.png)'
- en: As you can see from the plot, there is a positive correlation between the average
    number of rooms per dwelling (`RM`) and the house price, as expected. Now, we
    will see the magnitude of this relation and try to predict the housing prices
    by using this relation.
  prefs: []
  type: TYPE_NORMAL
  zh: 从该图可以看出，每个住宅的平均房间数（`RM`）与房价之间存在正相关，正如预期的那样。 现在，我们将看到这种关系的强度，并尝试使用这种关系来预测房价。
- en: Imagine that you have very limited knowledge of what linear regression is. Let's
    say that you just have familiarity with the equation but you don't know what an
    error function is, why we need iteration, what gradient descent is, and why we
    use it in linear regression models. In this case, what you would do is simply
    start passing some initial values for the coefficient and intercept to the equation
    before calculating the predicted value.
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您对线性回归的了解非常有限。 假设您只是熟悉方程式，但不知道误差函数是什么，为什么我们需要迭代，什么是梯度下降，以及为什么要在线性回归模型中使用它。
    在这种情况下，您要做的就是简单地开始传递系数的一些初始值，并在计算预测值之前截取方程。
- en: After you calculate several predicted values, you would compare them with the
    actual values and see how far you are from the reality. The next step would be
    to change the coefficient or intercept, or do both to see whether you can get
    closer results to the actual values. If you feel comfortable with that process,
    that's what our algorithm will do in smarter way.
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了几个预测值之后，您可以将它们与实际值进行比较，看看您与实际情况有多远。 下一步将是更改系数或截距，或者同时进行这两项操作，以查看是否可以使结果更接近实际值。
    如果您对此过程感到满意，这就是我们的算法将以更智能的方式进行的操作。
- en: 'In order to better understand the linear regression model steps, we separate
    the code into several blocks. First, let''s create a function which returns the
    prediction value as a result of the regression line:'
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解线性回归模型步骤，我们将代码分成几个块。 首先，让我们创建一个函数，该函数作为回归线的结果返回预测值：
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding function computes a linear regression model as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数计算线性回归模型如下：
- en: '![](img/2100916e-fad9-46a3-9ba0-ac870c514253.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2100916e-fad9-46a3-9ba0-ac870c514253.png)'
- en: 'Then, we need a cost function, which will be calculated in each iteration.
    As a `cost_function`, we will use the mean squared error, which will be the average
    of the total squared difference between predictions and actual values:'
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要一个成本函数，该函数将在每次迭代中进行计算。 作为`cost_function`，我们将使用均方误差，它是预测值与实际值之间总平方差的平均值：
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our last code block will be for updating the weights. When we talk about the
    weights, it's not only about coefficients of independent variables but also the
    intercept. Intercept is also known as bias. In order to update the weights logically,
    we need an iterative optimization algorithm which finds the minimum value of a
    given function. In this example, we will use the gradient descent method to minimize
    the loss function in each iteration. Let's discover what gradient descent does
    step by step.
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一个代码块将用于更新权重。 当我们讨论权重时，不仅涉及自变量的系数，还涉及截距。 拦截也称为偏差。 为了逻辑上更新权重，我们需要一种迭代优化算法，该算法可以找到给定函数的最小值。
    在此示例中，我们将使用梯度下降方法来最小化每次迭代中的损失函数。 让我们一步一步地发现梯度下降的作用。
- en: First of all, we should initialize the weights (intercept and coefficient) and
    calculate the mean squared error. Then, we need to calculate the gradient, which
    means looking at how the mean squared error changes when we change the weights.
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该初始化权重（截距和系数）并计算均方误差。 然后，我们需要计算梯度，这意味着查看当改变权重时均方误差如何变化。
- en: In order to change the weights in a smarter way, we need to understand which
    direction we have to change our coefficient and intercept. This means we should
    calculate the gradient of the error function when we change the weights. We can
    calculate the gradient by taking the partial derivative of a loss function for
    coefficients and intercept.
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以更智能的方式更改权重，我们需要了解我们必须更改系数和截距的方向。 这意味着我们在更改权重时应计算误差函数的梯度。 我们可以通过对系数和截距采用损失函数的偏导数来计算梯度。
- en: 'In univariate linear regression, we have only one coefficient. After we calculate
    the partial derivatives, the algorithm will adjust the weights and recalculate
    the mean squared error. This process will iterate until the updated weight does
    not reduce the means squared error anymore:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量线性回归中，我们只有一个系数。 在计算偏导数之后，该算法将调整权重并重新计算均方误差。 此过程将重复进行，直到更新的权重不再减小均方误差为止：
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code block defines the function for updating the weight and then
    returns the updated coefficient and intercept. Another important parameter in
    this function is `learning_rate`. This learning rate will decide the magnitude
    of the change:'
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码块定义了更新权重的函数，然后返回更新的系数并进行拦截。 此函数中的另一个重要参数是`learning_rate`。 学习速度将决定变化的幅度：
- en: '![](img/b23a29d5-1c29-4dfd-9884-fcda21bac1d0.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b23a29d5-1c29-4dfd-9884-fcda21bac1d0.png)'
- en: In the preceding graph, you can see that our loss function has the shape of ![](img/1814677e-e2ad-4f8f-8f6c-5207e3f3f629.png) as
    it's the sum of the squared error. With gradient descent, we are trying to find
    the minimum loss, as shown in the graph, where the partial derivatives are very
    close to zero. As we mentioned previously, we start our algorithm by initializing
    the weights, which means that we start from the points which are far from the
    minimum. In each iteration, we will update the weight, which decreases the loss.
    This means that given enough iterations, we will converge to the global minimum.
    The learning rate will decide how fast this convergence will happen.
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，您可以看到我们的损失函数为`x = y^2`的形状，因为它是平方误差的总和。 通过梯度下降，我们试图找到最小损耗，如图所示，其中偏导数非常接近零。
    如前所述，我们通过初始化权重来开始算法，这意味着我们从远离最小值的点开始。 在每次迭代中，我们将更新权重，从而减少损失。 这意味着给定足够的迭代次数，我们将收敛到全局最小值。
    学习速度将决定这种融合发生的速度。
- en: 'In other words, a high learning rate will look like a huge jump from one point
    to another when we update the weights. A low learning rate will approximate to
    the global minimum (the desired minimum loss point) slowly. As the learning rate
    is a hyperparameter, we need to set it before we run it:'
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当我们更新权重时，高学习率看起来就像是从一个点到另一个点的巨大跳跃。 低学习率将慢慢接近全局最小值（所需的最小损失点）。 由于学习率是一个超参数，因此我们需要在运行它之前进行设置：
- en: '![](img/6b467b81-8904-44ed-8832-0f4c6c71f77b.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b467b81-8904-44ed-8832-0f4c6c71f77b.png)'
- en: So, do we need to set a big or small learning rate? The answer is that we should
    find the optimal rate. If we set a big learning rate, our algorithm will be overshoot
    the minimum. We can easily miss the minimum as these jumps will never let our
    algorithm converge to the global minimum. On the other hand, if we set the learning
    rate too small, we may need a lot of iterations in order to converge.
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们需要设定学习率的大还是小？ 答案是我们应该找到最佳速率。 如果我们设定较高的学习率，我们的算法将超出最小值。 我们很容易错过最小值，因为这些跳跃永远不会让我们的算法收敛到全局最小值。
    另一方面，如果我们将学习率设置得太小，则可能需要进行大量迭代才能收敛。
- en: 'Having previous code blocks, it''s time to write the main function. The main
    function should follow the flow here like we discussed previously:'
  prefs: []
  type: TYPE_NORMAL
  zh: 具有先前的代码块，是时候编写主函数了。 主函数应遵循我们前面讨论的流程：
- en: '![](img/ea08a77a-7dae-470e-bf9e-5117542bdc10.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea08a77a-7dae-470e-bf9e-5117542bdc10.png)'
- en: 'Then, the code block for the main function should be as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，主函数的代码块应如下所示：
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we have all of the code blocks defined and we are ready to run our univariate
    model. Before we run the `train()` function, we need to set the hyperparameters
    and initial values for the coefficient and intercept. You can create the variables,
    set the values and then put these variables as parameters of the function, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经定义了所有代码块，并且可以运行单变量模型了。 在运行`train()`函数之前，我们需要设置系数和截距的超参数和初始值。 您可以创建变量，设置值，然后将这些变量作为函数的参数，如下所示：
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Or, you can pass these values as keyword arguments when you call the function:'
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以在调用函数时将这些值作为关键字参数传递：
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Our main function will return two arrays, which gives us the final values of
    intercept and coefficient. Additionally, the main function will return a list
    of loss values, which is the result of each iteration. These are the results of
    the mean squared error in each iteration. It is very useful to have this list
    in order to track how loss is changing in each iteration:'
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主函数将返回两个数组，这将为我们提供拦截和系数的最终值。 此外，主函数将返回损失值列表，这是每次迭代的结果。 这些是每次迭代中均方误差的结果。 拥有此列表以跟踪每次迭代中损耗的变化非常有用：
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you see from the coefficient value, that one unit increase in `RM` increases
    the housing price to around $8,575. Now, let''s calculate the predicted values
    by injecting the calculated intercept and coefficient into the regression formula.
    Then, we can plot the linear regression line and see how it fits our data:'
  prefs: []
  type: TYPE_NORMAL
  zh: 从系数值中可以看出，`RM`的每增加 1 单位，住房价格就会升至 8,575 美元左右。 现在，让我们通过将计算出的截距和系数注入回归公式中来计算预测值。
    然后，我们可以绘制线性回归线并查看其如何拟合我们的数据：
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](img/5c058ec9-ace7-40ca-b5bc-195bf5ffac67.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c058ec9-ace7-40ca-b5bc-195bf5ffac67.png)'
- en: In this section, we applied the univariate model by picking a single variable.
    In the following subsections, we will perform a multivariate linear regression
    model by adding more independent variables into our model, which means that we
    will have several coefficients to optimize for the best fit.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过选择一个变量来应用单变量模型。 在以下小节中，我们将通过在模型中添加更多自变量来执行多元线性回归模型，这意味着我们将有多个系数可进行优化以实现最佳拟合。
- en: Using linear regression to model housing prices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性回归建模房价
- en: 'In the section, we will perform multivariate linear regression for the same
    dataset. In contrast to the previous section, we will use the sklearn library
    to show you several ways of performing linear regression models. Before we start
    the linear regression model, we will trim the dataset proportionally from both
    sides by using the `trimboth()` method. By doing this, we will cut off the outliers:'
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对同一数据集执行多元线性回归。 与上一节相反，我们将使用 Sklearn 库向您展示执行线性回归模型的几种方法。 在开始线性回归模型之前，我们将使用`trimboth()`方法从两侧按比例修剪数据集。
    通过这样做，我们将消除异常值：
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you see in the preceding code block, our sample size has decreased from
    506 to 406 for each attribute and label column as we trimmed the data on the left
    and right by 10%:'
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的代码块中看到的那样，由于我们将左右数据修剪了 10% ，每个属性和标签列的样本量从 506 减少到 406：
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then use the `train_test_split()` method to split our dataset into train
    and test. This approach is very common in machine learning algorithms. You divide
    your data into two sets and let your model train (learn) before testing your model
    with the other part of the data. The reason for using this approach is to decrease
    the bias while you are validating your model. Each coefficient represents how
    much the target value is expected to change when we increase these samples by
    a single unit:'
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`train_test_split()`方法将数据集拆分为训练和测试。 这种方法在机器学习算法中非常普遍。 您将数据分为两组，然后让模型训练（学习），然后再使用数据的另一部分测试模型。
    使用这种方法的原因是为了减少验证模型时的偏差。 每个系数代表当我们将这些样本增加一个单位时，预期目标值将改变多少：
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/70631624-802f-4b84-b9b4-cdbc41eee7f1.png)'
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70631624-802f-4b84-b9b4-cdbc41eee7f1.png)'
- en: 'Let''s test our model in the preceding code block. In order to run our model
    on a dataset, we can use the `predict()` method. This method will run the given
    dataset on our model and return the results. Ideally, we are expecting the point
    to be distributed as an *x=y* line shape, in other words, such as a linear line
    which has a 45 degree angle. This is a perfect, 1-on-1 matching prediction. We
    cannot say that our prediction is perfect, but just by looking at the scatter
    plot, we can conclude that the prediction is not a bad one. We can look at two
    important metrics for the model''s performance as follows:'
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在前面的代码块中测试我们的模型。 为了在数据集上运行我们的模型，我们可以使用`predict()`方法。 此方法将在模型上运行给定的数据集并返回结果。
    理想情况下，我们期望该点以`x = y`线形分布，换言之，例如具有 45 度角的直线。 这是一个完美的一对一匹配预测。 我们不能说我们的预测是完美的，但是仅通过查看散点图，我们可以得出结论，该预测不是一个坏的预测。
    我们可以查看有关模型性能的两个重要指标，如下所示：
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first one is the mean squared error. You may have noticed that it decreases
    significantly when we add more independent variables into the model. The second
    one is ![](img/947d9164-2afa-44ba-94b6-66c92aa631ac.png), which is coefficient
    of determination, or in other words, the regression score. The coefficient of
    determination explains how much variance in a dependent variable is explained
    by your independent variables. In our model, the result is 0.91, which explains
    that 91% of the variance in housing prices can be explained by our thirteen features.
    As you can see, sklearn has many useful built-in functions for linear regression
    which may speed up your model building process. On the other hand, you can also
    easily build your linear regression model just by using numpy, which might give
    you more flexibility as you can control each part of your algorithm.
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是均方误差。 您可能已经注意到，当我们在模型中添加更多自变量时，它会大大降低。 第二个是 R 方，它是确定系数，或者换句话说，是回归得分。 确定系数可以解释您的自变量解释了因变量有多少方差。
    在我们的模型中，结果为 0.91，这说明我们的 13 个特征可以解释 91% 的房价差异。 如您所见，Sklearn 具有许多有用的线性回归内置函数，这些函数可以加快模型构建过程。
    另一方面，您也可以仅使用 NumPy 轻松构建线性回归模型，这可以为您提供更大的灵活性，因为您可以控制算法的每个部分。
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Linear regression is one of the most common techniques for modeling the relationship
    between continuous variables. The application of this method is very widely used
    in the industry. We started modeling part of the book on linear regression, not
    just because it's very popular, but because it's a relatively easy technique and
    contains most of the elements which almost every machine learning algorithm has.
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是对连续变量之间的关系进行建模的最常用技术之一。 这种方法的应用在工业上非常广泛。 我们开始对本书的一部分进行线性回归建模，这不仅是因为它非常流行，还因为它是一种相对简单的技术，并且包含了几乎每种机器学习算法都具有的大多数要素。
- en: In this chapter, we learned about supervised and unsupervised learning and built
    a linear regression model by using the Boston housing dataset. We touched upon
    different important concepts such as hyperparameters, loss functions, and gradient
    descent. The main purpose of this chapter was to give you sufficient knowledge
    so that you can build and tune a linear regression model and understand what it
    does step by step. We looked at two practical cases where we used univariate and
    multivariate linear regression. You have also experienced the usage of numpy and
    sklearn in linear regression. We highly encourage that you practice this further
    with different datasets and examine how the outcome changes when you change your
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了监督学习和无监督学习，并使用波士顿房屋数据集建立了线性回归模型。 我们谈到了不同的重要概念，例如超参数，损失函数和梯度下降。 本章的主要目的是为您提供足够的知识，以便您可以建立和调整线性回归模型并逐步了解它的作用。
    我们研究了两个使用单变量和多元线性回归的实际案例。 您还体验了 NumPy 和 Sklearn 在线性回归中的用法。 我们强烈建议您对不同的数据集进行进一步练习，并检查更改超参数时结果如何变化。
- en: In the next chapter, we will learn about the clustering method and practice
    it with an example on wholesale distributor dataset.
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习聚类方法，并通过批发分销商数据集上的示例进行实践。
