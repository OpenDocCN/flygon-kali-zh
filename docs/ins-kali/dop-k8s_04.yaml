- en: Working with Storage and Resources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理存储和资源
- en: In [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes* we introduced the basic function of Kubernetes. Once
    you start to deploy some containers by Kubernetes, you need to consider the application's
    data lifecycle and CPU/memory resource management.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e) *开始使用Kubernetes*中，我们介绍了Kubernetes的基本功能。一旦您开始通过Kubernetes部署一些容器，您需要考虑应用程序的数据生命周期和CPU/内存资源管理。
- en: 'In this chapter, we will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: How a container behaves with volume
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器如何处理卷
- en: Introduce Kubernetes volume functionalities
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Kubernetes卷功能
- en: Best practice and pitfalls of Kubernetes Persistent Volume
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes持久卷的最佳实践和陷阱
- en: Kubernetes resource management
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes资源管理
- en: Kubernetes volume management
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes卷管理
- en: Kubernetes and Docker use a local host disk by default. The Docker application
    may store and load any data onto the disk, for example, log data, temporary files,
    and application data. As long as the host has enough space and the application
    has necessary permission, data will exist as long as a container exists. In other
    words, when a container is closed the application exits, crashes, and reassigns
    a container to another host, and the data will be lost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes和Docker默认使用本地主机磁盘。Docker应用程序可以将任何数据存储和加载到磁盘上，例如日志数据、临时文件和应用程序数据。只要主机有足够的空间，应用程序有必要的权限，数据将存在于容器存在的时间内。换句话说，当容器关闭时，应用程序退出、崩溃并重新分配容器到另一个主机时，数据将丢失。
- en: Container volume lifecycle
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器卷的生命周期
- en: 'In order to understand Kubernetes volume management, you need to understand
    the Docker volume lifecycle. The following example is how Docker behaves with
    a volume when a container restarts:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解Kubernetes卷管理，您需要了解Docker卷的生命周期。以下示例是当容器重新启动时Docker的行为：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: On Kubernetes, it also needs to care pod restart. In the case of a resource
    shortage, Kubernetes may stop a container and then restart a container on the
    same or another Kubernetes node.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，还需要关心pod的重新启动。在资源短缺的情况下，Kubernetes可能会停止一个容器，然后在同一个或另一个Kubernetes节点上重新启动一个容器。
- en: 'The following example shows how Kubernetes behaves when there is a resource
    shortage. One pod is killed and restarted when an out of memory error is received:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了当资源短缺时Kubernetes的行为。当收到内存不足错误时，一个pod被杀死并重新启动：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sharing volume between containers within a pod
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在一个pod内部在容器之间共享卷
- en: '[Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes* described that multiple containers within the same Kubernetes
    pod can share the same pod IP address, network port, and IPC, therefore, applications
    can communicate with each other through a localhost network; however, the filesystem
    is segregated.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e) *开始使用Kubernetes*描述了同一个Kubernetes
    pod中的多个容器可以共享相同的pod IP地址、网络端口和IPC，因此，应用程序可以通过本地网络相互通信；但是，文件系统是分隔的。'
- en: 'The following diagram shows that **Tomcat** and **nginx** are in the same pod.
    Those applications can communicate with each other via localhost. However, they
    can''t access each other''s `config` file:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了**Tomcat**和**nginx**在同一个pod中。这些应用程序可以通过本地主机相互通信。但是，它们无法访问彼此的`config`文件：
- en: '![](../images/00046.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00046.jpeg)'
- en: Some applications won't affect these scenarios and behavior, but some applications
    may have some use cases that require them to use a shared directory or file. Therefore,
    developers and Kubernetes administrators need to be aware of the different types
    of stateless and stateful applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序不会影响这些场景和行为，但有些应用程序可能有一些使用案例，需要它们使用共享目录或文件。因此，开发人员和Kubernetes管理员需要了解不同类型的无状态和有状态应用程序。
- en: Stateless and stateful applications
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无状态和有状态的应用程序
- en: In terms of stateless applications, in this case use ephemeral volume. The application
    on the container doesn't need to preserve the data. Although stateless applications
    may write the data onto the filesystem while a container exists, but it is not
    important in terms of the application's lifecycle.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 就无状态应用程序而言，在这种情况下使用临时卷。容器上的应用程序不需要保留数据。虽然无状态应用程序可能会在容器存在时将数据写入文件系统，但在应用程序的生命周期中并不重要。
- en: For example, the `tomcat` container runs some web applications. It also writes
    an application log under `/usr/local/tomcat/logs/`, but it won't be affected if
    it loses a `log` file.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`tomcat`容器运行一些Web应用程序。它还在`/usr/local/tomcat/logs/`下写入应用程序日志，但如果丢失`log`文件，它不会受到影响。
- en: However, what if you start to analyze an application log? Need to preserve due
    to auditing purpose? In this scenario, Tomcat can still be stateless, but share
    the `/usr/local/tomcat/logs` volume to another container such as Logstash ([https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)).
    Then Logstash will send a log to the chosen analytic store, such as Elasticsearch
    ([https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您开始分析应用程序日志呢？需要出于审计目的保留吗？在这种情况下，Tomcat仍然可以是无状态的，但可以将`/usr/local/tomcat/logs`卷与Logstash等另一个容器共享（[https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)）。然后Logstash将日志发送到所选的分析存储，如Elasticsearch（[https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)）。
- en: 'In this case, the `tomcat` container and `logstash` container *must be in the
    same Kubernetes pod* and share the `/usr/local/tomcat/logs` volume as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`tomcat`容器和`logstash`容器*必须在同一个Kubernetes pod中*，并共享`/usr/local/tomcat/logs`卷，如下所示：
- en: '![](../images/00047.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00047.jpeg)'
- en: The preceding figure shows how Tomcat and Logstash can share the `log` file
    using the Kubernetes `emptyDir` volume ([https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)](https://kubernetes.io/docs/concepts/storage/volumes/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了Tomcat和Logstash如何使用Kubernetes的`emptyDir`卷共享`log`文件（[https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)](https://kubernetes.io/docs/concepts/storage/volumes/)。
- en: 'Tomcat and Logstash didn''t use network via localhost, but share the filesystem
    between `/usr/local/tomcat/logs` from the Tomcat container and `/mnt` from the
    Logstash container through Kubernetes `emptyDir` volume:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Tomcat和Logstash没有通过localhost使用网络，而是通过Kubernetes的`emptyDir`卷在Tomcat容器的`/usr/local/tomcat/logs`和Logstash容器的`/mnt`之间共享文件系统：
- en: '![](../images/00048.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00048.jpeg)'
- en: 'Let''s create `tomcat` and `logstash` pod, and then see whether Logstash can
    see the Tomcat application log under `/mnt`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建`tomcat`和`logstash` pod，然后看看Logstash是否能在`/mnt`下看到Tomcat应用程序日志：
- en: '![](../images/00049.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00049.jpeg)'
- en: In this scenario, in the final destination Elasticsearch must be stateful. In
    terms of stateful means use Persistent Volume. The Elasticsearch container must
    preserve the data even if the container is restarted. In addition, you do not
    need to configure the Elasticsearch container within the same pod as Tomcat/Logstash.
    Because Elasticsearch should be a centralized log datastore, it can be separate
    from the Tomcat/Logstash pod and scaled independently.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最终目的地的Elasticsearch必须是有状态的。有状态意味着使用持久卷。即使容器重新启动，Elasticsearch容器也必须保留数据。此外，您不需要在同一个pod中配置Elasticsearch容器和Tomcat/Logstash。因为Elasticsearch应该是一个集中的日志数据存储，它可以与Tomcat/Logstash
    pod分开，并独立扩展。
- en: Once you determine that your application needs a Persistent Volume, there are
    some different types of volume and different ways to manage Persistent Volumes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定您的应用程序需要持久卷，就有一些不同类型的卷和不同的管理持久卷的方法。
- en: Kubernetes Persistent Volume and dynamic provisioning
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes持久卷和动态配置
- en: Kubernetes supports a variety of Persistent Volume. For example, public cloud
    storage such as AWS EBS and Google Persistent Disk. It also supports network (distributed)
    filesystems such as NFS, GlusterFS, and Ceph. In addition, it can also support
    a block device such as iSCSI and Fibre Channel. Based on environment and infrastructure,
    a Kubernetes administrator can choose the best match types of Persistent Volume.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持各种持久卷。例如，公共云存储，如AWS EBS和Google持久磁盘。它还支持网络（分布式）文件系统，如NFS，GlusterFS和Ceph。此外，它还可以支持诸如iSCSI和光纤通道之类的块设备。根据环境和基础架构，Kubernetes管理员可以选择最匹配的持久卷类型。
- en: The following example is using GCP Persistent Disk as Persistent Volume. The
    first step is creating a GCP Persistent Disk and naming it `gce-pd-1`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用GCP持久磁盘作为持久卷。第一步是创建一个GCP持久磁盘，并将其命名为`gce-pd-1`。
- en: If you use AWS EBS or Google Persistent Disk, the Kubernetes node must be in
    the AWS or Google Cloud Platform.![](../images/00050.jpeg)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用AWS EBS或Google持久磁盘，则Kubernetes节点必须位于AWS或Google云平台中。![](../images/00050.jpeg)
- en: 'Then specify the name `gce-pd-1` in the `Deployment` definition:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在`Deployment`定义中指定名称`gce-pd-1`：
- en: '![](../images/00051.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00051.jpeg)'
- en: It will mount the Persistent Disk from GCE Persistent Disk to `/usr/local/tomcat/logs`,
    which can persist Tomcat application logs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它将从GCE持久磁盘挂载到`/usr/local/tomcat/logs`，可以持久保存Tomcat应用程序日志。
- en: Persistent Volume claiming the abstraction layer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷索赔抽象层
- en: Specifying a Persistent Volume into a configuration file directly, which makes
    a tight couple with a particular infrastructure. In previous example, this was
    Google Cloud Platform and also the disk name (`gce-pd-1`). From a container management
    point of view, pod definition shouldn't be locked-in to the specific environment
    because the infrastructure could be different based on the environment. The ideal
    pod definition should be flexible or abstract the actual infrastructure that specifies
    only volume name and mount point.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将持久卷直接指定到配置文件中，这将与特定基础架构紧密耦合。在先前的示例中，这是谷歌云平台，也是磁盘名称（`gce-pd-1`）。从容器管理的角度来看，pod定义不应该锁定到特定环境，因为基础架构可能会根据环境而不同。理想的pod定义应该是灵活的，或者抽象出实际的基础架构，只指定卷名称和挂载点。
- en: 'Therefore, Kubernetes provides an abstraction layer that associates between
    the pod and the Persistent Volume, which is called the **Persistent Volume Claim**
    (**PVC**). It allows us to decouple from the infrastructure. The Kubernetes administrator
    just needs to pre-allocate a necessary size of the Persistent Volume in advance.
    Then Kubernetes will bind between the Persistent Volume and PVC:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Kubernetes提供了一个抽象层，将pod与持久卷关联起来，称为**持久卷索赔**（**PVC**）。它允许我们与基础架构解耦。Kubernetes管理员只需预先分配必要大小的持久卷。然后Kubernetes将在持久卷和PVC之间进行绑定：
- en: '![](../images/00052.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00052.jpeg)'
- en: 'The following example is a definition of pod that uses PVC; let''s reuse the
    previous example (`gce-pd-1`) to register with Kubernetes first:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例是使用PVC的pod的定义；让我们首先重用之前的例子（`gce-pd-1`）在Kubernetes中注册：
- en: '![](../images/00053.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00053.jpeg)'
- en: Then, create a PVC that associates with Persistent Volume (`pv-1`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建一个与持久卷（`pv-1`）关联的PVC。
- en: 'Note that setting it as `storageClassName: ""` means, that it should explicitly
    use static provisioning. Some of the Kubernetes environments such as **Google
    Container Engine** (**GKE**), are already set up with Dynamic Provisioning. If
    we don''t specify `storageClassName: ""`, Kubernetes will ignore the existing
    `PersistentVolume` and allocates a new `PersistentVolume` when creating the `PersistentVolumeClaim`.![](../images/00054.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，将其设置为`storageClassName: ""`意味着它应明确使用静态配置。一些Kubernetes环境，如**Google容器引擎**（**GKE**），已经设置了动态配置。如果我们不指定`storageClassName:
    ""`，Kubernetes将忽略现有的`PersistentVolume`，并在创建`PersistentVolumeClaim`时分配新的`PersistentVolume`。![](../images/00054.jpeg)'
- en: 'Now, `tomcat` setting has been decoupled from the specific volume to "`pvc-1`":'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`tomcat`设置已经与特定卷“`pvc-1`”解耦：
- en: '![](../images/00055.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00055.jpeg)'
- en: Dynamic Provisioning and StorageClass
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态配置和StorageClass
- en: PVC gives a degree of flexibility for Persistent Volume management. However,
    pre-allocating some Persistent Volumes pools might not be cost efficient, especially
    in a public cloud.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PVC为持久卷管理提供了一定程度的灵活性。然而，预先分配一些持久卷池可能不够成本效益，特别是在公共云中。
- en: 'Kubernetes also helps this kind of situation by supporting Dynamic Provision
    for Persistent Volume. Kubernetes administrator defines the *provisioner* of the
    Persistent Volume, which is called `StorageClass`. Then, the Persistent Volume
    Claim asks `StorageClass` to dynamically allocate a Persistent Volume and then
    associates it with the PVC:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还通过支持持久卷的动态配置来帮助这种情况。Kubernetes管理员定义了持久卷的*provisioner*，称为`StorageClass`。然后，持久卷索赔要求`StorageClass`动态分配持久卷，然后将其与PVC关联起来：
- en: '![](../images/00056.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00056.jpeg)'
- en: 'In the following example, AWS EBS is used as the `StorageClass`, and then,
    when creating the PVC, `StorageClass` dynamically create EBS registers it with
    Kubernetes Persistent Volume, and then attaches to PVC:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，AWS EBS被用作`StorageClass`，然后，在创建PVC时，`StorageClass`动态创建EBS并将其注册到Kubernetes持久卷，然后附加到PVC：
- en: '![](../images/00057.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00057.jpeg)'
- en: 'Once `StorageClass` has been successfully created, create a PVC without PV,
    but specify the `StorageClass` name. In this example, this would be "`aws-sc`",
    as shown in the following screenshot:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`StorageClass`成功创建，就可以创建一个不带PV的PVC，但要指定`StorageClass`的名称。在这个例子中，这将是"`aws-sc`"，如下面的截图所示：
- en: '![](../images/00058.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00058.jpeg)'
- en: 'Then, PVC asks `StorageClass` to create a Persistent Volume automatically on
    AWS as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，PVC要求`StorageClass`在AWS上自动创建持久卷，如下所示：
- en: '![](../images/00059.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00059.jpeg)'
- en: 'Note that a Kubernetes provisioning tool such as kops ([https://github.com/kubernetes/kops](https://github.com/kubernetes/kops))
    and also Google Container Engine ([https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/))
    create a `StorageClass` by default. For example, kops sets up a default `StorageClass`
    as AWS EBS on an AWS environment. As well as Google Cloud Persistent disk on GKE.
    For more information, please refer to [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS* and [Chapter 10](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on GCP*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，诸如kops（[https://github.com/kubernetes/kops](https://github.com/kubernetes/kops)）和Google容器引擎（[https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)）等Kubernetes配置工具默认会创建`StorageClass`。例如，kops在AWS环境上设置了默认的AWS
    EBS `StorageClass`。Google容器引擎在GKE上设置了Google Cloud持久磁盘。有关更多信息，请参阅[第9章](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e)，*在AWS上使用Kubernetes*和[第10章](part0247.html#7BHQU0-6c8359cae3d4492eb9973d94ec3e4f1e)，*在GCP上使用Kubernetes*：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A problem case of ephemeral and persistent setting
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 临时和持久设置的问题案例
- en: You may determine your application as stateless, because `datastore` function
    is handled by another pod or system. However, there are some pitfalls that sometimes
    applications actually store important files that you aren't aware of. For example,
    Grafana ([https://grafana.com/grafana](https://grafana.com/grafana)), it connects
    time series datasources such as Graphite ([https://graphiteapp.org](https://graphiteapp.org))
    and InfluxDB ([https://www.influxdata.com/time-series-database/](https://www.influxdata.com/time-series-database/)),
    so that people may determine whether Grafana is a stateless application.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会将您的应用程序确定为无状态，因为`datastore`功能由另一个pod或系统处理。然而，有时应用程序实际上存储了您不知道的重要文件。例如，Grafana（[https://grafana.com/grafana](https://grafana.com/grafana)），它连接时间序列数据源，如Graphite（[https://graphiteapp.org](https://graphiteapp.org)）和InfluxDB（[https://www.influxdata.com/time-series-database/](https://www.influxdata.com/time-series-database/)），因此人们可以确定Grafana是否是一个无状态应用程序。
- en: However, Grafana itself also uses databases to store the user, organization,
    and dashboard metadata. By default, Grafana uses SQLite3 components and stores
    the database as `/var/lib/grafana/grafana.db`. Therefore, when a container is
    restarted, the Grafana setting will be all reset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Grafana本身也使用数据库来存储用户、组织和仪表板元数据。默认情况下，Grafana使用SQLite3组件，并将数据库存储为`/var/lib/grafana/grafana.db`。因此，当容器重新启动时，Grafana设置将被全部重置。
- en: 'The following example demonstrates how Grafana behaves with ephemeral volume:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了Grafana在临时卷上的行为：
- en: '![](../images/00060.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00060.jpeg)'
- en: 'Let''s create a Grafana `organizations` named `kubernetes org` as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`kubernetes org`的Grafana `organizations`，如下所示：
- en: '![](../images/00061.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00061.jpeg)'
- en: 'Then, look at the `Grafana` directory, there is a database file (`/var/lib/grafana/grafana.db`)
    timestamp that has been updated after creating a Grafana `organization`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，看一下`Grafana`目录，有一个数据库文件（`/var/lib/grafana/grafana.db`）的时间戳，在创建Grafana `organization`之后已经更新：
- en: '![](../images/00062.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00062.jpeg)'
- en: 'When the pod is deleted, ReplicaSet will start a new pod and check whether
    a Grafana `organization` exists or not:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当pod被删除时，ReplicaSet将启动一个新的pod，并检查Grafana `organization`是否存在：
- en: '![](../images/00063.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00063.jpeg)'
- en: 'It looks like the `sessions` directory has disappeared and `grafana.db` is
    also recreated by the Docker image again. Then if you access Web Console, the
    Grafana `organization` will also disappear:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来`sessions`目录已经消失，`grafana.db`也被Docker镜像重新创建。然后，如果您访问Web控制台，Grafana `organization`也会消失：
- en: '![](../images/00064.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00064.jpeg)'
- en: How about just using Persistent Volume for Grafana? But using ReplicaSet with
    Persistent Volume, it doesn't replicate (scale) properly. Because all of the pods
    attempt to mount the same Persistent Volume. In most cases, only the first pod
    can mount the Persistent Volume, then another pod will try to mount, and if it
    can't, it will give up. This happens if the Persistent Volume is capable of only
    RWO (read write once, only one pod can write).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用持久卷来处理Grafana呢？但是使用带有持久卷的ReplicaSet，它无法正确地复制（扩展）。因为所有的pod都试图挂载相同的持久卷。在大多数情况下，只有第一个pod可以挂载持久卷，然后另一个pod会尝试挂载，如果无法挂载，它将放弃。如果持久卷只能支持RWO（只能有一个pod写入），就会发生这种情况。
- en: 'In the following example, Grafana uses Persistent Volume to mount `/var/lib/grafana`;
    however, it can''t scale because Google Persistent Disk is RWO:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，Grafana使用持久卷挂载`/var/lib/grafana`；但是，它无法扩展，因为Google持久磁盘是RWO：
- en: '![](../images/00065.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00065.jpeg)'
- en: 'Even if the Persistent Volume has a capability of RWX (read write many, many
    pods can mount to read and write simultaneously), such as NFS, it won''t complain
    if multiple pods try to bind the same volume. However, we still need to consider
    whether multiple application instances can use the same folder/file or not. For
    example, if it replicates Grafana to two or more pods, it will be conflicted with
    multiple Grafana instances that try to write to the same `/var/lib/grafana/grafana.db`,
    and then data could be corrupted, as shown in the following screenshot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 即使持久卷具有RWX（多个pod可以同时挂载以读写），比如NFS，如果多个pod尝试绑定相同的卷，它也不会抱怨。但是，我们仍然需要考虑多个应用程序实例是否可以使用相同的文件夹/文件。例如，如果将Grafana复制到两个或更多的pod中，它将与尝试写入相同的`/var/lib/grafana/grafana.db`的多个Grafana实例发生冲突，然后数据可能会损坏，如下面的截图所示：
- en: '![](../images/00066.jpeg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00066.jpeg)'
- en: 'In this scenario, Grafana must use backend databases such as MySQL or PostgreSQL
    instead of SQLite3 as follows. It allows multiple Grafana instances to read/write
    Grafana metadata properly:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Grafana必须使用后端数据库，如MySQL或PostgreSQL，而不是SQLite3。这样可以使多个Grafana实例正确读写Grafana元数据。
- en: '![](../images/00067.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00067.jpeg)'
- en: Because RDBMS basically supports to connecting with multiple application instances
    via network, therefore, this scenario is perfectly suited being used by multiple
    pods. Note that Grafana supports using RDBMS as a backend metadata store; however,
    not all applications support RDBMS.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因为关系型数据库基本上支持通过网络连接多个应用程序实例，因此，这种情况非常适合多个pod使用。请注意，Grafana支持使用关系型数据库作为后端元数据存储；但是，并非所有应用程序都支持关系型数据库。
- en: 'For the Grafana configuration that uses MySQL/PostgreSQL, please visit the
    online documentation via:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用MySQL/PostgreSQL的Grafana配置，请访问在线文档：
- en: '[http://docs.grafana.org/installation/configuration/#database](http://docs.grafana.org/installation/configuration/#database).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://docs.grafana.org/installation/configuration/#database](http://docs.grafana.org/installation/configuration/#database)。'
- en: Therefore, the Kubernetes administrator carefully needs to monitor how an application
    behaves with volumes. And understand that in some use cases, just using Persistent
    Volume may not help because of issues that might arise when scaling pods.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Kubernetes管理员需要仔细监视应用程序在卷上的行为。并且要了解，在某些情况下，仅使用持久卷可能无法帮助，因为在扩展pod时可能会出现问题。
- en: If multiple pods need to access the centralized volume, then consider using
    the database as previously shown, if applicable. On the other hand, if multiple
    pods need an individual volume, consider using StatefulSet.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多个pod需要访问集中式卷，则考虑使用先前显示的数据库（如果适用）。另一方面，如果多个pod需要单独的卷，则考虑使用StatefulSet。
- en: Replicating pods with a Persistent Volume using StatefulSet
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StatefulSet复制具有持久卷的pod
- en: StatefulSet was introduced in Kubernetes 1.5; it consists of a bond between
    the pod and the Persistent Volume. When scaling a pod that increases or decreases,
    pod and Persistent Volume are created or deleted together.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet在Kubernetes 1.5中引入；它由Pod和持久卷之间的绑定组成。当扩展增加或减少Pod时，Pod和持久卷会一起创建或删除。
- en: 'In addition, pod creation process is serial. For example, when requesting Kubernetes
    to scale two additional StatefulSet, Kubernetes creates **Persistent Volume Claim
    1** and **Pod 1** first, and then creates **Persistent Volume Claim 2** and **Pod
    2**, but not simultaneously. It helps the administrator if an application registers
    to a registry during the application bootstrap:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Pod的创建过程是串行的。例如，当请求Kubernetes扩展两个额外的StatefulSet时，Kubernetes首先创建**持久卷索赔1**和**Pod
    1**，然后创建**持久卷索赔2**和**Pod 2**，但不是同时进行。如果应用程序在应用程序引导期间注册到注册表，这将有助于管理员：
- en: '![](../images/00068.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00068.jpeg)'
- en: Even if one pod is dead, StatefulSet preserves the position of the pod (pod
    name, IP address, and related Kubernetes metadata) and also the Persistent Volume.
    Then, it attempts to recreate a container that reassigns to the same pod and mounts
    the same Persistent Volume.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一个Pod死掉，StatefulSet也会保留Pod的位置（Pod名称、IP地址和相关的Kubernetes元数据），并且持久卷也会保留。然后，它会尝试重新创建一个容器，重新分配给同一个Pod并挂载相同的持久卷。
- en: 'It helps to keep the number of pods/Persistent Volumes and the application
    remains online using the Kubernetes scheduler:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes调度程序有助于保持Pod/持久卷的数量和应用程序保持在线：
- en: '![](../images/00069.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00069.jpeg)'
- en: StatefulSet with Persistent Volume requires Dynamic Provisioning and `StorageClass`
    because StatefulSet can be scalable. Kubernetes needs to know how to provision
    the Persistent Volume when adding more pods.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 具有持久卷的StatefulSet需要动态配置和`StorageClass`，因为StatefulSet可以进行扩展。当添加更多的Pod时，Kubernetes需要知道如何配置持久卷。
- en: Persistent Volume example
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷示例
- en: In this chapter, there are some Persistent Volume examples that have been introduced.
    Based on the environment and scenario, the Kubernetes administrator needs to configure
    Kubernetes properly.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，介绍了一些持久卷示例。根据环境和场景，Kubernetes管理员需要正确配置Kubernetes。
- en: The following are some examples that build Elasticsearch clusters using different
    role nodes to configure different types of Persistent Volume. They will help you
    to decide how to configure and manage the Persistent Volume.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用不同角色节点构建Elasticsearch集群以配置不同类型的持久卷的一些示例。它们将帮助您决定如何配置和管理持久卷。
- en: Elasticsearch cluster scenario
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch集群场景
- en: Elasticsearch is capable of setting up a cluster by using multiple nodes. As
    of Elasticsearch version 2.4, there are several different types, such as master,
    data, and coordinate nodes ([https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-node.html](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-node.html)).
    Each node has a different role and responsibility in the cluster, therefore the
    corresponding Kubernetes configuration and Persistent Volume should align with
    the proper settings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch能够通过使用多个节点来设置集群。截至Elasticsearch版本2.4，有几种不同类型的节点，如主节点、数据节点和协调节点（[https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-node.html](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-node.html)）。每个节点在集群中有不同的角色和责任，因此相应的Kubernetes配置和持久卷应该与适当的设置保持一致。
- en: 'The following diagram shows the components and roles of Elasticsearch nodes.
    The master node is the only node in the cluster that manages all Elasticsearch
    node registration and configuration. It can also have a backup node (master-eligible
    node) that can serve as the master node at any time:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Elasticsearch节点的组件和角色。主节点是集群中唯一管理所有Elasticsearch节点注册和配置的节点。它还可以有一个备用节点（有资格成为主节点的节点），可以随时充当主节点：
- en: '![](../images/00070.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00070.jpeg)'
- en: Data nodes hold and operate datastores in Elasticsearch. And the coordinating
    node handles HTTP requests from other applications, and then load balances/dispatches
    to the data nodes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据节点在Elasticsearch中保存和操作数据存储。协调节点处理来自其他应用程序的HTTP请求，然后进行负载均衡/分发到数据节点。
- en: Elasticsearch master node
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch主节点
- en: The master node is the only node in the cluster. In addition, other nodes need
    to point to the master node because of registration. Therefore, the master node
    should use Kubernetes StatefulSet to assign a stable DNS name, such as `es-master-1`.
    Therefore, we have to use the Kubernetes service to assign DNS with a headless
    mode that assigns the DNS name to the pod IP address directly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点是集群中唯一的节点。此外，其他节点需要指向主节点进行注册。因此，主节点应该使用Kubernetes StatefulSet来分配一个稳定的DNS名称，例如`es-master-1`。因此，我们必须使用Kubernetes服务以无头模式分配DNS，直接将DNS名称分配给pod
    IP地址。
- en: On the other hand, if the Persistent Volume is not required, because the master
    node does not need to persist an application's data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果不需要持久卷，因为主节点不需要持久化应用程序的数据。
- en: Elasticsearch master-eligible node
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch有资格成为主节点的节点
- en: The master-eligible node is a standby for the master node, and therefore there's
    no need to create another `Kubernetes` object. This means that scaling the master
    StatefulSet that assigns `es-master-2`, `es-master-3`, and `es-master-N` is enough.
    When the master node does not respond, there is a master node election within
    the master-eligible nodes to choose and elevate one node as the master node.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有资格成为主节点的节点是主节点的备用节点，因此不需要创建另一个`Kubernetes`对象。这意味着扩展主StatefulSet分配`es-master-2`、`es-master-3`和`es-master-N`就足够了。当主节点不响应时，在有资格成为主节点的节点中进行主节点选举，选择并提升一个节点为主节点。
- en: Elasticsearch data node
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch数据节点
- en: The Elasticsearch data node is responsible for storing the data. In addition,
    we need to scale out if greater data capacity and/or more query requests are needed.
    Therefore, we can use StatefulSet with Persistent Volume to stabilize the pod
    and Persistent Volume. On the other hand, there's no need to have the DNS name,
    therefore no need to setup Kubernetes service for Elasticsearch data node.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch数据节点负责存储数据。此外，如果需要更大的数据容量和/或更多的查询请求，我们需要进行横向扩展。因此，我们可以使用带有持久卷的StatefulSet来稳定pod和持久卷。另一方面，不需要有DNS名称，因此也不需要为Elasticsearch数据节点设置Kubernetes服务。
- en: Elasticsearch coordinating node
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch协调节点
- en: The coordinating node is a load balancer role in the Elasticsearch. Therefore,
    we need to scale out to handle HTTP traffic from external sources and persisting
    the data is not required. Therefore, we can use Kubernetes ReplicaSet with the
    Kubernetes service to expose the HTTP to the external service.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 协调节点是Elasticsearch中的负载均衡器角色。因此，我们需要进行横向扩展以处理来自外部来源的HTTP流量，并且不需要持久化数据。因此，我们可以使用带有Kubernetes服务的Kubernetes
    ReplicaSet来将HTTP暴露给外部服务。
- en: 'The following example shows the commands used when we create all of the preceding
    Elasticsearch nodes by Kubernetes:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了我们在Kubernetes中创建所有上述Elasticsearch节点时使用的命令：
- en: '![](../images/00071.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00071.jpeg)'
- en: 'In addition, the following screenshot is the result we obtain after creating
    the preceding instances:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以下截图是我们在创建上述实例后获得的结果：
- en: '![](../images/00072.jpeg)![](../images/00073.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: ！[](../images/00072.jpeg)![](../images/00073.jpeg)
- en: In this case, external service (Kubernetes node:`30020`) is an entry point for
    external applications. For testing purposes, let's install `elasticsearch-head`
    ([https://github.com/mobz/elasticsearch-head](https://github.com/mobz/elasticsearch-head))
    to visualize the cluster information.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，外部服务（Kubernetes节点：`30020`）是外部应用程序的入口点。为了测试目的，让我们安装`elasticsearch-head`（[https://github.com/mobz/elasticsearch-head](https://github.com/mobz/elasticsearch-head)）来可视化集群信息。
- en: 'Connect Elasticsearch coordination node to install the `elasticsearch-head`
    plugin:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将Elasticsearch协调节点连接到安装`elasticsearch-head`插件：
- en: '![](../images/00074.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: ！[](../images/00074.jpeg)
- en: 'Then, access any Kubernetes node, URL as `http://<kubernetes-node>:30200/_plugin/head`.
    The following UI contains the cluster node information:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，访问任何Kubernetes节点，URL为`http://<kubernetes-node>:30200/_plugin/head`。以下UI包含集群节点信息：
- en: '![](../images/00075.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: ！[](../images/00075.jpeg)
- en: The star icon indicates the Elasticsearch master node, the three black bullets
    are data nodes and the white circle bullet is the coordinator node.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 星形图标表示Elasticsearch主节点，三个黑色子弹是数据节点，白色圆形子弹是协调节点。
- en: 'In this configuration, if one data node is down, no service impact will occur,
    as shown in the following snippet:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种配置中，如果一个数据节点宕机，不会发生任何服务影响，如下面的片段所示：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../images/00076.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: ！[](../images/00076.jpeg)
- en: 'A few moments later, the new pod mounts the same PVC, which preserved `es-data-0`
    data. And then the Elasticsearch data node registers to master node again, after
    which the cluster health is back to green (normal), as shown in the following
    screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，新的pod挂载相同的PVC，保留了`es-data-0`的数据。然后Elasticsearch数据节点再次注册到主节点，之后集群健康状态恢复为绿色（正常），如下面的截图所示：
- en: '![](../images/00077.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: ！[](../images/00077.jpeg)
- en: Due to StatefulSet and Persistent Volume, the application data is not lost on
    `es-data-0`. If you need more disk space, increase the number of data nodes. If
    you need to support more traffic, increase the number of coordinator nodes. If
    a backup of the master node is required, increase the number of master nodes to
    make some master-eligible nodes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于StatefulSet和持久卷，应用程序数据不会丢失在`es-data-0`上。如果需要更多的磁盘空间，增加数据节点的数量。如果需要支持更多的流量，增加协调节点的数量。如果需要备份主节点，增加主节点的数量以使一些主节点有资格。
- en: Overall, the Persistent Volume combination of StatefulSet is very powerful,
    and can make the application flexible and scalable.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，StatefulSet的持久卷组合非常强大，可以使应用程序灵活和可扩展。
- en: Kubernetes resource management
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes资源管理
- en: '[Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes* mentioned that Kubernetes has a scheduler that manages
    Kubernetes node and then determines where to deploy a pod. When node has enough
    resources such as CPU and memory, Kubernetes administrator can feel free to deploy
    an application. However, once it reaches its resource limit, the Kubernetes scheduler
    behaves different based on its configuration. Therefore, the Kubernetes administrator
    has to understand how to configure and utilize machine resources.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)，*开始使用Kubernetes*提到Kubernetes有一个调度程序来管理Kubernetes节点，然后确定在哪里部署一个pod。当节点有足够的资源，如CPU和内存时，Kubernetes管理员可以随意部署应用程序。然而，一旦达到资源限制，Kubernetes调度程序根据其配置行为不同。因此，Kubernetes管理员必须了解如何配置和利用机器资源。'
- en: Resource Quality of Service
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源服务质量
- en: 'Kubernetes has the concept of **Resource QoS** (**Quality of Service**), which
    helps an administrator to assign and manage pods by different priorities. Based
    on the pod''s setting, Kubernetes classifies each pod as:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes有**资源QoS**（**服务质量**）的概念，它可以帮助管理员通过不同的优先级分配和管理pod。根据pod的设置，Kubernetes将每个pod分类为：
- en: Guaranteed pod
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guaranteed pod
- en: Burstable pod
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burstable pod
- en: BestEffort pod
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BestEffort pod
- en: The priority would be Guaranteed > Burstable > BestEffort, which means if the
    BestEffort pod and the Guaranteed pod exist in the same node, then when one of
    the pods consumes memory and to causes a node resource shortage, one of the BestEffort
    pods will be terminated to save the Guaranteed pod.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级将是Guaranteed > Burstable > BestEffort，这意味着如果BestEffort pod和Guaranteed pod存在于同一节点中，那么当其中一个pod消耗内存并导致节点资源短缺时，将终止其中一个BestEffort
    pod以保存Guaranteed pod。
- en: 'In order to configure Resource QoS, you have to set the resource request and/or
    resource limit in the pod definition. The following example is a definition of
    resource request and resource limit for nginx:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置资源QoS，您必须在pod定义中设置资源请求和/或资源限制。以下示例是nginx的资源请求和资源限制的定义：
- en: '[PRE4]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This example indicates the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例指示以下内容：
- en: '| **Type of resource definition** | **Resource name** | **Value** | **Mean**
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **资源定义类型** | **资源名称** | **值** | **含义** |'
- en: '| `requests` | `cpu` | `0.1` | At least 10% of 1 CPU core |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `requests` | `cpu` | `0.1` | 至少10%的1个CPU核心 |'
- en: '|  | `memory` | `10Mi` | At least 10 Mbytes of memory |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | `memory` | `10Mi` | 至少10兆字节的内存 |'
- en: '| `limits` | `cpu` | `0.5` | Maximum 50 % of 1 CPU core |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `limits` | `cpu` | `0.5` | 最大50%的1个CPU核心 |'
- en: '|  | `memory` | `300Mi` | Maximum 300 Mbyte of memory |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | `memory` | `300Mi` | 最大300兆字节的内存 |'
- en: 'For the CPU resource, acceptable value expressions for either cores (0.1, 0.2
    ... 1.0, 2.0) or millicpu (100m, 200m ... 1000m, 2000m). 1000 m is equivalent
    to 1 core. For example, if Kubernetes node has 2 cores CPU (or 1 core with hyperthreading),
    there are total of 2.0 cores or 2000 millicpu, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CPU资源，可接受的值表达式为核心（0.1、0.2……1.0、2.0）或毫核（100m、200m……1000m、2000m）。1000m相当于1个核心。例如，如果Kubernetes节点有2个核心CPU（或1个带超线程的核心），则总共有2.0个核心或2000毫核，如下所示：
- en: '![](../images/00078.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00078.jpeg)'
- en: 'If you run the nginx example (`requests.cpu: 0.1`), it occupies at least 0.1
    core, as shown in the following figure:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '如果运行nginx示例（`requests.cpu: 0.1`），它至少占用0.1个核心，如下图所示：'
- en: '![](../images/00079.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00079.jpeg)'
- en: 'As long as the CPU has enough spaces, it may occupy up to 0.5 cores (`limits.cpu:
    0.5`), as shown in the following figure:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '只要CPU有足够的空间，它可以占用最多0.5个核心（`limits.cpu: 0.5`），如下图所示：'
- en: '![](../images/00080.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00080.jpeg)'
- en: 'You can also see the configuration by using the `kubectl describe nodes` command
    as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`kubectl describe nodes`命令查看配置，如下所示：
- en: '![](../images/00081.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00081.jpeg)'
- en: Note that it shows a percentage that depends on the Kubernetes node's spec in
    the preceding example; as you can see the node has 1 core and 600 MB memory.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它显示的百分比取决于前面示例中Kubernetes节点的规格；如您所见，该节点有1个核心和600 MB内存。
- en: 'On the other hand, if it exceeds the memory limit, the Kubernetes scheduler
    determines that this pod is out of memory, and then it will kill a pod (`OOMKilled`):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果超出了内存限制，Kubernetes调度程序将确定该pod内存不足，然后它将终止一个pod（`OOMKilled`）：
- en: '[PRE5]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Configuring the BestEffort pod
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置BestEffort pod
- en: 'The BestEffort pod has the lowest priority in the Resource QoS configuration.
    Therefore, in case of a resource shortage, this pod will be the first one to be
    terminated. The use case of using BestEffort would be a stateless and recoverable
    application such as:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: BestEffort pod在资源QoS配置中具有最低的优先级。因此，在资源短缺的情况下，该pod将是第一个被终止的。使用BestEffort的用例可能是无状态和可恢复的应用程序，例如：
- en: Worker process
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Worker process
- en: Proxy or cache node
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理或缓存节点
- en: 'In the case of a resource shortage, this pod should yield CPU and memory resource
    to other higher priority pods. In order to configure a pod as the BestEffort pod,
    you need to set resource limit as 0, or not specify resource limit. For example:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源短缺的情况下，该pod应该将CPU和内存资源让给其他优先级更高的pod。为了将pod配置为BestEffort pod，您需要将资源限制设置为0，或者不指定资源限制。例如：
- en: '[PRE6]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that the resource setting is inherited by the `namespace default` setting.
    Therefore, if you intend to configure the pod as the BestEffort pod using the
    implicit setting, it might not configure as BestEffort if the namespace has a
    default resource setting as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，资源设置是由`namespace default`设置继承的。因此，如果您打算使用隐式设置将pod配置为BestEffort pod，如果命名空间具有以下默认资源设置，则可能不会配置为BestEffort：
- en: '![](../images/00082.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00082.jpeg)'
- en: 'In this case, if you deploy to the default namespace using implicit setting,
    it applies a default CPU request as `request.cpu: 0.1` and then it becomes Burstable.
    On the other hand, if you deploy to `blank-namespace`, apply `request.cpu: 0`,
    and then it will become BestEffort.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，如果您使用隐式设置部署到默认命名空间，它将应用默认的CPU请求，如`request.cpu: 0.1`，然后它将变成Burstable。另一方面，如果您部署到`blank-namespace`，应用`request.cpu:
    0`，然后它将变成BestEffort。'
- en: Configuring as the Guaranteed pod
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置为Guaranteed pod
- en: Guaranteed is the highest priority in Resource QoS. In the case of a resource
    shortage, the Kubernetes scheduler will try to retain the Guaranteed pod to the
    last.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Guaranteed是资源QoS中的最高优先级。在资源短缺的情况下，Kubernetes调度程序将尽力保留Guaranteed pod到最后。
- en: 'Therefore, the usage of a Guaranteed pod would be a mission critical node such
    as:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Guaranteed pod的使用将是诸如任务关键节点之类的节点：
- en: Backend database with Persistent Volume
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有持久卷的后端数据库
- en: Master node (such as Elasticsearch master node and HDFS name node)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点（例如Elasticsearch主节点和HDFS名称节点）
- en: 'In order to configure as the Guaranteed pod, explicitly set the resource limit
    and resource request as the same value, or only set the resource limit. However,
    again, if the namespace has default resource setting, it might cause different
    results:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将其配置为Guaranteed pod，明确设置资源限制和资源请求为相同的值，或者只设置资源限制。然而，再次强调，如果命名空间具有默认资源设置，可能会导致不同的结果：
- en: '[PRE7]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because Guaranteed pod has to set resource limit, if you are not 100% sure about
    the necessary CPU/memory resource of your application, especially maximum memory
    usage; you should use Burstable setting to monitor the application behavior for
    a while. Otherwise Kubernetes scheduler might terminate a pod (`OOMKilled`) even
    if the node has enough memory.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Guaranteed pod必须设置资源限制，如果您对应用程序的必要CPU/内存资源不是100%确定，特别是最大内存使用量；您应该使用Burstable设置一段时间来监视应用程序的行为。否则，即使节点有足够的内存，Kubernetes调度程序也可能终止pod（`OOMKilled`）。
- en: Configuring as Burstable pod
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置为Burstable pod
- en: The Burstable pod has a higher priority than BestEffort, but lower than Guaranteed.
    Unlike Guaranteed pod, resource limit setting is not mandatory; therefore pod
    can consume CPU and memory as much as possible while node resource is available.
    Therefore, it is good to be used by any type of application.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Burstable pod的优先级高于BestEffort，但低于Guaranteed。与Guaranteed pod不同，资源限制设置不是强制性的；因此，在节点资源可用时，pod可以尽可能地消耗CPU和内存。因此，它适用于任何类型的应用程序。
- en: If you already know the minimal memory size of an application, you should specify
    request resource, which helps Kubernetes scheduler to assign to the right node.
    For example, there are two nodes that have 1 GB memory each. Node 1 already assigns
    600 MB memory and node 2 assigns 200 MB memory to other pods.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经知道应用程序的最小内存大小，您应该指定请求资源，这有助于Kubernetes调度程序分配到正确的节点。例如，有两个节点，每个节点都有1GB内存。节点1已经分配了600MB内存，节点2分配了200MB内存给其他pod。
- en: 'If we create one more pod that has a resource request memory as 500 MB, then
    Kubernetes scheduler assigns this pod to node 2\. However, if the pod doesn''t
    have a resource request, the result will vary either node 1 or node 2\. Because
    Kubernetes doesn''t know how much memory this pod will consume:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建一个请求内存资源为 500 MB 的 pod，那么 Kubernetes 调度器会将此 pod 分配给节点 2。但是，如果 pod 没有资源请求，结果将在节点
    1 或节点 2 之间变化。因为 Kubernetes 不知道这个 pod 将消耗多少内存：
- en: '![](../images/00083.jpeg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00083.jpeg)'
- en: There is still important behavior of Resource QoS to discuss. The granularity
    of Resource QoS unit is pod level, not a container level. This means, if you configure
    a pod that has two containers, you intend to set container A as Guaranteed (request/limit
    are same value), and container B is Burstable (set only request). Unfortunately,
    Kubernetes configures this pod as Burstable because Kubernetes doesn't know what
    the limit of container B is.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个重要的资源 QoS 行为需要讨论。资源 QoS 单位的粒度是 pod 级别，而不是容器级别。这意味着，如果您配置了一个具有两个容器的 pod，您打算将容器
    A 设置为保证的（请求/限制值相同），容器 B 是可突发的（仅设置请求）。不幸的是，Kubernetes 会将此 pod 配置为可突发，因为 Kubernetes
    不知道容器 B 的限制是多少。
- en: 'The following example demonstrate that failed to configure as Guaranteed pod,
    it eventually configured as Burstable:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例表明未能配置为保证的 pod，最终配置为可突发的：
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Even though, change to configure resource limit only, but if container A has
    CPU limit only, then container B has memory limit only, then result will also
    be Burstable again because Kubernetes knows only either limit:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 即使改为仅配置资源限制，但如果容器 A 只有 CPU 限制，容器 B 只有内存限制，那么结果也会再次变为可突发，因为 Kubernetes 只知道限制之一：
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Therefore, if you intend to configure pod as Guaranteed, you must set all containers
    as Guaranteed.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您打算将 pod 配置为保证的，必须将所有容器设置为保证的。
- en: Monitoring resource usage
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控资源使用
- en: When you start to configure to set a resource request and/or limit, your pod
    may not be scheduled to deploy by Kubernetes scheduler due to insufficient resources.
    In order to understand allocatable resources and available resources, use the
    `kubectl describe nodes` command to see the status.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始配置资源请求和/或限制时，由于资源不足，您的 pod 可能无法被 Kubernetes 调度器部署。为了了解可分配资源和可用资源，请使用 `kubectl
    describe nodes` 命令查看状态。
- en: 'The following example shows one node that has 600 MB memory and one core CPU.
    So allocatable resources are as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示一个节点有 600 MB 内存和一个核心 CPU。因此，可分配资源如下：
- en: '![](../images/00084.jpeg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00084.jpeg)'
- en: 'However, this node already runs some Burstable pod (use resource request) already
    as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个节点已经运行了一些可突发的 pod（使用资源请求）如下：
- en: '![](../images/00085.jpeg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00085.jpeg)'
- en: 'The available memory is limited as approximately 20 MB. Therefore, if you submit
    Burstable pod that request more than 20 MB, it is never scheduled, as shown in
    the following screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 可用内存约为 20 MB。因此，如果您提交了请求超过 20 MB 的可突发的 pod，它将永远不会被调度，如下面的截图所示：
- en: '![](../images/00086.jpeg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00086.jpeg)'
- en: 'The error event can be captured by the `kubectl describe pod` command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 错误事件可以通过 `kubectl describe pod` 命令捕获：
- en: '![](../images/00087.jpeg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../images/00087.jpeg)'
- en: In this case, you need to add more Kubernetes nodes to support more resources.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您需要添加更多的 Kubernetes 节点来支持更多的资源。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered Stateless and Stateful applications that use
    ephemeral volume or Persistent Volume. Both have pitfalls when an application
    restarts or a pod scales. In addition, Persistent Volume management on Kubernetes
    has been kept enhanced to make it easier, as you can see from such tools as StatefulSet
    and Dynamic Provisioning.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经涵盖了使用临时卷或持久卷的无状态和有状态应用程序。当应用程序重新启动或 pod 扩展时，两者都存在缺陷。此外，Kubernetes 上的持久卷管理已经得到增强，使其更容易，正如您可以从
    StatefulSet 和动态配置等工具中看到的那样。
- en: Also, Resource QoS helps Kubernetes scheduler to assign a pod to the right node
    based on request and limit based on priorities.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，资源 QoS 帮助 Kubernetes 调度器根据优先级基于请求和限制将 pod 分配给正确的节点。
- en: The next chapter will introduce Kubernetes network and security, which configures
    pod and services more easier, and makes them scalable and secure.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍 Kubernetes 网络和安全性，这将使 pod 和服务的配置更加简单，并使它们更具可扩展性和安全性。
