- en: Chapter 2. Kubernetes – Core Concepts and Constructs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。Kubernetes-核心概念和构造
- en: This chapter will cover the core **Kubernetes** constructs, such as **pods**,
    **services**, **replication controllers**, and **labels**. A few simple application
    examples will be included to demonstrate each construct. The chapter will also
    cover basic operations for your cluster. Finally, **health checks** and **scheduling**
    will be introduced with a few examples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖核心**Kubernetes**构造，如**pod**、**服务**、**复制控制器**和**标签**。将包括一些简单的应用程序示例，以演示每个构造。本章还将介绍集群的基本操作。最后，将介绍**健康检查**和**调度**，并提供一些示例。
- en: 'This chapter will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主题：
- en: Kubernetes' overall architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes的整体架构
- en: Introduction to core Kubernetes constructs, such as pods, services, replication
    controllers, and labels
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍核心Kubernetes构造，如pod、服务、复制控制器和标签
- en: Understand how labels can ease management of a Kubernetes cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解标签如何简化Kubernetes集群的管理
- en: Understand how to monitor services and container health
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何监视服务和容器的健康状况
- en: Understand how to set up scheduling constraints based on available cluster resources
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何根据可用的集群资源设置调度约束。
- en: The architecture
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: Although **Docker** brings a helpful layer of abstraction and tooling around
    container management, Kubernetes brings similar assistance to orchestrating containers
    at scale as well as managing full application stacks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管**Docker**为容器管理带来了有用的抽象层和工具，但Kubernetes也为规模化编排容器以及管理完整的应用程序堆栈提供了类似的帮助。
- en: '**K8s** moves up the stack giving us constructs to deal with management at
    the application or service level. This gives us automation and tooling to ensure
    high availability, application stack, and service-wide portability. K8s also allows
    finer control of resource usage, such as CPU, memory, and disk space across our
    infrastructure.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**K8s**向上移动堆栈，为我们提供处理应用程序或服务级别管理的构造。这为我们提供了自动化和工具，以确保高可用性、应用程序堆栈和服务的广泛可移植性。K8s还允许更精细地控制资源使用，如CPU、内存和磁盘空间跨我们的基础设施。'
- en: Kubernetes provides this higher level of orchestration management by giving
    us key constructs to combine multiple containers, endpoints, and data into full
    application stacks and services. K8s then provides the tooling to manage the *when*,
    *where*, and *how many* of the stack and its components.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通过为我们提供关键构造来组合多个容器、端点和数据成为完整的应用程序堆栈和服务，从而提供了这种更高级别的编排管理。然后，K8s提供了管理堆栈及其组件的*何时*、*在哪里*和*多少*的工具。
- en: '![The architecture](../images/00019.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: ！[架构](../images/00019.jpeg)
- en: Figure 2.1\. Kubernetes core architecture
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1。Kubernetes核心架构
- en: In the preceding figure (Figure 2.1), we see the core architecture for Kubernetes.
    Most administrative interactions are done via the `kubectl` script and/or RESTful
    service calls to the API.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图（图2.1）中，我们看到了Kubernetes的核心架构。大多数管理交互是通过`kubectl`脚本和/或对API的RESTful服务调用完成的。
- en: Note the ideas of the *desired state* and *actual state* carefully. This is
    key to how Kubernetes manages the cluster and its workloads. All the pieces of
    K8s are constantly working to monitor the current *actual state* and synchronize
    it with the *desired state* defined by the administrators via the API server or
    `kubectl` script. There will be times when these states do not match up, but the
    system is always working to reconcile the two.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细注意*期望状态*和*实际状态*的概念。这是Kubernetes管理集群及其工作负载的关键。K8s的所有部分都在不断地监视当前的*实际状态*，并将其与管理员通过API服务器或`kubectl`脚本定义的*期望状态*进行同步。有时这些状态不匹配，但系统始终在努力协调两者。
- en: Master
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主节点
- en: Essentially, **master** is the brain of our cluster. Here, we have the core
    API server, which maintains RESTful web services for querying and defining our
    desired cluster and workload state. It's important to note that the control pane
    only accesses the master to initiate changes and not the nodes directly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，**master** 是我们集群的大脑。在这里，我们有核心 API 服务器，它维护用于查询和定义我们期望的集群和工作负载状态的 RESTful
    web 服务。重要的是要注意，控制面板只能访问主节点来启动更改，而不能直接访问节点。
- en: Additionally, the master includes the **scheduler**, which works with the API
    server to schedule workloads in the form of pods on the actual minion nodes. These
    pods include the various containers that make up our application stacks. By default,
    the basic Kubernetes scheduler spreads pods across the cluster and uses different
    nodes for matching pod replicas. Kubernetes also allows specifying necessary resources
    for each container, so scheduling can be altered by these additional factors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，主节点包括 **调度程序**，它与 API 服务器一起工作，在实际的 minion 节点上调度 pod 的工作负载。这些 pod 包括组成我们应用程序堆栈的各种容器。默认情况下，基本的
    Kubernetes 调度程序会在集群中分布 pod，并使用不同的节点来匹配 pod 副本。Kubernetes 还允许为每个容器指定必要的资源，因此调度可以通过这些额外的因素进行改变。
- en: The replication controller works with the API server to ensure that the correct
    number of pod replicas are running at any given time. This is exemplary of the
    *desired state* concept. If our replication controller is defining three replicas
    and our *actual state* is two copies of the pod running, then the scheduler will
    be invoked to add a third pod somewhere on our cluster. The same is true if there
    are too many pods running in the cluster at any given time. In this way, K8s is
    always pushing towards that *desired state*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 复制控制器与 API 服务器一起工作，以确保在任何给定时间运行正确数量的 pod 副本。这是 *期望状态* 概念的典范。如果我们的复制控制器定义了三个副本，而我们的
    *实际状态* 是运行两个 pod 副本，那么调度程序将被调用在集群的某个地方添加第三个 pod。如果在任何给定时间集群中运行的 pod 太多，也是如此。这样，K8s
    总是朝着那个 *期望状态* 前进。
- en: Finally, we have **etcd** running as a distributed configuration store. The
    Kubernetes state is stored here and etcd allows values to be watched for changes.
    Think of this as the brain's shared memory.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 **etcd** 作为分布式配置存储运行。Kubernetes 状态存储在这里，etcd 允许监视值的更改。可以将其视为大脑的共享内存。
- en: Node (formerly minions)
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点（以前是 minions）
- en: In each node, we have a couple of components. The **kublet** interacts with
    the API server to update state and to start new workloads that have been invoked
    by the scheduler.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点中，我们有一些组件。 **kublet** 与 API 服务器交互，更新状态并启动调度程序调用的新工作负载。
- en: '**Kube-proxy** provides basic load balancing and directs traffic destined for
    specific services to the proper pod on the backend. See the *Services* section
    later in this chapter.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kube-proxy** 提供基本的负载平衡，并将流向特定服务的流量定向到后端的正确 pod。请参阅本章后面的 *服务* 部分。'
- en: Finally, we have some default pods, which run various infrastructure services
    for the node. As we explored briefly in the previous chapter, the pods include
    services for **Domain Name System** (**DNS**), logging, and pod health checks.
    The default pod will run alongside our scheduled pods on every node.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一些默认的 pod，它们为节点运行各种基础设施服务。正如我们在上一章中简要探讨的那样，这些 pod 包括 **域名系统**（**DNS**）、日志记录和
    pod 健康检查的服务。默认的 pod 将与我们在每个节点上调度的 pod 并行运行。
- en: Note
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that in v1.0, **minion** was renamed to **node**, but there are still remnants
    of the term minion in some of the machine naming scripts and documentation that
    exists on the Web. For clarity, I've added the term minion in addition to node
    in a few places throughout the book.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 v1.0 中，**minion** 被重命名为 **node**，但在 Web 上仍然存在一些机器命名脚本和文档中的 minion 术语的残留。为了清晰起见，我在整本书中的一些地方除了
    node 外还添加了 minion 术语。
- en: Core constructs
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心构造
- en: Now, let's dive a little deeper and explore some of the core abstractions Kubernetes
    provides. These abstractions will make it easier to think about our applications
    and ease the burden of life cycle management, high availability, and scheduling.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入一点，探索一些Kubernetes提供的核心抽象。这些抽象将使我们更容易思考我们的应用程序，并减轻生命周期管理、高可用性和调度的负担。
- en: Pods
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pods
- en: Pods allow you to keep related containers close in terms of the network and
    hardware infrastructure. Data can live near the application, so processing can
    be done without incurring a high latency from network traversal. Similarly, common
    data can be stored on volumes that are shared between a number of containers.
    Pods essentially allow you to logically group containers and pieces of our application
    stacks together.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Pods允许您在网络和硬件基础设施方面将相关的容器保持紧密。数据可以靠近应用程序，因此处理可以在不产生网络遍历的高延迟的情况下进行。同样，常见的数据可以存储在多个容器之间共享的卷上。Pods基本上允许您逻辑地将容器和应用程序堆栈的部分组合在一起。
- en: While pods may run one or more containers inside, the pod itself may be one
    of many that is running on a Kubernetes (minion) node. As we'll see, pods give
    us a logical group of containers that we can then replicate, schedule, and balance
    service endpoints across.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然pod内部可能运行一个或多个容器，但pod本身可能是运行在Kubernetes（minion）节点上的众多pod之一。正如我们将看到的，pod给了我们一个逻辑上的容器组，我们可以复制、调度，并在服务端点之间平衡。
- en: Pod example
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod示例
- en: Let's take a quick look at a pod in action. We will spin up a **Node.js** application
    on the cluster. You'll need a GCE cluster running for this, so see [Chapter 1](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 1. Kubernetes and Container Operations"), *Kubernetes and Container Operations*,
    under the *Our first cluster* section, if you don't already have one started.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下pod的运行情况。我们将在集群上启动一个**Node.js**应用程序。为此，您需要运行一个GCE集群，请参阅[第1章](part0015_split_000.html#E9OE1-22fbdd9ef660435ca6bcc0309f05b1b7
    "第1章 Kubernetes和容器操作")，“Kubernetes和容器操作”下的“我们的第一个集群”部分，如果您还没有启动一个。
- en: Now, let's make a directory for our definitions. In this example, I will create
    a folder in the `/book-examples` subfolder under our home directory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为我们的定义创建一个目录。在这个例子中，我将在我们的主目录下的`/book-examples`子目录中创建一个文件夹。
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Downloading the example code**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您在[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载所购买的所有Packt Publishing图书的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便文件直接通过电子邮件发送给您。
- en: 'Use your favorite editor to create the following file:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您喜欢的编辑器创建以下文件：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 2-1*: `nodejs-pod.yaml`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-1*：`nodejs-pod.yaml`'
- en: 'This file creates a pod name `node-js-pod` with the latest `bitnami/apache`
    container running on port `80`. We can check this using the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件创建了一个名为`node-js-pod`的pod，其中运行着最新的`bitnami/apache`容器，运行在端口`80`上。我们可以使用以下命令来检查：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us a pod running the specified container. We can see more information
    on the pod by running the following command:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们运行指定容器的pod。我们可以通过运行以下命令查看有关pod的更多信息：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You''ll see a good deal of information, such as the pod''s status, IP address,
    and even relevant log events. You''ll note the pod IP address is a private `10.x.x.x`
    address, so we cannot access it directly from our local machine. Not to worry
    as the `kubectl exec` command mirrors Docker''s `exec` functionality. Using this
    feature, we can run a command inside a pod:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到大量信息，例如pod的状态、IP地址，甚至相关的日志事件。您会注意到pod的IP地址是一个私有的`10.x.x.x`地址，因此我们无法直接从本地机器访问它。不用担心，因为`kubectl
    exec`命令反映了Docker的`exec`功能。使用这个功能，我们可以在pod内运行一个命令：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tip
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: By default, this runs a command in the first container it finds, but you can
    select a specific one using the `-c` argument.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这会在找到的第一个容器中运行一个命令，但您可以使用`-c`参数选择特定的容器。
- en: After running, the command you should see some HTML code. We'll have a prettier
    view later in the chapter, but for now, we can see that our pod is indeed running
    as expected.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，您应该看到一些HTML代码。我们将在本章后面有一个更漂亮的视图，但现在，我们可以看到我们的pod确实按预期运行。
- en: Labels
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签
- en: Labels give us another level of categorization, which becomes very helpful in
    terms of everyday operations and management. Similar to tags, labels can be used
    as the basis of service discovery as well as a useful grouping tool for day-to-day
    operations and management tasks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 标签给了我们另一个层次的分类，这在日常操作和管理方面非常有帮助。类似于标签，标签也可以作为服务发现的基础，以及日常操作和管理任务的有用分组工具。
- en: Labels are just simple key-value pairs. You will see them on pods, replication
    controllers, services, and so on. The label acts as a selector and tells Kubernetes
    which resources to work with for a variety of operations. Think of it as a *filtering*
    option.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 标签只是简单的键值对。您会在pod、复制控制器、服务等上看到它们。标签充当选择器，并告诉Kubernetes对于各种操作应该使用哪些资源。把它想象成一个*过滤*选项。
- en: We will take a look at labels more in depth later in this chapter, but first,
    we will explore the remaining two constructs, services, and replication controllers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更深入地了解标签，但首先，我们将探索剩下的两个构造，服务和复制控制器。
- en: The container's afterlife
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器的来世
- en: As anyone in operations can attest, failures happen all the time. Containers
    and pods can and will crash, become corrupted, or maybe even just get accidentally
    shut off by a clumsy admin poking around on one of the nodes. Strong policy and
    security practices like enforcing least privilege curtail some of these incidents,
    but "involuntary workload slaughter happens" and is simply a fact of operations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如运维人员所证明的，故障经常发生。容器和pod可能会崩溃、损坏，甚至可能会被一个手忙脚乱的管理员在节点上无意中关闭。强大的策略和安全实践，比如强制执行最小特权，可以遏制一些这样的事件，但是“不经意的工作负载屠杀”是运维的一个事实。
- en: Luckily, Kubernetes provides two very valuable constructs to keep this somber
    affair all tidied up behind the curtains. Services and replication controllers
    give us the ability to keep our applications running with little interruption
    and graceful recovery.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes提供了两个非常有价值的构造，来将这个沉重的事情整理得井井有条。服务和复制控制器让我们能够保持我们的应用程序运行，几乎没有中断和优雅的恢复。
- en: Services
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务
- en: Services allow us to abstract access away from the consumers of our applications.
    Using a reliable endpoint, users and other programs can access pods running on
    your cluster seamlessly.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 服务让我们能够将对应用程序的访问抽象化。使用可靠的端点，用户和其他程序可以无缝地访问在您的集群上运行的pod。
- en: K8s achieves this by making sure that every node in the cluster runs a proxy
    named kube-proxy. As the name suggests, kube-proxy's job is to proxy communication
    from a service endpoint back to the corresponding pod that is running the actual
    application.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: K8s通过确保集群中的每个节点都运行一个名为kube-proxy的代理来实现这一点。顾名思义，kube-proxy的工作是将服务端点的通信代理回到运行实际应用程序的相应pod。
- en: '![Services](../images/00020.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: ！[服务](../images/00020.jpeg)
- en: Figure 2.2\. The kube-proxy architecture
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2。kube-proxy架构
- en: Membership in the service load balancing pool is determined by the use of selectors
    and labels. Pods with matching labels are added to the list of candidates where
    the service forwards traffic. A virtual IP address and port are used as the entry
    point for the service, and traffic is then forwarded to a random pod on a target
    port defined by either K8s or your definition file.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 服务负载均衡池中的成员资格是通过使用选择器和标签确定的。具有匹配标签的pod被添加到候选列表中，服务将流量转发到其中。虚拟IP地址和端口被用作服务的入口点，然后将流量转发到由K8s或您的定义文件定义的目标端口上的随机pod。
- en: Updates to service definitions are monitored and coordinated from the K8s cluster
    master and propagated to the **kube-proxy daemons** running on each node.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对服务定义的更新是从K8s集群主节点监视和协调的，并传播到运行在每个节点上的**kube-proxy守护程序**。
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: At the moment, kube-proxy is running on the node host itself. There are plans
    to containerize this and the kubelet by default in the future.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，kube-proxy正在节点主机上运行。未来计划默认将其容器化以及kubelet。
- en: Replication controllers
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制控制器
- en: '**Replication controllers** (**RCs**), as the name suggests, manage the number
    of nodes that a pod and included container images run on. They ensure that an
    instance of an image is being run with the specific number of copies.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**复制控制器**（**RCs**）顾名思义，管理pod和包含的容器映像运行的节点数量。它们确保特定数量的映像实例正在运行。'
- en: As you start to operationalize your containers and pods, you'll need a way to
    roll out updates, scale the number of copies running (both up and down), or simply
    ensure that at least one instance of your stack is always running. RCs create
    a high-level mechanism to make sure that things are operating correctly across
    the entire application and cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始使您的容器和pod运行起来时，您需要一种方法来推出更新，扩展运行的副本数量（增加和减少），或者简单地确保您的堆栈至少有一个实例一直在运行。RCs创建了一个高级机制，以确保整个应用程序和集群的正常运行。
- en: RCs are simply charged with ensuring that you have the desired scale for your
    application. You define the number of pod replicas you want running and give it
    a template for how to create new pods. Just like services, we will use selectors
    and labels to define a pod's membership in a replication controller.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: RCs负责确保您的应用程序具有所需的规模。您定义要运行的pod副本的数量，并为其提供创建新pod的模板。就像服务一样，我们将使用选择器和标签来定义复制控制器中pod的成员资格。
- en: Tip
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Kubernetes doesn't require the strict behavior of the replication controller.
    In fact, version 1.1 has a **job controller** in beta that can be used for short
    lived workloads which allow jobs to be run to a completion state.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes不要求复制控制器的严格行为。事实上，版本1.1中有一个**作业控制器**处于测试阶段，可以用于短期工作负载，允许作业运行到完成状态。
- en: Our first Kubernetes application
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个Kubernetes应用程序
- en: Before we move on, let's take a look at these three concepts in action. Kubernetes
    ships with a number of examples installed, but we will create a new example from
    scratch to illustrate some of the concepts.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们看看这三个概念是如何运作的。Kubernetes附带了许多示例安装，但我们将从头开始创建一个新示例，以阐明一些概念。
- en: We've already created a pod definition file, but as we learned, there are many
    advantages to running our pods via replication controllers. Again, using the `book-examples/02_example`
    folder we made earlier, we will create some definition files and start a cluster
    of Node.js servers using a replication controller approach. Additionally, we'll
    add a public face to it with a load-balanced service.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个pod定义文件，但正如我们所学到的，通过复制控制器来运行我们的pod有许多优势。同样，使用我们之前创建的`book-examples/02_example`文件夹，我们将创建一些定义文件，并使用复制控制器方法启动一个Node.js服务器集群。此外，我们将通过负载均衡服务为其添加一个公共界面。
- en: 'Use your favorite editor to create the following file:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您喜欢的编辑器创建以下文件：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 2-2*: `nodejs-controller.yaml`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-2*：`nodejs-controller.yaml`'
- en: 'This is the first resource definition file for our cluster, so let''s take
    a closer look. You''ll note that it has four first-level elements (`kind`, `apiVersion`,
    `metadata`, and `spec`). These are common among all top-level Kubernetes resource
    definitions:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们集群的第一个资源定义文件，让我们仔细看一下。您会注意到它有四个一级元素（`kind`，`apiVersion`，`metadata`和`spec`）。这些在所有顶级Kubernetes资源定义中都是常见的：
- en: '`Kind` tells K8s what type of resource we are creating. In this case, the type
    is `ReplicationController`. The `kubectl` script uses a single `create` command
    for all types of resources. The benefit here is that you can easily create a number
    of resources of various types without needing to specify individual parameters
    for each type. However, it requires that the definition files can identify what
    it is they are specifying.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Kind`告诉K8s我们正在创建的资源类型。在这种情况下，类型是`ReplicationController`。`kubectl`脚本使用单个`create`命令来创建所有类型的资源。这里的好处是您可以轻松地创建各种类型的资源，而无需为每种类型指定单独的参数。但是，这要求定义文件能够识别其所指定的内容。'
- en: '`ApiVersion` simply tells Kubernetes which version of the schema we are using.
    All examples in this book will be on `v1`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ApiVersion`简单地告诉Kubernetes我们正在使用的模式的版本。本书中的所有示例都将在`v1`上。'
- en: '`Metadata` is where we will give the resource a name and also specify labels
    that will be used to search and select resources for a given operation. The metadata
    element also allows you to create annotations, which are for nonidentifying information
    that might be useful for client tools and libraries.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Metadata`是我们将为资源指定名称并且还可以指定用于搜索和选择给定操作的资源的标签的地方。metadata元素还允许您创建注释，这些注释用于非标识信息，可能对客户端工具和库有用。'
- en: Finally, we have `spec`, which will vary based on the `kind` or type of resource
    we are creating. In this case, it's `ReplicationController`, which ensures the
    desired number of pods are running. The `replicas` element defines the desired
    number of pods, the `selector` tells the controller which pods to watch, and finally,
    the `template` element defines a template to launch a new pod. The `template`
    section contains the same pieces we saw in our pod definition earlier. An important
    thing to note is that the `selector` values need to match the `labels` values
    specified in the pod template. Remember that this matching is used to select the
    pods being managed.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有`spec`，它将根据我们正在创建的`kind`或资源类型而变化。在这种情况下，它是`ReplicationController`，它确保所需数量的pod正在运行。`replicas`元素定义了所需的pod数量，`selector`告诉控制器要监视哪些pod，最后，`template`元素定义了启动新pod的模板。`template`部分包含了我们之前在pod定义中看到的相同部分。需要注意的一点是，`selector`的值需要与pod模板中指定的`labels`值匹配。请记住，此匹配用于选择正在被管理的pod。
- en: 'Now, let''s take a look at the service definition:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下服务定义：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 2-3*: `nodejs-rc-service.yaml`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-3*：`nodejs-rc-service.yaml`'
- en: The YAML here is similar to the `ReplicationController`. The main difference
    is seen in the service `spec` element. Here, we define the `Service` type, listening
    `port`, and `selector`, which tells the `Service` proxy which pods can answer
    the service.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的YAML与`ReplicationController`类似。主要区别在于服务`spec`元素。在这里，我们定义了`Service`类型，监听`port`和`selector`，告诉`Service`代理哪些pod可以回答该服务。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Kubernetes supports both YAML and JSON formats for definition files.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持定义文件的YAML和JSON格式。
- en: 'Create the Node.js express replication controller:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Node.js express复制控制器：
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives us a replication controller that ensures that three copies of the
    container are always running:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个复制控制器，确保始终运行三个容器的副本：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: On GCE, this will create an external load balancer and forwarding rules, but
    you may need to add additional firewall rules. In my case, the firewall was already
    open for port `80`. However, you may need to open this port, especially if you
    deploy a service with ports other than `80` and `443`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCE上，这将创建一个外部负载均衡器和转发规则，但您可能需要添加额外的防火墙规则。在我的情况下，防火墙已经为端口`80`打开。但是，您可能需要打开此端口，特别是如果部署了除`80`和`443`之外的端口的服务。
- en: 'OK, now we have a running service, which means that we can access the Node.js
    servers from a reliable URL. Let''s take a look at our running services:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在我们有一个正在运行的服务，这意味着我们可以从可靠的URL访问Node.js服务器。让我们看看我们正在运行的服务：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图是上一个命令的结果：
- en: '![Our first Kubernetes application](../images/00021.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个Kubernetes应用](../images/00021.jpeg)'
- en: Figure 2.3\. Services listing
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 服务列表
- en: 'In the preceding figure (Figure 2.3), you should note that the **node-js**
    service running and, in the **IP(S)** column, you should have both a private and
    a public (**130.211.186.84** in the screenshot) IP address. Let''s see if we can
    connect by opening up the public address in a browser:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图（图2.3）中，您应该注意到正在运行的**node-js**服务，并且在**IP(S)**列中，您应该有一个私有和一个公共（屏幕截图中的130.211.186.84）IP地址。让我们尝试通过在浏览器中打开公共地址来连接：
- en: '![Our first Kubernetes application](../images/00022.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个Kubernetes应用](../images/00022.jpeg)'
- en: Figure 2.4\. Container info application
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 容器信息应用
- en: You should see something like Figure 2.4\. If we visit multiple times, you should
    note that the container name changes. Essentially, the service load balancer is
    rotating between available pods on the backend.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似图2.4的东西。如果我们多次访问，您应该注意到容器名称的更改。基本上，服务负载均衡器在后端可用的pod之间进行轮换。
- en: Note
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Browsers usually cache web pages, so to really see the container name change
    you may need to clear your cache or use a proxy like this one:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器通常会缓存网页，所以要真正看到容器名称的更改，您可能需要清除缓存或使用像这样的代理：
- en: '[https://hide.me/en/proxy](https://hide.me/en/proxy)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://hide.me/en/proxy](https://hide.me/en/proxy)'
- en: 'Let''s try playing chaos monkey a bit and kill off a few containers to see
    what Kubernetes does. In order to do this, we need to see where the pods are actually
    running. First, let''s list our pods:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试玩一下混沌猴，杀死一些容器，看看Kubernetes会做什么。为了做到这一点，我们需要看到pod实际在哪里运行。首先，让我们列出我们的pod：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图是上一个命令的结果：
- en: '![Our first Kubernetes application](../images/00023.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个Kubernetes应用](../images/00023.jpeg)'
- en: Figure 2.5\. Currently running pods
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 当前运行的pod
- en: 'Now, let''s get some more details on one of the pods running a `node-js` container.
    You can do this with the `describe` command with one of the pod names listed in
    the last command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对运行`node-js`容器的一个pod获取更多细节。您可以使用上一个命令中列出的一个pod名称来执行`describe`命令：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![Our first Kubernetes application](../images/00024.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个Kubernetes应用](../images/00024.jpeg)'
- en: Figure 2.6\. Pod description
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6\. Pod描述
- en: 'You should see the preceding output. The information we need is the **Node:**
    section. Let''s use the node name to **SSH** (short for **Secure Shell**) into
    the (minion) node running this workload:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到前面的输出。我们需要的信息是**Node:**部分。让我们使用节点名称**SSH**（**安全外壳**的缩写）进入运行此工作负载的（从属）节点：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once SSHed into the node, if we run a `sudo docker ps` command, we should see
    at least two containers: one running the `pause` image and one running the actual
    `node-express-info` image. You may see more if the K8s scheduled more than one
    replica on this node. Let''s grab the container ID of the `jonbaier/node-express-info`
    image (not `gcr.io/google_containers/pause`) and kill it off to see what happens.
    Save this container ID somewhere for later:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过SSH进入节点，如果我们运行`sudo docker ps`命令，我们应该至少看到两个容器：一个运行`pause`镜像，另一个运行实际的`node-express-info`镜像。如果K8s在此节点上安排了多个副本，您可能会看到更多。让我们获取`jonbaier/node-express-info`镜像的容器ID（而不是`gcr.io/google_containers/pause`），并将其杀死以查看会发生什么。将此容器ID保存在某个地方以备后用：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Unless you are really quick you'll probably note that there is still a `node-express-info`
    container running, but look closely and you'll note that the `container id` is
    different and the creation time stamp shows only a few seconds ago. If you go
    back to the service URL, it is functioning like normal. Go ahead and exit the
    SSH session for now.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您非常迅速，否则您可能会注意到仍然有一个`node-express-info`容器在运行，但仔细观察，您会注意到`容器ID`不同，并且创建时间戳显示仅几秒前。如果您返回到服务URL，它会像正常一样运行。现在可以退出SSH会话。
- en: Here, we are already seeing Kubernetes playing the role of on-call operations
    ensuring that our application is always running.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经看到Kubernetes扮演了值班运维的角色，确保我们的应用程序始终在运行。
- en: 'Let''s see if we can find any evidence of the outage. Go to the **Events**
    page in the Kubernetes UI. You can find it on the main K8s dashboard under **Events**
    in the **Views** menu. Alternatively, you can just use the following URL, adding
    `your master ip`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否可以找到任何中断的证据。转到Kubernetes UI中的**Events**页面。您可以在主K8s仪表板的**Views**菜单下的**Events**中找到它。或者，您可以使用以下URL，添加`您的主节点IP`：
- en: '`https://`**`<your master ip>`**`/api/v1/proxy/namespaces/kube-system/services/kube-ui/#/dashboard/events`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`https://`**`<您的主节点IP>`**`/api/v1/proxy/namespaces/kube-system/services/kube-ui/#/dashboard/events`'
- en: 'You will see a screen similar to the following screenshot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到类似以下截图的屏幕：
- en: '![Our first Kubernetes application](../images/00025.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个Kubernetes应用](../images/00025.jpeg)'
- en: Figure 2.7\. Kubernetes UI event page
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7\. Kubernetes UI事件页面
- en: You should see three recent events. First, Kubernetes pulls the image. Second,
    it creates a new container with the pulled image. Finally, it starts that container
    again. You'll note that, from the time stamps, this all happens in less than a
    second. Time taken may vary based on cluster size and image pulls, but the recovery
    is very quick.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到三个最近的事件。首先，Kubernetes拉取镜像。其次，它使用拉取的镜像创建一个新的容器。最后，它再次启动该容器。您会注意到，从时间戳来看，所有这些都发生在不到一秒的时间内。所需的时间可能会根据集群大小和镜像拉取而有所不同，但恢复非常迅速。
- en: More on labels
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于标签的内容
- en: 'As mentioned previously, labels are just simple key-value pairs. They are available
    on pods, replication controllers, services, and more. If you recall our service
    YAML, in *Listing 2-3*: `nodejs-rc-service.yaml`, there was a `selector` attribute.
    The `selector` tells Kubernetes which labels to use in finding pods to forward
    traffic for that service.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，标签只是简单的键值对。它们适用于pod、复制控制器、服务等。如果您回忆一下我们的服务YAML，在*清单2-3*：`nodejs-rc-service.yaml`中，有一个`selector`属性。`selector`告诉Kubernetes在查找要为该服务转发流量的pod时使用哪些标签。
- en: 'K8s allows users to work with labels directly on replication controllers and
    services. Let''s modify our replicas and services to include a few more labels.
    Once again, use your favorite editor and create these two files as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: K8s允许用户直接在复制控制器和服务上使用标签。让我们修改我们的副本和服务，包括一些额外的标签。再次使用您喜欢的编辑器，并创建以下两个文件：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '*Listing 2-4*: `nodejs-labels-controller.yaml`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-4*：`nodejs-labels-controller.yaml`'
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Listing 2-5*: `nodejs-labels-service.yaml`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-5*：`nodejs-labels-service.yaml`'
- en: 'Create the replication controller and service as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 创建复制控制器和服务如下：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s take a look at how we can use labels in everyday management. The following
    table shows us the options to select labels:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何在日常管理中使用标签。以下表格向我们展示了选择标签的选项：
- en: '| Operators | Description | Example |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 运算符 | 描述 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `= or ==` | You can use either style to select keys with values equal to
    the string on the right | `name = apache` |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `=或==` | 您可以使用任一样式来选择键值等于右侧字符串的键 | `name = apache` |'
- en: '| `!=` | Select keys with values that do not equal the string on the right
    | `Environment != test` |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `!=` | 选择键值不等于右侧字符串的键 | `Environment != test` |'
- en: '| `In` | Select resources whose labels have keys with values in this set |
    `tier in (web, app)` |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: 选择具有键值在此集合中的标签的资源 | `tier in (web, app)` |
- en: '| `Notin` | Select resources whose labels have keys with values not in this
    set | `tier not in (lb, app)` |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: 选择具有键值不在此集合中的标签的资源 | `tier not in (lb, app)` |
- en: '| `<Key name>` | Use a key name only to select resources whose labels contain
    this key | `tier` |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `<键名>` | 仅使用键名来选择包含此键的标签的资源 | `tier` |'
- en: '*Table 1: Label selectors*'
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表1：标签选择器*'
- en: 'Let''s try looking for replicas with `test` deployments:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试查找具有`test`部署的副本：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图是上述命令的结果：
- en: '![More on labels](../images/00026.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![标签更多信息](../images/00026.jpeg)'
- en: Figure 2.8\. Replication controller listing
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8。复制控制器清单
- en: 'You''ll notice that it only returns the replication controller we just started.
    How about services with a label named `component`? Use the following command:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到它只返回我们刚刚启动的复制控制器。带有名为`component`的标签的服务呢？使用以下命令：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图是上述命令的结果：
- en: '![More on labels](../images/00027.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![标签更多信息](../images/00027.jpeg)'
- en: Figure 2.9\. Listing of services with a label named "component"
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9。带有名为"component"的标签的服务清单
- en: 'Here, we see the core Kubernetes service only. Finally, let''s just get the
    `node-js` servers we started in this chapter. See the following command:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只看到了核心Kubernetes服务。最后，让我们只获取本章中启动的`node-js`服务器。看以下命令：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图是上述命令的结果：
- en: '![More on labels](../images/00028.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![标签更多信息](../images/00028.jpeg)'
- en: Figure 2.10\. Listing of services with a label name and a value of "node-js"
    or "nodejs-labels"
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10。带有名称为"node-js"或"nodejs-labels"的标签的服务清单
- en: 'Additionally, we can perform management tasks across a number of pods and services.
    For example, we can kill all replication controllers that are part of the `demo`
    deployment (if we had any running) as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以在许多pod和服务之间执行管理任务。例如，我们可以杀死所有属于`demo`部署的复制控制器（如果有运行的话），如下所示：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Otherwise, kill all services that are not part of a `production` or `test`
    deployment (again, if we had any running), as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，杀死所有不属于`production`或`test`部署的服务（再次，如果有运行的话），如下所示：
- en: '[PRE24]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It's important to note that while label selection is quite helpful in day-to-day
    management tasks it does require proper deployment hygiene on our part. We need
    to make sure that we have a tagging standard and that it is actively followed
    in the resource definition files for everything we run on Kubernetes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，虽然标签选择在日常管理任务中非常有帮助，但这需要我们正确部署。我们需要确保我们有一个标签标准，并且在我们在Kubernetes上运行的所有资源定义文件中积极遵循它。
- en: Tip
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'While we used service definition YAML files to create our services thus far,
    you can actually create them using a `kubectl` command only. To try this out,
    first run the `get pods` command and get one of the `node-js` pod names. Next,
    use the following `expose` command to create a service endpoint for just that
    pod:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用服务定义的YAML文件来创建我们的服务，但实际上，你可以只使用`kubectl`命令来创建它们。要尝试这样做，首先运行`get pods`命令，并获取一个`node-js`
    pod名称。接下来，使用以下`expose`命令为该pod创建一个服务端点：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will create a service named `testing-vip` and also a public `vip` (load
    balancer IP) that can be used to access this pod over port `80`. There''s a number
    of other optional parameters that can be used. These can be found with the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为`testing-vip`的服务，还将创建一个公共的`vip`（负载均衡器IP），可以用来通过端口`80`访问这个pod。还有许多其他可选参数可以使用。这些可以在以下找到：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Health checks
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: Kubernetes provides two layers of health checking. First, in the form of HTTP
    or TCP checks, K8s can attempt to connect to a particular endpoint and give a
    status of healthy on a successful connection. Second, application-specific health
    checks can be performed using command line scripts.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了两层健康检查。首先，以HTTP或TCP检查的形式，K8s可以尝试连接到特定的端点，并在成功连接时给出健康状态。其次，可以使用命令行脚本执行特定于应用程序的健康检查。
- en: 'Let''s take a look at a few health checks in action. First, we''ll create a
    new controller with a health check:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些健康检查的实际操作。首先，我们将创建一个带有健康检查的新控制器：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*Listing 2-6*: `nodejs-health-controller.yaml`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-6*：`nodejs-health-controller.yaml`'
- en: Note the addition of the `livenessprobe` element. This is our core health check
    element. From there, we can specify `httpGet`, `tcpScoket`, or `exec`. In this
    example, we use `httpGet` to perform a simple check for a URI on our container.
    The probe will check the path and port specified and restart the pod if it doesn't
    successfully return.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`livenessprobe`元素的添加。这是我们的核心健康检查元素。从那里，我们可以指定`httpGet`、`tcpScoket`或`exec`。在这个例子中，我们使用`httpGet`来对我们容器上的URI执行简单的检查。探测器将检查指定的路径和端口，并在没有成功返回时重新启动pod。
- en: Tip
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Status codes between `200` and `399` are all considered healthy by the probe.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 探测器认为状态码在`200`和`399`之间的都是健康的。
- en: Finally, `initialDelaySeconds` gives us the flexibility to delay health checks
    until the pod has finished initializing. `timeoutSeconds` is simply the timeout
    value for the probe.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`initialDelaySeconds`给了我们灵活性，可以延迟健康检查，直到pod完成初始化。`timeoutSeconds`只是探测的超时值。
- en: 'Let''s use our new health check-enabled controller to replace the old `node-js`
    RC. We can do this using the `replace` command, which will replace the replication
    controller definition:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的新的启用了健康检查的控制器来替换旧的`node-js` RC。我们可以使用`replace`命令来执行这个操作，它将替换复制控制器的定义：
- en: '[PRE28]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Replacing the RC on it's own won't replace our containers because it still has
    three healthy pods from our first run. Let's kill off those pods and let the updated
    `ReplicationController` replace them with containers that have health checks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅替换RC本身不会替换我们的容器，因为它仍然有三个来自我们第一次运行的健康Pod。让我们杀死这些Pod，并让更新的“ReplicationController”用具有健康检查的容器替换它们。
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, after waiting a minute or two, we can list the pods in an RC and grab
    one of the pod IDs to inspect a bit deeper with the `describe` command:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，等待一两分钟后，我们可以列出RC中的pod，并获取一个pod ID，然后使用“describe”命令进行更深入的检查：
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是上述命令的结果：
- en: '![Health checks](../images/00029.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![健康检查](../images/00029.jpeg)'
- en: Figure 2.11\. Description of "node-js" replication controller
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11。 "node-js"复制控制器的描述
- en: 'Then, using the following command for one of the pods:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，针对其中一个pod使用以下命令：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是上述命令的结果：
- en: '![Health checks](../images/00030.jpeg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![健康检查](../images/00030.jpeg)'
- en: Figure 2.12\. Description of "node-js-1m3cs" pod
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12。 "node-js-1m3cs" pod的描述
- en: 'Depending on your timing, you will likely have a number of events for the pod.
    Within a minute or two, you''ll note a pattern of *killing*, *started*, and *created*
    events repeating over and over again. You should also see an unhealthy event described
    as **Liveness probe failed: Cannot GET /status/**. This is our health check failing
    because we don''t have a page responding at `/status`.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '根据您的时间安排，您可能会对pod有许多事件。一两分钟后，您会注意到一系列“killing”、“started”和“created”事件不断重复。您还应该看到一个描述为**Liveness
    probe failed: Cannot GET /status/**的不健康事件。这是因为我们在“/status”上没有响应页面，导致我们的健康检查失败。'
- en: You may note that if you open a browser to the service load balancer address,
    it still responds with a page. You can find the load balancer IP with a `kubectl
    get services` command.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到，如果您打开浏览器到服务负载均衡器地址，它仍然会响应一个页面。您可以使用“kubectl get services”命令找到负载均衡器IP。
- en: This is happening for a number of reasons. First, the health check is simply
    failing because `/status` doesn't exist, but the page where the service is pointed
    is still functioning normally. Second, the `livenessProbe` is only charged with
    restarting the container on a health check fail. There is a separate `readinessProbe`
    that will remove a container from the pool of pods answering service endpoints.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于多种原因造成的。首先，健康检查失败是因为“/status”不存在，但服务指向的页面仍然正常运行。其次，“livenessProbe”只负责在健康检查失败时重新启动容器。还有一个单独的“readinessProbe”，它将从回答服务端点的Pod池中移除容器。
- en: Let's modify the health check for a page that does exist in our container, so
    we have a proper health check. We'll also add a readiness check and point it to
    the nonexistent status page. Open the `nodejs-health-controller.yaml` file and
    modify the `spec` section to match *Listing 2-7* and save it as `nodejs-health-controller-2.yaml`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改健康检查，使其针对容器中存在的页面，这样我们就有了一个合适的健康检查。我们还将添加一个就绪检查，并将其指向不存在的状态页面。打开“nodejs-health-controller.yaml”文件，并修改“spec”部分以匹配*清单2-7*，然后将其保存为“nodejs-health-controller-2.yaml”。
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*Listing 2-7*: `nodejs-health-controller-2.yaml`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-7*：`nodejs-health-controller-2.yaml`'
- en: This time, we will delete the old RC, which will kill the pods with it, and
    create a new RC with our updated YAML file.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们将删除旧的RC，这将与之一起删除pod，并使用我们更新的YAML文件创建一个新的RC。
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now when we describe one of the pods, we only see the creation of the pod and
    the container. However, you''ll note that the service load balancer IP no longer
    works. If we run the `describe` command on one of the new nodes we''ll note a
    **Readiness probe failed** error message, but the pod itself continues running.
    If we change the readiness probe path to `path: /`, we will again be able to fulfill
    requests from the main service. Open up `nodejs-health-controller-2.yaml` in an
    editor and make that update now. Then, once again remove and recreate the replication
    controller:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '现在当我们描述其中一个pod时，我们只看到了pod和容器的创建。但是，您会注意到服务负载均衡器IP不再起作用。如果我们在新节点中运行`describe`命令，我们会注意到一个**就绪探针失败**的错误消息，但是pod本身仍在运行。如果我们将就绪探针路径更改为`path:
    /`，我们将再次能够满足主服务的请求。现在在编辑器中打开`nodejs-health-controller-2.yaml`并进行更新。然后，再次删除并重新创建复制控制器：'
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now the load balancer IP should work once again. Keep these pods around as we
    will use them again in [Chapter 3](part0028_split_000.html#QMFO2-22fbdd9ef660435ca6bcc0309f05b1b7
    "Chapter 3. Core Concepts – Networking, Storage, and Advanced Services"), *Core
    Concepts – Networking, Storage, and Advanced Services*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在负载均衡器IP应该再次起作用。保留这些pod，因为我们将在[第3章](part0028_split_000.html#QMFO2-22fbdd9ef660435ca6bcc0309f05b1b7
    "第3章。核心概念-网络、存储和高级服务")中再次使用它们，*核心概念-网络、存储和高级服务*。
- en: TCP checks
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCP检查
- en: 'Kubernetes also supports health checks via simple TCP socket checks and also
    with custom command-line scripts. The following snippets are examples of what
    both use cases look like in the YAML file:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还支持通过简单的TCP套接字检查和自定义命令行脚本进行健康检查。以下片段是两种用例在YAML文件中的示例：
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '*Listing 2-8*: *Health check using command-line script*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-8*：*使用命令行脚本进行健康检查*'
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*Listing 2-9*: *Health check using simple TCP Socket connection*'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-9*：*使用简单TCP套接字连接进行健康检查*'
- en: Life cycle hooks or graceful shutdown
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生命周期钩子或优雅关闭
- en: As you run into failures in real-life scenarios, you may find that you want
    to take additional action before containers are shutdown or right after they are
    started. Kubernetes actually provides life cycle hooks for just this kind of use
    case.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中遇到故障时，您可能会发现希望在容器关闭之前或刚启动后采取额外的操作。Kubernetes实际上为这种情况提供了生命周期钩子。
- en: 'The following example controller definition defines both a `postStart` and
    a `preStop` action to take place before Kubernetes moves the container into the
    next stage of its life cycle¹:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例控制器定义了`postStart`和`preStop`动作，在Kubernetes将容器移入其生命周期的下一阶段之前执行¹：
- en: '[PRE37]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '*Listing 2-10*: `apache-hooks-controller.yaml`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-10*：`apache-hooks-controller.yaml`'
- en: You'll note for the `postStart` hook we define an `httpGet` action, but for
    the `preStop` hook, I define an `exec` action. Just as with our health checks,
    the `httpGet` action attempts to make an HTTP call to the specific endpoint and
    port combination while the `exec` action runs a local command in the container.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，对于`postStart`钩子，我们定义了一个`httpGet`动作，但是对于`preStop`钩子，我定义了一个`exec`动作。就像我们的健康检查一样，`httpGet`动作尝试对特定的端点和端口组合进行HTTP调用，而`exec`动作在容器中运行本地命令。
- en: 'The `httpGet` and `exec` action are both supported for the `postStart` and
    `preStop` hooks. In the case of `preStop`, a parameter named `reason` will be
    sent to the handler as a parameter. See the following table (Table 2.1) for valid
    values:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`httpGet`和`exec`动作都支持`postStart`和`preStop`钩子。对于`preStop`，将一个名为`reason`的参数作为参数发送到处理程序。有关有效值，请参阅以下表（表2.1）：'
- en: '| Reason parameter | Failure Description |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 原因参数 | 失败描述 |'
- en: '| --- | --- |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Delete** | Delete command issued via `kubectl` or the API |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| **删除** | 通过`kubectl`或API发出的删除命令 |'
- en: '| **Health** | Health check fails |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| **健康** | 健康检查失败 |'
- en: '| **Dependency** | Dependency failure such as a disk mount failure or a default
    infrastructure pod crash |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| **依赖** | 依赖失败，如磁盘挂载失败或默认基础设施pod崩溃 |'
- en: '*Table 2.1\. Valid preStop reasons¹*'
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表2.1\. 有效的preStop原因¹*'
- en: It's important to note that hook calls are delivered *at least once*. Therefore,
    any logic in the action should gracefully handles multiple calls. Another important
    note is that `postStart` runs before a pod enters its ready state. If the hook
    itself fails, the pod will be considered unhealthy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，挂钩调用至少会传递一次。因此，操作中的任何逻辑都应该优雅地处理多次调用。另一个重要的注意事项是`postStart`在pod进入就绪状态之前运行。如果挂钩本身失败，pod将被视为不健康。
- en: Application scheduling
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序调度
- en: Now that we understand how to run containers in pods and even recover from failure,
    it may be useful to understand how new containers are scheduled on our cluster
    nodes.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何在pod中运行容器，甚至从故障中恢复，了解如何在我们的集群节点上调度新容器可能是有用的。
- en: As mentioned earlier, the default behavior for the Kubernetes scheduler is to
    spread container replicas across the nodes in our cluster. In the absence of all
    other constraints, the scheduler will place new pods on nodes with the least number
    of other pods belonging to matching services or replication controllers.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Kubernetes调度程序的默认行为是在集群中的节点上分布容器副本。在没有其他约束条件的情况下，调度程序将在具有匹配服务或复制控制器的其他pod数量最少的节点上放置新的pod。
- en: Additionally, the scheduler provides the ability to add constraints based on
    resources available to the node. Today, that includes minimum CPU and memory allocations.
    In terms of Docker, these use the **cpu-shares** and **memory limit flags** under
    the covers.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，调度程序提供了根据节点上可用资源添加约束的能力。目前，这包括最小的CPU和内存分配。在Docker方面，这些在底层使用**cpu-shares**和**memory
    limit flags**。
- en: When additional constraints are defined, Kubernetes will check a node for available
    resources. If a node does not meet all the constraints, it will move to the next.
    If no nodes can be found that meet the criteria, then we will see a scheduling
    error in the logs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义了额外的约束条件时，Kubernetes将检查节点上的可用资源。如果一个节点不满足所有约束条件，它将移动到下一个节点。如果找不到满足条件的节点，则我们将在日志中看到调度错误。
- en: The Kubernetes roadmap also has plans to support networking and storage. Because
    scheduling is such an important piece of overall operations and management for
    containers, we should expect to see many additions in this area as the project
    grows.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes路线图还计划支持网络和存储。因为调度对于容器的整体运营和管理非常重要，所以我们应该期望在项目发展过程中在这个领域看到许多增加。
- en: Scheduling example
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度示例
- en: 'Let''s take a look at a quick example of setting some resource limits. If we
    look at our K8s dashboard, we can get a quick snapshot of the current state of
    resource usage on our cluster using `https://`**`<your master ip>`**`/api/v1/proxy/namespaces/kube-system/services/kube-ui`,
    as shown in the following screenshot:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一个设置一些资源限制的例子。如果我们查看我们的K8s仪表板，我们可以使用`https://`**`<your master ip>`**`/api/v1/proxy/namespaces/kube-system/services/kube-ui`快速获取我们集群上资源使用的当前状态快照，如下面的屏幕截图所示：
- en: '![Scheduling example](../images/00031.jpeg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![调度示例](../images/00031.jpeg)'
- en: Figure 2.13\. Kube UI dashboard
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13\. Kube UI 仪表板
- en: 'In this case, we have fairly low CPU utilization, but a decent chunk of memory
    in use. Let''s see what happens when I try to spin up a few more pods, but this
    time, we will request 512 Mi for memory and 1500 m for the CPU. We''ll use 1500
    m to specify 1.5 CPUs, since each node only has 1 CPU, this should result in failure.
    Here''s an example of RC definition:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的CPU利用率相当低，但内存使用相当大。让我们看看当我尝试启动更多的pod时会发生什么，但这次，我们将请求512 Mi的内存和1500
    m的CPU。我们将使用1500 m来指定1.5个CPU，因为每个节点只有1个CPU，这应该会导致失败。以下是RC定义的示例：
- en: '[PRE38]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '*Listing 2-11*: `nodejs-constraints-controller.yaml`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单2-11*：`nodejs-constraints-controller.yaml`'
- en: 'To open the preceding file, use the following command:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 打开上述文件，请使用以下命令：
- en: '[PRE39]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The replication controller completes successfully, but if we run a `get pods`
    command, we''ll note the `node-js-constraints` pods are stuck in a pending state.
    If we look a little closer with the `describe pods/<pod-id>` command, we''ll note
    a scheduling error:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 复制控制器成功完成，但如果我们运行`get pods`命令，我们会注意到`node-js-constraints` pods处于挂起状态。如果我们使用`describe
    pods/<pod-id>`命令仔细观察一下，我们会注意到一个调度错误：
- en: '[PRE40]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是上述命令的结果：
- en: '![Scheduling example](../images/00032.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![调度示例](../images/00032.jpeg)'
- en: Figure 2.14\. Pod description
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14. Pod描述
- en: Note that the **failedScheduling** error listed in events is accompanied by
    **Failed for reason PodFitsResources and possibly others** on our screen. As you
    can see, Kubernetes could not find a fit in the cluster that met all the constraints
    we defined.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，事件中列出的**failedScheduling**错误伴随着我们屏幕上的**Failed for reason PodFitsResources
    and possibly others**。正如您所看到的，Kubernetes无法在集群中找到符合我们定义的所有约束的适配项。
- en: If we now modify our CPU constraint down to 500 m, and then recreate our replication
    controller, we should have all three pods running within a few moments.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在将CPU约束修改为500 m，然后重新创建我们的复制控制器，我们应该在几分钟内让所有三个pod运行起来。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We've taken a look at the overall architecture for Kubernetes as well as the
    core constructs provided to build your services and application stacks. You should
    have a better understanding of how these abstractions make it easier to manage
    the life cycle of your stack and/or services as a whole and not just the individual
    components. Additionally, we took a first-hand look at how to manage some simple
    day-to-day tasks using pods, services, and replication controllers. We also looked
    at how to use Kubernetes to automatically respond to outages via health checks.
    Finally, we explored the Kubernetes scheduler and some of the constraints users
    can specify to influence scheduling placement.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看了一下Kubernetes的整体架构，以及提供的核心构造，用于构建您的服务和应用程序堆栈。您应该更好地了解这些抽象如何使管理堆栈和/或服务的生命周期变得更容易，而不仅仅是单个组件。此外，我们首次了解了如何使用pod、服务和复制控制器来管理一些简单的日常任务。我们还看了一下如何使用Kubernetes来自动响应健康检查中的故障。最后，我们探讨了Kubernetes调度程序以及用户可以指定的一些约束，以影响调度放置。
- en: Footnotes
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚注
- en: ¹[https://github.com/GoogleCloudPlatform/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks](https://github.com/GoogleCloudPlatform/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ¹[https://github.com/GoogleCloudPlatform/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks](https://github.com/GoogleCloudPlatform/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks)
