- en: '*Chapter 2*: Kubernetes Networking'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第2章*：Kubernetes网络'
- en: 'When thousands of microservices are running in a Kubernetes cluster, you may
    be curious about how these microservices communicate with each other as well as
    with the internet. In this chapter, we will unveil all the communication paths
    in a Kubernetes cluster. We want you to not only know how the communication happens
    but to also look into the technical details with a security mindset: a regular
    communication channel can always be abused as part of the kill chain.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当成千上万的微服务在Kubernetes集群中运行时，您可能会好奇这些微服务如何相互通信以及与互联网通信。在本章中，我们将揭示Kubernetes集群中所有通信路径。我们希望您不仅了解通信是如何发生的，还要以安全意识查看技术细节：常规通信渠道总是可以作为kill链的一部分被滥用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Overview of the Kubernetes network model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络模型概述
- en: Communicating inside a pod
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在pod内部通信
- en: Communicating between pods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在pod之间通信
- en: Introducing the Kubernetes service
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入Kubernetes服务
- en: Introducing the CNI and CNI plugins
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入CNI和CNI插件
- en: Overview of the Kubernetes network model
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络模型概述
- en: Applications running on a Kubernetes cluster are supposed to be accessible either
    internally from the cluster or externally, from outside the cluster. The implication
    from the network's perspective is there may be a **Uniform Resource Identifier**
    (**URI**) or **Internet Protocol** (**IP**) address associated with the application.
    Multiple applications can run on the same Kubernetes worker node, but how can
    they expose themselves without conflicting with each other? Let's take a look
    at this problem together, and then dive into the Kubernetes network model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群上运行的应用程序应该可以从集群内部或外部访问。从网络的角度来看，这意味着应用程序可能与**统一资源标识符**（**URI**）或**互联网协议**（**IP**）地址相关联。多个应用程序可以在同一Kubernetes工作节点上运行，但它们如何在不与彼此冲突的情况下暴露自己呢？让我们一起来看看这个问题，然后深入了解Kubernetes网络模型。
- en: Port-sharing problems
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端口共享问题
- en: 'Traditionally, if there are two different applications running on the same
    machine where the machine IP is public and the two applications are publicly accessible,
    then the two applications cannot listen on the same port in the machine. If they
    both try to listen on the same port in the same machine, one application will
    not launch as the port is in use. A simple illustration of this is provided in
    the following diagram:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，如果在同一台机器上运行两个不同的应用程序，其中机器IP是公共的，并且这两个应用程序是公开访问的，那么这两个应用程序不能在机器上监听相同的端口。如果它们都尝试在同一台机器的相同端口上监听，由于端口被使用，一个应用程序将无法启动。下图提供了这个问题的简单说明：
- en: '![Figure 2.1 – Port-sharing conflict on node (applications)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1 - 节点上的端口共享冲突（应用程序）'
- en: '](image/B15566_02_001.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_001.jpg)'
- en: Figure 2.1 – Port-sharing conflict on node (applications)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 - 节点上的端口共享冲突（应用程序）
- en: 'In order to address the port-sharing confliction issue, the two applications
    need to use different ports. Obviously, the limitation here is that the two applications
    have to share the same IP address. What if they have their own IP address while
    still sitting on the same machine? This is the pure Docker approach. This helps
    if the application does not need to expose itself externally, as illustrated in
    the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决端口共享冲突问题，这两个应用程序需要使用不同的端口。显然，这里的限制是这两个应用程序必须共享相同的IP地址。如果它们有自己的IP地址，但仍然位于同一台机器上会怎样？这就是纯Docker的方法。如果应用程序不需要外部暴露自己，这将有所帮助，如下图所示：
- en: '![Figure 2.2 – Port-sharing conflict on node (containers)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 - 节点上的端口共享冲突（容器）'
- en: '](image/B15566_02_002.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_002.jpg)'
- en: Figure 2.2 – Port-sharing conflict on node (containers)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 - 节点上的端口共享冲突（容器）
- en: In the preceding diagram, both applications have their own IP address so that
    they can both listen on port **80**. They can communicate with each other as they
    are in the same subnet (for example, a Docker bridge). However, if both applications
    need to expose themselves externally through binding the container port to the
    host port, they can't bind on the same port **80**. At least one of the port bindings
    will fail. As shown in the preceding diagram, container **B** can't bind to host
    port **80** as the host port **80** is occupied by container **A**. The port-sharing
    confliction issue still exists.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，两个应用程序都有自己的IP地址，因此它们都可以监听端口80。它们可以相互通信，因为它们位于同一个子网中（例如，一个Docker桥接）。然而，如果两个应用程序都需要通过将容器端口绑定到主机端口来在外部公开自己，它们就不能绑定在相同的端口80上。至少一个端口绑定将失败。如上图所示，容器B无法绑定到主机端口80，因为主机端口80被容器A占用。端口共享冲突问题仍然存在。
- en: Dynamic port configuration brings a lot of complexity to the system regarding
    port allocation and application discovery; however, Kubernetes does not take this
    approach. Let's discuss the Kubernetes approach for solving this issue.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 动态端口配置给系统带来了很多复杂性，涉及端口分配和应用程序发现；然而，Kubernetes并不采取这种方法。让我们讨论一下Kubernetes解决这个问题的方法。
- en: Kubernetes network model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes网络模型
- en: In a Kubernetes cluster, every pod gets its own IP address. This means applications
    can communicate with each other at a pod level. The beauty of this design is that
    it offers a clean, backward-compatible model where pods act like **Virtual Machines**
    (**VMs**) or physical hosts from the perspective of port allocation, naming, service
    discovery, load balancing, application configuration, and migration. Containers
    inside the same pod share the same IP address. It's very unlikely that similar
    applications that use the same default port (Apache and nginx) will run inside
    the same pod. In reality, applications bundled inside the same container usually
    have a dependency or serve different purposes, and it is up to the application
    developers to bundle them together. A simple example would be that, in the same
    pod, there is a **HyperText Transfer Protocol** (**HTTP**) server or an nginx
    container to serve static files, and the main web application to serve dynamic
    content.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中，每个Pod都有自己的IP地址。这意味着应用程序可以在Pod级别相互通信。这种设计的美妙之处在于，它提供了一个清晰、向后兼容的模型，其中Pod在端口分配、命名、服务发现、负载平衡、应用程序配置和迁移方面的表现就像虚拟机（VM）或物理主机一样。同一Pod内的容器共享相同的IP地址。很少有类似的应用程序会在同一Pod内使用相同的默认端口（如Apache和nginx）。实际上，捆绑在同一容器内的应用程序通常具有依赖性或提供不同的目的，这取决于应用程序开发人员将它们捆绑在一起。一个简单的例子是，在同一个Pod中，有一个超文本传输协议（HTTP）服务器或一个nginx容器来提供静态文件，以及一个用于提供动态内容的主Web应用程序。
- en: 'Kubernetes leverages CNI plugins to implement the IP address allocation, management,
    and pod communication. However, all the plugins need to follow the two fundamental
    requirements listed here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes利用CNI插件来实现IP地址分配、管理和Pod通信。然而，所有插件都需要遵循以下两个基本要求：
- en: Pods on a node can communicate with all pods in all nodes without using **Network
    Address Translation** (**NAT**).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点上的Pod可以与所有节点上的所有Pod进行通信，而无需使用网络地址转换（NAT）。
- en: Agents such as `kubelet` can communicate with pods in the same node.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 诸如`kubelet`之类的代理可以与同一节点上的Pod进行通信。
- en: These two preceding requirements enforce the simplicity of migrating applications
    inside the VM to a pod.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个先前的要求强制了在虚拟机内迁移应用程序到Pod的简单性。
- en: 'The IP address assigned to each pod is a private IP address or a cluster IP
    address that is not publicly accessible. Then, how, can an application become
    publicly accessible without conflicting with other applications in the cluster?
    The Kubernetes service is the one that surfaces the internal application to the
    public. We will dive deeper into the Kubernetes service concept in later sections.
    For now, it will be useful to summarize the content of this chapter with a diagram,
    as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给每个pod的IP地址是一个私有IP地址或集群IP地址，不对公众开放。那么，一个应用程序如何在不与集群中的其他应用程序发生冲突的情况下变得对公众可访问呢？Kubernetes服务就是将内部应用程序暴露给公众的方式。我们将在后面的章节中更深入地探讨Kubernetes服务的概念。现在，用下面的图表总结本章内容将会很有用：
- en: '![Figure 2.3 – Service exposed to the internet'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 - 服务暴露给互联网'
- en: '](image/B15566_02_003.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_003.jpg)'
- en: Figure 2.3 – Service exposed to the internet
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 - 服务暴露给互联网
- en: 'In the previous diagram, there is a **k8s cluster** where there are four applications
    running in two pods: **Application A** and **Application B** are running in **Pod
    X**, and they share the same pod IP address—**100.97.240.188**—while they are
    listening on port **8080** and **9090** respectively. Similarly, **Application
    C** and **Application D** are running in **Pod Y** and listening on port **8000**
    and **9000** respectively. All these four applications are accessible from the
    public via the following public-facing Kubernetes services: **svc.a.com**, **svc.b.com**,
    **svc.c.com**, and **svc.d.com**. The pods (**X** and **Y** in this diagram) can
    be deployed in one single worker node or replicated across 1,000 nodes. However,
    it makes no difference from a user''s or a service''s perspective. Although the
    deployment in the diagram is quite unusual, there is still a need to deploy more
    than one container inside the same pod. It''s time to take a look into the containers''
    communication inside the same pod.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个图表中，有一个**k8s集群**，其中有四个应用程序在两个pod中运行：**应用程序A**和**应用程序B**在**Pod X**中运行，并共享相同的pod
    IP地址—**100.97.240.188**—它们分别在端口**8080**和**9090**上监听。同样，**应用程序C**和**应用程序D**在**Pod
    Y**中运行，并分别在端口**8000**和**9000**上监听。所有这四个应用程序都可以通过以下面向公众的Kubernetes服务进行访问：**svc.a.com**，**svc.b.com**，**svc.c.com**和**svc.d.com**。这个图表中的pod（X和Y）可以部署在一个单独的工作节点上，也可以在1000个节点上复制。然而，从用户或服务的角度来看，这并没有什么区别。尽管图表中的部署方式相当不寻常，但仍然需要在同一个pod内部部署多个容器。现在是时候来看看同一个pod内部容器之间的通信了。
- en: Communicating inside a pod
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在pod内部通信
- en: Containers inside the same pod share the same pod IP address. Usually, it is
    up to application developers to bundle the container images together and to resolve
    any possible resource usage conflicts such as port listening. In this section,
    we will dive into the technical details of how the communication happens among
    the containers inside the pod and will also highlight the communications that
    take place beyond the network level.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 同一个pod内的容器共享相同的pod IP地址。通常，将容器映像捆绑在一起并解决可能的资源使用冲突（如端口监听）是应用程序开发人员的责任。在本节中，我们将深入探讨容器内部通信的技术细节，并强调超出网络层面的通信。
- en: Linux namespaces and the pause container
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linux命名空间和暂停容器
- en: 'Linux namespaces are a feature of the Linux kernel to partition resources for
    isolation purposes. With namespaces assigned, a set of processes sees one set
    of resources, while another set of processes sees another set of resources. Namespaces
    are a major fundamental aspect of modern container technology. It is important
    for readers to understand this concept in order to know Kubernetes in depth. So,
    we set forth all the Linux namespaces with explanations. Since Linux kernel version
    4.7, there are seven kinds of namespaces, listed as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Linux命名空间是Linux内核的一个特性，用于分区资源以进行隔离。使用分配的命名空间，一组进程看到一组资源，而另一组进程看到另一组资源。命名空间是现代容器技术的一个重要基本方面。读者理解这个概念对于深入了解Kubernetes很重要。因此，我们列出了所有的Linux命名空间并进行了解释。自Linux内核版本4.7以来，有七种类型的命名空间，如下所示：
- en: '**cgroup**: Isolate cgroup and root directory. cgroup namespaces virtualize
    the view of a process''s cgroups. Each cgroup namespace has its own set of cgroup
    root directories.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cgroup**：隔离cgroup和根目录。cgroup命名空间虚拟化了进程cgroup的视图。每个cgroup命名空间都有自己的cgroup根目录集。'
- en: '**IPC**: Isolate System V **Interprocess Communication** (**IPC**) objects
    or **Portable Operating System Interface** (**POSIX**) message queues.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IPC**：隔离System V进程间通信（IPC）对象或POSIX消息队列。'
- en: '**Network**: Isolate network devices, protocol stacks, ports, IP routing tables,
    firewall rules, and more.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：隔离网络设备、协议栈、端口、IP路由表、防火墙规则等。'
- en: '**Mount**: Isolate mount points. Thus, the processes in each of the mount namespace
    instances will see distinct single-directory hierarchies.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挂载**：隔离挂载点。因此，每个挂载命名空间实例中的进程将看到不同的单目录层次结构。'
- en: '**PID**: Isolate **process IDs** (**PIDs**). Processes in different PID namespaces
    can have the same PID.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PID**：隔离进程ID（PIDs）。不同PID命名空间中的进程可以具有相同的PID。'
- en: '**User**: Isolate user IDs and group IDs, the root directory, keys, and capabilities.
    A process can have a different user and group ID inside and outside a user namespace.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户**：隔离用户ID和组ID、根目录、密钥和功能。在用户命名空间内外，进程可以具有不同的用户和组ID。'
- en: '**Unix Time Sharing (UTS)**: Isolate the two system identifiers: the hostname
    and **Network Information Service** (**NIS**) domain name.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unix时间共享（UTS）：隔离两个系统标识符：主机名和网络信息服务（NIS）域名。
- en: 'Though each of these namespaces is powerful and serves an isolation purpose
    on different resources, not all of them are adopted for containers inside the
    same pod. Containers inside the same pod share at least the same IPC namespace
    and network namespace; as a result, K8s needs to resolve potential conflicts in
    port usage. There will be a loopback interface created, as well as the virtual
    network interface, with an IP address assigned to the pod. A more detailed diagram
    will look like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个命名空间都很强大，并且在不同资源上提供隔离目的，但并非所有命名空间都适用于同一Pod内的容器。同一Pod内的容器至少共享相同的IPC命名空间和网络命名空间；因此，K8s需要解决端口使用可能的冲突。将创建一个回环接口，以及分配给Pod的IP地址的虚拟网络接口。更详细的图表将如下所示：
- en: '![Figure 2.4 – Containers inside a pod'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 - Pod内的容器'
- en: '](image/B15566_02_004.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_004.jpg)'
- en: Figure 2.4 – Containers inside a pod
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 - Pod内的容器
- en: In this diagram, there is one **Pause** container running inside the pod alongside
    containers **A** and **B**. If you **Secure Shell** (**SSH**) into a Kubernetes
    cluster node and run the `docker ps` command inside the node, you will see at
    least one container that was started with the `pause` command. The `pause` command
    suspends the current process until a signal is received. Basically, these containers
    do nothing but sleep. Despite the lack of activity, the **Pause** container plays
    a critical role in the pod. It serves as a placeholder to hold the network namespace
    for all other containers in the same pod. Meanwhile, the **Pause** container acquires
    an IP address for the virtual network interface that will be used by all other
    containers to communicate with each other and the outside world.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图表中，有一个**Pause**容器与容器**A**和**B**一起运行在同一个pod中。如果您通过**Secure Shell**（**SSH**）进入Kubernetes集群节点并在节点内运行`docker
    ps`命令，您将看到至少一个使用`pause`命令启动的容器。`pause`命令会暂停当前进程，直到接收到信号。基本上，这些容器什么也不做，只是休眠。尽管没有活动，**Pause**容器在pod中扮演着关键的角色。它作为一个占位符，为同一个pod中的所有其他容器持有网络命名空间。与此同时，**Pause**容器获取了一个IP地址，用于所有其他容器之间以及与外部世界通信的虚拟网络接口。
- en: Beyond network communication
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越网络通信
- en: We decide to go beyond network communication a little bit among the containers
    in the same pod. The reason for doing so is that the communication path could
    sometimes become part of the kill chain. Thus, it is very important to know the
    possible ways to communicate among entities. You will see more coverage of this
    in [*Chapter 3*](B15566_03_Final_ASB_ePub.xhtml#_idTextAnchor091), *Threat Modeling*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定在同一个pod中的容器之间稍微超越网络通信。这样做的原因是通信路径有时可能成为杀伤链的一部分。因此，了解实体之间可能的通信方式非常重要。您将在[*第3章*](B15566_03_Final_ASB_ePub.xhtml#_idTextAnchor091)中看到更多相关内容，*威胁建模*。
- en: 'Inside a pod, all containers share the same IPC namespace so that containers
    can communicate via the IPC object or a POSIX message queue. Besides the IPC channel,
    containers inside the same pod can also communicate via a shared mounted volume.
    The mounted volume could be a temporary memory, host filesystem, or cloud storage.
    If the volume is mounted by containers in the Pod, then containers can read and
    write the same files in the volume. Last but not least, in beta, since the 1.12
    Kubernetes release, the `shareProcessNamespace` feature finally graduates to stable
    in 1.17\. To allow containers within a pod to share a common PID namespace, users
    can simply set the `shareProcessNamespace` option in the Podspec. The result of
    this is that **Application A** in **Container A** is now able to see **Application
    B** in **Container B**. Since they''re both in the same PID namespace, they can
    communicate using signals such as SIGTERM, SIGKILL, and so on. This communication
    can be seen in the following diagram:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个pod内，所有容器共享相同的IPC命名空间，以便容器可以通过IPC对象或POSIX消息队列进行通信。除了IPC通道，同一个pod内的容器还可以通过共享挂载卷进行通信。挂载的卷可以是临时内存、主机文件系统或云存储。如果卷被Pod中的容器挂载，那么容器可以读写卷中的相同文件。最后但并非最不重要的是，在1.12
    Kubernetes版本的beta版中，`shareProcessNamespace`功能最终在1.17版本中稳定。用户可以简单地在Podspec中设置`shareProcessNamespace`选项，以允许pod内的容器共享一个公共PID命名空间。其结果是**Container
    A**中的**Application A**现在能够看到**Container B**中的**Application B**。由于它们都在相同的PID命名空间中，它们可以使用诸如SIGTERM、SIGKILL等信号进行通信。这种通信可以在以下图表中看到：
- en: '![Figure 2.5 – Possible communication between containers inside a pod'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 - pod内部容器之间可能的通信'
- en: '](image/B15566_02_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_005.jpg)'
- en: Figure 2.5 – Possible communication between containers inside a pod
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 - pod内部容器之间可能的通信
- en: As the previous diagram shows, containers inside the same pod can communicate
    to each other via a network, an IPC channel, a shared volume, and through signals.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: As the previous diagram shows, containers inside the same pod can communicate
    to each other via a network, an IPC channel, a shared volume, and through signals.
- en: Communicating between pods
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Communicating between pods
- en: 'Kubernetes pods are dynamic beings and ephemeral. When a set of pods is created
    from a deployment or a DaemonSet, each pod gets its own IP address; however, when
    patching happens or a pod dies and restarts, pods may have a new IP address assigned.
    This leads to two fundamental communication problems, given a set of pods (frontend)
    needs to communicate to another set of pods (backend), detailed as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kubernetes pods are dynamic beings and ephemeral. When a set of pods is created
    from a deployment or a DaemonSet, each pod gets its own IP address; however, when
    patching happens or a pod dies and restarts, pods may have a new IP address assigned.
    This leads to two fundamental communication problems, given a set of pods (frontend)
    needs to communicate to another set of pods (backend), detailed as follows:'
- en: Given that the IP addresses may change, what are the valid IP addresses of the
    target pods?
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Given that the IP addresses may change, what are the valid IP addresses of the
    target pods?
- en: Knowing the valid IP addresses, which pod should we communicate to?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knowing the valid IP addresses, which pod should we communicate to?
- en: Now, let's jump into the Kubernetes service as it is the solution for these
    two problems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Now, let's jump into the Kubernetes service as it is the solution for these
    two problems.
- en: The Kubernetes service
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: The Kubernetes service
- en: The Kubernetes service is an abstraction of a grouping of sets of pods with
    a definition of how to access the pods. The set of pods targeted by a service
    is usually determined by a selector based on pod labels. The Kubernetes service
    also gets an IP address assigned, but it is virtual. The reason to call it a virtual
    IP address is that, from a node's perspective, there is neither a namespace nor
    a network interface bound to a service as there is with a pod. Also, unlike pods,
    the service is more stable, and its IP address is less likely to be changed frequently.
    Sounds like we should be able to solve the two problems mentioned earlier. First,
    define a service for the target sets of pods with a proper selector configured;
    secondly, let some magic associated with the service decide which target pod is
    to receive the request. So, when we look at pod-to-pod communication again, we're
    in fact talking about pod-to-service (then to-pod) communication.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: The Kubernetes service is an abstraction of a grouping of sets of pods with
    a definition of how to access the pods. The set of pods targeted by a service
    is usually determined by a selector based on pod labels. The Kubernetes service
    also gets an IP address assigned, but it is virtual. The reason to call it a virtual
    IP address is that, from a node's perspective, there is neither a namespace nor
    a network interface bound to a service as there is with a pod. Also, unlike pods,
    the service is more stable, and its IP address is less likely to be changed frequently.
    Sounds like we should be able to solve the two problems mentioned earlier. First,
    define a service for the target sets of pods with a proper selector configured;
    secondly, let some magic associated with the service decide which target pod is
    to receive the request. So, when we look at pod-to-pod communication again, we're
    in fact talking about pod-to-service (then to-pod) communication.
- en: 'So, what''s the magic behind the service? Now, we''ll introduce the great network
    magician: the `kube-proxy` component.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'So, what''s the magic behind the service? Now, we''ll introduce the great network
    magician: the `kube-proxy` component.'
- en: kube-proxy
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kube-proxy
- en: 'You may guess what `kube-proxy` does by its name. Generally, what a proxy (not
    a reverse proxy) does is, it passes the traffic between the client and the servers
    over two connections: inbound from the client and outbound to the server. So,
    what `kube-proxy` does to solve the two problems mentioned earlier is that it
    forwards all the traffic whose destination is the target service (the virtual
    IP) to the pods grouped by the service (the actual IP); meanwhile, `kube-proxy`
    watches the Kubernetes control plane for the addition or removal of the service
    and endpoint objects (pods). In order to do this simple task well, `kube-proxy`
    has evolved a few times.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据`kube-proxy`的名称猜到它的作用。一般来说，代理（不是反向代理）的作用是在客户端和服务器之间通过两个连接传递流量：从客户端到服务器的入站连接和从服务器到客户端的出站连接。因此，`kube-proxy`为了解决前面提到的两个问题，会将所有目标服务（虚拟IP）的流量转发到由服务分组的pod（实际IP）；同时，`kube-proxy`会监视Kubernetes控制平面，以便添加或删除服务和端点对象（pod）。为了很好地完成这个简单的任务，`kube-proxy`已经发展了几次。
- en: User space proxy mode
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户空间代理模式
- en: 'The `kube-proxy` component in the user space proxy mode acts like a real proxy.
    First, `kube-proxy` will listen on a random port on the node as a proxy port for
    a particular service. Any inbound connection to the proxy port will be forwarded
    to the service''s backend pods. When `kube-proxy` needs to decide which backend
    pod to send requests to, it takes the `SessionAffinity` setting of the service
    into account. Secondly, `kube-proxy` will install **iptables rules** to forward
    any traffic whose destination is the target service (virtual IP) to the proxy
    port, which proxies the backend port. The following diagram from the Kubernetes
    documentation illustrates this well:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用户空间代理模式中的`kube-proxy`组件就像一个真正的代理。首先，`kube-proxy`将在节点上的一个随机端口上作为特定服务的代理端口进行监听。任何对代理端口的入站连接都将被转发到服务的后端pod。当`kube-proxy`需要决定将请求发送到哪个后端pod时，它会考虑服务的`SessionAffinity`设置。其次，`kube-proxy`将安装**iptables规则**，将任何目标服务（虚拟IP）的流量转发到代理端口，代理端口再将流量转发到后端端口。来自Kubernetes文档的以下图表很好地说明了这一点：
- en: '![Figure 2.6 – kube-proxy user space proxy mode'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6 - kube-proxy用户空间代理模式'
- en: '](image/B15566_02_006.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_006.jpg)'
- en: Figure 2.6 – kube-proxy user space proxy mode
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 - kube-proxy用户空间代理模式
- en: By default, `kube-proxy` in user space mode uses a round-robin algorithm to
    choose which backend pod to forward the requests to. The downside of this mode
    is obvious. The traffic forwarding is done in the user space. This means that
    packets are marshaled into the user space and then marshaled back to the kernel
    space on every trip through the proxy. The solution is not ideal from a performance
    perspective.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，用户空间模式中的`kube-proxy`使用循环算法来选择要将请求转发到的后端pod。这种模式的缺点是显而易见的。流量转发是在用户空间中完成的。这意味着数据包被编组到用户空间，然后在代理的每次传输中被编组回内核空间。从性能的角度来看，这种解决方案并不理想。
- en: iptables proxy mode
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: iptables代理模式
- en: 'The `kube-proxy` component in the iptables proxy mode offloads the forwarding
    traffic job to `netfilter` using iptables rules. `kube-proxy` in the iptables
    proxy mode is only responsible for maintaining and updating the iptables rules.
    Any traffic targeted to the service IP will be forwarded to the backend pods by
    `netfilter`, based on the iptables rules managed by `kube-proxy`. The following
    diagram from the Kubernetes documentation illustrates this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: iptables代理模式中的`kube-proxy`组件将转发流量的工作交给了`netfilter`，使用iptables规则。在iptables代理模式中的`kube-proxy`只负责维护和更新iptables规则。任何针对服务IP的流量都将根据`kube-proxy`管理的iptables规则由`netfilter`转发到后端pod。来自Kubernetes文档的以下图表说明了这一点：
- en: '![Figure 2.7 – kube-proxy iptables proxy mode'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.7 - kube-proxy iptables代理模式'
- en: '](image/B15566_02_007.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_007.jpg)'
- en: Figure 2.7 – kube-proxy iptables proxy mode
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 - kube-proxy iptables代理模式
- en: Compared to the user space proxy mode, the advantage of the iptables mode is
    obvious. The traffic will no longer go through the kernel space to the user space
    and then back to the kernel space. Instead, it will be forwarded in the kernel
    space directly. The overhead is much lower. The disadvantage of this mode is the
    error handling required. For a case where `kube-proxy` runs in the iptables proxy
    mode, if the first selected pod does not respond, the connection will fail. While
    in the user space mode, however, `kube-proxy` would detect that the connection
    to the first pod had failed and then automatically retry with a different backend
    pod.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与用户空间代理模式相比，iptables模式的优势是显而易见的。流量不再经过内核空间到用户空间，然后再返回内核空间。相反，它将直接在内核空间中转发。开销大大降低。这种模式的缺点是需要错误处理。例如，如果`kube-proxy`在iptables代理模式下运行，如果第一个选择的pod没有响应，连接将失败。然而，在用户空间模式下，`kube-proxy`会检测到与第一个pod的连接失败，然后自动重试与不同的后端pod。
- en: IPVS proxy mode
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IPVS代理模式
- en: 'The `kube-proxy` component in the **IP Virtual Server** (**IPVS**) proxy mode
    manages and leverages the IPVS rule to forward the targeted service traffic to
    the backend pods. Just as with iptables rules, IPVS rules also work in the kernel.
    IPVS is built on top of `netfilter`. It implements transport-layer load balancing
    as part of the Linux kernel, incorporated into **Linux Virtual Server** (**LVS**).
    LVS runs on a host and acts as a load balancer in front of a cluster of real servers,
    and any **Transmission Control Protocol** (**TCP**)- or **User Datagram Protocol**
    (**UDP**)-based traffic to the IPVS service will be forwarded to the real servers.
    This makes the IPVS service of the real servers appear as virtual services on
    a single IP address. IPVS is a perfect match with the Kubernetes service. The
    following diagram from the Kubernetes documentation illustrates this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**IP Virtual Server**（**IPVS**）代理模式中的`kube-proxy`组件管理和利用IPVS规则，将目标服务流量转发到后端pod。就像iptables规则一样，IPVS规则也在内核中工作。IPVS建立在`netfilter`之上。它作为Linux内核的一部分实现传输层负载均衡，纳入**Linux
    Virtual Server**（**LVS**）中。LVS在主机上运行，并充当一组真实服务器前面的负载均衡器，任何传输控制协议（TCP）或用户数据报协议（UDP）流量都将被转发到真实服务器。这使得真实服务器的IPVS服务看起来像单个IP地址上的虚拟服务。IPVS与Kubernetes服务完美匹配。以下来自Kubernetes文档的图表说明了这一点：'
- en: '![Figure 2.8 – kube-proxy IPVS proxy mode'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.8 - kube-proxy IPVS代理模式'
- en: '](image/B15566_02_008.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_008.jpg)'
- en: Figure 2.8 – kube-proxy IPVS proxy mode
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 - kube-proxy IPVS代理模式
- en: 'Compared to the iptables proxy mode, both IPVS rules and iptables rules work
    in the kernel space. However, iptables rules are evaluated sequentially for each
    incoming packet. The more rules there are, the longer the process. The IPVS implementation
    is different from iptables: it uses a hash table managed by the kernel to store
    the destination of a packet so that it has lower latency and faster rules synchronization
    than iptables rules. IPVS mode also provides more options for load balancing.
    The only limitation for using IPVS mode is that you must have IPVS Linux available
    on the node for `kube-proxy` to consume.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与iptables代理模式相比，IPVS规则和iptables规则都在内核空间中工作。然而，iptables规则会对每个传入的数据包进行顺序评估。规则越多，处理时间越长。IPVS的实现与iptables不同：它使用由内核管理的哈希表来存储数据包的目的地，因此具有比iptables规则更低的延迟和更快的规则同步。IPVS模式还提供了更多的负载均衡选项。使用IPVS模式的唯一限制是必须在节点上有可供`kube-proxy`使用的IPVS
    Linux。
- en: Introducing the Kubernetes service
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kubernetes服务
- en: Kubernetes deployments create and destroy pods dynamically. For a general three-tier
    web architecture, this can be a problem if the frontend and backend are different
    pods. Frontend pods don't know how to connect to the backend. Network service
    abstraction in Kubernetes resolves this problem.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes部署动态创建和销毁pod。对于一般的三层Web架构，如果前端和后端是不同的pod，这可能是一个问题。前端pod不知道如何连接到后端。Kubernetes中的网络服务抽象解决了这个问题。
- en: The Kubernetes service enables network access for a logical set of pods. The
    logical set of pods are usually defined using labels. When a network request is
    made for a service, it selects all the pods with a given label and forwards the
    network request to one of the selected pods.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务使一组逻辑pod能够进行网络访问。通常使用标签来定义一组逻辑pod。当对服务发出网络请求时，它会选择所有具有特定标签的pod，并将网络请求转发到所选pod中的一个。
- en: 'A Kubernetes service is defined using a **YAML Ain''t Markup Language** (**YAML**)
    file, as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**YAML Ain't Markup Language** (**YAML**)文件定义了Kubernetes服务，如下所示：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this YAML file, the following applies:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个YAML文件中，以下内容适用：
- en: The `type` property defines how the service is exposed to the network.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`type`属性定义了服务如何向网络公开。'
- en: The `selector` property defines the label for the Pods.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`selector`属性定义了Pod的标签。'
- en: The `port` property is used to define the port exposed internally in the cluster.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`port`属性用于定义在集群内部公开的端口。'
- en: The `targetPort` property defines the port on which the container is listening.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`targetPort`属性定义了容器正在侦听的端口。'
- en: 'Services are usually defined with a selector, which is a label attached to
    pods that need to be in the same service. A service can be defined without a selector.
    This is usually done to access external services or services in a different namespace.
    Services without selectors are mapped to a network address and a port using an
    endpoint object, as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 服务通常使用选择器来定义，选择器是附加到需要在同一服务中的pod的标签。服务可以在没有选择器的情况下定义。这通常是为了访问外部服务或不同命名空间中的服务。没有选择器的服务将使用端点对象映射到网络地址和端口，如下所示：
- en: '[PRE1]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This endpoint object will route traffic for `192:123.1.22:3909` to the attached
    service.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此端点对象将路由流量`192:123.1.22:3909`到附加的服务。
- en: Service discovery
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'To find Kubernetes services, developers either use environment variables or
    the **Domain Name System** (**DNS**), detailed as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到Kubernetes服务，开发人员可以使用环境变量或**Domain Name System** (**DNS**)，详细如下：
- en: '**Environment variables**: When a service is created, a set of environment
    variables of the form `[NAME]_SERVICE_HOST` and `[NAME]_SERVICE_PORT` are created
    on the nodes. These environment variables can be used by other pods or applications
    to reach out to the service, as illustrated in the following code snippet:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**环境变量**：创建服务时，在节点上创建了一组环境变量，形式为`[NAME]_SERVICE_HOST`和`[NAME]_SERVICE_PORT`。其他pod或应用程序可以使用这些环境变量来访问服务，如下面的代码片段所示：'
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**DNS**: The DNS service is added to Kubernetes as an add-on. Kubernetes supports
    two add-ons: CoreDNS and Kube-DNS. DNS services contain a mapping of the service
    name to IP addresses. Pods and applications use this mapping to connect to the
    service.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**DNS**：DNS服务作为附加组件添加到Kubernetes中。Kubernetes支持两个附加组件：CoreDNS和Kube-DNS。DNS服务包含服务名称到IP地址的映射。Pod和应用程序使用此映射来连接到服务。'
- en: Clients can locate the service IP from environment variables as well as through
    a DNS query, and there are different types of services to serve different types
    of client.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端可以通过环境变量和DNS查询来定位服务IP，而且有不同类型的服务来为不同类型的客户端提供服务。
- en: Service types
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务类型
- en: 'A service can have four different types, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 服务可以有四种不同的类型，如下所示：
- en: '**ClusterIP**: This is the default value. This service is only accessible within
    the cluster. A Kubernetes proxy can be used to access the ClusterIP services externally.
    Using `kubectl` proxy is preferable for debugging but is not recommended for production
    services as it requires `kubectl` to be run as an authenticated user.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterIP**：这是默认值。此服务只能在集群内访问。可以使用Kubernetes代理来外部访问ClusterIP服务。使用`kubectl`代理进行调试是可取的，但不建议用于生产服务，因为它需要以经过身份验证的用户身份运行`kubectl`。'
- en: '**NodePort**: This service is accessible via a static port on every node. NodePorts
    expose one service per port and require manual management of IP address changes.
    This also makes NodePorts unsuitable for production environments.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodePort**：此服务可以通过每个节点上的静态端口访问。NodePorts每个端口暴露一个服务，并需要手动管理IP地址更改。这也使得NodePorts不适用于生产环境。'
- en: '**LoadBalancer**: This service is accessible via a load balancer. A node balancer
    per service is usually an expensive option.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoadBalancer**：此服务可以通过负载均衡器访问。通常每个服务都有一个节点负载均衡器是一个昂贵的选择。'
- en: '**ExternalName**: This service has an associated **Canonical Name Record**
    (**CNAME**) that is used to access the service.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ExternalName**：此服务有一个关联的**规范名称记录**（**CNAME**），用于访问该服务。'
- en: There are a few types of service to use and they work on layer 3 and layer 4
    of the OSI model. None of them is able to route a network request at layer 7\.
    For routing requests to applications, it would be ideal if the Kubernetes service
    supported such a feature. Let's see, then, how an ingress object can help here.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种要使用的服务类型，它们在OSI模型的第3层和第4层上工作。它们都无法在第7层路由网络请求。为了路由请求到应用程序，如果Kubernetes服务支持这样的功能将是理想的。那么，让我们看看Ingress对象如何在这里帮助。
- en: Ingress for routing external requests
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于路由外部请求的Ingress
- en: 'Ingress is not a type of service but is worth mentioning here. Ingress is a
    smart router that provides external **HTTP/HTTPS** (short for **HyperText Transfer
    Protocol Secure**) access to a service in a cluster. Services other than HTTP/HTTPS
    can only be exposed for the NodePort or LoadBalancer service types. An Ingress
    resource is defined using a YAML file, like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress不是一种服务类型，但在这里值得一提。Ingress是一个智能路由器，为集群中的服务提供外部**HTTP/HTTPS**（**超文本传输安全协议**）访问。除了HTTP/HTTPS之外的服务只能暴露给NodePort或LoadBalancer服务类型。Ingress资源是使用YAML文件定义的，就像这样：
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This minimal ingress spec forwards all traffic from the `testpath` route to
    the `service-1` route.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最小的Ingress规范将`testpath`路由的所有流量转发到`service-1`路由。
- en: 'Ingress objects have five different variations, listed as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress对象有五种不同的变体，列举如下：
- en: '**Single-service Ingress**: This exposes a single service by specifying a default
    backend and no rules, as illustrated in the following code block:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单服务Ingress**：通过指定默认后端和没有规则来暴露单个服务，如下面的代码块所示：'
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This ingress exposes a dedicated IP address for `service-1`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Ingress暴露了一个专用IP地址给`service-1`。
- en: '**Simple fanout**: A fanout configuration routes traffic from a single IP to
    multiple services based on the **Uniform Resource Locator** (**URL**), as illustrated
    in the following code block:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单的分流**：分流配置根据**统一资源定位符**（**URL**）将来自单个IP的流量路由到多个服务，如下面的代码块所示：'
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This configuration allows requests to `foo.com/foo` to reach out to `service-1`
    and for `foo.com/bar` to connect to `service-2`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置允许`foo.com/foo`的请求到达`service-1`，并且`foo.com/bar`连接到`service-2`。
- en: '**Name-based virtual hosting**: This configuration uses multiple hostnames
    for a single IP to reach out to different services, as illustrated in the following
    code block:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于名称的虚拟主机**：此配置使用多个主机名来达到一个IP到达不同服务的目的，如下面的代码块所示：'
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This configuration allows requests to `foo.com` to connect to `service-1` and
    requests to `bar.com` to connect to `service-2`. The IP address allocated to both
    services is the same in this case.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置允许`foo.com`的请求连接到`service-1`，`bar.com`的请求连接到`service-2`。在这种情况下，两个服务分配的IP地址是相同的。
- en: '**Transport Layer Security (TLS)**: A secret can be added to the ingress spec
    to secure the endpoints, as illustrated in the following code block:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传输层安全性（TLS）：可以向入口规范添加一个秘密以保护端点，如下面的代码块所示：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With this configuration, the `secret-tls` secret provides the private key and
    certificate for the endpoint.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个配置，`secret-tls`提供了端点的私钥和证书。
- en: '**Load balancing**: A load balancing ingress provides a load balancing policy,
    which includes the load balancing algorithm and weight scheme for all ingress
    objects.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载平衡：负载平衡入口提供负载平衡策略，其中包括所有入口对象的负载平衡算法和权重方案。
- en: In this section, we introduced the basic concept of the Kubernetes service,
    including ingress objects. These are all Kubernetes objects. However, the actual
    network communication magic is done by several components, such as `kube-proxy`.
    Next, we will introduce the CNI and CNI plugins, which is the foundation that
    serves the network communication of a Kubernetes cluster.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了Kubernetes服务的基本概念，包括入口对象。这些都是Kubernetes对象。然而，实际的网络通信魔术是由几个组件完成的，比如`kube-proxy`。接下来，我们将介绍CNI和CNI插件，这是为Kubernetes集群的网络通信提供服务的基础。
- en: Introducing the CNI and CNI plugins
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍CNI和CNI插件
- en: 'In Kubernetes, **CNI** stands for the **Container Network Interface**. CNI
    is a **Cloud Native Computing Foundation** (**CNCF**) project—you can find further
    information on GitHub here: [https://github.com/containernetworking/cni](https://github.com/containernetworking/cni).
    Basically, there are three things in this project: a specification, libraries
    for writing plugins to configure network interfaces in Linux containers, and some
    supported plugins. When people talk about the CNI, they usually make reference
    to either the specification or the CNI plugins. The relationship between the CNI
    and CNI plugins is that the CNI plugins are executable binaries that implement
    the CNI specification. Now, let''s look into the CNI specification and plugins
    at a high level, and then we will give a brief introduction to one of the CNI
    plugins, Calico.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，CNI代表容器网络接口。CNI是云原生计算基金会（CNCF）的一个项目-您可以在GitHub上找到更多信息：[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)。基本上，这个项目有三个东西：一个规范，用于编写插件以配置Linux容器中的网络接口的库，以及一些支持的插件。当人们谈论CNI时，他们通常指的是规范或CNI插件。CNI和CNI插件之间的关系是CNI插件是可执行的二进制文件，实现了CNI规范。现在，让我们高层次地看看CNI规范和插件，然后我们将简要介绍CNI插件之一，Calico。
- en: CNI specification and plugins
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI规范和插件
- en: 'The CNI specification is only concerned with the network connectivity of containers
    and removing allocated resources when the container is deleted. Let me elaborate
    more on this. First, from a container runtime''s perspective, the CNI spec defines
    an interface for the **Container Runtime Interface** (**CRI**) component (such
    as Docker) to interact with—for example, add a container to a network interface
    when a container is created, or delete the network interface when a container
    dies. Secondly, from a Kubernetes network model''s perspective, since CNI plugins
    are actually another flavor of Kubernetes network plugins, they have to comply
    with Kubernetes network model requirements, detailed as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: CNI规范只关注容器的网络连接性，并在容器删除时移除分配的资源。让我更详细地解释一下。首先，从容器运行时的角度来看，CNI规范为**容器运行时接口**（**CRI**）组件（如Docker）定义了一个接口，用于与之交互，例如在创建容器时向网络接口添加容器，或者在容器死亡时删除网络接口。其次，从Kubernetes网络模型的角度来看，由于CNI插件实际上是Kubernetes网络插件的另一种类型，它们必须符合Kubernetes网络模型的要求，详细如下：
- en: Pods on a node can communicate with all pods in all the nodes without using
    NAT.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点上的pod可以与所有节点上的所有pod进行通信，而无需使用NAT。
- en: Agents such as `kubelet` can communicate with pods in the same node.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubelet`等代理可以与同一节点中的pod进行通信。'
- en: 'There are a handful of CNI plugins available to choose—just to name a few:
    Calico, Cilium, WeaveNet, Flannel, and so on. The CNI plugins'' implementation
    varies, but in general, what CNI plugins do is similar. They carry out the following
    tasks:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些可供选择的CNI插件，比如Calico、Cilium、WeaveNet、Flannel等。CNI插件的实施各不相同，但总的来说，CNI插件的功能类似。它们执行以下任务：
- en: Manage network interfaces for containers
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理容器的网络接口
- en: Allocate IP addresses for pods. This is usually done via calling other **IP
    Address Management** (**IPAM**) plugins such as `host-local`
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为pod分配IP地址。这通常是通过调用其他**IP地址管理**（**IPAM**）插件（如`host-local`）来完成的。
- en: Implement network policies (optional)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施网络策略（可选）
- en: 'The network policy implementation is not required in the CNI specification,
    but when DevOps choose which CNI plugins to use, it is important to take security
    into consideration. Alexis Ducastel''s article ([https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560](https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560))
    did a good comparison of the mainstream CNI plugins with the latest update in
    April 2019\. The security comparison is notable, as can be seen in the following
    screenshot:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: CNI规范中不要求实施网络策略，但是当DevOps选择要使用的CNI插件时，考虑安全性是很重要的。Alexis Ducastel的文章（[https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560](https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560)）在2019年4月进行了主流CNI插件的良好比较。安全性比较值得注意，如下截图所示：
- en: '![Figure 2.9 – CNI plugins comparison'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.9 - CNI插件比较'
- en: '](image/B15566_02_009.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_009.jpg)'
- en: Figure 2.9 – CNI plugins comparison
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 - CNI插件比较
- en: You may notice that the majority of the CNI plugins on the list don't support
    encryption. Flannel does not support Kubernetes network policies, while `kube-router`
    supports ingress network policies only.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到列表中大多数CNI插件都不支持加密。Flannel不支持Kubernetes网络策略，而`kube-router`仅支持入口网络策略。
- en: 'As Kubernetes comes with the default `kubenet` plugin, in order to use CNI
    plugins in a Kubernetes cluster, users must pass the `--network-plugin=cni` command-line
    option and specify a configuration file via the `--cni-conf-dir` flag or in the
    `/etc/cni/net.d` default directory. The following is a sample configuration defined
    within the Kubernetes cluster so that `kubelet` may know which CNI plugin to interact
    with:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Kubernetes默认使用`kubenet`插件，为了在Kubernetes集群中使用CNI插件，用户必须通过`--network-plugin=cni`命令行选项传递，并通过`--cni-conf-dir`标志或在`/etc/cni/net.d`默认目录中指定配置文件。以下是在Kubernetes集群中定义的示例配置，以便`kubelet`知道要与哪个CNI插件交互：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The CNI configuration file tells `kubelet` to use Calico as a CNI plugin and
    use `host-local` to allocate IP addresses to pods. In the list, there is another
    CNI plugin called `portmap` that is used to support `hostPort`, which allows container
    ports to be exposed on the host IP.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: CNI配置文件告诉`kubelet`使用Calico作为CNI插件，并使用`host-local`来为pod分配IP地址。在列表中，还有另一个名为`portmap`的CNI插件，用于支持`hostPort`，允许容器端口在主机IP上暴露。
- en: 'When creating a cluster with **Kubernetes Operations** (**kops**), you can
    also specify the CNI plugin you would like to use, as illustrated in the following
    code block:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用**Kubernetes Operations**（**kops**）创建集群时，您还可以指定要使用的CNI插件，如下面的代码块所示：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, the cluster is created using the `calico` CNI plugin.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，集群是使用`calico` CNI插件创建的。
- en: Calico
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Calico
- en: 'Calico is an open source project that enables cloud-native application connectivity
    and policy. It integrates with major orchestration systems such as Kubernetes,
    Apache Mesos, Docker, and OpenStack. Compared to other CNI plugins, here are a
    few things about Calico worth highlighting:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Calico是一个开源项目，可以实现云原生应用的连接和策略。它与主要的编排系统集成，如Kubernetes、Apache Mesos、Docker和OpenStack。与其他CNI插件相比，Calico有一些值得强调的优点：
- en: Calico provides a flat IP network, which means there will be no IP encapsulation
    appended to the IP message (no overlays). Also, this means that each IP address
    assigned to the pod is fully routable. The ability to run without an overlay provides
    exceptional throughput characteristics.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Calico提供了一个扁平的IP网络，这意味着IP消息中不会附加IP封装（没有覆盖）。这也意味着分配给pod的每个IP地址都是完全可路由的。无需覆盖即可运行的能力提供了出色的吞吐特性。
- en: Calico has better performance and less resource consumption, according to Alexis
    Ducastel's experiments.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Alexis Ducastel的实验，Calico具有更好的性能和更少的资源消耗。
- en: Calico offers a more comprehensive network policy compared to Kubernetes' built-in
    network policy. Kubernetes' network policy can only define whitelist rules, while
    Calico network policies can define blacklist rules (deny).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与Kubernetes内置的网络策略相比，Calico提供了更全面的网络策略。Kubernetes的网络策略只能定义白名单规则，而Calico网络策略可以定义黑名单规则（拒绝）。
- en: 'When integrating Calico into Kubernetes, you will see three components running
    inside the Kubernetes cluster, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将Calico集成到Kubernetes中时，您会看到以下三个组件在Kubernetes集群中运行：
- en: The `calico/node` is a DaemonSet service, which means that it runs on every
    node in the cluster. It is responsible for programming and routing kernel routes
    to local workloads, and enforces the local filtering rules required by the current
    network policies in the cluster. It is also responsible for broadcasting the routing
    tables to other nodes to keep the IP routes in sync across the cluster.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calico/node`是一个DaemonSet服务，这意味着它在集群中的每个节点上运行。它负责为本地工作负载编程和路由内核路由，并强制执行集群中当前网络策略所需的本地过滤规则。它还负责向其他节点广播路由表，以保持集群中IP路由的同步。'
- en: The CNI plugin binaries. This includes two binary executables (`calico` and
    `calico-ipam`) and a configuration file that integrates directly with the Kubernetes
    `kubelet` process on each node. It watches the pod creation event and then adds
    pods to the Calico networking.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI插件二进制文件。这包括两个可执行二进制文件（`calico`和`calico-ipam`）以及一个配置文件，直接与每个节点上的Kubernetes
    `kubelet`进程集成。它监视pod创建事件，然后将pod添加到Calico网络中。
- en: The Calico Kubernetes controllers, running as a standalone pod, monitor the
    Kubernetes **application programming interface** (**API**) to keep Calico in sync.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico Kubernetes控制器作为一个独立的pod运行，监视Kubernetes **应用程序编程接口**（**API**）以保持Calico同步。
- en: Calico is a popular CNI plugin and also the default CNI plugin in **Google Kubernetes
    Engine** (**GKE**). Kubernetes administrators have full freedom to choose whatever
    CNI plugin fits their requirement. Just keep in mind that security is essential
    and is one of the decision factors. We've talked a lot about the Kubernetes network
    in the previous sections. Let's quickly review this again before you forget.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Calico是一个流行的CNI插件，也是**Google Kubernetes Engine**（**GKE**）中的默认CNI插件。Kubernetes管理员完全可以自由选择符合其要求的CNI插件。只需记住安全性是至关重要的决定因素之一。在前面的章节中，我们已经谈了很多关于Kubernetes网络的内容。在你忘记之前，让我们快速回顾一下。
- en: Wrapping up
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In a Kubernetes cluster, every pod gets an IP address assigned, but this is
    an internal IP address and not accessible externally. Containers inside the same
    pod can communicate with each other via the name network interface, as they share
    the same network namespace. Containers inside the same pod also need to resolve
    the port resource conflict problem; however, this is quite unlikely to happen
    as applications run in different containers grouped in the same pod for a specific
    purpose. Also, it is worth noting that containers inside the same pod can communicate
    beyond the network through shared volume, IPC channel, and process signals.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中，每个pod都被分配了一个IP地址，但这是一个内部IP地址，无法从外部访问。同一pod中的容器可以通过名称网络接口相互通信，因为它们共享相同的网络命名空间。同一pod中的容器还需要解决端口资源冲突的问题；然而，这种情况发生的可能性非常小，因为应用程序在同一pod中的不同容器中运行，目的是特定的。此外，值得注意的是，同一pod中的容器可以通过共享卷、IPC通道和进程信号进行网络通信。
- en: 'The Kubernetes service helps pod-to-pod communication to be stabilized, as
    pods are usually ephemeral. The service also gets an IP address assigned but this
    is virtual, meaning no network interface is created for the service. The `kube-proxy`
    network magician actually routes all traffic to the target service to the backend
    pods. There are three different modes of `kube-proxy`: user space proxy, iptables
    proxy, and IPVS proxy. The Kubernetes service not only provides support for pod-to-pod
    communication but also enables communication from external sources.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务有助于稳定pod之间的通信，因为pod通常是短暂的。该服务也被分配了一个IP地址，但这是虚拟的，意味着没有为服务创建网络接口。`kube-proxy`网络魔术师实际上将所有流量路由到目标服务的后端pod。`kube-proxy`有三种不同的模式：用户空间代理、iptables代理和IPVS代理。Kubernetes服务不仅提供了对pod之间通信的支持，还能够实现来自外部源的通信。
- en: 'There are a few ways to expose services so that they are accessible from external
    sources such as NodePort, LoadBalancer, and ExternalName. Also, you can create
    an Ingress object to achieve the same goal. Finally, though it is hard, we''ll
    use the following single diagram to try to consolidate most of the knowledge we
    want to highlight in this chapter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以公开服务，使其可以从外部源访问，例如NodePort、LoadBalancer和ExternalName。此外，您可以创建一个Ingress对象来实现相同的目标。最后，虽然很难，但我们将使用以下单个图表来尝试整合我们在本章中要强调的大部分知识：
- en: '![Figure 2.10 – Communications: inside pod, among pods, and from external sources'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.10 - 通信：pod内部、pod之间以及来自外部的源'
- en: '](image/B15566_02_010.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B15566_02_010.jpg)'
- en: 'Figure 2.10 – Communications: inside pod, among pods, and from external sources'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 - 通信：pod内部，pod之间，以及来自外部来源
- en: There is nearly always a load balancer sitting in front of a Kubernetes cluster.
    With the different service types we mentioned previously, this could be a single
    service that is exposed via the load balancer (this is service **A**), or it could
    be exposed via a NodePort. This is service **B** using node port **30000** in
    both nodes to accept external traffic. Though ingress is not a service type, it
    is powerful and cost-efficient compared to a LoadBalancer-type service. Service
    **C** and service **D** routing is controlled by the same ingress object. Every
    pod in the cluster may have an internal communication topology in the preceding
    callout diagram.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个Kubernetes集群前面都有一个负载均衡器。根据我们之前提到的不同服务类型，这可能是一个通过负载均衡器公开的单个服务（这是服务**A**），或者它可以通过NodePort公开。这是服务**B**，在两个节点上使用节点端口**30000**来接受外部流量。虽然Ingress不是一种服务类型，但与LoadBalancer类型服务相比，它更强大且成本效益更高。服务**C**和服务**D**的路由由同一个Ingress对象控制。集群中的每个pod可能在前面的标注图中有一个内部通信拓扑。
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by discussing the typical port resource conflict
    problem and how the Kubernetes network model tries to avoid this while maintaining
    good compatibility for migrating applications from the VM to Kubernetes pods.
    Then, we talked about the communication inside a pod, among pods, and from external
    sources to pods.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先讨论了典型的端口资源冲突问题，以及Kubernetes网络模型如何在避免这一问题的同时保持对从VM迁移到Kubernetes pod的应用程序的良好兼容性。然后，我们讨论了pod内部的通信，pod之间的通信，以及来自外部来源到pod的通信。
- en: Last but not least, we covered the basic concept of CNI and introduced how Calico
    works in the Kubernetes environment. After the first two chapters, we hope you
    have a basic understanding of how Kubernetes components work and how things communicate
    with each other.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，我们介绍了CNI的基本概念，并介绍了Calico在Kubernetes环境中的工作原理。在前两章中，我们希望您对Kubernetes组件的工作方式以及各个组件之间的通信有了基本的了解。
- en: In the next chapter, we're going to talk about threat modeling a Kubernetes
    cluster.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论威胁建模Kubernetes集群。
- en: Questions
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: In a Kubernetes cluster, is the IP address assigned to a pod or a container?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中，IP地址分配给pod还是容器？
- en: What are the Linux namespaces that will be shared among containers inside the
    same pod?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个pod内部，哪些Linux命名空间将被容器共享？
- en: What is a pause container and what is it for?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暂停容器是什么，它有什么作用？
- en: What are the types of Kubernetes services?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes服务有哪些类型？
- en: What is the advantage of using Ingress other than the LoadBalancer type service?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了LoadBalancer类型的服务，使用Ingress的优势是什么？
- en: Further reading
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'If you want to build your own CNI plugin or evaluate Calico more, do check
    out the following links:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想构建自己的CNI插件或评估Calico更多，请查看以下链接：
- en: '[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)'
- en: '[https://docs.projectcalico.org/v3.11/reference/architecture/](https://docs.projectcalico.org/v3.11/reference/architecture/)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.projectcalico.org/v3.11/reference/architecture/](https://docs.projectcalico.org/v3.11/reference/architecture/)'
- en: '[https://docs.projectcalico.org/v3.11/getting-started/kubernetes/installation/integration](https://docs.projectcalico.org/v3.11/getting-started/kubernetes/installation/integration)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.projectcalico.org/v3.11/getting-started/kubernetes/installation/integration](https://docs.projectcalico.org/v3.11/getting-started/kubernetes/installation/integration)'
