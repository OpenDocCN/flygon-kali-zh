- en: Orchestrators
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编排器
- en: In the previous chapter, we introduced Docker Compose, a tool that allows us
    to work with multi-service applications that are defined in a declarative way
    on a single Docker host.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了Docker Compose，这是一个允许我们在单个Docker主机上以声明方式定义多服务应用程序的工具。
- en: This chapter introduces the concept of orchestrators. It teaches us why orchestrators
    are needed, and how they work conceptually. This chapter will also provide an
    overview of the most popular orchestrators and list a few of their pros and cons.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了编排器的概念。它教会我们为什么需要编排器，以及它们在概念上是如何工作的。本章还将概述最流行的编排器，并列出它们的一些优缺点。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: What are orchestrators and why do we need them?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排器是什么，为什么我们需要它们？
- en: The tasks of an orchestrator
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排器的任务
- en: Overview of popular orchestrators
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行编排器概述
- en: 'After finishing this chapter, you will be able to do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将能够做到以下几点：
- en: Name three to four tasks for which an orchestrator is responsible
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列举编排器负责的三到四个任务
- en: List two to three of the most popular orchestrators
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列举两到三个最流行的编排器
- en: Explain to an interested layman, in your own words, and with appropriate analogies,
    why we need container orchestrators
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用你自己的话和适当的类比向一个感兴趣的外行解释为什么我们需要容器编排器
- en: What are orchestrators and why do we need them?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编排器是什么，为什么我们需要它们？
- en: In [Chapter 9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml)*, Distributed Application
    Architecture*, we learned which patterns and best practices are commonly used
    to successfully build, ship, and run a highly distributed application. Now, if
    our distributed application is containerized, then we're facing the exact same
    problems or challenges that a non-containerized distributed application faces.
    Some of these challenges are those that were discussed in [Chapter 9](bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml), *Distributed
    Application Architecture*—service discovery, load balancing, scaling, and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章]（bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml）*，分布式应用架构*中，我们了解了成功构建、部署和运行高度分布式应用程序常用的模式和最佳实践。现在，如果我们的分布式应用程序是容器化的，那么我们将面临与非容器化分布式应用程序面临的完全相同的问题或挑战。其中一些挑战是在[第9章]（bbbf480e-3d5a-4ad7-94e9-fae735b025ae.xhtml）*，分布式应用架构*中讨论过的——服务发现、负载均衡、扩展等等。
- en: Similar to what Docker did with containers—standardizing the packaging and shipping
    of software with the introduction of those containers—we would like to have some
    tool or infrastructure software that handles all or most of the challenges mentioned.
    This software turns out to be what we call container orchestrators or, as we also
    call them, orchestration engines.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Docker对容器所做的事情——通过引入这些容器来标准化软件的打包和交付——我们希望有一些工具或基础设施软件来处理提到的所有或大部分挑战。这个软件就是我们所说的容器编排器，或者我们也称之为编排引擎。
- en: 'If what I just said doesn''t make much sense to you yet, then let''s look at
    it from a different angle. Take an artist who plays an instrument. They can play
    wonderful music to an audience all on their own—just the artist and their instrument.
    But now take an orchestra of musicians. Put them all in a room, give them the
    notes of a symphony, ask them to play it, and leave the room. Without any director,
    this group of very talented musicians would not be able to play this piece in
    harmony; it would more or less sound like a cacophony. Only if the orchestra has
    a conductor, who orchestrates the group of musicians, will the resulting music
    of the orchestra be enjoyable to our ears:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我刚才说的对你来说还不太有意义，那么让我们从另一个角度来看。拿一个演奏乐器的艺术家来说。他们可以独自为观众演奏美妙的音乐 - 只有艺术家和他们的乐器。但现在想象一个由音乐家组成的管弦乐团。把他们都放在一个房间里，给他们一首交响乐的音符，让他们演奏，并离开房间。没有指挥，这群非常有才华的音乐家将无法和谐地演奏这首曲子；它听起来或多或少会像一片杂音。只有管弦乐团有一个指挥，来指挥这群音乐家，管弦乐团的音乐才会让我们的耳朵愉悦：
- en: '![](assets/a0425d38-20e7-4cc7-b2f5-5680c5ae5b2a.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '*那么，我们期望一个值得投资的编排者为我们执行哪些任务呢？* 让我们详细看一下。以下列表显示了在撰写本文时，企业用户通常期望从他们的编排者那里得到的最重要的任务。'
- en: 'A container orchestrator is like the conductor of an orchestraSource: https://it.wikipedia.org/wiki/Giuseppe_Lanzetta#/media/File:UMB_5945.JPGLicense: https://creativecommons.org/licenses/by-sa/3.0/deed.en'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排者就像管弦乐团的指挥
- en: Instead of musicians, we now have containers, and instead of different instruments,
    we have containers that have different requirements to the container hosts to
    run. And instead of the music being played at varying tempi, we have containers
    that communicate with each other in particular ways, and have to scale up and
    scale down. In this regard, a container orchestrator has very much the same role
    as a conductor in an orchestra. It makes sure that the containers and other resources in a
    cluster play together in harmony.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你现在能更清楚地看到容器编排者是什么，以及为什么我们需要它。假设你确认了这个问题，我们现在可以问自己编排者将如何实现预期的结果，即确保集群中的所有容器和谐地相互配合。嗯，答案是，编排者必须执行非常具体的任务，类似于管弦乐团的指挥也有一系列任务要执行，以驯服和同时提升管弦乐团。
- en: I hope that you can now see more clearly what a container orchestrator is, and
    why we need one. Assuming that you confirm this question, we can now ask ourselves
    how the orchestrator is going to achieve the expected outcome, namely, to make
    sure that all the containers in the cluster play with each other in harmony. Well,
    the answer is, the orchestrator has to execute very specific tasks, similar to
    the way in which the conductor of an orchestra also has a set of tasks that they
    execute in order to tame and, at the same time, elevate the orchestra.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://it.wikipedia.org/wiki/Giuseppe_Lanzetta#/media/File:UMB_5945.JPGLicense: https://creativecommons.org/licenses/by-sa/3.0/deed.en
- en: The tasks of an orchestrator
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编排者的任务
- en: '*So, **what are the tasks that we expect an orchestrator worth its money to
    execute for us?* Let''s look at them in detail. The following list shows the most
    important tasks that, at the time of writing, enterprise users typically expect
    from their orchestrator.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有的不是音乐家，而是容器，不同的乐器，而是对容器主机运行的不同要求。音乐以不同的速度演奏，我们有以特定方式相互通信的容器，并且需要扩展和缩减。在这方面，容器编排者与管弦乐团的指挥有着非常相似的角色。它确保集群中的容器和其他资源和谐地相互配合。
- en: Reconciling the desired state
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协调所需的状态
- en: When using an orchestrator, you tell it, in a declarative way, how you want
    it to run a given application or application service. We learned  what declarative
    versus imperative means in [Chapter 11](412c6f55-a00b-447f-b22a-47b305453507.xhtml)*,
    Docker Compose*. Part of this declarative way of describing the application service
    that we want to run includes elements such as which container image to use, how
    many instances of this service to run, which ports to open, and more. This declaration
    of the properties of our application service is what we call the *desired state*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用编排器时，您以声明方式告诉它如何运行特定的应用程序或应用程序服务。我们在《Docker Compose》的[第11章]中学到了声明式与命令式的含义。描述我们想要运行的应用程序服务的声明方式包括诸如要使用哪个容器镜像、要运行多少个此服务的实例、要打开哪些端口等元素。我们称这些应用服务属性的声明为“期望状态”。
- en: So, when we now tell the orchestrator for the first time to create such a new
    application service based on the declaration, then the orchestrator makes sure
    to schedule as many containers in the cluster as requested. If the container image
    is not yet available on the target nodes of the cluster where the containers are
    supposed to run, then the scheduler makes sure that they're first downloaded from
    the image registry. Next, the containers are started with all the settings, such
    as networks to which to attach, or ports to expose. The orchestrator works as
    hard as it can to exactly match, in reality, the cluster to the declaration.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们现在首次告诉编排器根据声明创建这样一个新的应用服务时，编排器会确保在集群中安排尽可能多的容器。如果容器镜像尚未在集群的目标节点上可用，调度程序会确保首先从镜像注册表中下载它们。接下来，容器将以所有设置启动，例如要附加到的网络或要公开的端口。编排器会尽其所能确保将集群与声明的状态完全匹配。
- en: Once our service is up and running as requested, that is, it is running in the
    desired state, then the orchestrator continues to monitor it. Each time the orchestrator
    discovers a discrepancy between the actual state of the service and its desired
    state, it again tries its best to reconcile the desired state.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的服务按要求启动并运行，也就是说，它以期望的状态运行，那么编排器会继续监视它。每当编排器发现服务的实际状态与期望状态之间存在差异时，它会再次尽力调解期望状态。
- en: 'What could such a discrepancy between the actual and desired states of an application
    service be? Well, let''s say one of the replicas of the service, that is, one
    of the containers, crashes due to, say, a bug, then the orchestrator will discover
    that the actual state differs from the desired state in the number of replicas:
    there is one replica missing. The orchestrator will immediately schedule a new
    instance to another cluster node, which replaces the crashed instance. Another
    discrepancy could be that there are too many instances of the application service
    running, if the service has been scaled down. In this case, the orchestrator will
    just randomly kill as many instances as needed in order to achieve parity between
    the actual and the desired number of instances. Yet another discrepancy could
    be when the orchestrator discovers that there is an instance of the application
    service running a wrong (maybe old) version of the underlying container image.
    By now, you should get the picture, right?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序服务的实际状态与期望状态之间可能存在什么差异呢？比如说，服务的一个副本，也就是一个容器，由于某种原因崩溃了，编排器会发现实际状态与期望状态之间的差异在于副本的数量：缺少一个副本。编排器会立即将一个新实例调度到另一个集群节点，以替换崩溃的实例。另一个差异可能是应用程序服务的实例数量过多，如果服务已经缩减。在这种情况下，编排器将随机关闭所需数量的实例，以实现实际实例和期望实例数量之间的平衡。另一个差异可能是编排器发现应用程序服务的一个实例运行了错误（可能是旧）版本的底层容器映像。到现在为止，你应该明白了吧？
- en: Thus, instead of us actively monitoring our application's services that are
    running in the cluster and correcting any deviation from the desired state, we
    delegate this tedious task to the orchestrator. This works very well provided
    we use a declarative and not an imperative way of describing the desired state
    of our application services.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不需要主动监视集群中运行的应用程序服务，并纠正与期望状态的任何偏差，而是将这一繁琐的任务委托给编排器。只要我们使用声明性而不是命令式的方式描述应用程序服务的期望状态，这种方法就非常有效。
- en: Replicated and global services
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制和全局服务
- en: There are two quite different types of services that we might want to run in
    a cluster that is managed by an orchestrator. They are *replicated *and *global *services.
    A replicated service is a service that is required to run in a specific number
    of instances, say 10\. A global service, in turn, is a service that is required
    to have exactly one instance running on every single worker node of the cluster.
    I have used the term *worker node* here. In a cluster that is managed by an orchestrator,
    we typically have two types of nodes, *managers* and *workers.* A manager node is usually exclusively
    used by the orchestrator to manage the cluster and does not run any other workload.
    Worker nodes, in turn, run the actual applications.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在由编排器管理的集群中，我们可能想要运行两种完全不同类型的服务。它们是*复制*和*全局*服务。复制服务是指需要在特定数量的实例中运行的服务，比如说10个。而全局服务则是要求集群中每个工作节点上都运行一个实例的服务。我在这里使用了“工作节点”这个术语。在由编排器管理的集群中，通常有两种类型的节点，即*管理节点*和*工作节点*。管理节点通常由编排器专门用于管理集群，不运行任何其他工作负载。而工作节点则运行实际的应用程序。
- en: So, the orchestrator makes sure that, for a global service, an instance of it
    is running on every single worker node, no matter how many there are. We do not
    need to care about the number of instances, but only that on each node, it is
    guaranteed to run a single instance of the service.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，编排器确保对于全局服务，无论有多少个工作节点，它都在每个工作节点上运行一个实例。我们不需要关心实例的数量，只需要确保在每个节点上都保证运行服务的单个实例。
- en: Once again, we can fully rely on the orchestrator to handle this. In a replicated
    service, we will always be guaranteed to find the exact desired number of instances,
    while for a global service, we can be assured that on every worker node, there
    will always run exactly one instance of the service. The orchestrator will always
    work as hard as it can to guarantee this desired state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以完全依赖编排器来处理这个问题。在复制的服务中，我们总是能够找到确切所需数量的实例，而对于全局服务，我们可以确保在每个工作节点上始终运行服务的一个实例。编排器将尽其所能保证这种期望状态。
- en: In Kubernetes, a global service is also called a **DaemonSet**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，全局服务也被称为**DaemonSet**。
- en: Service discovery
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: When we describe an application service in a declarative way, we are never supposed
    to tell the orchestrator on which cluster nodes the different instances of the
    service have to run. We leave it up to the orchestrator to decide which nodes
    best fit this task.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们以声明方式描述应用服务时，我们永远不应该告诉编排器服务的不同实例必须在哪些集群节点上运行。我们让编排器决定哪些节点最适合这项任务。
- en: It is, of course, technically possible to instruct the orchestrator to use very
    deterministic placement rules, but this would be an anti-pattern, and is not recommended
    at all, other than in very special edge cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，从技术上讲，指示编排器使用非常确定性的放置规则是可能的，但这将是一种反模式，不建议在非常特殊的边缘情况之外使用。
- en: So, if we now assume that the orchestration engine has complete and free will
    as to where to place individual instances of the application service and, furthermore,
    that instances can crash and be rescheduled by the orchestrator to different nodes,
    then we will realize that it is a futile task for us to keep track of where the
    individual instances are running at any given time. Even better, we shouldn't
    even try to know this, since it is not important.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们现在假设编排引擎完全自由地决定放置应用服务的各个实例的位置，而且实例可能会崩溃并由编排器重新安排到不同的节点，那么我们会意识到，我们无法追踪每个实例在任何给定时间运行在哪里是一项徒劳的任务。更好的是，我们甚至不应该尝试知道这一点，因为这并不重要。
- en: OK, you might say, but what about if I have two services, A and B, and Service
    A relies on Service B; *shouldn't any given instance of Service A know where it
    can find an instance of Service B? *
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，你可能会说，但如果我有两个服务，A和B，服务A依赖于服务B；*服务A的任何给定实例都应该知道在哪里可以找到服务B的实例吗？*
- en: Here, I have to say loudly and clearly—no, it shouldn't. This kind of knowledge
    is not desirable in a highly distributed and scalable application. Rather, we
    should rely on the orchestrator to provide us with the information that we need
    in order to reach the other service instances that we depend on. It is a bit like
    in the old days of telephony, when we could not directly call our friends, but
    had to call the phone company's central office, where some operator would then
    route us to the correct destination. In our case, the orchestrator plays the role
    of the operator, routing a request coming from an instance of Service A to an
    available instance of Service B. This whole process is called **service discovery**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我必须大声明确地说——不，不应该。在高度分布式和可扩展的应用程序中，这种知识是不可取的。相反，我们应该依赖编排器为我们提供所需的信息，以便访问我们依赖的其他服务实例。这有点像在电话的旧时代，当我们不能直接打电话给朋友，而必须打电话给电话公司的中央办公室，那里的一些操作员会将我们路由到正确的目的地。在我们的情况下，编排器扮演操作员的角色，将来自服务A实例的请求路由到可用的服务B实例。整个过程被称为**服务发现**。
- en: Routing
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 路由
- en: We have learned so far that in a distributed application, we have many interacting
    services. When Service A interacts with Service B, it happens through the exchange
    of data packets. These data packets need to somehow be funneled from Service A
    to Service B. This process of funneling the data packets from a source to a destination
    is also called **routing**. As authors or operators of an application, we do expect
    the orchestrator to take over this task of routing. As we will see in later chapters,
    routing can happen on different levels. It is like in real life. Suppose you're
    working in a big company in one of their office buildings. Now, you have a document
    that needs to be forwarded to another employee of the company. The internal post
    service will pick up the document from your outbox, and take it to the post office
    located in the same building. If the target person works in the same building,
    the document can then be directly forwarded to that person. If, on the other hand,
    the person works in another building of the same block, the document will be forwarded
    to the post office in that target building, from where it is then distributed
    to the receiver through the internal post service. Thirdly, if the document is
    targeted at an employee working in another branch of the company that is located
    in a different city or even country, then the document is forwarded to an external
    postal service such as UPS, which will transport it to the target location, from
    where, once again, the internal post service takes over and delivers it to the
    recipient.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到在分布式应用中，有许多相互作用的服务。当服务A与服务B交互时，它是通过数据包的交换来实现的。这些数据包需要以某种方式从服务A传输到服务B。这个从源到目的地传输数据包的过程也被称为**路由**。作为应用的作者或操作者，我们期望编排器来接管这个路由任务。正如我们将在后面的章节中看到的，路由可以发生在不同的层面。就像在现实生活中一样。假设你在一家大公司的办公楼里工作。现在，你有一份需要转发给公司另一名员工的文件。内部邮件服务将从你的发件箱中取出文件，并将其送到同一建筑物内的邮局。如果目标人员在同一建筑物内工作，文件可以直接转发给该人员。另一方面，如果该人员在同一街区的另一栋建筑物内工作，文件将被转发到目标建筑物的邮局，然后通过内部邮件服务分发给接收者。第三，如果文件的目标是公司位于不同城市甚至不同国家的另一分支机构的员工，那么文件将被转发给UPS等外部邮政服务，后者将把它运送到目标地点，然后再次由内部邮件服务接管并将其送达收件人。
- en: Similar things happen when routing data packets between application services
    that are running in containers. The source and target containers can be located on
    the same cluster node, which corresponds to the situation where both employees
    work in the same building. The target container can be running on a different
    cluster node, which corresponds to the situation where the two employees work
    in different buildings of the same block. Finally, the third situation is when
    a data packet comes from outside of the cluster and has to be routed to the target
    container that is running inside the cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当在容器中运行的应用服务之间路由数据包时，类似的事情会发生。源容器和目标容器可以位于同一集群节点上，这对应于两名员工在同一建筑物内工作的情况。目标容器可以在不同的集群节点上运行，这对应于两名员工在同一街区的不同建筑物内工作的情况。最后，第三种情况是当数据包来自集群外部并且必须路由到集群内部运行的目标容器时。
- en: All these situations, and more, have to be handled by the orchestrator.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器必须处理所有这些情况，以及更多。
- en: Load balancing
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: In a highly available distributed application, all components have to be redundant.
    That means that every application service has to be run in multiple instances,
    so that if one instance fails, the service as a whole is still operational.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在高可用的分布式应用中，所有组件都必须是冗余的。这意味着每个应用服务都必须以多个实例运行，以便如果一个实例失败，整个服务仍然可用。
- en: To make sure that all instances of a service are actually doing work and are
    not just sitting around idle, you have to make sure that the requests for service
    are distributed equally to all the instances. This process of distributing workload
    among service instances is called **load balancing**. Various algorithms exist
    for how the workload can be distributed. Usually, a load balancer works using
    the so-called round robin algorithm, which makes sure that the workload is distributed
    equally to the instances using a cyclic algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保一个服务的所有实例实际上都在工作，而不是闲置，您必须确保对服务的请求均匀分布到所有实例。这种在服务实例之间分配工作负载的过程称为负载均衡。存在各种算法来分配工作负载。通常，负载均衡器使用所谓的轮询算法，确保工作负载使用循环算法均匀分布到实例上。
- en: Once again, we expect the orchestrator to take care of the load balancing requests
    from one service to another, or from external sources to internal services.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们期望编排器处理从一个服务到另一个服务的负载均衡请求，或者从外部来源到内部服务的请求。
- en: Scaling
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: When running our containerized, distributed application in a cluster that is
    managed by an orchestrator, we additionally want an easy way to handle expected
    or unexpected increases in workload. To handle an increased workload, we usually
    just schedule additional instances of a service that is experiencing this increased
    load. Load balancers will then automatically be configured to distribute the workload
    over more available target instances.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当在由编排器管理的集群中运行我们的容器化分布式应用程序时，我们还希望有一种简单的方式来处理预期或意外的工作负载增加。为了处理增加的工作负载，我们通常会安排正在经历增加负载的服务的额外实例。然后负载均衡器将自动配置为在更多可用的目标实例之间分发工作负载。
- en: But in real-life scenarios, the workload varies over time. If we look at a shopping
    site such as Amazon, it might have a high load during peak hours in the evening,
    when everyone is at home and shopping online; it may experience extreme loads
    during special days such as Black Friday; and it may experience very little traffic
    early in the morning. Thus, services need to not just be able to scale up, but
    also to scale down when the workload goes down.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但在现实场景中，工作负载会随时间变化而变化。如果我们看一个像亚马逊这样的购物网站，它在晚上高峰时段可能会有很高的负载，当每个人都在家里网上购物；在特殊的日子，比如黑色星期五，它可能会经历极端的负载；而在早晨可能会经历很少的流量。因此，服务不仅需要能够扩展，还需要在工作负载减少时能够缩减。
- en: We also expect orchestrators to distribute the instances of a service in a meaningful
    way when scaling up or down. It would not be wise to schedule all instances of
    the service on the same cluster node, since, if that node goes down, the whole
    service goes down. The scheduler of the orchestrator, which is responsible for
    the placement of the containers, needs to also consider not placing all instances
    into the same rack of computers, since, if the power supply of the rack fails,
    again, the whole service is affected. Furthermore, service instances of critical
    services should even be distributed across data centers in order to avoid outages.
    All these decisions, and many more, are the responsibility of the orchestrator.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还期望编排器在扩展时以有意义的方式分发服务的实例。将所有服务实例安排在同一集群节点上是不明智的，因为如果该节点宕机，整个服务就会宕机。编排器的调度程序负责容器的放置，还需要考虑不将所有实例放置在同一台计算机机架上，因为如果机架的电源供应失败，整个服务将受到影响。此外，关键服务的服务实例甚至应该分布在数据中心，以避免中断。所有这些决定，以及许多其他决定，都是编排器的责任。
- en: In the cloud, instead of computer racks, the term '**availability zones**' is
    often used.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中，通常使用“可用区”这个术语，而不是计算机机架。
- en: Self-healing
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自愈
- en: These days, orchestrators are very sophisticated and can do a lot for us to
    maintain a healthy system. Orchestrators monitor all containers that are running
    in the cluster, and they automatically replace crashed or unresponsive ones with
    new instances. Orchestrators monitor the health of cluster nodes, and take them
    out of the scheduler loop if a node becomes unhealthy or is down. A workload that
    was located on those nodes is automatically rescheduled to different available
    nodes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，编排器非常复杂，可以为我们做很多事情来维护一个健康的系统。编排器监视集群中运行的所有容器，并自动用新实例替换崩溃或无响应的容器。编排器监视集群节点的健康状况，并在节点变得不健康或宕机时将其从调度循环中移除。原本位于这些节点上的工作负载会自动重新调度到其他可用节点上。
- en: All these activities, where the orchestrator monitors the current state and
    automatically repairs the damage or reconciles the desired state, lead to a so-called **self-healing** system.
    We do not, in most cases, have to actively engage and repair damage. The orchestrator
    will do this for us automatically.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些活动，编排器监视当前状态并自动修复损坏或协调期望状态，导致了所谓的**自愈**系统。在大多数情况下，我们不需要积极参与和修复损害。编排器会自动为我们完成这些工作。
- en: However, there are a few situations that the orchestrator cannot handle without
    our help. Imagine a situation where we have a service instance running in a container.
    The container is up and running and, from the outside, looks perfectly healthy.
    But, the application running inside it is in an unhealthy state. The application
    did not crash, it just is not able to work as it was originally designed anymore. *How
    could the orchestrator possibly know about this without us giving it a hint?* It
    can't! Being in an unhealthy or invalid state means something completely different
    for each application service. In other words, the health status is service dependent.
    Only the authors of the service, or its operators, know what health means in the
    context of a service.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一些情况编排器无法在没有我们帮助的情况下处理。想象一种情况，我们有一个运行在容器中的服务实例。容器正在运行，并且从外部看起来非常健康。但是，容器内部运行的应用程序处于不健康状态。应用程序没有崩溃，只是不能再像最初设计的那样工作了。编排器怎么可能在没有我们提示的情况下知道这一点呢？它不可能！处于不健康或无效状态对每个应用服务来说意味着完全不同。换句话说，健康状态是与服务相关的。只有服务的作者或其操作者知道在服务的上下文中健康意味着什么。
- en: 'Now, orchestrators define seams or probes, over which an application service
    can communicate to the orchestrator about what state it is in. Two fundamental
    types of probe exist:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编排器定义了应用服务可以与其通信的接口或探针。存在两种基本类型的探针：
- en: The service can tell the orchestrator that it is healthy or not
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务可以告诉编排器它的健康状态
- en: The service can tell the orchestrator that it is ready or temporarily unavailable
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务可以告诉编排器它已经准备好或者暂时不可用
- en: How the service determines either of the preceding answers is totally up to
    the service. The orchestrator only defines how it is going to ask, for example,
    through an `HTTP GET` request, or what type of answers it is expecting, for example,
    `OK` or `NOT OK.`
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 服务如何确定前面提到的任一答案完全取决于服务本身。编排器只定义了它将如何询问，例如通过`HTTP GET`请求，或者它期望的答案类型，例如`OK`或`NOT
    OK`。
- en: If our services implement logic in order to answer the preceding health or availability
    questions, then we have a truly self-healing system, since the orchestrator can
    kill unhealthy service instances and replace them with new healthy ones, and it
    can take service instances that are temporarily unavailable out of the load balancer's
    round robin.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的服务实现了逻辑来回答前面提到的健康或可用性问题，那么我们就拥有了一个真正的自愈系统，因为编排器可以终止不健康的服务实例并用新的健康实例替换它们，还可以将暂时不可用的服务实例从负载均衡器的轮询中移除。
- en: Zero downtime deployments
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零停机部署
- en: These days, it gets harder and harder to justify a complete downtime for a mission-critical
    application that needs to be updated. Not only does that mean missed opportunities,
    but it can also result in a damaged reputation for the company. Customers using
    the application are no longer prepared to accept such an inconvenience, and will
    turn away quickly. Furthermore, our release cycles get shorter and shorter. Where,
    in the past, we would have one or two new releases per year, these days, a lot
    of companies update their applications multiple times a week, or even multiple
    times per day.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，很难再为需要更新的关键任务应用程序辩解完全停机。这不仅意味着错失机会，还可能导致公司声誉受损。使用该应用程序的客户不再愿意接受这样的不便，并会迅速离开。此外，我们的发布周期变得越来越短。在过去，我们每年可能会有一两次新版本发布，但如今，许多公司每周甚至每天多次更新他们的应用程序。
- en: The solution to that problem is to come up with a zero downtime application
    update strategy. The orchestrator needs to be able to update individual application
    services, batch-wise. This is also called **rolling updates**. At any given time,
    only one or a few of the total number of instances of a given service are taken
    down and replaced by the new version of the service. Only if the new instances
    are operational, and do not produce any unexpected errors or show any misbehavior,
    will the next batch of instances be updated. This is repeated until all instances
    are replaced with their new version. If, for some reason, the update fails, then
    we expect the orchestrator to automatically roll the updated instances back to
    their previous version.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是提出一个零停机应用程序更新策略。编排器需要能够逐批更新单个应用程序服务。这也被称为**滚动更新**。在任何给定时间，只有给定服务的总实例数中的一个或几个会被关闭，并被该服务的新版本替换。只有新实例是可操作的，并且不会产生任何意外错误或显示任何不当行为，才会更新下一批实例。这一过程重复进行，直到所有实例都被替换为它们的新版本。如果由于某种原因更新失败，那么我们期望编排器自动将更新的实例回滚到它们的先前版本。
- en: Other possible zero downtime deployments are blue–green deployments and canary
    releases. In both cases, the new version of a service is installed in parallel
    with the current, active version. But initially, the new version is only accessible
    internally. Operations can then run smoke tests against the new version, and when
    the new version seems to be running just fine, then, in the case of a blue–green
    deployment, the router is switched from the current blue version, to the new green
    version. For some time, the new green version of the service is closely monitored
    and, if everything is fine, the old blue version can be decommissioned. If, on
    the other hand, the new green version does not work as expected, then it is only
    a matter of setting the router back to the old blue version in order to achieve
    a complete rollback.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可能的零停机部署包括蓝绿部署和金丝雀发布。在这两种情况下，服务的新版本与当前活动版本并行安装。但最初，新版本只能在内部访问。运营人员可以对新版本运行烟雾测试，当新版本似乎运行良好时，就可以在蓝绿部署的情况下，将路由器从当前蓝色版本切换到新的绿色版本。一段时间内，新的绿色版本的服务将受到密切监控，如果一切正常，旧的蓝色版本就可以被废弃。另一方面，如果新的绿色版本不如预期那样工作，那么只需将路由器设置回旧的蓝色版本，就可以实现完全回滚。
- en: In the case of a canary release, the router is configured in such a way that
    it funnels a tiny percentage, say 1%, of the overall traffic through the new version
    of the service, while 99% of the traffic is still routed through the old version.
    The behavior of the new version is closely monitored and compared to the behavior
    of the old version. If everything looks good, then the percentage of the traffic
    that is funneled through the new service is slightly increased. This process is
    repeated until 100% of the traffic is routed through the new service. If the new
    service has run for a while and everything looks good, then the old service can
    be decommissioned.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀发布的情况下，路由器被配置为将整体流量的一小部分，比如1%，引导到服务的新版本，而仍然有99%的流量通过旧版本路由。新版本的行为受到密切监视，并与旧版本的行为进行比较。如果一切正常，那么通过新服务引导的流量百分比会略微增加。这个过程会重复，直到100%的流量通过新服务路由。如果新服务运行一段时间并且一切正常，那么旧服务可以被停用。
- en: Most orchestrators support at least the rolling update type of zero downtime
    deployment out of the box. Blue–green deployments and canary releases are often
    quite easy to implement.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数编排器至少支持开箱即用的滚动更新类型的零停机部署。蓝绿部署和金丝雀发布通常很容易实现。
- en: Affinity and location awareness
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲和性和位置感知
- en: Sometimes, certain application services require the availability of dedicated
    hardware on the nodes on which they run. For example, I/O-bound services require
    cluster nodes with an attached high-performance **solid-state drive** (**SSD**),
    or some services that are used for machine learning, or similar, require an **Accelerated
    Processing Unit** (**APU**). Orchestrators allow us to define node affinities
    per application service. The orchestrator will then make sure that its scheduler
    only schedules containers on cluster nodes that fulfill the required criteria.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，某些应用服务需要节点上专用硬件的可用性。例如，I/O密集型服务需要具有附加高性能**固态硬盘**（**SSD**）的集群节点，或者用于机器学习等用途的某些服务需要**加速处理单元**（**APU**）。编排器允许我们为每个应用服务定义节点亲和性。然后，编排器将确保其调度程序仅在满足所需条件的集群节点上调度容器。
- en: Defining an affinity to a particular node should be avoided; this would introduce
    a single point of failure and thus compromise high availability. Always define
    a set of multiple cluster nodes as the target for an application service.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 避免将亲和力定义为特定节点；这将引入单点故障，从而损害高可用性。始终将多个集群节点定义为应用服务的目标。
- en: Some orchestration engines also support what is called **location awareness** or **geo
    awareness**. What this means is that you can ask the orchestrator to equally distribute
    instances of a service over a set of different locations. You could, for example,
    define a `datacenter` label, with the possible `west`, `center`, and `east` values,
    and apply the label to all of the cluster nodes with the value that corresponds
    to the geographical region in which the respective node is located. Then, you
    instruct the orchestrator to use this label for the geo awareness of a certain
    application service. In this case, if you request nine replicas of the service,
    then the orchestrator would make sure that three instances are deployed to the
    nodes in each of the three data centers—west, center, and east.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一些编排引擎还支持所谓的**位置感知**或**地理感知**。这意味着您可以要求编排器将服务的实例均匀分布在不同位置的一组位置上。例如，您可以定义一个`数据中心`标签，其可能的值为`西`、`中`和`东`，并将该标签应用于具有对应于各自节点所在地理区域的值的所有集群节点。然后，您指示编排器使用此标签来进行某个应用服务的地理感知。在这种情况下，如果您请求该服务的九个副本，那么编排器将确保将三个实例部署到每个数据中心的节点中——西、中和东。
- en: Geo awareness can even be defined hierarchically; for example, you can have
    a data center as the top-level discriminator, followed by the availability zone.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 地理意识甚至可以按层次定义；例如，您可以将数据中心作为最高级别的判别器，然后是可用区。
- en: Geo awareness, or location awareness, is used to decrease the probability of
    outages due to power supply failures or data center outages. If the application
    instances are distributed across nodes, availability zones, or even data centers,
    it is extremely unlikely that everything will go down at once. One region will
    always be available.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 地理意识或位置意识用于减少由电源供应故障或数据中心故障导致的中断的概率。如果应用实例分布在节点、可用区甚至数据中心之间，那么一切同时崩溃的可能性极小。总会有一个地区是可用的。
- en: Security
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全
- en: These days, security in IT is a very hot topic. Cyber warfare is at an all-time
    high. Most high-profile companies have been victims of hacker attacks, with very
    costly consequences. One of the worst nightmares of each **chief information officer** (**CIO**)
    or **chief technology officer** (**CTO**) is to wake up in the morning and hear
    in the news that their company has become a victim of a hacker attack, and that
    sensitive information has been stolen or compromised.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，IT安全是一个非常热门的话题。网络战争达到了历史最高点。大多数知名公司都曾是黑客攻击的受害者，造成了非常昂贵的后果。每个首席信息官（CIO）或首席技术官（CTO）最糟糕的噩梦之一就是早上醒来听到自己的公司成为黑客攻击的受害者，并且敏感信息被窃取或泄露的消息。
- en: To counter most of these security threats, we need to establish a secure software
    supply chain, and enforce security defense in depth. Let's look at some of the
    tasks that you can expect from an enterprise-grade orchestrator.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对抗大多数安全威胁，我们需要建立一个安全的软件供应链，并在深度上强制执行安全防御。让我们来看看您可以从企业级编排器中期望的一些任务。
- en: Secure communication and cryptographic node identity
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全通信和加密节点身份
- en: First and foremost, we want to make sure that our cluster that is managed by
    the orchestrator is secure. Only trusted nodes can join the cluster. Each node
    that joins the cluster gets a cryptographic node identity, and all communication
    between the nodes must be encrypted. For this, nodes can use **M****utual Transport
    Layer Security** (**MTLS**). In order to authenticate nodes of the cluster with
    each other, certificates are used. These certificates are automatically rotated
    periodically, or on request, to protect the system in case a certificate is leaked.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们希望确保由编排器管理的集群是安全的。只有受信任的节点才能加入集群。加入集群的每个节点都会获得一个加密的节点身份，并且节点之间的所有通信必须加密。为此，节点可以使用相互传输层安全（MTLS）。为了相互认证集群的节点，使用证书。这些证书会定期自动轮换，或者根据请求进行轮换，以保护系统以防证书泄露。
- en: 'The communication that happens in a cluster can be separated into three types.
    You talk about communication planes—management, control, and data planes:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中发生的通信可以分为三种类型。您可以谈论通信平面-管理、控制和数据平面：
- en: The management plane is used by the cluster managers, or masters, to, for example,
    schedule service instances, execute health checks, or create and modify any other
    resources in the cluster, such as data volumes, secrets, or networks.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理平面由集群管理器或主节点使用，例如，调度服务实例，执行健康检查，或创建和修改集群中的任何其他资源，如数据卷、密钥或网络。
- en: The control plane is used to exchange important state information between all
    nodes of the cluster. This kind of information is, for example, used to update
    the local IP tables on clusters, which are used for routing purposes.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面用于在集群的所有节点之间交换重要的状态信息。例如，这种信息用于更新用于路由目的的集群上的本地IP表。
- en: The data plane is where the actual application services communicate with each
    other and exchange data.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据平面是实际应用服务相互通信和交换数据的地方。
- en: Normally, orchestrators mainly care about securing the management and control
    plane. Securing the data plane is left to the user, although the orchestrator
    may facilitate this task.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，编排器主要关心保护管理和控制平面。保护数据平面留给用户，尽管编排器可能会促进这项任务。
- en: Secure networks and network policies
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全网络和网络策略
- en: When running application services, not every service needs to communicate with
    every other service in the cluster. Thus, we want the ability to sandbox services
    from each other, and only run those services in the same networking sandbox that
    absolutely need to communicate with each other. All other services and all network
    traffic coming from outside of the cluster should have no possibility of accessing
    the sandboxed services.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行应用服务时，并非每个服务都需要与集群中的其他服务通信。因此，我们希望能够将服务相互隔离，并且只在绝对需要相互通信的情况下在相同的网络沙盒中运行这些服务。所有其他服务和来自集群外部的所有网络流量都不应该有可能访问被隔离的服务。
- en: There are at least two ways in which this network-based sandboxing can happen.
    We can either use a **software-defined network** (**SDN**) to group application
    services, or we can have one flat network, and use network policies to control
    who does and does not have access to a particular service or group of services.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有两种网络沙盒化的方式。我们可以使用软件定义网络（SDN）来分组应用服务，或者我们可以使用一个扁平网络，并使用网络策略来控制谁有权访问特定服务或服务组。
- en: Role-based access control (RBAC)
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于角色的访问控制（RBAC）
- en: One of the most important tasks (next to security) that an orchestrator must
    fulfill in order to make it enterprise-ready is to provide role-based access to
    the cluster and its resources. RBAC defines how subjects, users, or groups of
    users of the system, organized into teams, and so on, can access and manipulate
    the system. It makes sure that unauthorized personnel cannot do any harm to the
    system, nor can they see any of the available resources in the system that they're
    not supposed to know of or see.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器必须履行的最重要任务之一（除了安全性）是为集群及其资源提供基于角色的访问。RBAC定义了系统的主体、用户或用户组，组织成团队等如何访问和操作系统。它确保未经授权的人员无法对系统造成任何伤害，也无法看到他们不应该知道或看到的系统中的任何可用资源。
- en: A typical enterprise might have user groups such as Development, QA, and Prod,
    and each of those groups can have one or many users associated with it. John Doe,
    the developer, is a member of the Development group and, as such, can access resources
    that are dedicated to the development team, but he cannot access, for example,
    the resources of the Prod team, of which Ann Harbor is a member. She, in turn,
    cannot interfere with the Development team's resources.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的企业可能有开发、QA和生产等用户组，每个组都可以有一个或多个用户与之关联。开发人员约翰·多伊是开发组的成员，因此可以访问专门为开发团队提供的资源，但他不能访问例如生产团队的资源，其中安·哈伯是成员。反过来，她也不能干扰开发团队的资源。
- en: One way of implementing RBAC is through the definition of **grants**. A grant
    is an association between a subject, a role, and a resource collection. Here,
    a role is comprised of a set of access permissions to a resource. Such permissions
    can be to create, stop, remove, list, or view containers; to deploy a new application
    service; to list cluster nodes or view the details of a cluster node; and many
    more.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实施RBAC的一种方式是通过定义授权。授权是主体、角色和资源集合之间的关联。在这里，角色由对资源的一组访问权限组成。这些权限可以是创建、停止、删除、列出或查看容器；部署新的应用服务；列出集群节点或查看集群节点的详细信息；以及许多其他权限。
- en: A resource collection is a group of logically related resources of the cluster,
    such as application services, secrets, data volumes, or containers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 资源集合是集群中逻辑相关的资源的组合，例如应用服务、秘密、数据卷或容器。
- en: Secrets
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 秘密
- en: In our daily life, we have loads of secrets. Secrets are information that is
    not meant to be publicly known, such as the username and password combination
    that you use to access your online bank account, or the code to your cell phone
    or your locker at the gym.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的日常生活中，我们有很多秘密。秘密是不应该公开知道的信息，比如你用来访问在线银行账户的用户名和密码组合，或者你手机或健身房储物柜的密码。
- en: When writing software, we often need to use secrets, too. For example, we need
    a certificate to authenticate our application service with the external service
    that we want to access, or we need a token to authenticate and authorize our service
    when accessing some other API. In the past, developers, for convenience, have
    just hardcoded those values, or put them in clear text in some external configuration
    files. There, this very sensitive information has been accessible to a broad audience,
    which, in reality, should never have had the opportunity to see those secrets.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写软件时，我们经常也需要使用秘密。例如，我们需要一个证书来验证我们的应用服务与我们想要访问的外部服务进行身份验证，或者我们需要一个令牌来在访问其他API时验证和授权我们的服务。过去，为了方便起见，开发人员通常会将这些值硬编码，或者将它们以明文形式放在一些外部配置文件中。在那里，这些非常敏感的信息对广大观众都是可访问的，而实际上，他们本不应该有机会看到这些秘密。
- en: Luckily, these days, orchestrators offer what's called secrets to deal with
    such sensitive information in a highly secure way. Secrets can be created by authorized
    or trusted personnel. The values of those secrets are then encrypted and stored
    in the highly available cluster state database. The secrets, since they are encrypted,
    are now secure at rest. Once a secret is requested by an authorized application
    service, the secret is only forwarded to the cluster nodes that actually run an
    instance of that particular service, and the secret value is never stored on the
    node but mounted into the container in a `tmpfs` RAM-based volume. Only inside
    the respective container is the secret value available in clear text.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这些天，编排器提供了所谓的秘密，以高度安全的方式处理这些敏感信息。秘密可以由授权或信任的人员创建。这些秘密的值然后被加密并存储在高可用的集群状态数据库中。由于这些秘密是加密的，所以它们现在在静态时是安全的。一旦一个被授权的应用服务请求一个秘密，该秘密只会被转发到实际运行该特定服务实例的集群节点，并且秘密值永远不会存储在节点上，而是挂载到容器中的`tmpfs`基于RAM的卷中。只有在相应的容器内，秘密值才以明文形式可用。
- en: We already mentioned that the secrets are secure at rest. Once they are requested
    by a service, the cluster manager, or master, decrypts the secret and sends it
    over the wire to the target nodes. *So, what about the secrets being secure in
    transit?* Well, we learned earlier that the cluster nodes use MTLS for their communication,
    and so the secret, although transmitted in clear text, is still secure, since
    data packets will be encrypted by MTLS. Thus, secrets are secure both at rest
    and in transit. Only services that are authorized to use secrets will ever have
    access to those secret values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，秘密在静态时是安全的。一旦它们被服务、集群管理器或主节点请求，主节点会解密秘密并将其通过网络发送到目标节点。*那么，秘密在传输过程中安全吗？*嗯，我们之前了解到集群节点使用MTLS进行通信，因此即使秘密以明文传输，也仍然是安全的，因为数据包将被MTLS加密。因此，秘密在静态和传输过程中都是安全的。只有被授权使用秘密的服务才能访问这些秘密值。
- en: Content trust
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容信任
- en: For added security, we want to make sure that only trusted images run in our
    production cluster. Some orchestrators allow us to configure a cluster so that
    it can only ever run signed images. Content trust and signing images is all about
    making sure that the authors of the image are the ones that we expect them to
    be, namely, our trusted developers or, even better, our trusted CI server. Furthermore,
    with content trust, we want to guarantee that the image that we get is fresh,
    and is not an old and maybe vulnerable image. And finally, we want to make sure
    that the image cannot be compromised by malicious hackers in transit. The latter
    is often called a **man-in-the-middle** (**MITM**) attack.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加安全性，我们希望确保只有受信任的图像在我们的生产集群中运行。一些编排器允许我们配置集群，以便它只能运行经过签名的图像。内容信任和签署图像的目的在于确保图像的作者是我们所期望的人，即我们信任的开发人员，甚至更好的是我们信任的CI服务器。此外，通过内容信任，我们希望保证我们获取的图像是新鲜的，而不是旧的，可能存在漏洞的图像。最后，我们希望确保图像在传输过程中不会被恶意黑客篡改。后者通常被称为**中间人**（**MITM**）攻击。
- en: By signing images at the source, and validating the signature at the target,
    we can guarantee that the images that we want to run are not compromised.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在源头签署图像，并在目标处验证签名，我们可以保证我们想要运行的图像没有被篡改。
- en: Reverse uptime
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逆向正常运行时间
- en: The last point I want to discuss in the context of security is reverse uptime. *What
    do we mean by that?* Imagine that you have configured and secured a production
    cluster. On this cluster, you're running a few mission-critical applications of
    your company. Now, a hacker has managed to find a security hole in one of your
    software stacks, and has gained root access to one of your cluster nodes. That
    alone is already bad enough but, even worse, this hacker could now mask their
    presence on this node, on which they have root access, after all, and then use
    it as a base to attack other nodes in your cluster.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在安全性的背景下讨论的最后一点是逆向正常运行时间。*这是什么意思呢？*想象一下，你已经配置和保护了一个生产集群。在这个集群上，你正在运行公司的一些关键应用程序。现在，一个黑客设法在你的软件堆栈中找到了一个安全漏洞，并且已经获得了对你的集群节点的root访问权限。这本身已经够糟糕了，但更糟糕的是，这个黑客现在可以掩盖他们在这个节点上的存在，毕竟他们已经有了root访问权限，然后将其用作攻击你的集群中其他节点的基地。
- en: Root access in Linux or any Unix-type operating system means that you can do
    anything on this system. It is the highest level of access that someone can have.
    In Windows, the equivalent role is that of an administrator.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux或任何Unix类型的操作系统中，root访问权限意味着你可以在这个系统上做任何事情。这是某人可以拥有的最高级别的访问权限。在Windows中，相当于这个角色的是管理员。
- en: But *what if we leverage the fact that containers are ephemeral and cluster
    nodes are quickly provisioned, usually in a matter of minutes if fully automated?* We
    just kill each cluster node after a certain uptime of, say, 1 day. The orchestrator
    is instructed to drain the node and then exclude it from the cluster. Once the
    node is out of the cluster, it is torn down and replaced by a freshly provisioned
    node.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，*如果我们利用容器是短暂的，集群节点通常可以快速配置，通常在几分钟内完全自动化的情况下呢？*我们只需在一定的正常运行时间后关闭每个集群节点，比如说1天。编排器被指示排空节点，然后将其从集群中排除。一旦节点离开集群，它就会被拆除并被一个新配置的节点所取代。
- en: That way, the hacker has lost their base and the problem has been eliminated.
    This concept is not yet broadly available, though, but to me it seems to be a
    huge step toward increased security and, as far as I have discussed it with engineers
    who are working in this area, it is not difficult to implement.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，黑客就失去了他们的基地，问题也被消除了。尽管这个概念目前还没有广泛应用，但对我来说，这似乎是向增加安全性迈出的一大步，而且据我与在这个领域工作的工程师讨论，实施起来并不困难。
- en: Introspection
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内省
- en: So far, we have discussed a lot of tasks for which the orchestrator is responsible,
    and that it can execute in a completely autonomous way. But there is also the
    need for human operators to be able to see and analyze what's currently running
    on the cluster, and in what state or health the individual applications are. For
    all this, we need the possibility of introspection. The orchestrator needs to
    surface crucial information in a way that is easily consumable and understandable.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了许多由编排器负责的任务，它可以完全自主地执行。但是，人类操作员也需要能够查看和分析集群上当前运行的内容，以及个别应用程序的状态或健康状况。为了做到这一点，我们需要进行内省。编排器需要以易于消化和理解的方式呈现关键信息。
- en: The orchestrator should collect system metrics from all the cluster nodes and
    make them accessible to the operators. Metrics include CPU, memory and disk usage,
    network bandwidth consumption, and more. The information should be easily available
    on a node-per-node basis, as well as in an aggregated form.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器应该从所有集群节点收集系统指标，并使其对操作员可访问。指标包括CPU、内存和磁盘使用情况、网络带宽消耗等。这些信息应该以逐个节点的方式轻松获取，以及以汇总形式获取。
- en: We also want the orchestrator to give us access to logs that are produced by
    service instances or containers. Even more, the orchestrator should provide us
    with `exec` access to each and every container if we have the correct authorization
    to do so. With `exec` access to containers, you can then debug misbehaving containers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望编排器能够让我们访问由服务实例或容器产生的日志。此外，如果我们有正确的授权，编排器还应该为我们提供对每个容器的`exec`访问权限。有了对容器的`exec`访问权限，您就可以调试行为不端的容器。
- en: In highly distributed applications, where each request to the application goes
    through numerous services until it is completely handled, tracing requests is
    a really important task. Ideally, the orchestrator supports us in implementing
    a tracing strategy, or gives us some good guidelines to follow.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在高度分布式的应用程序中，每个对应用程序的请求都要经过多个服务，直到完全处理，跟踪请求是一项非常重要的任务。理想情况下，编排器支持我们实施跟踪策略，或者给我们一些好的遵循指南。
- en: Finally, human operators can best monitor a system when working with a graphical
    representation of all the collected metrics and logging and tracing information.
    Here, we are speaking about dashboards. Every decent orchestrator should offer
    at least some basic dashboard with a graphical representation of the most critical
    system parameters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，人类操作员在使用所有收集到的指标、日志和跟踪信息的图形表示时，可以最好地监视系统。在这里，我们谈论的是仪表板。每个体面的编排器都应该提供至少一些基本的仪表板，以图形方式表示最关键的系统参数。
- en: However, human operators are not the only ones concerned about introspection.
    We also need to be able to connect external systems with the orchestrator in order
    to consume this information. There needs to be an API available, over which external
    systems can access data such as cluster state, metrics, and logs, and use this
    information to make automated decisions, such as creating pager or phone alerts,
    sending out emails, or triggering an alarm siren if some thresholds are exceeded
    by the system.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人类操作员并不是唯一关心内省的人。我们还需要能够将外部系统连接到编排器，以便消费这些信息。需要提供一个API，通过该API，外部系统可以访问集群状态、指标和日志等数据，并利用这些信息做出自动决策，例如创建警报或电话警报、发送电子邮件，或者在系统超过某些阈值时触发警报。
- en: Overview of popular orchestrators
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行编排器的概述
- en: At the time of writing, there are many orchestration engines out there and in
    use, but there are a few clear winners. The number one spot is clearly held by
    Kubernetes, which reigns supreme. A distant second is Docker's own SwarmKit, followed by others
    such as Apache Mesos, AWS **Elastic Container Service** (**ECS**), or Microsoft **Azure
    Container Service** (**ACS**).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，有许多编排引擎在使用中，但有一些明显的赢家。第一名显然是由Kubernetes占据，它统治着。遥遥领先的第二名是Docker自己的SwarmKit，其次是其他一些，如Apache
    Mesos，AWS弹性容器服务（ECS），或Microsoft Azure容器服务（ACS）。
- en: Kubernetes
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes was originally designed by Google and later donated to the **Cloud
    Native Computing Foundation** (**CNCF**). Kubernetes was modeled after Google's
    proprietary Borg system, which has been running containers on a super massive
    scale for years. Kubernetes was Google's attempt to go back to the drawing board,
    and completely start over and design a system that incorporates all the lessons
    that were learned with Borg.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes最初由Google设计，后来捐赠给了云原生计算基金会（CNCF）。Kubernetes是模仿Google专有的Borg系统而设计的，该系统多年来一直在超大规模上运行容器。Kubernetes是Google重新设计的尝试，完全重新开始并设计一个系统，其中包含了与Borg学到的所有教训。
- en: Contrary to Borg, which is proprietary technology, Kubernetes was open sourced
    early on. This was a very wise choice by Google, since it attracted a huge number
    of contributors from outside of the company and, over only a couple of years,
    an even more massive ecosystem evolved around Kubernetes. You can rightfully say
    that Kubernetes is the darling of the community in the container orchestration
    space. No other orchestrator has been able to produce so much hype and attract
    so many talented people who are willing to contribute in a meaningful way to the
    success of the project as a contributor or an early adopter.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与专有技术Borg相反，Kubernetes在早期就开源了。这是Google的一个非常明智的选择，因为它吸引了大量来自公司外部的贡献者，仅仅在短短几年内，Kubernetes周围的生态系统更加庞大。你可以说Kubernetes是容器编排领域社区的宠儿。没有其他编排器能够产生如此多的炒作，并吸引如此多愿意以有意义的方式为项目的成功做出贡献的人才，无论是作为贡献者还是早期采用者。
- en: In that regard, Kubernetes in the container orchestration space looks to me
    very much like what Linux has become in the server operating system space. Linux
    has become the *de facto* standard of server operating systems. All relevant companies,
    such as Microsoft, IBM, Amazon, Red Hat, and even Docker, have embraced Kubernetes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，Kubernetes在容器编排领域对我来说非常像Linux在服务器操作系统领域所变成的。Linux已经成为服务器操作系统的事实标准。所有相关公司，如微软、IBM、亚马逊、红帽，甚至Docker，都已经接受了Kubernetes。
- en: 'And there is one thing that cannot be denied: Kubernetes was designed from
    the very beginning for massive scalability. After all, it was designed with Google
    Borg in mind.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有一件事是无法否认的：Kubernetes从一开始就被设计用于大规模扩展。毕竟，它是以Google Borg为目标而设计的。
- en: One negative aspect that you could be voiced against Kubernetes is that it is
    still complex to set up and manage, at least at the time of writing. There is
    a significant hurdle to overcome for newcomers. The first step is steep, but once
    you have worked with this orchestrator for a while, it all makes sense. The overall
    design is carefully thought through and executed very well.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以提出反对Kubernetes的一个负面方面是，至少在撰写本文时，它仍然很复杂，设置和管理起来。对于新手来说，这是一个重大障碍。第一步是艰难的，但一旦你使用这个编排器一段时间，一切就会变得清晰。整体设计经过深思熟虑，执行得非常好。
- en: In release 1.10 of Kubernetes, whose **general availability** (**GA**) was in
    March 2018, most of the initial shortcomings compared to other orchestrators such
    as Docker Swarm have been eliminated. For example, security and confidentiality
    is now not only an afterthought, but an integral part of the system.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的1.10版本中，与其他编排器（如Docker Swarm）相比，最初的缺点大多已经消除。例如，安全性和保密性现在不仅仅是一个事后的考虑，而是系统的一个组成部分。
- en: New features are implemented at a tremendous speed. New releases are happening
    every 3 months or so, more precisely, about every 100 days. Most of the new features
    are demand-driven, that is, companies using Kubernetes to orchestrate their mission-critical
    applications can voice their needs. This makes Kubernetes enterprise-ready. It
    would be wrong to assume that this orchestrator is only for start-ups and not
    for risk-averse enterprises. The contrary is the case. *On what do I base this
    claim?* Well, my claim is justified by the fact that companies such as Microsoft,
    Docker, and Red Hat, whose clients are mostly big enterprises, have fully embraced
    Kubernetes, and provide enterprise-grade support for it if it is used and integrated
    into their enterprise offerings.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 新功能以惊人的速度实施。新版本大约每3个月发布一次，更确切地说，大约每100天发布一次。大多数新功能都是需求驱动的，也就是说，使用Kubernetes来编排其关键任务应用程序的公司可以提出他们的需求。这使得Kubernetes适合企业使用。认为这个编排器只适用于初创企业而不适用于风险规避型企业是错误的。相反的情况是。*我基于什么来做出这个断言？*嗯，我的断言是有根据的，因为像微软、Docker和红帽这样的公司，他们的客户大多是大型企业，已经完全接受了Kubernetes，并为其提供企业级支持，如果它被用于并集成到他们的企业产品中。
- en: Kubernetes supports both Linux and Windows containers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持Linux和Windows容器。
- en: Docker Swarm
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: It is well known that Docker popularized and commoditized software containers.
    Docker did not invent containers, but standardized them and made them broadly
    available, not least by offering the free image registry—Docker Hub. Initially,
    Docker focused mainly on the developer and the development life cycle. However,
    companies that started to use and love containers soon also wanted to use them
    not just during the development or testing of new applications, but also to run
    those applications in production.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，Docker推广和商品化了软件容器。Docker并没有发明容器，但是标准化了它们，并使其广泛可用，其中包括提供免费镜像注册表—Docker Hub。最初，Docker主要关注开发人员和开发生命周期。然而，开始使用和喜爱容器的公司很快也希望不仅在开发或测试新应用程序时使用它们，而且在生产中运行这些应用程序时也使用它们。
- en: Initially, Docker had nothing to offer in that space, so other companies jumped
    into that vacuum and offered help to the users. But it didn't take long, and Docker
    recognized that there was a huge demand for a simple yet powerful orchestrator.
    Docker's first attempt was a product called classic swarm. It was a standalone
    product that enabled users to create a cluster of Docker host machines that could
    be used to run and scale their containerized applications in a highly available
    and self-healing way.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Docker在这个领域没有什么可提供的，所以其他公司跳进这个真空并为用户提供帮助。但是没过多久，Docker意识到有一个对于一个简单而强大的编排器的巨大需求。Docker的第一次尝试是一个名为经典Swarm的产品。它是一个独立的产品，使用户能够创建一个Docker主机集群，可以用于以高可用和自愈的方式运行和扩展其容器化应用程序。
- en: The setup of a classic Docker swarm, though, was hard. A lot of complicated
    manual steps were involved. Customers loved the product, but struggled with its
    complexity. So, Docker decided it could do better. It went back to the drawing
    board and came up with SwarmKit. SwarmKit was introduced at DockerCon 2016 in
    Seattle, and was an integral part of the newest version of the Docker engine.
    Yes, you got that right; SwarmKit was, and still is to this day, an integral part
    of the Docker engine. Thus, if you install a Docker host, you automatically have
    SwarmKit available with it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，经典Docker Swarm的设置很困难。涉及许多复杂的手动步骤。客户喜欢这个产品，但在处理其复杂性时遇到了困难。因此，Docker决定可以做得更好。它回到了起点，并提出了SwarmKit。SwarmKit在2016年的DockerCon大会上在西雅图推出，并成为最新版本的Docker引擎的一个重要组成部分。是的，你没听错；SwarmKit是，直到今天仍然是Docker引擎的一个重要组成部分。因此，如果你安装了Docker主机，你自动就有了SwarmKit。
- en: SwarmKit was designed with simplicity and security in mind. The mantra was,
    and still is, that it has to be almost trivial to set up a swarm, and that the
    swarm has to be highly secure out of the box. Docker Swarm operates on the assumption
    of least privilege.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: SwarmKit的设计理念是简单和安全。其口号是，几乎可以轻松地设置一个Swarm，并且Swarm在开箱即用时必须具有高度安全性。Docker Swarm的运行基于最低权限的假设。
- en: Installing a complete, highly available Docker swarm is literally as simple
    as starting with `docker swarm init` on the first node in the cluster, which becomes
    the so-called leader, and then `docker swarm join <join-token>` on all other nodes.
    `join-token` is generated by the leader during initialization. The whole process
    takes fewer that 5 minutes on a swarm with up to 10 nodes. If it is automated,
    it takes even less time.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中的第一个节点上使用`docker swarm init`开始安装完整、高可用的Docker Swarm，这个节点成为所谓的领导者，然后在所有其他节点上使用`docker
    swarm join <join-token>`。`join-token`是在初始化期间由领导者生成的。整个过程在具有多达10个节点的集群上不到5分钟。如果自动化，时间会更短。
- en: As I already mentioned, security was top on the list of must-haves when Docker
    designed and developed SwarmKit. Containers provide security by relying on Linux kernel namespaces
    and cgroups, as well as Linux syscall whitelisting (seccomp), and the support
    of Linux capabilities and the **Linux security module** (**LSM**). Now, on top
    of that, SwarmKit adds MTLS and secrets that are encrypted at rest and in transit.
    Furthermore, Swarm defines the so-called **container network model** (**CNM**),
    which allows for SDNs that provide sandboxing for application services that are
    running on the swarm.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，安全性是Docker设计和开发SwarmKit时的首要考虑因素。容器通过依赖Linux内核命名空间和cgroups、Linux系统调用白名单（seccomp）以及对Linux功能和Linux安全模块（LSM）的支持来提供安全性。现在，在此基础上，SwarmKit还增加了MTLS和在静态和传输中加密的秘密。此外，Swarm定义了所谓的容器网络模型（CNM），允许为在集群上运行的应用服务提供沙盒环境的SDN。
- en: Docker SwarmKit supports both Linux and Windows containers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Docker SwarmKit支持Linux和Windows容器。
- en: Apache Mesos and Marathon
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Mesos和Marathon
- en: Apache Mesos is an open source project, and was originally designed to make
    a cluster of servers or nodes look like one single big server from the outside.
    Mesos is software that makes the management of computer clusters simple. Users
    of Mesos should not have to care about individual servers, but just assume they
    have a gigantic pool of resources at their disposal, which corresponds to the
    aggregate of all the resources of all the nodes in the cluster.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mesos是一个开源项目，最初旨在使服务器或节点集群从外部看起来像一个单一的大服务器。Mesos是一种使计算机集群管理变得简单的软件。Mesos的用户不必关心单个服务器，只需假设他们拥有一个庞大的资源池，这对应于集群中所有节点的所有资源的总和。
- en: Mesos, in IT terms, is already pretty old, at least compared to the other orchestrators.
    It was first publicly presented in 2009, but at that time, of course, it wasn't
    designed to run containers, since Docker didn't even exist yet. Similar to what
    Docker does with containers, Mesos uses Linux cgroups to isolate resources such
    as CPU, memory, or disk I/O for individual applications or services.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从IT术语上讲，Mesos已经相当古老，至少与其他编排器相比是这样。它首次公开亮相是在2009年，但当时当然并不是为了运行容器，因为当时甚至还没有Docker。与Docker对容器的处理方式类似，Mesos使用Linux
    cgroups来隔离CPU、内存或磁盘I/O等资源，以便为单个应用程序或服务提供资源隔离。
- en: Mesos is really the underlying infrastructure for other interesting services
    built on top of it. From the perspective of containers specifically, Marathon is
    important. Marathon is a container orchestrator that runs on top of Mesos, which
    is able to scale to thousands of nodes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos实际上是其他建立在其之上的有趣服务的基础基础设施。从容器的角度来看，Marathon非常重要。Marathon是一个运行在Mesos之上的容器编排器，能够扩展到数千个节点。
- en: Marathon supports multiple container runtimes, such as Docker or its own Mesos
    containers. It supports not only stateless, but also stateful, application services,
    for example, databases such as PostgreSQL or MongoDB. Similar to Kubernetes and
    Docker SwarmKit, it supports many of the features that were described earlier
    in this chapter, such as high availability, health checks, service discovery,
    load balancing, and location awareness, to name but a few of the most important
    ones.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Marathon支持多个容器运行时，如Docker或其自己的Mesos容器。它不仅支持无状态的应用服务，还支持有状态的应用服务，例如像PostgreSQL或MongoDB这样的数据库。与Kubernetes和Docker
    SwarmKit类似，它支持本章前面描述的许多功能，例如高可用性、健康检查、服务发现、负载均衡和位置感知等等。
- en: Although Mesos and, to a certain extent, Marathon, are rather mature projects,
    their reach is relatively limited. It seems to be most popular in the area of
    big data, that is, to run data-crunching services such as Spark or Hadoop.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Mesos和在一定程度上Marathon是相当成熟的项目，但它们的影响范围相对有限。它似乎在大数据领域最受欢迎，即运行诸如Spark或Hadoop之类的数据处理服务。
- en: Amazon ECS
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亚马逊ECS
- en: 'If you are looking for a simple orchestrator and have already heavily bought
    into the AWS ecosystem, then Amazon''s ECS might be the right choice for you.
    It is important to point out one very important limitation of ECS: if you buy
    into this container orchestrator, then you lock yourself into AWS. You will not
    be able to easily port an application that is running on ECS to another platform
    or cloud.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找一个简单的编排器，并且已经深度融入了AWS生态系统，那么亚马逊的ECS可能是您的正确选择。但是，有一点非常重要的限制需要指出：如果您选择了这个容器编排器，那么您就将自己锁定在AWS中。您将无法轻松地将在ECS上运行的应用程序迁移到另一个平台或云上。
- en: Amazon promotes its ECS service as a highly scalable, fast container management
    service that makes it easy to run, stop, and manage Docker containers on a cluster.
    Next to running containers, ECS gives direct access to many other AWS services
    from the application services that run inside the containers. This tight and seamless
    integration with many popular AWS services is what makes ECS compelling for users
    who are looking for an easy way to get their containerized applications up and
    running in a robust and highly scalable environment. Amazon also provides its
    own private image registry.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊将其ECS服务宣传为一个高度可扩展、快速的容器管理服务，可以轻松在集群上运行、停止和管理Docker容器。除了运行容器，ECS还可以直接访问容器内运行的应用服务的许多其他AWS服务。这种与许多热门AWS服务的紧密无缝集成，使ECS对于寻求在强大且高度可扩展的环境中轻松运行其容器化应用的用户非常具有吸引力。亚马逊还提供自己的私有镜像注册表。
- en: With AWS ECS, you can use Fargate to have it fully manage the underlying infrastructure,
    allowing you to concentrate exclusively on deploying containerized applications,
    and you do not have to care about how to create and manage a cluster of nodes.
    ECS supports both Linux and Windows containers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AWS ECS，您可以使用Fargate来完全管理底层基础设施，让您专注于部署容器化应用程序，而不必关心如何创建和管理节点集群。ECS支持Linux和Windows容器。
- en: In summary, ECS is simple to use, highly scalable, and well integrated with
    other popular AWS services; but it is not as powerful as, say, Kubernetes or Docker
    SwarmKit, and it is only available on Amazon AWS.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，ECS使用简单，高度可扩展，并与其他热门的AWS服务很好地集成在一起；但它不像Kubernetes或Docker SwarmKit那样强大，并且仅在Amazon
    AWS上可用。
- en: Microsoft ACS
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微软ACS
- en: 'Similar to what we said about ECS, we can claim the same for Microsoft''s ACS.
    It is a simple container orchestration service that makes sense if you are already
    heavily invested in the Azure ecosystem. I should say the same as I have pointed
    out for Amazon ECS: if you buy into ACS, then you lock yourself in to the offerings
    of Microsoft. It will not be easy to move your containerized applications from
    ACS to any other platform or cloud.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们对ECS所说的类似，我们也可以对微软的ACS提出同样的要求。如果您已经在Azure生态系统中投入了大量资金，那么ACS是一个有意义的容器编排服务。我应该说与我为Amazon
    ECS指出的相同：如果您选择ACS，那么您就会将自己锁定在微软的产品中。将容器化应用程序从ACS移动到其他平台或云将不容易。
- en: ACS is Microsoft's container service, which supports multiple orchestrators
    such as Kubernetes, Docker Swarm, and Mesos DC/OS. With Kubernetes becoming more
    and more popular, the focus of Microsoft has clearly shifted to that orchestrator.
    Microsoft has even rebranded its service and called it **Azure Kubernetes Service** (**AKS**)
    in order to put the focus on Kubernetes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ACS是微软的容器服务，支持多个编排器，如Kubernetes、Docker Swarm和Mesos DC/OS。随着Kubernetes变得越来越受欢迎，微软的重点显然已经转移到了该编排器上。微软甚至重新将其服务命名为Azure
    Kubernetes Service（AKS），以便将重点放在Kubernetes上。
- en: 'AKS manages, for you, a hosted Kubernetes or Docker Swarm or DC/OS environment
    in Azure, so that you can concentrate on the applications that you want to deploy,
    and you don''t have to care about configuring the infrastructure. Microsoft, in
    its own words, claims the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: AKS为您管理在Azure中托管的Kubernetes或Docker Swarm或DC/OS环境，这样您就可以专注于要部署的应用程序，而不必关心配置基础设施。微软自己声称如下：
- en: '"AKS makes it quick and easy to deploy and manage containerized applications
    without container orchestration expertise. It also eliminates the burden of ongoing
    operations and maintenance by provisioning, upgrading, and scaling resources on
    demand, without taking your applications offline."'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: “AKS使得快速轻松地部署和管理容器化应用程序成为可能，而无需容器编排专业知识。它还通过根据需求提供、升级和扩展资源来消除持续运营和维护的负担，而不会使您的应用程序下线。”
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter demonstrated why orchestrators are needed in the first place, and
    how they conceptually work. It pointed out which orchestrators are the most prominent
    ones at the time of writing, and discussed the main commonalities and differences
    between the various orchestrators.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本章阐明了为什么首先需要编排器，以及它们在概念上是如何工作的。它指出了在撰写时最突出的编排器，并讨论了各种编排器之间的主要共同点和区别。
- en: The next chapter will introduce Docker’s native orchestrator, SwarmKit. It will
    elaborate on all the concepts and objects that SwarmKit uses to deploy and run
    a distributed, resilient, robust, and highly available application in a cluster—on-premises
    or in the cloud.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍Docker的本地编排器SwarmKit。它将详细阐述SwarmKit用于在集群（本地或云中）部署和运行分布式、有弹性、健壮和高可用应用所使用的所有概念和对象。
- en: Questions
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions to assess your learning progress:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题以评估您的学习进度：
- en: Why do we need an orchestrator? Provide two or three reasons.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么需要编排器？提供两到三个理由。
- en: Name three to four typical responsibilities of an orchestrator.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出编排器的三到四个典型职责。
- en: Name at least two container orchestrators, as well as the main sponsors behind
    them.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请至少列出两个容器编排器，以及它们背后的主要赞助商。
- en: Further reading
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'The following links provide some deeper insight into orchestration-related
    topics:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了有关编排相关主题的更深入的见解：
- en: Kubernetes—production-grade orchestration:[https://kubernetes.io/.](https://kubernetes.io/)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes-生产级编排：[https://kubernetes.io/.](https://kubernetes.io/)
- en: An overview of Docker Swarm mode:[https://docs.docker.com/engine/swarm/.](https://docs.docker.com/engine/swarm/)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm模式概述：[https://docs.docker.com/engine/swarm/.](https://docs.docker.com/engine/swarm/)
- en: Marathon, A container orchestration platform for Mesos and DC/OS: [https://](https://mesosphere.github.io/marathon/)[mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marathon，Mesos和DC/OS的容器编排平台：[https://](https://mesosphere.github.io/marathon/)[mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/)
- en: Containers and orchestration are explained:[http://bit.ly/2DFoQgx.](https://bit.ly/2npjrEl)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释容器和编排：[http://bit.ly/2DFoQgx.](https://bit.ly/2npjrEl)
