- en: Troubleshooting Guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除指南
- en: We have come so far and I am sure you are enjoying each and every moment of
    this challenging and joyful learning journey. I will not say that this book ends
    after this chapter, but rather you are completing the first milestone. This milestone
    opens the doors for learning and implementing a new paradigm in the cloud with
    microservice-based design. I would like to reaffirm that integration testing is
    an important way to test the interaction between microservices and APIs. While
    working on your sample application **online table reservation system** (**OTRS**),
    I am sure you have faced many challenges, especially while debugging the application.
    Here, we will cover a few of the practices and tools that will help you to troubleshoot
    the deployed application, Docker containers, and host machines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经走了这么远，我相信您正在享受这个充满挑战和快乐的学习之旅的每一刻。我不会说这本书在本章之后结束，而是您正在完成第一个里程碑。这个里程碑为在云中使用基于微服务的设计学习和实施打开了大门。我想重申，集成测试是测试微服务和API之间交互的重要方式。在处理您的示例应用程序**在线订餐系统**（**OTRS**）时，我相信您已经面临了许多挑战，特别是在调试应用程序时。在这里，我们将介绍一些实践和工具，这些将帮助您排除已部署应用程序、Docker容器和主机的故障。
- en: 'This chapter covers the following three topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下三个主题：
- en: Logging and the ELK stack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志和ELK堆栈
- en: Use of correlation ID for service calls using Zipkin and Sleuth
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Zipkin和Sleuth进行服务调用的相关ID的使用
- en: Dependencies and versions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖关系和版本
- en: Logging and the ELK stack
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志和ELK堆栈
- en: Can you imagine debugging any issue without seeing a log on the production system?
    Simply, no, as it would be difficult to go back in time. Therefore, we need logging.
    Logs also give us warning signals about the system if they are designed and coded
    that way. Logging and log analysis is an important step for troubleshooting any
    issue, and also for throughput, capacity, and monitoring the health of the system.
    Therefore, having a very good logging platform and strategy will enable effective
    debugging. Logging is one of the most important key components of software development
    in the initial days.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您能想象在生产系统上看不到日志的情况下调试任何问题吗？简单地说，不行，因为回溯将会很困难。因此，我们需要日志记录。日志还可以为我们提供关于系统的警告信号，如果它们是以这种方式设计和编码的话。日志记录和日志分析是解决任何问题的重要步骤，也是用于吞吐量、容量和监视系统健康状况的重要步骤。因此，拥有一个非常好的日志记录平台和策略将能够实现有效的调试。日志记录是软件开发最重要的关键组件之一。
- en: 'Microservices are generally deployed using image containers such as Docker
    that provide the log with commands that help you to read logs of services deployed
    inside the containers. Docker and Docker Compose provide commands to stream the
    log output of running services within the container and in all containers respectively.
    Please refer to the following `logs` command of Docker and Docker Compose:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务通常使用诸如Docker之类的镜像容器部署，这些容器提供了帮助您读取容器内部部署的服务日志的命令。Docker和Docker Compose提供了命令来流式传输容器内运行服务的日志输出，以及所有容器的日志输出。请参考Docker和Docker
    Compose的以下`logs`命令：
- en: '**Docker logs command:** **Usage:** `docker logs [OPTIONS] <CONTAINER NAME>`  **Fetch
    the logs of a container:**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker日志命令：** **用法：** `docker logs [OPTIONS] <CONTAINER NAME>`  **获取容器的日志：**'
- en: '`**-f, --follow Follow log output**`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-f, --follow 跟踪日志输出**`'
- en: '`**--help Print usage**`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--help 打印用法**`'
- en: '`**--since="" Show logs since timestamp**`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--since="" 显示自时间戳以来的日志**`'
- en: '`**-t, --timestamps Show timestamps**`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-t, --timestamps 显示时间戳**`'
- en: '`**--tail="all" Number of lines to show from the end of the logs**`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--tail="all" 显示日志末尾的行数**`'
- en: '**Docker Compose logs command:** `**Usage: docker-compose logs [options] [SERVICE...]**`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker Compose日志命令：** `**用法：docker-compose logs [options] [SERVICE...]**`'
- en: '**Options:**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项：**'
- en: '`**--no-color Produce monochrome output**`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--no-color 生成单色输出**`'
- en: '`**-f, --follow Follow log output**`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-f, --follow 跟踪日志输出**`'
- en: '`**-t, --timestamps Show timestamps**`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-t, --timestamps 显示时间戳**`'
- en: '`**--tail Number of lines to show from the end of the logs for each container**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--tail 显示每个容器日志末尾的行数**`'
- en: '**[SERVICES...] Service representing the container - you can give multiple**`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**[SERVICES...] 表示容器的服务 - 您可以提供多个**`'
- en: These commands help you to explore the logs of microservices and other processes
    running inside the containers. As you can see, using the above commands would
    be a challenging task when you have a higher number of services. For example,
    if you have tens or hundreds of microservices, it would be very difficult to track
    each microservice log. Similarly, you can imagine, even without containers, how
    difficult it would be to monitor logs individually. Therefore, you can assume
    the difficulty of exploring and correlating the logs of tens to hundreds of containers.
    It is time-consuming and adds very little value.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令帮助您探索微服务和其他在容器内运行的进程的日志。正如您所见，当您有大量服务时，使用上述命令将是一项具有挑战性的任务。例如，如果您有数十甚至数百个微服务，跟踪每个微服务的日志将非常困难。同样，即使没有容器，单独监视日志也会很困难。因此，您可以想象探索和关联数十到数百个容器的日志的困难程度。这是耗时的，并且增加了很少的价值。
- en: Therefore, a log aggregator and visualizing tools such as the ELK stack come
    to our rescue. It will be used for centralizing logging. We'll explore this in
    the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，日志聚合器和可视化工具，如ELK堆栈，成为我们的救星。它将用于日志的集中记录。我们将在下一节中探讨这一点。
- en: A brief overview
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要概述
- en: 'The **Elasticsearch, Logstash, Kibana** (**ELK**) stack is a chain of tools
    that performs log aggregation, analysis, visualization, and monitoring. The ELK
    stack provides a complete logging platform that allows you to analyze, visualize,
    and monitor all of your logs, including all types of product logs and system logs.
    If you already know about the ELK stack, please skip to the next section. Here,
    we''ll provide a brief introduction to each tool in the ELK Stack:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**Elasticsearch, Logstash, Kibana** (**ELK**)堆栈是一系列工具，用于执行日志聚合、分析、可视化和监视。ELK堆栈提供了一个完整的日志记录平台，允许您分析、可视化和监视所有类型的产品日志和系统日志，包括所有类型的日志。如果您已经了解ELK堆栈，请跳到下一节。在这里，我们将对ELK堆栈中的每个工具进行简要介绍：'
- en: '![](img/87defb1f-0f40-4d90-8f8d-0be85aaf64c6.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87defb1f-0f40-4d90-8f8d-0be85aaf64c6.png)'
- en: 'ELK overview (source: elastic.co)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ELK概述（来源：elastic.co）
- en: Elasticsearch
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Elasticsearch is one of the most popular enterprise full text search engines.
    It is open source software. It is distributable and supports multi-tenancy. A
    single Elasticsearch server stores multiple indexes (each index represents a database),
    and a single query can search the data of multiple indexes. It is a distributed
    search engine and supports clustering.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是最受欢迎的企业全文搜索引擎之一。它是开源软件。它是可分发的并支持多租户。单个Elasticsearch服务器存储多个索引（每个索引代表一个数据库），单个查询可以搜索多个索引的数据。它是一个分布式搜索引擎并支持集群。
- en: It is readily scalable and can provide near-real-time searches with a latency
    of 1 second. It is developed in Java using Apache Lucene. Apache Lucene is also
    free and open source, and it provides the core of Elasticsearch, also known as
    the informational retrieval software library.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以轻松扩展，并且可以提供具有1秒延迟的几乎实时搜索。它是使用Apache Lucene开发的Java。Apache Lucene也是免费和开源的，它提供了Elasticsearch的核心，也被称为信息检索软件库。
- en: Elasticsearch APIs are extensive in nature and very elaborative. Elasticsearch
    provides a JSON-based schema, less storage, and represents data models in JSON.
    Elasticsearch APIs use JSON documents for HTTP requests and responses.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch的API非常广泛且非常详细。Elasticsearch提供基于JSON的模式，占用较少的存储空间，并使用JSON文档进行HTTP请求和响应。
- en: Logstash
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Logstash
- en: Logstash is an open source data collection engine with real-time pipeline capabilities.
    In simple words, it collects, parses, processes, and stores the data. Since Logstash
    has data pipeline capabilities, it helps you to process any event data, such as
    logs, from a variety of systems. Logstash runs as an agent that collects the data,
    parses it, filters it, and sends the output to a designated app, such as Elasticsearch,
    or simple standard output on a console.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash是具有实时管道功能的开源数据收集引擎。简单来说，它收集、解析、处理和存储数据。由于Logstash具有数据管道功能，它可以帮助您处理来自各种系统的任何事件数据，例如日志。Logstash作为一个代理运行，收集数据，解析数据，过滤数据，并将输出发送到指定的应用程序，例如Elasticsearch，或者在控制台上进行简单的标准输出。
- en: 'It also has a very good plugin ecosystem (image sourced from [www.elastic.co](http://www.elastic.co)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 它还具有非常好的插件生态系统（图片来源：[www.elastic.co](http://www.elastic.co)）：
- en: '![](img/d8951d1d-10a5-4118-bf8d-6a6174fb9975.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8951d1d-10a5-4118-bf8d-6a6174fb9975.jpg)'
- en: Logstash ecosystem
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash生态系统
- en: Kibana
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is an open source analytics and visualization web application. It is
    designed to work with Elasticsearch. You use Kibana to search, view, and interact
    with data stored in Elasticsearch indices.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana是一个开源的分析和可视化Web应用程序。它旨在与Elasticsearch配合使用。您可以使用Kibana搜索、查看和与存储在Elasticsearch索引中的数据进行交互。
- en: It is a browser-based web application that lets you perform advanced data analysis
    and visualize your data in a variety of charts, tables, and maps. Moreover, it
    is a zero-configuration application. Therefore, it neither needs any coding nor
    additional infrastructure after installation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个基于浏览器的Web应用程序，可以让您执行高级数据分析，并以各种图表、表格和地图可视化您的数据。此外，它是一个零配置应用程序。因此，在安装后，它既不需要任何编码，也不需要额外的基础设施。
- en: ELK stack setup
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELK堆栈设置
- en: Generally, these tools are installed individually and then configured to communicate
    with each other. The installation of these components is pretty straightforward.
    Download the installable artifact from the designated location and follow the
    installation steps, as shown in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些工具是单独安装的，然后配置为相互通信。这些组件的安装非常简单。从指定位置下载可安装的工件，并按照下一节中显示的安装步骤进行安装。
- en: The installation steps provided below are part of a basic setup which is required
    for setting up the ELK stack you want to run. Since this installation was done
    on my localhost machine, I have used the host localhost. It can be changed easily
    with any respective hostname that you want.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面提供的安装步骤是ELK堆栈设置所需的基本设置的一部分。由于此安装是在我的本地主机上完成的，我使用了主机localhost。可以轻松更改为您想要的任何主机名。
- en: Installing Elasticsearch
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Elasticsearch
- en: 'To install Elasticsearch, we can use the Elasticsearch Docker image:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Elasticsearch，我们可以使用Elasticsearch Docker镜像：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also install Elasticsearch by following these steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以按照以下步骤安装Elasticsearch：
- en: Download the latest Elasticsearch distribution from [https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)下载最新的Elasticsearch分发版。
- en: Unzip it to the desired location in your system.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其解压到系统中所需的位置。
- en: Make sure the latest Java version is installed and the `JAVA_HOME` environment
    variable is set.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保安装了最新的Java版本并设置了`JAVA_HOME`环境变量。
- en: Go to Elasticsearch home and run `bin/elasticsearch` on Unix-based systems and
    `bin/elasticsearch.bat` on Windows.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到Elasticsearch主页，在Unix系统上运行`bin/elasticsearch`，在Windows上运行`bin/elasticsearch.bat`。
- en: 'Open any browser and hit `http://localhost:9200/`. On successful installation,
    it should provide you with a JSON object similar to the following:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开任何浏览器并访问`http://localhost:9200/`。安装成功后，它应该为您提供类似以下的JSON对象：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By default, the GUI is not installed. You can install one by executing the
    following command from the `bin` directory; make sure the system is connected
    to the internet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GUI未安装。您可以通过从`bin`目录执行以下命令来安装一个；确保系统连接到互联网：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you are using the Elasticsearch image, then run the Docker image (later,
    we'll use `docker-compose` to run the ELK stack together).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您正在使用Elasticsearch镜像，则运行Docker镜像（稍后，我们将使用`docker-compose`一起运行ELK堆栈）。
- en: Now, you can access the GUI interface with the URL `http://localhost:9200/_plugin/head/`.
    You can replace `localhost` and `9200` with your respective hostname and port
    number.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以使用URL `http://localhost:9200/_plugin/head/`访问GUI界面。您可以将`localhost`和`9200`替换为您的主机名和端口号。
- en: Installing Logstash
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Logstash
- en: 'To install Logstash, we can use the Logstash Docker image:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Logstash，我们可以使用Logstash Docker镜像：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can also install Logstash by performing the following steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过执行以下步骤安装Logstash：
- en: Download the latest Logstash distribution from [https://www.elastic.co/downloads/logstash](https://www.elastic.co/downloads/logstash).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.elastic.co/downloads/logstash](https://www.elastic.co/downloads/logstash)下载最新的Logstash发行版。
- en: Unzip it to the desired location in your system.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其解压缩到系统中所需的位置。
- en: 'Prepare a configuration file, as shown. It instructs Logstash to read input
    from given files and passes it to Elasticsearch (see the following `config` file;
    Elasticsearch is represented by localhost and the `9200` port). It is the simplest
    configuration file. To add filters and learn more about Logstash, you can explore
    the Logstash reference documentation available at [https://www.elastic.co/guide/en/logstash/current/index.html](https://www.elastic.co/guide/en/logstash/current/index.html):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 准备一个配置文件，如所示。它指示Logstash从给定文件中读取输入，并将其传递给Elasticsearch（参见以下`config`文件；Elasticsearch由localhost和`9200`端口表示）。这是最简单的配置文件。要添加过滤器并了解更多关于Logstash的信息，您可以查看[https://www.elastic.co/guide/en/logstash/current/index.html](https://www.elastic.co/guide/en/logstash/current/index.html)上提供的Logstash参考文档：
- en: As you can see, the OTRS `service` log and `edge-server` log are added as input.
    Similarly, you can also add log files of other microservices.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，OTRS `service`日志和`edge-server`日志被添加为输入。同样，您还可以添加其他微服务的日志文件。
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Go to Logstash home and run `bin/logstash agent -f logstash.conf` on Unix-based
    systems and `bin/logstash.bat agent -f logstash.conf` on Windows. Here, Logstash
    is executed using the `agent` command. The Logstash agent collects data from the
    sources provided in the input field in the configuration file and sends the output
    to Elasticsearch. Here, we have not used the filters, because otherwise it may
    process the input data before providing it to Elasticsearch.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到Logstash主目录，并在Unix系统上运行`bin/logstash agent -f logstash.conf`，在Windows上运行`bin/logstash.bat
    agent -f logstash.conf`。在这里，Logstash使用`agent`命令执行。Logstash代理从配置文件中提供的输入字段中收集数据，并将输出发送到Elasticsearch。在这里，我们没有使用过滤器，否则它可能会在将数据提供给Elasticsearch之前处理输入数据。
- en: Similarly, you can run Logstash using the downloaded Docker image (later, we'll
    use the `docker-compose` to run the ELK stack together).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以使用下载的Docker镜像运行Logstash（稍后，我们将使用`docker-compose`一起运行ELK堆栈）。
- en: Installing Kibana
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Kibana
- en: 'To install Kibana, we can use the Kibana Docker image:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Kibana，我们可以使用Kibana Docker镜像：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also install the Kibana web application by performing the following
    steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过执行以下步骤安装Kibana Web应用程序：
- en: 'Download the latest Kibana distribution from: [https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana).'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana)下载最新的Kibana发行版。
- en: Unzip it to the desired location in your system.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其解压缩到系统中所需的位置。
- en: 'Open the configuration file `config/kibana.yml` from the Kibana home directory
    and point the `elasticsearch.url` to the previously configured Elasticsearch instance:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Kibana主目录中的配置文件`config/kibana.yml`，并将`elasticsearch.url`指向先前配置的Elasticsearch实例：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Go to Kibana home and run `bin/kibana agent -f logstash.conf` on Unix-based
    systems and `bin/kibana.bat agent -f logstash.conf` on Windows.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到Kibana主目录，并在Unix系统上运行`bin/kibana agent -f logstash.conf`，在Windows上运行`bin/kibana.bat
    agent -f logstash.conf`。
- en: If you are using the Kibana Docker image, then you can run the Docker image
    (later, we'll use docker-compose to run the ELK stack together).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用Kibana Docker镜像，则可以运行Docker镜像（稍后，我们将使用docker-compose一起运行ELK堆栈）。
- en: Now, you can access the Kibana app from your browser using the URL `http://localhost:5601/`.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以使用URL `http://localhost:5601/`从浏览器访问Kibana应用程序。
- en: To learn more about Kibana, explore the Kibana reference documentation at [https://www.elastic.co/guide/en/kibana/current/getting-started.html](https://www.elastic.co/guide/en/kibana/current/getting-started.html).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关Kibana的更多信息，请查看[Kibana参考文档](https://www.elastic.co/guide/en/kibana/current/getting-started.html)。
- en: As we followed the preceding steps, you may have noticed that it requires some
    amount of effort. If you want to avoid a manual setup, you can Dockerize it. If
    you don't want to put effort into creating the Docker container of the ELK stack,
    you can choose one from Docker Hub. On Docker Hub, there are many ready-made ELK
    stack Docker images. You can try different ELK containers and choose the one that
    suits you the most. `willdurand/elk` is the most downloaded container and is easy
    to start, working well with Docker Compose.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述步骤操作，您可能已经注意到它需要一定的努力。如果您不想手动设置，可以将其Docker化。如果您不想花费精力创建ELK堆栈的Docker容器，可以从Docker
    Hub中选择一个。在Docker Hub上，有许多现成的ELK堆栈Docker镜像。您可以尝试不同的ELK容器，并选择最适合您的那个。`willdurand/elk`是最受欢迎的容器，并且很容易开始，与Docker
    Compose配合使用效果很好。
- en: Running the ELK stack using Docker Compose
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Compose运行ELK堆栈
- en: 'ELK images available on elastic.co''s own Docker repository have the XPack
    package enabled by default at the time of writing this section. In the future,
    it may be optional. Based on XPack availability in ELK images, you can modify
    the docker-compose file `docker-compose-elk.yml`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在elastic.co的Docker存储库中提供的ELK镜像在撰写本节时默认启用了XPack包。将来可能是可选的。根据ELK镜像中XPack的可用性，您可以修改docker-compose文件`docker-compose-elk.yml`：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once you save the ELK Docker Compose file, you can run the ELK stack using
    the following command (the command is run from the directory that contains the
    Docker Compose file):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 保存ELK Docker Compose文件后，可以使用以下命令运行ELK堆栈（该命令是从包含Docker Compose文件的目录运行的）：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output for the preceding command is as shown in the following screenshot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下所示：
- en: '![](img/c32104c2-3d7a-4822-a5f1-15d7e7be0b52.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c32104c2-3d7a-4822-a5f1-15d7e7be0b52.png)'
- en: Running the ELK stack using Docker Compose
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker Compose运行ELK堆栈
- en: 'If volume is not used, the environment pipeline does not work. For a Windows
    environment such as Windows 7, where normally volume is hard to configure, you
    can copy the pipeline CONF file inside the container and restart the Logstash
    container:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不使用卷，环境管道将无法工作。对于Windows 7等Windows环境，通常很难配置卷，您可以将管道CONF文件复制到容器内并重新启动Logstash容器：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Please restart the Logstash container after copying the pipeline CONF file
    `pipeline/logstash.conf`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制管道CONF文件`pipeline/logstash.conf`后，请重新启动Logstash容器：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Pushing logs to the ELK stack
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将日志推送到ELK堆栈
- en: We are done making the ELK stack available for consumption. Now, Logstash just
    needs a log stream that can be indexed by Elasticsearch. Once the Elasticsearch
    index of logs is created, logs can be accessed and processed on the Kibana dashboard.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使ELK堆栈可供使用。现在，Logstash只需要一个可以由Elasticsearch索引的日志流。一旦创建了Elasticsearch日志索引，就可以在Kibana仪表板上访问和处理日志。
- en: To push the logs to Logstash, we need to make the following changes in our service
    code. We need to add logback and logstash-logback encoder dependencies in OTRS
    services.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要将日志推送到Logstash，我们需要在我们的服务代码中进行以下更改。我们需要在OTRS服务中添加logback和logstash-logback编码器依赖项。
- en: 'Add the following dependencies in the `pom.xml` file:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下依赖项：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We also need to configure the logback by adding `logback.xml` to `src/main/resources`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要通过将`logback.xml`添加到`src/main/resources`来配置logback。
- en: 'The `logback.xml` file will look something like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`logback.xml`文件将如下所示：'
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the destination is `192.168.99.100:5001`, where Logstash is hosted; you
    can change it based on your configuration. For the encoder, the `net.logstash.logback.encoder.LogstashEncoder`
    class is used. The value of the `spring.application.name` property should be set
    to the service for which it is configured. Simiarly, a shutdown hook is added,
    so that once the service is stopped, all resources should be released and cleaned.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，目的地是`192.168.99.100:5001`，Logstash托管在那里；您可以根据您的配置进行更改。对于编码器，使用`net.logstash.logback.encoder.LogstashEncoder`类。`spring.application.name`属性的值应设置为配置的服务。同样，添加了一个关闭挂钩，因此一旦服务停止，所有资源都应该被释放和清理。
- en: You want to start services after the ELK stack is available, so services can
    push the logs to Logstash.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望在ELK堆栈可用后启动服务，以便服务可以将日志推送到Logstash。
- en: Once the ELK stack and services are up, you can check the ELK stack to view
    the logs. You want to wait for a few minutes after starting the ELK stack and
    then access the following URLs (replace the IP based on your configuration).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦ELK堆栈和服务启动，您可以检查ELK堆栈以查看日志。您需要在启动ELK堆栈后等待几分钟，然后访问以下URL（根据您的配置替换IP）。
- en: 'To check whether Elasticsearch is up, access the following URL:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查Elasticsearch是否可用，请访问以下URL：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To check whether indexes have been created or not, access either of the following
    URLs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查索引是否已创建，请访问以下任一URL：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the Logstash index is done (you may have a few service endpoints to generate
    some logs), access Kibana:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Logstash索引完成（您可能有一些服务端点来生成一些日志），访问Kibana：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tips for ELK stack implementation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELK堆栈实现的提示
- en: 'The following are some useful tips for implementing the ELK stack:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些实施ELK堆栈的有用提示：
- en: To avoid any data loss and handle the sudden spike of input load, using a broker
    such as Redis or RabbitMQ is recommended between Logstash and Elasticsearch.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免任何数据丢失并处理突然的输入负载激增，建议在Logstash和Elasticsearch之间使用Redis或RabbitMQ等代理。
- en: Use an odd number of nodes for Elasticsearch if you are using clustering to
    prevent the split-brain problem.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在使用集群，为Elasticsearch使用奇数个节点以防止分裂脑问题。
- en: In Elasticsearch, always use the appropriate field type for given data. This
    will allow you to perform different checks; for example, the `int` field type
    will allow you to perform `("http_status:<400")` or `("http_status:=200")`. Similarly,
    other field types also allow you to perform similar checks.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Elasticsearch中，始终针对给定数据使用适当的字段类型。这将允许您执行不同的检查；例如，`int`字段类型将允许您执行`("http_status:<400")`或`("http_status:=200")`。同样，其他字段类型也允许您执行类似的检查。
- en: Use of correlation ID for service calls
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于服务调用的相关ID的使用
- en: When you make a call to any REST endpoint and if any issue pops up, it is difficult
    to trace the issue and its root origin because each call is made to a server,
    and this call may call another, and so on and so forth. This makes it very difficult
    to figure out how one particular request was transformed and what it was called.
    Normally, an issue that is caused by one service can have domino effect on other
    services or can fail other service operation. It is very difficult to track and
    may require an enormous amount of effort. If it is monolithic, you know that you
    are looking in the right direction, but microservices make it difficult to understand
    what the source of the issue is and where you should get your data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当您调用任何REST端点并且出现任何问题时，很难跟踪问题及其根源，因为每个调用都是向服务器发出的，而这个调用可能会调用另一个，依此类推。这使得很难弄清楚一个特定请求是如何转换的，以及它被调用了什么。通常，由一个服务引起的问题可能会对其他服务产生连锁效应，或者可能会导致其他服务操作失败。跟踪起来非常困难，可能需要大量的努力。如果是单片，您知道自己正在朝着正确的方向努力，但是微服务使得很难理解问题的来源以及您应该从哪里获取数据。
- en: Let's see how we can tackle this problem
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们看看我们如何解决这个问题
- en: By using a correlation ID that is passed across all calls, it allows you to
    track each request and track the route easily. Each request will have its unique
    correlation ID. Therefore, when we debug any issue, the correlation ID is our
    starting point. We can follow it and, along the way, we can find out what went
    wrong.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用在所有调用中传递的相关ID，它允许您跟踪每个请求并轻松跟踪路由。每个请求都将有其唯一的相关ID。因此，当我们调试任何问题时，相关ID是我们的起点。我们可以跟随它，并且在途中，我们可以找出出了什么问题。
- en: The correlation ID requires some extra development effort, but it's effort well
    spent as it helps a lot in the long run. When a request travels between different
    microservices, you will be able to see all interactions and which service has
    problems.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关联ID需要额外的开发工作，但这些努力是值得的，因为它在长期内会帮助很多。当一个请求在不同的微服务之间传递时，你将能够看到所有的交互以及哪个服务出了问题。
- en: This is not something new or invented for microservices. This pattern is already
    being used by many popular products such as Microsoft SharePoint.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是为微服务而新发明的东西。这种模式已经被许多流行产品使用，比如Microsoft SharePoint。
- en: Use of Zipkin and Sleuth for tracking
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Zipkin和Sleuth进行跟踪
- en: For the OTRS application, we'll make use of Zipkin and Sleuth for tracking.
    It provides trace IDs and span IDs and a nice UI to trace the requests. More importantly,
    you can find out the time taken by each request in Zipkin and it allows you to
    drill down to find out the request that makes maximum time for serving the request.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于OTRS应用程序，我们将使用Zipkin和Sleuth进行跟踪。它提供跟踪ID和跨度ID以及一个漂亮的UI来跟踪请求。更重要的是，你可以在Zipkin中找出每个请求所花费的时间，并且它允许你深入挖掘，找出为满足请求所花费的最长时间的请求。
- en: 'In the following screenshot, you can see the time taken by the `findById` API
    call of the restaurant as well as the trace ID of the same request. It also shows
    the span ID:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，你可以看到餐厅`findById` API调用所花费的时间，以及相同请求的跟踪ID。它还显示了跨度ID：
- en: '![](img/b2422251-7e3e-4319-947a-3d747347f8b8.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2422251-7e3e-4319-947a-3d747347f8b8.png)'
- en: Total time taken and trace ID of restaurant `findById` API call
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 餐厅`findById` API调用的总时间和跟踪ID
- en: We'll stick to the following steps to configure the Zipkin and Sleuth in OTRS
    services.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将坚持以下步骤来配置OTRS服务中的Zipkin和Sleuth。
- en: 'You just need to add Sleuth and Sleuth-Zipkin dependencies to enable the tracking
    and request tracing:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要添加Sleuth和Sleuth-Zipkin依赖项来启用跟踪和请求追踪：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Access the Zipkin dashboard and find out the time taken by different requests.
    Replace the port if the default port is changed. Please make sure that services
    are up before making use of Zipkin:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 访问Zipkin仪表板，找出不同请求所花费的时间。如果默认端口已更改，请更换端口。在使用Zipkin之前，请确保服务已启动：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, if the ELK stack is configured and up, then you can use this trace ID
    to find the appropriate logs in Kibana, as shown in following screenshot. The
    X-B3-TraceId field is available in Kibana, which is used to filter the logs based
    on trace ID:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果ELK堆栈已配置并启动，那么你可以使用这个跟踪ID在Kibana中找到相应的日志，如下图所示。X-B3-TraceId字段在Kibana中可用，用于根据跟踪ID过滤日志：
- en: '![](img/04a4d229-71fa-4d6d-97b5-0289e3e650cd.png)Kibana dashboard - search
    based on request trace ID'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/04a4d229-71fa-4d6d-97b5-0289e3e650cd.png)Kibana仪表板 - 基于请求跟踪ID进行搜索'
- en: Dependencies and versions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖和版本
- en: Two common problems that we face in product development are cyclic dependencies
    and API versions. We'll discuss them in terms of microservice-based architecture.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在产品开发中我们面临的两个常见问题是循环依赖和API版本。我们将从微服务架构的角度来讨论它们。
- en: Cyclic dependencies and their impact
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环依赖及其影响
- en: Generally, monolithic architecture has a typical layer model, whereas microservices
    carry the graph model. Therefore, microservices may have cyclic dependencies.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，单体架构具有典型的层模型，而微服务采用图模型。因此，微服务可能存在循环依赖。
- en: Therefore, it is necessary to keep a dependency check on microservice relationships.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有必要对微服务关系进行依赖检查。
- en: 'Let us have a look at the following two cases:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下两种情况：
- en: If you have a cycle of dependencies between your microservices, you are vulnerable
    to distributed stack overflow errors when a certain transaction might be stuck
    in a loop. For example, when a restaurant table is being reserved by a person.
    In this case, the restaurant needs to know the person (`findBookedUser`), and
    the person needs to know the restaurant at a given time (`findBookedRestaurant`).
    If it is not designed well, these services may call each other in a loop. The
    result may be a stack overflow generated by JVM.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的微服务之间存在循环依赖，那么当某个事务可能陷入循环时，你就容易遭受分布式堆栈溢出错误。例如，当一个人预订餐厅的桌子时。在这种情况下，餐厅需要知道这个人（`findBookedUser`），而这个人需要在特定时间找到餐厅（`findBookedRestaurant`）。如果设计不好，这些服务可能会相互调用，导致堆栈溢出错误由JVM生成。
- en: If two services share a dependency and you update that other service's API in
    a way that could affect them, you'll need to update all three at once. This brings
    up questions such as, which should you update first? In addition, how do you make
    this a safe transition?
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个服务共享一个依赖，并且你以一种可能影响它们的方式更新了另一个服务的API，那么你需要同时更新所有三个服务。这带来了一些问题，比如，你应该先更新哪一个？此外，如何使这个过渡安全？
- en: Analyzing dependencies while designing the system
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在设计系统时分析依赖关系
- en: Therefore, it is important while designing the microservices to establish the
    proper relationship between different services internally to avoid any cyclic
    dependencies.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在设计微服务时，建立不同服务之间的适当关系以避免任何循环依赖是很重要的。
- en: It is a design issue and must be addressed, even if it requires a refactoring
    of the code.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个设计问题，必须加以解决，即使需要重构代码。
- en: Maintaining different versions
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维护不同版本
- en: When you have more services, it means different release cycles for each of them,
    which adds to this complexity by introducing different versions of services, in
    that there will be different versions of the same REST services. Reproducing the
    solution to a problem will prove to be very difficult when it has gone in one
    version and returns in a newer one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有更多的服务时，这意味着每个服务都有不同的发布周期，这会通过引入不同版本的服务增加复杂性，因为相同的REST服务将有不同的版本。当问题在一个版本中出现并在新版本中返回时，解决方案的再现将变得非常困难。
- en: Let's explore more
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们更深入地探索
- en: The versioning of APIs is important because, over time, APIs change. Your knowledge
    and experience improves with time, and that leads to changes in APIs. Changing
    APIs may break existing client integrations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: API的版本很重要，因为随着时间的推移，API会发生变化。您的知识和经验随着时间的推移而提高，这导致API的变化。更改API可能会破坏现有的客户端集成。
- en: 'Therefore, there are various ways to manage the API versions. One of these
    is using the version in the path that we have used in this book; some also use
    the HTTP header. The HTTP header could be a custom request header or you could
    use `Accept Header` for representing the calling API version. For more information
    on how versions are handled using HTTP headers, please refer to *RESTful Java
    Patterns and Best Practices* by Bhakti Mehta, Packt Publishing: [https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices](https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有各种方法来管理API版本。其中一种方法是使用我们在本书中使用的路径中的版本；有些人还使用HTTP标头。HTTP标头可以是自定义请求标头，或者您可以使用`Accept
    Header`来表示调用API版本。有关如何使用HTTP标头处理版本的更多信息，请参阅Bhakti Mehta的*RESTful Java Patterns
    and Best Practices*，Packt Publishing：[https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices](https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices)。
- en: It is very important while troubleshooting any issue that your microservices
    are implemented to produce the version numbers in logs. In addition, ideally,
    you should avoid any instance where you have too many versions of any microservice.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在排除故障时，非常重要的一点是您的微服务被实施为在日志中生成版本号。此外，理想情况下，您应该避免任何微服务有太多版本的情况。
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'This following links will have more information:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接将提供更多信息：
- en: 'Elasticsearch: [https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Elasticsearch: [https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)'
- en: 'Logstash: [https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Logstash: [https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)'
- en: 'Kibana: [https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kibana: [https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)'
- en: '`willdurand/elk`: ELK Docker image'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`willdurand/elk`: ELK Docker image'
- en: '*Mastering Elasticsearch - Second* *Edition*: [https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition](https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*掌握Elasticsearch-第二版*：[https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition](https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition)'
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have explored the ELK stack overview and installation. In
    the ELK stack, Elasticsearch is used for storing the logs and service queries
    from Kibana. Logstash is an agent that runs on each server that you wish to collect
    logs from. Logstash reads the logs, filters/transforms them, and provides them
    to Elasticsearch. Kibana reads/queries the data from Elasticsearch and presents
    it in tabular or graphical visualizations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了ELK堆栈的概述和安装。在ELK堆栈中，Elasticsearch用于存储来自Kibana的日志和服务查询。Logstash是在每台希望收集日志的服务器上运行的代理。Logstash读取日志，过滤/转换日志，并将其提供给Elasticsearch。Kibana从Elasticsearch读取/查询数据，并以表格或图形形式呈现数据。
- en: We also understand the utility of having the correlation ID while debugging
    issues. At the end of this chapter, we also discovered the shortcomings of a few
    microservice designs. It was a challenging task to cover all of the topics relating
    to microservices in this book, so I tried to include as much relevant information
    as possible with precise sections with references, which allow you to explore
    more. Now, I would like to let you start implementing the concepts we have learned
    in this chapter in your workplace or in your personal projects. This will not
    only give you hands-on experience, but may also allow you to master microservices.
    In addition, you will also be able to participate in local meetups and conferences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解到在调试问题时具有关联ID的实用性。在本章末尾，我们还发现了一些微服务设计的缺点。在本书中涵盖所有与微服务相关的主题是一项具有挑战性的任务，因此我尽量包含尽可能多的相关信息，并提供了带有参考文献的精确章节，这些章节可以让您进行更多探索。现在，我想让您开始在工作场所或个人项目中实施我们在本章中学到的概念。这不仅会让您获得实践经验，还可能让您精通微服务。此外，您还可以参加当地的聚会和会议。
