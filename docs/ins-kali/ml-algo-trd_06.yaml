- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: The Machine Learning Process
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习过程
- en: 'This chapter starts Part 2 of this book, where we''ll illustrate how you can
    use a range of supervised and unsupervised **machine learning** (**ML**) models
    for trading. We will explain each model''s assumptions and use cases before we
    demonstrate relevant applications using various Python libraries. The categories
    of models that we will cover in Parts 2-4 include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始了本书的第二部分，我们将说明您可以如何使用一系列监督和无监督的**机器学习**（**ML**）模型进行交易。在演示相关应用之前，我们将解释每个模型的假设和用例，并使用各种Python库。我们将在第2-4部分涵盖的模型类别包括：
- en: Linear models for the regression and classification of cross-section, time series,
    and panel data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于截面、时间序列和面板数据的线性模型的回归和分类
- en: Generalized additive models, including nonlinear tree-based models, such as
    decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义加性模型，包括非线性基于树的模型，如决策树
- en: Ensemble models, including random forest and gradient-boosting machines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成模型，包括随机森林和梯度提升机
- en: Unsupervised linear and nonlinear methods for dimensionality reduction and clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于降维和聚类的无监督线性和非线性方法
- en: Neural network models, including recurrent and convolutional architectures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络模型，包括循环和卷积架构
- en: Reinforcement learning models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习模型
- en: We will apply these models to the market, fundamental, and alternative data
    sources introduced in the first part of this book. We will build on the material
    covered so far by demonstrating how to embed these models in a trading strategy
    that translates model signals into trades, how to optimize portfolio, and how
    to evaluate strategy performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这些模型应用到本书第一部分介绍的市场、基本和替代数据来源。我们将通过演示如何将这些模型嵌入到将模型信号转化为交易的交易策略中，如何优化投资组合以及如何评估策略绩效来扩展到目前为止涵盖的材料。
- en: There are several aspects that many of these models and their applications have
    in common. This chapter covers these common aspects so that we can focus on model-specific
    usage in the following chapters. They include the overarching goal of learning
    a functional relationship from data by optimizing an objective or loss function.
    They also include the closely related methods of measuring model performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些模型及其应用有几个共同的方面。本章涵盖了这些共同方面，以便我们可以在接下来的章节中专注于特定模型的使用。它们包括通过优化目标或损失函数来学习数据的功能关系的总体目标。它们还包括密切相关的测量模型性能的方法。
- en: We'll distinguish between unsupervised and supervised learning and outline use
    cases for algorithmic trading. We'll contrast supervised regression and classification
    problems and the use of supervised learning for statistical inference of relationships
    between input and output data, along with its use for the prediction of future
    outputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将区分无监督学习和监督学习，并概述算法交易的用例。我们将对比监督回归和分类问题以及使用监督学习进行输入和输出数据之间关系的统计推断，以及用于预测未来输出的监督学习的用途。
- en: We'll also illustrate how prediction errors are due to the model's bias or variance,
    or because of a high noise-to-signal ratio in the data. Most importantly, we'll
    present methods to diagnose sources of errors like overfitting and improve your
    model's performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将说明预测错误是由于模型的偏差或方差，或者是由于数据中高噪音信号比导致的。最重要的是，我们将介绍诊断过拟合等错误来源的方法，并改善您模型的性能。
- en: 'In this chapter, we will cover the following topics relevant to applying the
    ML workflow in practice:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下与在实践中应用ML工作流相关的主题：
- en: How supervised and unsupervised learning from data works
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习如何从数据中工作
- en: Training and evaluating supervised learning models for regression and classification
    tasks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估用于回归和分类任务的监督学习模型
- en: How the bias-variance trade-off impacts predictive performance
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡如何影响预测性能
- en: How to diagnose and address prediction errors due to overfitting
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何诊断和解决由于过度拟合而导致的预测错误
- en: Using cross-validation to optimize hyperparameters with a focus on time-series
    data
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证来优化超参数，重点放在时间序列数据上
- en: Why financial data requires additional attention when testing out-of-sample
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试样本外时，为什么金融数据需要额外关注
- en: If you are already quite familiar with ML, feel free to skip ahead and dive
    right into learning how to use ML models to produce and combine alpha factors
    for an algorithmic trading strategy. This chapter's directory in the GitHub repository
    contains the code examples and lists additional resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经对ML非常熟悉，可以随意跳过并直接学习如何使用ML模型为算法交易策略生成和组合Alpha因子。本章的GitHub存储库中包含代码示例和其他资源列表。
- en: How machine learning from data works
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中学习机器学习
- en: 'Many definitions of ML revolve around the automated detection of meaningful
    patterns in data. Two prominent examples include:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多ML的定义都围绕数据中有意义模式的自动检测。两个著名的例子包括：
- en: AI pioneer **Arthur Samuelson** defined ML in 1959 as a subfield of computer
    science that gives computers the ability to learn without being explicitly programmed.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI先驱**Arthur Samuelson**在1959年将ML定义为计算机科学的一个子领域，使计算机能够在没有明确编程的情况下学习。
- en: '**Tom Mitchell**, one of the current leaders in the field, pinned down a well-posed
    learning problem more specifically in 1998: a computer program learns from experience
    with respect to a task and a performance measure of whether the performance of
    the task improves with experience (Mitchell 1997).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tom Mitchell**，该领域的现任领导者之一，1998年更具体地确定了一个明确定义的学习问题：计算机程序从任务和绩效度量的角度学习经验，以确定任务的绩效是否随经验改善（Mitchell
    1997）。'
- en: Experience is presented to an algorithm in the form of training data. The principal
    difference from previous attempts of building machines that solve problems is
    that the rules that an algorithm uses to make decisions are learned from the data,
    as opposed to being programmed by humans as was the case, for example, for expert
    systems prominent in the 1980s.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 经验以训练数据的形式呈现给算法。与以前构建解决问题的机器的尝试的主要区别在于，算法用于做出决策的规则是从数据中学习的，而不是像在20世纪80年代突出的专家系统的情况下由人类编程的。
- en: Recommended textbooks that cover a wide range of algorithms and general applications
    include James et al (2013), Hastie, Tibshirani, and Friedman (2009), Bishop (2006),
    and Mitchell (1997).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的覆盖广泛的算法和一般应用的教科书包括James等人（2013），Hastie，Tibshirani和Friedman（2009），Bishop（2006）和Mitchell（1997）。
- en: The challenge – matching the algorithm to the task
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 挑战-将算法与任务匹配
- en: The key challenge of automated learning is to identify patterns in the training
    data that are meaningful when generalizing the model's learning to new data. There
    are a large number of potential patterns that a model could identify, while the
    training data only constitutes a sample of the larger set of phenomena that the
    algorithm may encounter when performing the task in the future.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自动学习的关键挑战是识别在训练数据中有意义的模式，当将模型的学习泛化到新数据时。模型可能识别的潜在模式数量很大，而训练数据只构成了算法在未来执行任务时可能遇到的更大现象集合的样本。
- en: The infinite number of functions that could have generated the observed outputs
    from the given input makes the search process for the true function impossible,
    without restricting the eligible set of candidates. The types of patterns that
    an algorithm is capable of learning are limited by the size of its **hypothesis
    space** that contains the functions it can possibly represent. It is also limited
    by the amount of information provided by the sample data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以生成给定输入的观察输出的无限数量的函数使得真实函数的搜索过程变得不可能，除非限制候选集的资格。算法能够学习的模式类型受其假设空间的大小限制，该空间包含其可能表示的函数。它还受样本数据提供的信息量的限制。
- en: The size of the hypothesis space varies significantly between algorithms, as
    we will see in the following chapters. On the one hand, this limitation enables
    a successful search, and on the other hand, it implies an inductive bias that
    may lead to poor performance when the algorithm generalizes from the training
    sample to new data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设空间的大小在算法之间变化很大，我们将在接下来的章节中看到。一方面，这种限制使得成功搜索成为可能，另一方面，它意味着归纳偏差可能导致算法从训练样本到新数据的泛化时表现不佳。
- en: Hence, the key challenge becomes how to choose a model with a hypothesis space
    large enough to contain a solution to the learning problem, yet small enough to
    ensure reliable learning and generalization given the size of the training data.
    With more informative data, a model with a larger hypothesis space has a better
    chance of being successful.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，关键挑战是如何选择一个假设空间足够大以包含学习问题的解决方案，但又足够小以确保在给定训练数据的情况下可靠学习和泛化。有了更多信息的数据，假设空间更大的模型成功的机会更大。
- en: The **no-free-lunch theorem** states that there is no universal learning algorithm.
    Instead, a learner's hypothesis space has to be tailored to a specific task using
    prior knowledge about the task domain in order for the search for meaningful patterns
    that generalize well to succeed (Gómez and Rojas 2015).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: “没有免费午餐”定理指出没有通用的学习算法。相反，学习者的假设空间必须使用关于任务领域的先验知识来定制，以便成功搜索到能够很好泛化的有意义模式（Gómez和Rojas
    2015）。
- en: We will pay close attention to the assumptions that a model makes about data
    relationships for a specific task throughout this chapter and emphasize the importance
    of matching these assumptions with empirical evidence gleaned from data exploration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将密切关注模型对特定任务的数据关系所做的假设，并强调将这些假设与从数据探索中获得的经验证据相匹配的重要性。
- en: There are several categories of machine learning tasks that differ by purpose,
    available information, and, consequently, the learning process itself. The main
    categories are supervised, unsupervised, and reinforcement learning, and we will
    review their key differences next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同目的、可用信息和因此学习过程本身而不同的机器学习任务类别。主要类别包括监督学习、无监督学习和强化学习，接下来我们将审查它们的主要区别。
- en: Supervised learning – teaching by example
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习-以示例教学
- en: '**Supervised learning** is the most commonly used type of ML. We will dedicate
    most of the chapters in this book to applications in this category. The term *supervised*
    implies the presence of an outcome variable that guides the learning process—that
    is, it teaches the algorithm the correct solution to the task at hand. Supervised
    learning aims to capture a functional input-output relationship from individual
    samples that reflect this relationship and to apply its learning by making valid
    statements about new data.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是最常用的ML类型。我们将在本书的大部分章节中致力于这一类应用。术语*监督*意味着存在一个结果变量，指导学习过程-也就是说，它教会算法如何正确解决手头的任务。监督学习旨在从反映这种关系的个体样本中捕获功能输入-输出关系，并通过对新数据做出有效的陈述来应用其学习。'
- en: Depending on the field, the output variable is also interchangeably called the
    label, target, or outcome, as well as the endogenous or left-hand side variable.
    We will use *y*[i] for outcome observations *i* = 1, ..., *N*, or *y* for a (column)
    vector of outcomes. Some tasks come with several outcomes and are called **multilabel
    problems**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据领域的不同，输出变量也可以互换地称为标签、目标或结果，以及内生或左侧变量。我们将使用*y*[i]表示结果观测*i*=1，…，*N*，或*y*表示（列）向量的结果。有些任务有多个结果，称为**多标签问题**。
- en: The input data for a supervised learning problem is also known as features,
    as well as exogenous or right-hand side variables. We use *x*[i] for a vector
    of features with observations *i* = 1, ..., *N*, or *X* in matrix notation, where
    each column contains a feature and each row an observation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题的输入数据也称为特征，也称为外生变量或右侧变量。我们使用*x*[i]表示具有观察*i*= 1，...，*N*的特征向量，或者在矩阵表示中使用*X*，其中每列包含一个特征，每行包含一个观察结果。
- en: The solution to a supervised learning problem is a function ![](img/B15439_06_001.png)
    that represents what the model learned about the input-output relationship from
    the sample and approximates the true relationship, represented by ![](img/B15439_06_002.png).
    This function can potentially be used to infer statistical associations or even
    causal relationships among variables of interest beyond the sample, or it can
    be used to predict outputs for new input data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 解决监督学习问题的解决方案是一个函数![](img/B15439_06_001.png)，它表示模型从样本中学到的关于输入-输出关系的知识，并且近似真实关系，由![](img/B15439_06_002.png)表示。这个函数可能被用来推断样本之外感兴趣变量之间的统计关联或者因果关系，或者用来预测新输入数据的输出。
- en: The task of learning an input-outcome relationship from data that permits accurate
    predictions of outcomes for new inputs faces important trade-offs. More complex
    models have more moving parts that are capable of representing more nuanced relationships.
    However, they are also more likely to learn random noise particular to the training
    sample, as opposed to a systematic signal that represents a general pattern. When
    this happens, we say the model is **overfitting** to the training data. In addition,
    complex models may also be more difficult to inspect, making it more difficult
    to understand the nature of the learned relationship or the drivers of specific
    predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中学习输入-输出关系以便对新输入进行准确预测面临重要的权衡。更复杂的模型有更多的部分，能够表示更微妙的关系。然而，它们也更有可能学习与训练样本特定的随机噪音，而不是代表一般模式的系统信号。当这种情况发生时，我们说模型对训练数据**过拟合**。此外，复杂的模型可能更难以检查，这使得更难以理解学习关系的性质或特定预测的驱动因素。
- en: Overly simple models, on the other hand, will miss complex signals and deliver
    biased results. This trade-off is known as the **bias-variance trade-off** in
    supervised learning, but conceptually, this also applies to the other forms of
    ML where too simple or too complex models may perform poorly beyond the training
    data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，过于简单的模型将错过复杂的信号并产生有偏见的结果。这种权衡在监督学习中被称为**偏差-方差权衡**，但在概念上，这也适用于其他形式的机器学习，即太简单或太复杂的模型可能在训练数据之外表现不佳。
- en: Unsupervised learning – uncovering useful patterns
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习 - 发现有用的模式
- en: When solving an **unsupervised learning** problem, we only observe the features
    and have no measurements of the outcome. Instead of predicting future outcomes
    or inferring relationships among variables, unsupervised algorithms aim to identify
    structure in the input that permits a new representation of the information contained
    in the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决**无监督学习**问题时，我们只观察特征，没有结果的测量。无监督算法的目标不是预测未来结果或推断变量之间的关系，而是识别输入中的结构，以便获得数据中包含的信息的新表示。
- en: Frequently, the measure of success is the contribution of the result to the
    solution of some other problem. This includes identifying commonalities, or clusters,
    among observations, or transforming features to obtain a compressed summary that
    captures relevant information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的衡量标准通常是结果对解决其他问题的贡献。这包括识别观察结果之间的共同点或群集，或者转换特征以获得捕捉相关信息的压缩摘要。
- en: The key challenge is that unsupervised algorithms have to accomplish their mission
    without the guidance provided by outcome information. As a consequence, we are
    often unable to evaluate the result against a ground truth as in the supervised
    case, and its quality may be in the eye of the beholder. However, sometimes, we
    can evaluate its contribution to a downstream task, for example when dimensionality
    reduction enables better predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关键挑战在于，无监督算法必须在没有结果信息的指导下完成其任务。因此，我们通常无法像在监督情况下那样根据基本事实评估结果，其质量可能取决于观察者。然而，有时，我们可以评估其对下游任务的贡献，例如当降维使得更好的预测成为可能时。
- en: There are numerous approaches, from well-established cluster algorithms to cutting-edge
    deep learning models, and several relevant use cases for our purposes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法，从成熟的群集算法到尖端的深度学习模型，以及一些相关的用例。
- en: Use cases – from risk management to text processing
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用例 - 从风险管理到文本处理
- en: 'There are numerous trading use cases for unsupervised learning that we will
    cover in later chapters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习有许多交易用例，我们将在后面的章节中介绍：
- en: Grouping securities with similar risk and return characteristics (see **hierarchical
    risk parity** in *Chapter 13*, *Data-Driven Risk Factors and Asset Allocation
    with Unsupervised Learning*)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将具有相似风险和回报特征的证券分组（见*第13章*，*基于数据驱动的风险因素和无监督学习的资产配置*中的**分层风险平衡**）
- en: Finding a small number of risk factors driving the performance of a much larger
    number of securities using **principal component analysis** (*Chapter 13*, *Data-Driven
    Risk Factors and Asset Allocation with Unsupervised Learning*) or **autoencoders**
    (*Chapter 19*, *RNN for Multivariate Time Series and Sentiment Analysis*)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**主成分分析**（*第13章*，*基于数据驱动的风险因素和无监督学习的资产配置*）或**自动编码器**（*第19章*，*用于多变量时间序列和情感分析的RNN*）找到驱动大量证券表现的少数风险因素
- en: Identifying latent topics in a body of documents (for example, earnings call
    transcripts) that comprise the most important aspects of those documents (*Chapter
    14*, *Text Data for Trading – Sentiment Analysis*)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别文档集合（例如，收益电话转录）中的潜在主题，这些主题包括这些文档的最重要方面（*第14章*，*用于交易的文本数据 - 情感分析*）
- en: At a high level, these applications rely on methods to identify clusters and
    methods to reduce the dimensionality of the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，这些应用依赖于识别聚类的方法和减少数据维度的方法。
- en: Cluster algorithms – seeking similar observations
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类算法 - 寻找相似的观察
- en: Cluster algorithms apply a concept of similarity to identify observations or
    data attributes that contain comparable information. They summarize a dataset
    by assigning a large number of data points to a smaller number of clusters. They
    do this so that the cluster members are more closely related to each other than
    to members of other clusters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法应用了相似性的概念，以识别包含可比较信息的观察或数据属性。它们通过将大量数据点分配给较少数量的聚类来总结数据集。它们这样做是为了使聚类成员彼此之间的关系比与其他聚类成员的关系更密切。
- en: 'Cluster algorithms differ in what they assume about how the various groupings
    were generated and what makes them alike. As a result, they tend to produce alternative
    types of clusters and should thus be selected based on the characteristics of
    the data. Some prominent examples are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法在它们对各种分组生成方式和相似性的假设方面存在差异。因此，它们倾向于产生不同类型的聚类，因此应根据数据的特征进行选择。一些著名的例子包括：
- en: '**K-means clustering**: Data points belong to one of the *k* clusters of equal
    size that take an elliptical form.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K均值聚类**：数据点属于等大小的*k*个椭圆形聚类之一。'
- en: '**Gaussian mixture models**: Data points have been generated by any of the
    various multivariate normal distributions.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合模型**：数据点由各种多元正态分布之一生成。'
- en: '**Density-based clusters**: Clusters are of arbitrary shape and defined only
    by the existence of a minimum number of nearby data points.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的聚类**：聚类具有任意形状，仅由附近数据点的最小数量的存在来定义。'
- en: '**Hierarchical clusters**: Data points belong to various supersets of groups
    that are formed by successively merging smaller clusters.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**：数据点属于由逐渐合并较小聚类形成的各种超集。'
- en: Dimensionality reduction – compressing information
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维 - 压缩信息
- en: '**Dimensionality reduction** produces new data that captures the most important
    information contained in the source data. Rather than grouping data into clusters
    while retaining the original data, these algorithms transform the data with the
    goal of using fewer features to represent the original information.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**：生成新数据，捕捉源数据中包含的最重要信息。这些算法不是将数据分组成聚类并保留原始数据，而是通过使用更少的特征来表示原始信息的目标来转换数据。'
- en: 'Algorithms differ with respect to how they transform data and, thus, the nature
    of the resulting compressed dataset, as shown in the following list:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在它们如何转换数据以及因此产生的压缩数据集的性质方面存在差异，如下列表所示：
- en: '**Principal component analysis (PCA)**: Finds the linear transformation that
    captures most of the variance in the existing dataset'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**：找到捕捉现有数据集中大部分方差的线性转换'
- en: '**Manifold learning**: Identifies a nonlinear transformation that yields a
    lower-dimensional representation of the data'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流形学习**：识别产生数据的非线性转换，得到数据的低维表示'
- en: '**Autoencoders**: Uses a neural network to compress data nonlinearly with minimal
    loss of information'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器**：使用神经网络以最小的信息损失非线性地压缩数据'
- en: We will dive deeper into these unsupervised learning models in several of the
    following chapters, including important applications to **natural language processing**
    (**NLP**) in the form of topic modeling and Word2vec feature extraction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几章中更深入地探讨这些无监督学习模型，包括在**自然语言处理**（**NLP**）中的重要应用，如主题建模和Word2vec特征提取。
- en: Reinforcement learning – learning by trial and error
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习 - 通过试错学习
- en: '**Reinforcement learning** (**RL**) is the third type of ML. It centers on
    an agent that needs to pick an action at each time step, based on information
    provided by the environment. The agent could be a self-driving car, a program
    playing a board game or a video game, or a trading strategy operating in a certain
    security market. You find an excellent introduction in *Sutton and Barto (2018)*.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是第三种ML类型。它以一个需要在每个时间步选择动作的代理为中心，根据环境提供的信息。代理可以是自动驾驶汽车，玩棋盘游戏或视频游戏的程序，或在某个证券市场上运行的交易策略。您可以在*Sutton
    and Barto (2018)*中找到一个很好的介绍。'
- en: 'The agent aims to choose the action that yields the highest reward over time,
    based on a set of observations that describes the current state of the environment.
    It is both dynamic and interactive: the stream of positive and negative rewards
    impacts the algorithm''s learning, and actions taken now may influence both the
    environment and future rewards.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的目标是选择随时间产生最高奖励的动作，基于描述环境当前状态的一组观察结果。它既是动态的又是交互式的：正面和负面奖励的流影响算法的学习，当前采取的行动可能影响环境和未来的奖励。
- en: The agent needs to take action right from start and learns in an "online" fashion,
    one example at a time as it goes along. The learning process follows a trial-and-error
    approach. This is because the agent needs to manage the trade-off between exploiting
    a course of action that has yielded a certain reward in the past and exploring
    new actions that may increase the reward in the future. RL algorithms optimize
    the agent's learning using dynamical systems theory and, in particular, the optimal
    control of Markov decision processes with incomplete information.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 代理需要从一开始就采取行动，并以“在线”方式学习，一次一个示例。学习过程遵循试错方法。这是因为代理需要在利用过去产生一定奖励的行动和探索可能在未来增加奖励的新行动之间进行权衡。RL算法使用动态系统理论和特别是具有不完整信息的马尔可夫决策过程的最优控制来优化代理的学习。
- en: RL differs from supervised learning, where the training data lays out both the
    context and the correct decision for the algorithm. It is tailored to interactive
    settings where the outcomes only become available over time and learning must
    proceed in a continuous fashion as the agent acquires new experience.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不同于监督学习，监督学习中，训练数据同时提供了上下文和算法的正确决策。它专门针对交互式设置，其中结果只能随着时间的推移才能获得，并且学习必须在代理获取新经验的过程中持续进行。
- en: However, some of the most notable progress in **artificial intelligence** (**AI**)
    involves RL, which uses deep learning to approximate functional relationships
    between actions, environments, and future rewards. It also differs from unsupervised
    learning because feedback on the actions will be available, albeit with a delay.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**人工智能**（**AI**）中一些最显著的进展涉及强化学习，它使用深度学习来逼近行为、环境和未来回报之间的功能关系。它也不同于无监督学习，因为对行为的反馈将是可用的，尽管有延迟。
- en: RL is particularly suitable for algorithmic trading because the model of a return-maximizing
    agent in an uncertain, dynamic environment has much in common with an investor
    or a trading strategy that interacts with financial markets. We will introduce
    RL approaches to building an algorithmic trading strategy in *Chapter 21*, *Generative
    Adversarial Networks for Synthetic Time-Series Data*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习特别适用于算法交易，因为在不确定的、动态的环境中，回报最大化代理的模型与与金融市场互动的投资者或交易策略有很多共同之处。我们将在*第21章*，*用于合成时间序列数据的生成对抗网络*中介绍强化学习方法来构建算法交易策略。
- en: The machine learning workflow
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: Developing an ML solution for an algorithmic trading strategy requires a systematic
    approach to maximize the chances of success while economizing on resources. It
    is also very important to make the process transparent and replicable in order
    to facilitate collaboration, maintenance, and later refinements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为算法交易策略开发一个机器学习解决方案需要系统化的方法，以最大化成功的机会，同时节约资源。非常重要的是，为了促进协作、维护和后续改进，使过程透明和可复制。
- en: 'The following chart outlines the key steps, from problem definition to the
    deployment of a predictive solution:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表概述了关键步骤，从问题定义到预测解决方案的部署：
- en: '![](img/B15439_06_01.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_01.png)'
- en: 'Figure 6.1: Key steps of the machine learning workflow'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：机器学习工作流程的关键步骤
- en: 'The process is iterative throughout, and the effort required at different stages
    will vary according to the project. Generally, however, this process should include
    the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程都是迭代的，并且不同阶段需要的工作量会根据项目而变化。然而，通常情况下，这个过程应该包括以下步骤：
- en: Frame the problem, identify a target metric, and define success.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建问题，确定目标指标，并定义成功。
- en: Source, clean, and validate the data.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源，清洗和验证数据。
- en: Understand your data and generate informative features.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解您的数据并生成信息性特征。
- en: Pick one or more machine learning algorithms suitable for your data.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个或多个适合您数据的机器学习算法。
- en: Train, test, and tune your models.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练，测试和调整您的模型。
- en: Use your model to solve the original problem.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的模型解决原始问题。
- en: We will walk through these steps in the following sections using a simple example
    to illustrate some of the key points.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中通过一个简单的例子来走一遍这些步骤，以说明一些关键点。
- en: Basic walkthrough – k-nearest neighbors
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本步骤 - k最近邻
- en: The `machine_learning_workflow.ipynb` notebook in this chapter's folder of this
    book's GitHub repository contains several examples that illustrate the machine
    learning workflow using a dataset of house prices.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的GitHub存储库中这一章的文件夹中的`machine_learning_workflow.ipynb`笔记本包含了几个例子，演示了使用房价数据集的机器学习工作流程。
- en: We will use the fairly straightforward **k-nearest neighbors** (**KNN**) algorithm,
    which allows us to tackle both regression and classification problems. In its
    default scikit-learn implementation, it identifies the *k* nearest data points
    (based on the Euclidean distance) to make a prediction. It predicts the most frequent
    class among the neighbors or the average outcome in the classification or regression
    case, respectively.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相当直接的**k最近邻**（**KNN**）算法，它允许我们解决回归和分类问题。在其默认的scikit-learn实现中，它识别出* k *最近的数据点（基于欧几里德距离）来进行预测。在分类或回归情况下，它分别预测邻居中最频繁的类别或平均结果。
- en: The `README` for this chapter on GitHub links to additional resources; see Bhatia
    and Vandana (2010) for a brief survey.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章的GitHub上的`README`链接到其他资源；请参阅Bhatia和Vandana（2010）进行简要调查。
- en: Framing the problem – from goals to metrics
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建问题 - 从目标到指标
- en: The starting point for any machine learning project is the use case it ultimately
    aims to address. Sometimes, this goal will be statistical inference in order to
    identify an association or even a causal relationship between variables. Most
    frequently, however, the goal will be the prediction of an outcome to yield a
    trading signal.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习项目的起点是它最终的应用案例。有时，这个目标将是统计推断，以便识别变量之间的关联或甚至因果关系。然而，最常见的目标将是预测结果以产生交易信号。
- en: Both inference and prediction tasks rely on metrics to evaluate how well a model
    achieves its objective. Due to their prominence in practice, we will focus on
    common objective functions and the corresponding error metrics for predictive
    models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 推理和预测任务都依赖于指标来评估模型实现其目标的程度。由于它们在实践中的突出地位，我们将重点关注用于预测模型的常见目标函数和相应的误差指标。
- en: 'We distinguish prediction tasks by the nature of the output: a continuous output
    variable poses a **regression** problem, a categorical variable implies **classification**,
    and the special case of ordered categorical variables represents a **ranking**
    problem.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过输出的性质来区分预测任务：连续输出变量构成了一个**回归**问题，分类变量意味着**分类**，而有序分类变量的特殊情况代表了一个**排名**问题。
- en: You can often frame a given problem in different ways. The task at hand may
    be how to efficiently combine several alpha factors. You could frame this task
    as a regression problem that aims to predict returns, a binary classification
    problem that aims to predict the direction of future price movements, or a multiclass
    problem that aims to assign stocks to various performance classes such as return
    quintiles.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您经常可以以不同的方式构建给定问题。手头的任务可能是如何有效地结合几个α因子。您可以将这个任务构建为一个回归问题，旨在预测回报，一个二元分类问题，旨在预测未来价格走势的方向，或者一个多类问题，旨在将股票分配到不同的绩效类别，如回报五分位数。
- en: In the following section, we will introduce these objectives and look at how
    to measure and interpret related error metrics.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将介绍这些目标，并看看如何测量和解释相关的误差指标。
- en: Prediction versus inference
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测与推断
- en: The functional relationship produced by a supervised learning algorithm can
    be used for inference—that is, to gain insights into how the outcomes are generated.
    Alternatively, you can use it to predict outputs for unknown inputs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法产生的功能关系可以用于推断 - 也就是说，可以洞察结果是如何生成的。或者，您可以用它来预测未知输入的输出。
- en: For algorithmic trading, we can use inference to estimate the statistical association
    of the returns of an asset with a risk factor. This implies, for instance, assessing
    how likely this observation is due to noise, as opposed to an actual influence
    of the risk factor. Prediction, in turn, can be used to forecast the risk factor,
    which can help predict the asset return and price and be translated into a trading
    signal.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于算法交易，我们可以使用推断来估计资产回报与风险因素的统计关联。这意味着，例如，评估这种观察是否由噪音引起的可能性，而不是风险因素的实际影响。反过来，预测可以用来预测风险因素，这有助于预测资产回报和价格，并转化为交易信号。
- en: Statistical inference is about drawing conclusions from sample data about the
    parameters of the underlying probability distribution or the population. Potential
    conclusions include hypothesis tests about the characteristics of the distribution
    of an individual variable, or the existence or strength of numerical relationships
    among variables. They also include the point or interval estimates of metrics.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 统计推断是关于从样本数据中得出关于潜在概率分布或总体参数的结论。潜在的结论包括关于单个变量分布特征的假设检验，或者变量之间的数值关系的存在或强度的假设检验。它们还包括指标的点估计或区间估计。
- en: Inference depends on the assumptions about the process that originally generated
    the data. We will review these assumptions and the tools that are used for inference
    with linear models where they are well established. More complex models make fewer
    assumptions about the structural relationship between input and output. Instead,
    they approach the task of function approximation with fewer restrictions, while
    treating the data-generating process as a black box.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 推断取决于最初生成数据的过程的假设。我们将回顾这些假设以及用于线性模型推断的工具，这些假设在那里已经得到确认。更复杂的模型对输入和输出之间的结构关系做出较少的假设。相反，它们以较少的限制来处理函数逼近的任务，同时将生成数据的过程视为黑匣子。
- en: These models, including decision trees, ensemble models, and neural networks,
    have gained in popularity because they often outperform on prediction tasks. However,
    we will see that there have been numerous recent efforts to increase the transparency
    of complex models. Random forests, for example, have recently gained a framework
    for statistical inference (Wager and Athey 2019).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型，包括决策树、集成模型和神经网络，因为它们通常在预测任务上表现优异而变得受欢迎。然而，我们将看到最近有许多努力增加复杂模型的透明度。例如，随机森林最近获得了统计推断的框架（Wager
    and Athey 2019）。
- en: Causal inference – correlation does not imply causation
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因果推断 - 相关性并不意味着因果关系
- en: Causal inference aims to identify relationships where certain input values imply
    certain outputs—for example, a certain constellation of macro variables causing
    the price of a given asset to move in a certain way, while assuming all other
    variables remain constant.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断旨在确定某些输入值暗示某些输出的关系 - 例如，假设所有其他变量保持不变，某些宏观变量的特定组合导致给定资产价格以某种方式变化。
- en: Statistical inference about relationships among two or more variables produces
    measures of correlation. Correlation can only be interpreted as a causal relationship
    when several other conditions are met—for example, when alternative explanations
    or reverse causality has been ruled out.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于两个或更多变量之间的关系的统计推断产生了相关性的度量。只有在满足其他几个条件时，相关性才能被解释为因果关系，例如，当排除了替代解释或逆向因果关系时。
- en: Meeting these conditions requires an experimental setting where all relevant
    variables of interest can be fully controlled to isolate causal relationships.
    Alternatively, quasi-experimental settings expose units of observations to changes
    in inputs in a randomized way. It does this to rule out that other observable
    or unobservable features are responsible for the observed effects of the change
    in the environment.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 满足这些条件需要一个实验设置，可以完全控制所有感兴趣的相关变量，以隔离因果关系。或者，准实验设置以随机的方式暴露观察单位对输入的变化，以排除其他可观察或不可观察的特征对环境变化的观察效果负责的可能性。
- en: These conditions are rarely met, so inferential conclusions need to be treated
    with care. The same applies to the performance of predictive models that also
    rely on the statistical association between features and outputs, which may change
    with other factors that are not part of the model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些条件很少被满足，因此推断性结论需要谨慎对待。同样适用于依赖于特征和输出之间的统计关联的预测模型的性能，这种关联可能会随着不属于模型的其他因素的改变而改变。
- en: The non-parametric nature of the KNN model does not lend itself well to inference,
    so we'll postpone this step in the workflow until we encounter linear models in
    *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: KNN模型的非参数特性不利于推断，因此我们将在*第7章*，*线性模型 - 从风险因素到回报预测*中遇到线性模型时将此步骤推迟到工作流程中。
- en: Regression – popular loss functions and error metrics
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归 - 流行的损失函数和错误指标
- en: Regression problems aim to predict a continuous variable. The **root-mean-square
    error** (**RMSE**) is the most popular loss function and error metric, not least
    because it is differentiable. The loss is symmetric, but larger errors weigh more
    in the calculation. Using the square root has the advantage that we can measure
    the error in the units of the target variable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题旨在预测连续变量。**均方根误差**（**RMSE**）是最受欢迎的损失函数和错误指标，至少因为它是可微的。损失是对称的，但较大的错误在计算中权重更大。使用平方根的优势在于我们可以用目标变量的单位来衡量误差。
- en: The **root-mean-square of the log of the error** (**RMSLE**) is appropriate
    when the target is subject to exponential growth. Its asymmetric penalty weighs
    negative errors less than positive errors. You can also log-transform the target
    prior to training the model and then use the RMSE, as we'll do in the example
    later in this section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**误差的对数的均方根**（**RMSLE**）在目标受到指数增长时是合适的。它的非对称惩罚使负误差的权重小于正误差。您还可以在训练模型之前对目标进行对数变换，然后使用RMSE，就像我们将在本节的示例中所做的那样。'
- en: The **mean of the absolute errors** (**MAE**) and **median of the absolute errors**
    (**MedAE**) are symmetric but do not give more weight to larger errors. The MedAE
    is robust to outliers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**绝对误差的平均值**（**MAE**）和**绝对误差的中位数**（**MedAE**）是对称的，但不会给较大的错误更多的权重。MedAE对异常值具有鲁棒性。'
- en: The **explained variance score** computes the proportion of the target variance
    that the model accounts for and varies between 0 and 1\. The **R2 score** is also
    called the coefficient of determination and yields the same outcome if the mean
    of the residuals is 0, but can differ otherwise. In particular, it can be negative
    when calculated on out-of-sample data (or for a linear regression without intercept).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释方差得分**计算模型解释的目标方差的比例，介于0和1之间变化。**R2得分**也称为确定系数，如果残差的平均值为0，则产生相同的结果，但在其他情况下可能会有所不同。特别是在对外样本数据（或没有截距的线性回归）计算时可能为负数。'
- en: 'The following table defines the formulas used for calculation and the corresponding
    scikit-learn function that can be imported from the metrics module. The `scoring`
    parameter is used in combination with automated train-test functions (such as
    `cross_val_score` and `GridSearchCV`), which we''ll will introduce later in this
    section, and which are illustrated in the accompanying notebook:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格定义了用于计算的公式和相应的scikit-learn函数，可以从metrics模块导入。`scoring`参数与自动化的训练-测试函数（如`cross_val_score`和`GridSearchCV`）结合使用，我们将在本节稍后介绍，并在附带的笔记本中进行说明：
- en: '| Name | Formula | scikit-learn function | Scoring parameter |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 公式 | scikit-learn函数 | Scoring参数 |'
- en: '| Mean squared error | ![](img/B15439_06_003.png) | `mean_squared_error` |
    `neg_mean_squared_error` |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 均方误差 | ![](img/B15439_06_003.png) | `mean_squared_error` | `neg_mean_squared_error`
    |'
- en: '| Mean squared log error | ![](img/B15439_06_004.png) | `mean_squared_log_error`
    | `neg_mean_squared_log_error` |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 均方对数误差 | ![](img/B15439_06_004.png) | `mean_squared_log_error` | `neg_mean_squared_log_error`
    |'
- en: '| Mean absolute error | ![](img/B15439_06_005.png) | `mean_absolute_error`
    | `neg_mean_absolute_error` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 平均绝对误差 | ![](img/B15439_06_005.png) | `mean_absolute_error` | `neg_mean_absolute_error`
    |'
- en: '| Median absolute error | ![](img/B15439_06_006.png) | `median_absolute_error`
    | `neg_median_absolute_error` |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 中位绝对误差 | ![](img/B15439_06_006.png) | `median_absolute_error` | `neg_median_absolute_error`
    |'
- en: '| Explained variance | ![](img/B15439_06_007.png) | `explained_variance_score`
    | `explained_variance` |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 解释方差 | ![](img/B15439_06_007.png) | `explained_variance_score` | `explained_variance`
    |'
- en: '| R² score | ![](img/B15439_06_008.png) | `r2_score` | `r2` |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| R²得分 | ![](img/B15439_06_008.png) | `r2_score` | `r2` |'
- en: '*Figure 6.2* shows the various error metrics for the house price regression
    that we''ll compute in the notebook:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.2*显示了我们将在笔记本中计算的房价回归的各种错误指标：'
- en: '![](img/B15439_06_02.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_02.png)'
- en: 'Figure 6.2: In-sample regression errors'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：样本内回归误差
- en: The `sklearn` function also supports multilabel evaluation—that is, assigning
    multiple outcome values to a single observation; see the documentation referenced
    on GitHub for more details.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`函数还支持多标签评估 - 即将多个结果值分配给单个观察结果；有关更多详细信息，请参阅GitHub上引用的文档。'
- en: Classification – making sense of the confusion matrix
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类 - 理解混淆矩阵的含义
- en: Classification problems have categorical outcome variables. Most predictors
    will output a score to indicate whether an observation belongs to a certain class.
    In the second step, these scores are then translated into actual predictions using
    a threshold value.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题具有分类结果变量。大多数预测器将输出一个分数，以指示观察结果是否属于某个类。在第二步中，这些分数然后使用阈值值转换为实际预测。
- en: In the binary case, with a positive and a negative class label, the score typically
    varies between zero and one or is normalized accordingly. Once the scores are
    converted into predictions of one class or the other, there can be four outcomes,
    since each of the two classes can be either correctly or incorrectly predicted.
    With more than two classes, there can be more cases if you differentiate between
    the several potential mistakes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元情况下，具有正类和负类标签，得分通常在零和一之间变化，或者相应地进行了归一化。一旦将分数转换为一类或另一类的预测，就可能有四种结果，因为两个类中的每一个都可以被正确或错误地预测。如果有两个以上的类别，如果区分了几种潜在的错误，就可能有更多的情况。
- en: All error metrics are computed from the breakdown of predictions across the
    four fields of the 2×2 confusion matrix that associates actual and predicted classes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所有错误指标都是根据2×2混淆矩阵的四个领域的预测来计算的，该矩阵将实际类和预测类相关联。
- en: 'The metrics listed in the following table, such as accuracy, evaluate a model
    for a given threshold:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表中列出的度量，如准确度，评估了给定阈值的模型：
- en: '![](img/B15439_06_03.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_03.png)'
- en: 'Figure 6.3: Confusion matrix and related error metrics'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：混淆矩阵和相关错误度量
- en: The classifier usually doesn't output calibrated probabilities. Instead, the
    threshold used to distinguish positive from negative cases is itself a decision
    variable that should be optimized, taking into account the costs and benefits
    of correct and incorrect predictions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器通常不会输出校准的概率。相反，用于区分正负案例的阈值本身是一个决策变量，应该进行优化，考虑到正确和错误预测的成本和收益。
- en: All things equal, a lower threshold tends to imply more positive predictions,
    with a potentially rising false positive rate, whereas for a higher threshold,
    the opposite is likely to be true.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其他条件相同，较低的阈值往往意味着更多的正预测，可能会导致不断上升的假阳性率，而较高的阈值则相反。
- en: Receiver operating characteristics the area under the curve
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 接收器操作特性曲线下面积
- en: The **receiver operating characteristics** (**ROC**) curve allows us to visualize,
    compare, and select classifiers based on their performance. It computes the pairs
    of **true positive rates** (**TPR**) and **false positive rates** (**FPR**) that
    result from using all predicted scores as a threshold to produce class predictions.
    It visualizes these pairs inside a square with unit side length.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收器操作特性**（**ROC**）曲线允许我们根据其性能可视化、比较和选择分类器。它计算使用所有预测分数作为阈值产生类别预测时得到的**真正率**（**TPR**）和**假正率**（**FPR**）的配对。它在单位边长的正方形内可视化这些配对。'
- en: Random predictions (weighted to take into account class imbalance), on average,
    yield equal TPR and FPR that appear on the diagonal, which becomes the benchmark
    case. Since an underperforming classifier would benefit from relabeling the predictions,
    this benchmark also becomes the minimum.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随机预测（加权以考虑类别不平衡），平均而言，产生等于对角线上的TPR和FPR，这成为基准情况。由于表现不佳的分类器会受益于重新标记预测，因此这个基准也成为最低点。
- en: The **area under the curve** (**AUC**) is defined as the area under the ROC
    plot that varies between 0.5 and the maximum of 1\. It is a summary measure of
    how well the classifier's scores are able to rank data points with respect to
    their class membership. More specifically, the AUC of a classifier has the important
    statistical property of representing the probability that the classifier will
    rank a randomly chosen positive instance higher than a randomly chosen negative
    instance, which is equivalent to the Wilcoxon ranking test (Fawcett 2006). In
    addition, the AUC has the benefit of not being sensitive to class imbalances.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**曲线下面积**（**AUC**）被定义为ROC图下的面积，介于0.5和最大值1之间。它是分类器分数能够根据其类成员资格对数据点进行排名的摘要度量。更具体地说，分类器的AUC具有重要的统计特性，代表分类器将随机选择的正实例排在随机选择的负实例之前的概率，这相当于Wilcoxon排名检验（Fawcett
    2006）。此外，AUC的好处是不会对类别不平衡敏感。'
- en: Precision-recall curves – zooming in on one class
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线-放大一个类别
- en: 'When predictions for one of the classes are of particular interest, precision
    and recall curves visualize the trade-off between these error metrics for different
    thresholds. Both measures evaluate the quality of predictions for a particular
    class. The following list shows how they are applied to the positive class:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当某个类别的预测特别重要时，精确度和召回率曲线可视化不同阈值下这些错误度量之间的权衡。这两个度量评估了特定类别的预测质量。以下列表显示了它们如何应用于正类：
- en: '**Recall** measures the share of actual positive class members that a classifier
    predicts as positive for a given threshold. It originates from information retrieval
    and measures the share of relevant documents successfully identified by a search
    algorithm.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**衡量分类器对于给定阈值预测为正的实际正类成员的份额。它源自信息检索，并衡量搜索算法成功识别的相关文档的份额。'
- en: '**Precision**, in contrast, measures the share of positive predictions that
    are correct.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**相反，衡量了正确的正预测的份额。'
- en: Recall typically increases with a lower threshold, but precision may decrease.
    Precision-recall curves visualize the attainable combinations and allow for the
    optimization of the threshold, given the costs and benefits of missing a lot of
    relevant cases or producing lower-quality predictions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率通常随着较低的阈值而增加，但精确度可能会降低。精确度-召回率曲线可视化了可达到的组合，并允许优化阈值，考虑到错过大量相关案例或产生质量较低的预测的成本和收益。
- en: The **F1 score** is a harmonic mean of precision and recall for a given threshold,
    and can be used to numerically optimize the threshold, all while taking into account
    the relative weights that these two metrics should assume.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1分数**是给定阈值下精确度和召回率的调和平均值，并且可以用于在考虑这两个度量应该承担的相对权重的情况下，数值优化阈值。'
- en: '*Figure 6.4* illustrates the ROC curve and corresponding AUC, alongside the
    precision-recall curve and the F1 score, which, using equal weights for precision
    and recall, yields an optimal threshold of 0.37\. The chart has been taken from
    the accompanying notebook, where you can find the code for the KNN classifier
    that operates on binarized housing prices:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.4*说明了ROC曲线和相应的AUC，以及精确度-召回率曲线和F1分数，使用精确度和召回率的相等权重，得出了0.37的最佳阈值。该图表取自附带的笔记本，您可以在其中找到操作二值化房价的KNN分类器的代码：'
- en: '![](img/B15439_06_04.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_04.png)'
- en: 'Figure 6.4: Receiver-Operating Characteristics, Precision-Recall Curve, and
    F1 Scores charts'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：接收器操作特性、精确度-召回率曲线和F1分数图表
- en: Collecting and preparing the data
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集和准备数据
- en: We already addressed important aspects of how to source market, fundamental,
    and alternative data in *Chapter 2*, *Market and Fundamental Data – Sources and
    Techniques,* and *Chapter 3*, *Alternative Data for Finance – Categories and Use
    Cases*. We will continue to work with various examples of these sources as we
    illustrate the application of the various models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*第2章*《市场和基本数据-来源和技术》和*第3章*《金融替代数据-类别和用例》中讨论了如何获取市场、基本和替代数据的重要方面。我们将继续使用这些来源的各种示例，以说明各种模型的应用。
- en: In addition to market and fundamental data, we will also acquire and transform
    text data as we explore natural language processing and image data when we look
    at image processing and recognition. Besides obtaining, cleaning, and validating
    the data, we may need to assign labels such as sentiment for news articles or
    timestamps to align it with trading data typically available in a time-series
    format.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了市场和基本数据外，当我们探索自然语言处理和图像处理时，我们还将获取和转换文本数据以及图像数据。除了获取、清理和验证数据外，我们可能需要为新闻文章分配情感标签或为时间戳分配标签，以便将其与通常以时间序列格式可用的交易数据对齐。
- en: It is also important to store it in a format that enables quick exploration
    and iteration. We recommend the HDF and parquet formats (see *Chapter 2*, *Market
    and Fundamental Data – Sources and Techniques*). For data that does not fit into
    memory and requires distributed processing on several machines, Apache Spark is
    often the best solution for interactive analysis and machine learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据存储在能够快速探索和迭代的格式中也很重要。我们推荐HDF和parquet格式（参见*第2章*《市场和基本数据-来源和技术》）。对于不适合内存并需要在多台机器上进行分布式处理的数据，Apache
    Spark通常是交互式分析和机器学习的最佳解决方案。
- en: Exploring, extracting, and engineering features
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索、提取和工程特征
- en: 'Understanding the distribution of individual variables and the relationships
    among outcomes and features is the basis for picking a suitable algorithm. This
    typically starts with **visualizations** such as scatter plots, as illustrated
    in the accompanying notebook and shown in *Figure 6.5*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 了解单个变量的分布以及结果和特征之间的关系是选择合适算法的基础。这通常从诸如散点图的**可视化**开始，如附带的笔记本中所示，并在*图6.5*中显示：
- en: '![](img/B15439_06_05.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_05.png)'
- en: 'Figure 6.5: Pairwise scatter plots of outcome and features'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：结果和特征的成对散点图
- en: It also includes **numerical evaluations** ranging from linear metrics like
    correlation to nonlinear statistics, such as the Spearman rank correlation coefficient
    that we encountered when we introduced the information coefficient in *Chapter
    4*, *Financial Feature Engineering – How to Research Alpha Factors*. There are
    also information-theoretic measures, such as mutual information, which we'll illustrate
    in the next subsection.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包括从线性度量到非线性统计的**数值评估**，例如我们在*第4章*《金融特征工程-如何研究Alpha因子》中介绍信息系数时遇到的Spearman秩相关系数。还有信息论度量，如互信息，我们将在下一小节中进行说明。
- en: 'A systematic exploratory analysis is also the basis of what is often the single
    most important ingredient of a successful predictive model: the **engineering
    of features** that extract information contained in the data, but which are not
    necessarily accessible to the algorithm in their raw form. Feature engineering
    benefits from domain expertise, the application of statistics and information
    theory, and creativity.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 系统性的探索性分析也是成功预测模型中通常最重要的一个因素的基础：**特征工程**，它提取了数据中包含的信息，但这些信息在原始形式下不一定对算法可访问。特征工程受益于领域专业知识、统计和信息论的应用以及创造力。
- en: It relies on smart data transformations that effectively tease out the systematic
    relationship between input and output data. There are many choices that include
    outlier detection and treatment, functional transformations, and the combination
    of several variables, including unsupervised learning. We will illustrate examples
    throughout, but will emphasize that this central aspect of the ML workflow is
    best learned through experience. Kaggle is a great place to learn from other data
    scientists who share their experiences with the community.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 它依赖于有效地揭示输入和输出数据之间系统关系的智能数据转换。有许多选择，包括异常值检测和处理、功能转换以及包括无监督学习在内的多个变量的组合。我们将在整个过程中举例说明，但要强调的是，这个机器学习工作流的核心方面最好通过经验学习。Kaggle是一个很好的地方，可以向其他数据科学家学习并与社区分享他们的经验。
- en: Using information theory to evaluate features
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用信息论评估特征
- en: The **mutual information** (**MI**) between a feature and the outcome is a measure
    of the mutual dependence between the two variables. It extends the notion of correlation
    to nonlinear relationships. More specifically, it quantifies the information obtained
    about one random variable through the other random variable.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 特征和结果之间的**互信息**（**MI**）是两个变量之间相互依赖的度量。它扩展了对非线性关系的相关性概念。更具体地，它量化了通过另一个随机变量获得的信息。
- en: 'The concept of MI is closely related to the fundamental notion of entropy of
    a random variable. Entropy quantifies the amount of information contained in a
    random variable. Formally, the mutual information—*I*(*X*, *Y*)—of two random
    variables, *X* and *Y*, is defined as the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: MI的概念与随机变量的熵的基本概念密切相关。熵量化了随机变量中包含的信息量。形式上，两个随机变量*X*和*Y*的互信息*I*(*X*, *Y*)定义如下：
- en: '![](img/B15439_06_009.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_009.png)'
- en: The sklearn function implements `feature_selection.mutual_info_regression`,
    which computes the mutual information between all features and a continuous outcome
    to select the features that are most likely to contain predictive information.
    There is also a classification version (see the sklearn documentation for more
    details). The `mutual_information.ipynb` notebook contains an application for
    the financial data we created in *Chapter 4*, *Financial Feature Engineering –
    How to Research Alpha Factors*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn函数实现了`feature_selection.mutual_info_regression`，它计算所有特征与连续结果之间的互信息，以选择最有可能包含预测信息的特征。还有一个分类版本（有关更多详细信息，请参阅sklearn文档）。`mutual_information.ipynb`笔记本包含了我们在*第4章*，*金融特征工程-如何研究Alpha因子*中创建的金融数据的应用。
- en: Selecting an ML algorithm
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择ML算法
- en: The remainder of this book will introduce several model families, ranging from
    linear models, which make fairly strong assumptions about the nature of the functional
    relationship between input and output variables, to deep neural networks, which
    make very few assumptions. As mentioned in the introductory section, fewer assumptions
    will require more data with significant information about the relationship so
    that the learning process can be successful.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的其余部分将介绍几个模型家族，从线性模型，它对输入和输出变量之间的功能关系的性质做出了相当强的假设，到深度神经网络，它几乎没有假设。正如在介绍部分提到的，更少的假设将需要更多包含关于关系的重要信息的数据，以便学习过程能够成功。
- en: We will outline the key assumptions and how to test them where applicable as
    we introduce these models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在介绍这些模型时概述关键假设及如何测试它们。
- en: Design and tune the model
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计和调整模型
- en: The ML process includes steps to diagnose and manage model complexity based
    on estimates of the model's generalization error. An important goal of the ML
    process is to obtain an unbiased estimate of this error using a statistically
    sound and efficient procedure. Key to managing the model design and tuning process
    is an understanding of how the bias-variance tradeoff relates to under- and overfitting.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ML过程包括诊断和管理模型复杂性的步骤，基于对模型泛化误差的估计。ML过程的一个重要目标是使用统计上健全和高效的程序获得对这种误差的无偏估计。管理模型设计和调整过程的关键在于理解偏差-方差权衡与欠拟合和过拟合的关系。
- en: The bias-variance trade-off
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: The prediction errors of an ML model can be broken down into reducible and irreducible
    parts. The irreducible part is due to random variation (noise) in the data due
    to, for example, the absence of relevant variables, natural variation, or measurement
    errors. The reducible part of the generalization error, in turn, can be broken
    down into errors due to **bias** and **variance**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型的预测误差可以分解为可减少和不可减少的部分。不可减少的部分是由数据中的随机变化（噪音）引起的，例如由于缺乏相关变量、自然变化或测量误差。可减少的泛化误差又可以分解为由**偏差**和**方差**引起的误差。
- en: 'Both result from discrepancies between the true functional relationship and
    the assumptions made by the machine learning algorithm, as detailed in the following
    list:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者都是由机器学习算法对真实功能关系和假设之间的差异造成的，如下列表所详细说明的：
- en: '**Error due to bias**: The hypothesis is too simple to capture the complexity
    of the true functional relationship. As a result, whenever the model attempts
    to learn the true function, it makes systematic mistakes and, on average, the
    predictions will be similarly biased. This is also called *underfitting*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差导致的误差**：假设过于简单，无法捕捉真实功能关系的复杂性。因此，每当模型试图学习真实函数时，都会出现系统性错误，平均而言，预测也会有类似的偏差。这也被称为*欠拟合*。'
- en: '**Error due to variance**: The algorithm is overly complex in view of the true
    relationship. Instead of capturing the true relationship, it overfits the data
    and extracts patterns from the noise. As a result, it learns different functional
    relationships from each sample, and out-of-sample predictions will vary widely.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差导致的误差**：算法过于复杂，无法捕捉真实关系。它过拟合数据，并从噪音中提取模式。因此，它从每个样本中学习到不同的功能关系，样本外的预测将变化很大。'
- en: Underfitting versus overfitting – a visual example
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合-一个视觉示例
- en: '*Figure 6.6* illustrates overfitting by measuring the in-sample error of approximations
    of a *sine* function by increasingly complex polynomials. More specifically, we
    draw a random sample with some added noise (*n* = 30) to learn a polynomial of
    varying complexity (see the code in the notebook, `bias_variance.ipynb`). The
    model predicts new data points, and we capture the mean-squared error for these
    predictions.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.6*通过测量对*sine*函数的逼近的样本内误差来说明过拟合，逼近的多项式复杂度越来越高。更具体地说，我们随机抽取一些带有一些噪音的样本（*n*
    = 30）来学习不同复杂度的多项式（请参见笔记本中的代码`bias_variance.ipynb`）。模型预测新数据点，我们捕获这些预测的均方误差。'
- en: The left-hand panel of *Figure 6.6* shows a polynomial of degree 1; a straight
    line clearly underfits the true function. However, the estimated line will not
    differ dramatically from one sample drawn from the true function to the next.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.6*的左侧面板显示了一个1次多项式；一条直线明显地低估了真实函数。然而，估计的线在从真实函数中抽取的一个样本到下一个样本中不会有明显的差异。'
- en: 'The middle panel shows that a degree 5 polynomial approximates the true relationship
    reasonably well on the interval from about ![](img/B15439_06_010.png) until ![](img/B15439_06_011.png).
    On the other hand, a polynomial of degree 15 fits the small sample almost perfectly,
    but provides a poor estimate of the true relationship: it overfits to the random
    variation in the sample data points, and the learned function will vary strongly
    as a function of the sample:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 中间面板显示，5次多项式在从大约![](img/B15439_06_010.png)到![](img/B15439_06_011.png)的区间上合理地逼近了真实关系。另一方面，15次多项式几乎完美地适应了小样本，但提供了对真实关系的糟糕估计：它对样本数据点的随机变化过拟合，学习到的函数将随着样本的变化而变化：
- en: '![](img/B15439_06_06.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_06.png)'
- en: 'Figure 6.6: A visual example of overfitting with polynomials'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：多项式过拟合的视觉示例
- en: How to manage the bias-variance trade-off
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何处理偏差-方差权衡
- en: To further illustrate the impact of overfitting versus underfitting, we'll try
    to learn a Taylor series approximation of the *sine* function of the ninth degree
    with some added noise. *Figure 6.7* shows the in- and-out-of-sample errors and
    the out-of-sample predictions for polynomials that underfit, overfit, and provide
    an approximately correct level of flexibility with degrees 1, 15, and 9, respectively,
    to 100 random samples of the true function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明过拟合与欠拟合的影响，我们将尝试学习一个带有一些噪音的九次幂的正弦函数的泰勒级数近似。图6.7显示了欠拟合、过拟合和提供大致正确灵活性水平的多项式的样本内外误差和样本外预测，分别为1、15和9次，对真实函数的100个随机样本。
- en: The left-hand panel shows the distribution of the errors that result from subtracting
    the true function values from the predictions. The high bias but low variance
    of an underfit polynomial of degree 1 compares to the low bias but exceedingly
    high variance of the errors for an overfitting polynomial of degree 15\. The underfit
    polynomial produces a straight line with a poor in-sample fit that is significantly
    off-target out of sample. The overfit model shows the best fit in-sample with
    the smallest dispersion of errors, but the price is a large variance out-of-sample.
    The appropriate model that matches the functional form of the true model performs,
    on average, by far the best on out-of-sample data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧面板显示了从预测值中减去真实函数值得到的误差分布。欠拟合的一次幂多项式的高偏差但低方差与过拟合的15次幂多项式的误差的低偏差但极高方差相比。欠拟合的多项式产生了一条直线，样本内拟合差，但样本外明显偏离目标。过拟合模型在样本内表现最佳，误差分散最小，但代价是样本外方差很大。与真实模型的函数形式相匹配的适当模型在样本外数据上平均表现得最好。
- en: 'The right-hand panel of *Figure 6.7* shows the actual predictions rather than
    the errors to visualize the different types of fit in practice:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧面板显示了实际预测而不是误差，以便实际可视化不同类型的拟合：
- en: '![](img/B15439_06_07.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_07.png)'
- en: 'Figure 6.7: Errors and out-of-sample predictions for polynomials of different
    degrees'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：不同次数多项式的误差和样本外预测
- en: Learning curves
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习曲线
- en: A learning curve plots the evolution of train and test errors against the size
    of the dataset used to learn the functional relationship. It helps to diagnose
    the bias-variance trade-off for a given model, and also answer the question of
    whether increasing the sample size might improve predictive performance. A model
    with a high bias will have a high but similar training error, both in-sample and
    out-of-sample. An overfit model will have a very low training but much higher
    test errors.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线绘制了训练和测试误差随着用于学习函数关系的数据集大小的演变。它有助于诊断给定模型的偏差-方差权衡，并回答增加样本量是否可能改善预测性能的问题。高偏差的模型将在样本内和样本外都有高但相似的训练误差。过拟合模型将有非常低的训练误差，但测试误差要高得多。
- en: '*Figure 6.8* shows how the out-of-sample error for the overfitted model declines
    as the sample size increases, suggesting that it may benefit from additional data
    or tools to limit the model''s complexity, such as regularization. Regularization
    adds data-driven constraints to the model''s complexity; we''ll introduce this
    technique in *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts*.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8显示了过拟合模型的样本外误差随着样本量增加而下降，表明它可能受益于额外数据或限制模型复杂性的工具，如正则化。正则化为模型的复杂性增加了数据驱动的约束；我们将在第7章“线性模型-从风险因素到收益预测”中介绍这一技术。
- en: 'Underfit models, in contrast, require either more features or need to increase
    their capacity to capture the true relationship:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，欠拟合模型要么需要更多特征，要么需要增加其容量以捕捉真实关系：
- en: '![](img/B15439_06_08.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_08.png)'
- en: 'Figure 6.8: Learning curves and bias-variance tradeoff'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：学习曲线和偏差-方差权衡
- en: How to select a model using cross-validation
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用交叉验证选择模型
- en: There are usually several candidate models for your use case, and the task of
    choosing one of them is known as the **model selection problem**. The goal is
    to identify the model that will produce the lowest prediction error when given
    new data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您的用例会有几个候选模型，选择其中一个的任务被称为“模型选择问题”。目标是识别在给定新数据时将产生最低预测误差的模型。
- en: A good choice requires an unbiased estimate of this generalization error, which,
    in turn, requires testing the model on data that was not part of model training.
    Otherwise, the model would have already been able to peek at the "solution" and
    learn something about the prediction task ahead of time that will inflate its
    performance.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的选择需要对泛化误差进行无偏估计，而这又需要在模型训练中没有涉及的数据上测试模型。否则，模型可能已经能够窥视“解决方案”，并提前了解有关预测任务的信息，从而夸大了其性能。
- en: 'To avoid this, we only use part of the available data to train the model and
    set aside another part of the data to validate its performance. The resulting
    estimate of the model''s prediction error on new data will only be unbiased if
    absolutely no information about the validation set leaks into the training set,
    as shown in *Figure 6.9*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们只使用部分可用数据来训练模型，并将另一部分数据保留下来验证其性能。对新数据的模型预测误差的估计只有在验证集的信息绝对不泄漏到训练集中时才是无偏的，如图6.9所示：
- en: '![](img/B15439_06_09.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_09.png)'
- en: 'Figure 6.9: Training and test set'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：训练集和测试集
- en: '**Cross-validation** (**CV**) is a popular strategy for model selection. The
    main idea behind CV is to split the data one or several times. This is done so
    that each split is used once as a validation set and the remainder as a training
    set: part of the data (the training sample) is used to train the algorithm, and
    the remaining part (the validation sample) is used to estimate the algorithm''s
    predictive performance. Then, CV selects the algorithm with the smallest estimated
    error or risk.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**（**CV**）是一种流行的模型选择策略。CV背后的主要思想是将数据拆分一次或多次。这样做是为了使每个拆分都被用作一次验证集，其余部分被用作训练集：数据的一部分（训练样本）用于训练算法，剩下的部分（验证样本）用于估计算法的预测性能。然后，CV选择具有最小估计误差或风险的算法。'
- en: Several methods can be used to split the available data. They differ in terms
    of the amount of data used for training, the variance of the error estimates,
    the computational intensity, and whether structural aspects of the data are taken
    into account when splitting the data, such as maintaining the ratio between class
    labels.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种方法来拆分可用数据。它们在使用的数据量、误差估计的方差、计算强度以及在拆分数据时是否考虑数据的结构方面有所不同，例如保持类标签之间的比例。
- en: While the data-splitting heuristic is very general, a key assumption of CV is
    that the data is **independently and identically distributed** (**IID**). In the
    following section and throughout this book, we will emphasize that **time-series
    data** requires a different approach because it usually does not meet this assumption.
    Moreover, we need to ensure that splits respect the temporal order to avoid **lookahead
    bias**. We'll do this by including some information from the future that we aim
    to predict in the historical training set.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据拆分启发式方法非常普遍，但CV的一个关键假设是数据是**独立同分布**（**IID**）。在接下来的章节和整本书中，我们将强调**时间序列数据**需要不同的方法，因为它通常不符合这一假设。此外，我们需要确保拆分尊重时间顺序，以避免**前瞻性偏差**。我们将通过在历史训练集中包含一些我们希望预测的未来信息来做到这一点。
- en: Model selection often involves hyperparameter tuning, which may result in many
    CV iterations. The resulting validation score of the best-performing model will
    be subject to **multiple testing bias**, which reflects the sampling noise inherent
    in the CV process. As a result, it is no longer a good estimate of the generalization
    error. For an unbiased estimate of the error rate, we have to estimate the score
    from a fresh dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择通常涉及超参数调整，这可能导致许多CV迭代。最佳模型的结果验证分数将受到**多重检验偏差**的影响，这反映了CV过程中固有的抽样噪声。因此，它不再是泛化误差的良好估计。为了对误差率进行无偏估计，我们必须从新数据集中估计分数。
- en: 'For this reason, we use a three-way split of the data, as shown in *Figure
    6.10*: one part is used in cross-validation and is repeatedly split into a training
    and validation set. The remainder is set aside as a hold-out set that is only
    used once after, cross-validation is complete to generate an unbiased test error
    estimate.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用数据的三向拆分，如*图6.10*所示：一部分用于交叉验证，并反复拆分为训练集和验证集。其余部分被设置为保留集，只在交叉验证完成后使用一次，以生成无偏的测试误差估计。
- en: 'We will illustrate this method as we start building ML models in the next chapter:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章开始构建机器学习模型时说明这种方法：
- en: '![](img/B15439_06_10.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_10.png)'
- en: 'Figure 6.10: Train, validation, and hold-out test set'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：训练、验证和保留测试集
- en: How to implement cross-validation in Python
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在Python中实现交叉验证
- en: 'We will illustrate various options for splitting data into training and test
    sets. We''ll do this by showing how the indices of a mock dataset with 10 observations
    are assigned to the train and test set (see `cross_validation.py` for details),
    as shown in following code:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将说明将数据拆分为训练集和测试集的各种选项。我们将通过展示如何将具有10个观察结果的模拟数据集的索引分配给训练集和测试集（有关详细信息，请参见`cross_validation.py`），如下面的代码所示：
- en: '[PRE0]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Scikit-learn's CV functionality, which we'll demonstrate in this section, can
    be imported from `sklearn.model_selection`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的CV功能可以从`sklearn.model_selection`中导入，我们将在本节中进行演示。
- en: 'For a single split of your data into a training and a test set, use `train_test_split`,
    where the `shuffle` parameter, by default, ensures the randomized selection of
    observations. You can ensure replicability by seeding the random number generator
    by setting `random_state`. There is also a `stratify` parameter, which ensures
    for a classification problem that the train and test sets will contain approximately
    the same proportion of each class. The result looks as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据拆分为训练集和测试集的单个拆分，请使用`train_test_split`，其中`shuffle`参数默认情况下确保随机选择观察结果。您可以通过设置`random_state`来种子化随机数生成器来确保可复制性。还有一个`stratify`参数，它确保对于分类问题，训练集和测试集将包含大致相同比例的每个类。结果如下所示：
- en: '[PRE1]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, we train a model using all data except row numbers `6` and `9`,
    which will be used to generate predictions and measure the errors given on the
    known labels. This method is useful for quick evaluation but is sensitive to the
    split, and the standard error of the performance measure estimate will be higher.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用除行号`6`和`9`之外的所有数据来训练模型，这些行将用于生成预测并根据已知标签给出的错误进行测量。这种方法对于快速评估很有用，但对拆分敏感，性能测量估计的标准误差会更高。
- en: KFold iterator
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KFold迭代器
- en: 'The `KFold` iterator produces several disjunct splits and assigns each of these
    splits once to the validation set, as shown in the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`KFold`迭代器产生几个不相交的拆分，并将这些拆分中的每一个分配一次给验证集，如下面的代码所示：'
- en: '[PRE2]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In addition to the number of splits, most CV objects take a `shuffle` argument
    that ensures randomization. To render results reproducible, set the `random_state`
    as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拆分数量之外，大多数CV对象还接受一个`shuffle`参数，以确保随机化。为了使结果可重现，设置`random_state`如下：
- en: '[PRE3]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Leave-one-out CV
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 留一交叉验证
- en: 'The original CV implementation used a **leave-one-out method** that used each
    observation once as the validation set, as shown in the following code:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的CV实现使用了一种**留一法**，每次将每个观察值作为验证集使用，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This maximizes the number of models that are trained, which increases computational
    costs. While the validation sets do not overlap, the overlap of training sets
    is maximized, driving up the correlation of models and their prediction errors.
    As a result, the variance of the prediction error is higher for a model with a
    larger number of folds.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这最大化了训练的模型数量，增加了计算成本。虽然验证集不重叠，但训练集的重叠被最大化，推高了模型和它们的预测误差的相关性。因此，对于具有更多折叠的模型，预测误差的方差更高。
- en: Leave-P-Out CV
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 留P法CV
- en: 'A similar version to leave-one-out CV is **leave-P-out CV**, which generates
    all possible combinations of `p` data rows, as shown in the following code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与留一法CV类似的版本是**留P法CV**，它生成所有可能的`p`数据行的组合，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ShuffleSplit
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ShuffleSplit
- en: 'The `ShuffleSplit` class creates independent splits with potentially overlapping
    validation sets, as shown in the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`ShuffleSplit`类创建具有潜在重叠验证集的独立拆分，如下面的代码所示：'
- en: '[PRE6]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Challenges with cross-validation in finance
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 金融交叉验证的挑战
- en: A key assumption for the cross-validation methods discussed so far is the IID
    distribution of the samples available for training.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的交叉验证方法的一个关键假设是用于训练的样本的IID分布。
- en: For financial data, this is often not the case. On the contrary, financial data
    is neither independently nor identically distributed because of serial correlation
    and time-varying standard deviation, also known as **heteroskedasticity** (see
    *Chapter 7*, *Linear Models – From Risk Factors to Return Forecasts,* and *Chapter
    9*, *Time Series Models for Volatility Forecasts and Statistical Arbitrage*, for
    more details). `TimeSeriesSplit` in the `sklearn.model_selection` module aims
    to address the linear order of time-series data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于金融数据来说，情况通常并非如此。相反，金融数据既不是独立分布的，也不是同分布的，因为存在串行相关性和时间变化的标准差，也称为**异方差性**（有关更多细节，请参见*第7章*，*线性模型-从风险因素到收益预测*，以及*第9章*，*时间序列模型用于波动率预测和统计套利*）。`sklearn.model_selection`模块中的`TimeSeriesSplit`旨在解决时间序列数据的线性顺序。
- en: Time series cross-validation with scikit-learn
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行时间序列交叉验证
- en: The time-series nature of the data implies that cross-validation produces a
    situation where data from the future will be used to predict data from the past.
    This is unrealistic at best and data snooping at worst, to the extent that future
    data reflects past events.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的时间序列性意味着交叉验证产生了一种情况，未来的数据将被用来预测过去的数据。这充其量是不现实的，最坏的情况是数据窥探，因为未来的数据反映了过去的事件。
- en: 'To address time dependency, the `TimeSeriesSplit` object implements a walk-forward
    test with an expanding training set, where subsequent training sets are supersets
    of past training sets, as shown in the following code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决时间依赖性，`TimeSeriesSplit`对象实现了一个向前行走的测试，其中扩展训练集，后续训练集是过去训练集的超集，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can use the `max_train_size` parameter to implement walk-forward cross-validation,
    where the size of the training set remains constant over time, similar to how
    Zipline tests a trading algorithm. Scikit-learn facilitates the design of custom
    cross-validation methods using **subclassing**, which we will implement in the
    following chapters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`max_train_size`参数来实现向前交叉验证，其中训练集的大小随时间保持不变，类似于Zipline测试交易算法的方式。Scikit-learn通过**子类化**来方便设计自定义交叉验证方法，我们将在接下来的章节中实现。
- en: Purging, embargoing, and combinatorial CV
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清除、禁运和组合CV
- en: For financial data, labels are often derived from overlapping data points because
    returns are computed from prices across multiple periods. In the context of trading
    strategies, the result of a model's prediction, which may imply taking a position
    in an asset, can only be known later when this decision is evaluated—for example,
    when a position is closed out.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于金融数据，标签通常是从重叠的数据点中派生出来的，因为收益是根据多个时期的价格计算出来的。在交易策略的背景下，模型的预测结果，可能意味着对某种资产采取头寸，只有在以后才能知道，当这个决定被评估时，例如当头寸被平仓时。
- en: The risks include the leakage of information from the test into the training
    set, which would very likely artificially inflate performance. We need to address
    this risk by ensuring that all data is point-in-time—that is, truly available
    and known at the time it is used as the input for a model. For example, financial
    disclosures may refer to a certain time period but only become available later.
    If we include this information too early, our model might do much better in hindsight
    than it would have under realistic circumstances.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 风险包括测试信息泄漏到训练集中，这很可能会人为地夸大性能。我们需要通过确保所有数据都是点对点的来解决这一风险——也就是说，在用作模型输入时，数据确实是可用和已知的。例如，财务披露可能涉及某个时间段，但只有在以后才能获得。如果我们过早地包含这些信息，我们的模型在事后可能会比在现实情况下表现得更好。
- en: 'Marcos Lopez de Prado, one of the leading practitioners and academics in the
    field, has proposed several methods to address these challenges in his book, *Advances
    in Financial Machine Learning* (2018). Techniques to adapt cross-validation to
    the context of financial data and trading include:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Marcos Lopez de Prado，该领域的主要从业者和学者之一，在他的书《金融机器学习进展》（2018年）中提出了几种方法来解决这些挑战。调整交叉验证以适应金融数据和交易背景的技术包括：
- en: '**Purging**: Eliminate training data points where the evaluation occurs after
    the prediction of a point-in-time data point in the validation set to avoid look-ahead
    bias.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清除**：消除训练数据点，其中评估发生在验证集中的点对点数据点的预测之后，以避免前瞻性偏见。'
- en: '**Embargoing**: Further eliminate training samples that follow a test period.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁运**：进一步消除跟随测试期间的训练样本。'
- en: '**Combinatorial cross-validation**: Walk-forward CV severely limits the historical
    paths that can be tested. Instead, given *T* observations, compute all possible
    train/test splits for *N*<*T* groups that each maintain their order, and purge
    and embargo potentially overlapping groups. Then, train the model on all combinations
    of *N*-*k* groups while testing the model on the remaining *k* groups. The result
    is a much larger number of possible historical paths.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合交叉验证**：向前CV严重限制了可以测试的历史路径。相反，给定*T*观测值，计算每个维持其顺序的*N*<*T*组的所有可能的训练/测试拆分，并清除和禁止潜在重叠的组。然后，在剩余的*k*组上训练模型的所有*N*-*k*组的组合，同时测试模型。结果是可能的历史路径数量大大增加。'
- en: Prado's *Advances in Financial Machine Learning* contains sample code to implement
    these approaches; the code is also available via the new Python library, timeseriescv.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 普拉多的《金融机器学习进展》包含了实现这些方法的示例代码；该代码也可以通过新的Python库timeseriescv获得。
- en: Parameter tuning with scikit-learn and Yellowbrick
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn和Yellowbrick进行参数调整
- en: Model selection typically involves repeated cross-validation of the out-of-sample
    performance of models using different algorithms (such as linear regression and
    random forest) or different configurations. Different configurations may involve
    changes to hyperparameters or the inclusion or exclusion of different variables.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择通常涉及使用不同算法（如线性回归和随机森林）或不同配置重复交叉验证模型的样本外性能。不同的配置可能涉及对超参数的更改或包含或排除不同变量。
- en: The Yellowbrick library extends the scikit-learn API to generate diagnostic
    visualization tools to facilitate the model-selection process. These tools can
    be used to investigate relationships among features, analyze classification or
    regression errors, monitor cluster algorithm performance, inspect the characteristics
    of text data, and help with model selection. We will demonstrate validation and
    learning curves that provide valuable information during the parameter-tuning
    phase—see the `machine_learning_workflow.ipynb` notebook for implementation details.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick库扩展了scikit-learn API，生成诊断可视化工具，以便于模型选择过程。这些工具可用于研究特征之间的关系，分析分类或回归错误，监视聚类算法的性能，检查文本数据的特征，并帮助模型选择。我们将演示提供有价值信息的验证和学习曲线，这些信息在参数调整阶段非常有用
    - 有关实现细节，请参见`machine_learning_workflow.ipynb`笔记本。
- en: Validation curves – plotting the impact of hyperparameters
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证曲线 - 绘制超参数的影响
- en: Validation curves (see the left-hand panel in *Figure 6.11*) visualize the impact
    of a single hyperparameter on a model's cross-validation performance. This is
    useful to determine whether the model underfits or overfits the given dataset.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 验证曲线（参见*图6.11*的左侧面板）可视化单个超参数对模型交叉验证性能的影响。这有助于确定模型是否对给定数据集欠拟合或过拟合。
- en: In our example of `KNeighborsRegressor`, which only has a single hyperparameter,
    the number of neighbors is *k*. Note that model complexity increases as the number
    of neighbors drop because the model can now make predictions for more distinct
    areas in the feature space.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`KNeighborsRegressor`示例中，只有一个超参数，邻居的数量是*k*。请注意，随着邻居数量的减少，模型复杂度增加，因为模型现在可以对特征空间中更多不同的区域进行预测。
- en: 'We can see that the model underfits for values of *k* above 20\. The validation
    error drops as we reduce the number of neighbors and make our model more complex.
    For values below 20, the model begins to overfit as training and validation errors
    diverge and average out-of-sample performance quickly deteriorates:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当*k*的值大于20时，模型出现欠拟合。随着邻居数量的减少和模型变得更加复杂，验证错误也会下降。对于小于20的值，模型开始出现过拟合，因为训练和验证错误出现分歧，平均样本外表现迅速恶化：
- en: '![](img/B15439_06_11.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15439_06_11.png)'
- en: 'Figure 6.11: Validation and learning curves'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：验证和学习曲线
- en: Learning curves – diagnosing the bias-variance trade-off
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习曲线 - 诊断偏差-方差权衡
- en: The learning curve (see the right-hand panel of *Figure 6.11* for our house
    price regression example) helps determine whether a model's cross-validation performance
    would benefit from additional data, and whether the prediction errors are more
    driven by bias or by variance.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线（参见我们房价回归示例的右侧面板*图6.11*）有助于确定模型的交叉验证性能是否会受益于额外的数据，并且预测错误是更多地受偏差还是方差驱动。
- en: More data is unlikely to improve performance if training and cross-validation
    scores converge. At this point, it is important to evaluate whether the model
    performance meets expectations, determined by a human benchmark. If this is not
    the case, then you should modify the model's hyperparameter settings to better
    capture the relationship between the features and the outcome, or choose a different
    algorithm with a higher capacity to capture complexity.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练和交叉验证分数趋于一致，那么更多的数据不太可能提高性能。在这一点上，评估模型性能是否符合人类基准非常重要。如果不符合，则应修改模型的超参数设置，以更好地捕捉特征和结果之间的关系，或选择具有更高复杂性的不同算法。
- en: In addition, the variation of train and test errors shown by the shaded confidence
    intervals provides clues about the bias and variance sources of the prediction
    error. Variability around the cross-validation error is evidence of variance,
    whereas variability for the training set suggests bias, depending on the size
    of the training error.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由阴影置信区间显示的训练和测试错误的变化提供了关于预测错误的偏差和方差来源的线索。交叉验证错误的变异性是方差的证据，而训练集的变异性则表明偏差，具体取决于训练错误的大小。
- en: In our example, the cross-validation performance has continued to drop, but
    the incremental improvements have shrunk, and the errors have plateaued, so there
    are unlikely to be many benefits from a larger training set. On the other hand,
    the data is showing substantial variance given the range of validation errors
    compared to that shown for the training errors.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，交叉验证性能持续下降，但增量改进已经减少，错误已经趋于稳定，因此增大训练集不太可能带来太多好处。另一方面，数据显示出相当大的方差，与训练错误相比，验证错误的范围较大。
- en: Parameter tuning using GridSearchCV and pipeline
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GridSearchCV和pipeline进行参数调整
- en: Since hyperparameter tuning is a key ingredient of the machine learning workflow,
    there are tools to automate this process. The scikit-learn library includes a
    `GridSearchCV` interface that cross-validates all combinations of parameters in
    parallel, captures the result, and automatically trains the model using the parameter
    setting that performed best during cross-validation on the full dataset.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于超参数调整是机器学习工作流程的关键组成部分，因此有工具可以自动化这个过程。scikit-learn库包括一个`GridSearchCV`接口，可以并行交叉验证所有参数组合，捕获结果，并在完整数据集上自动训练使用在交叉验证中表现最佳的参数设置的模型。
- en: In practice, the training and validation set often requires some processing
    prior to cross-validation. Scikit-learn offers the `Pipeline` to also automate
    any feature-processing steps while using `GridSearchCV`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，训练和验证集通常需要在交叉验证之前进行一些处理。Scikit-learn提供了`Pipeline`，可以在使用`GridSearchCV`的同时自动化任何特征处理步骤。
- en: You can look at the implementation examples in the included `machine_learning_workflow.ipynb`
    notebook to see these tools in action.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看附带的`machine_learning_workflow.ipynb`笔记本中的实现示例，以了解这些工具的实际运用。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the challenge of learning from data and looked
    at supervised, unsupervised, and reinforcement models as the principal forms of
    learning that we will study in this book to build algorithmic trading strategies.
    We discussed the need for supervised learning algorithms to make assumptions about
    the functional relationships that they attempt to learn. They do this to limit
    the search space while incurring an inductive bias that may lead to excessive
    generalization errors.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了从数据中学习的挑战，并将监督学习、无监督学习和强化学习模型作为我们将在本书中研究的主要学习形式，以构建算法交易策略。我们讨论了监督学习算法需要对它们试图学习的功能关系做出假设的必要性。他们这样做是为了限制搜索空间，同时产生归纳偏差，可能导致过度泛化错误。
- en: We presented key aspects of the machine learning workflow, introduced the most
    common error metrics for regression and classification models, explained the bias-variance
    trade-off, and illustrated the various tools for managing the model selection
    process using cross-validation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了机器学习工作流程的关键方面，介绍了回归和分类模型最常见的错误度量标准，解释了偏差-方差权衡，并举例说明了使用交叉验证来管理模型选择过程的各种工具。
- en: In the following chapter, we will dive into linear models for regression and
    classification to develop our first algorithmic trading strategies that use machine
    learning.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究用于开发我们的第一个机器学习算法交易策略的回归和分类线性模型。
