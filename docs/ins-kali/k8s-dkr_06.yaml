- en: '*Chapter 4*: Deploying Kubernetes Using KinD'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：使用KinD部署Kubernetes'
- en: One of the largest obstacles to learning Kubernetes is having enough resources
    to create a cluster for testing or development. Like most IT professionals, we
    like to have a Kubernetes cluster on our laptops for demonstrations and for testing
    products in general.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 学习Kubernetes最大的障碍之一是拥有足够的资源来创建用于测试或开发的集群。像大多数IT专业人员一样，我们喜欢在笔记本电脑上拥有一个Kubernetes集群，用于演示和测试产品。
- en: Often, you may have a need to run multiple clusters for a complex demonstration,
    such as a multi-cluster service mesh or testing **kubefed2**. These scenarios
    would require multiple servers to create the necessary clusters, which, in turn,
    would require a lot of RAM and a hypervisor.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您可能需要运行多个集群进行复杂的演示，比如多集群服务网格或测试**kubefed2**。这些场景将需要多台服务器来创建必要的集群，这又需要大量的RAM和一个虚拟化程序。
- en: To do full testing on a multiple cluster scenario, you would need to create
    six nodes for each cluster. If you created the clusters using virtual machines,
    you would need to have enough resources to run 6 virtual machines. Each of the
    machines would have an overhead including disk space, memory, and CPU utilization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在多集群场景下进行全面测试，您需要为每个集群创建六个节点。如果您使用虚拟机创建集群，您需要有足够的资源来运行6个虚拟机。每台机器都会有一些开销，包括磁盘空间、内存和CPU利用率。
- en: But what if you could create a cluster using just containers? Using containers,
    rather than full virtual machines, will give you the ability to run additional
    nodes due to the reduced system requirements, create and delete clusters in minutes
    with a single command, script cluster creation, and allow you to run multiple
    clusters on a single host.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您可以只使用容器来创建一个集群呢？使用容器而不是完整的虚拟机将使您能够由于降低的系统要求而运行额外的节点，用单个命令在几分钟内创建和删除集群，脚本化集群创建，并允许您在单个主机上运行多个集群。
- en: Using containers to run a Kubernetes cluster provides you with an environment
    that would be difficult for most people to deploy using virtual machines, or physical
    hardware due to resource constraints. To explain how to run a cluster using only
    containers locally, we will use KinD to create a Kubernetes cluster on your Docker
    host. We will deploy a multi-node cluster that you will use in future chapters
    to test and deploy components such as Ingress controllers, authentication, RBAC,
    security policies, and more.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用容器来运行Kubernetes集群为您提供了一个环境，对于大多数人来说，使用虚拟机或物理硬件部署将会很困难，因为资源受限。为了解释如何在本地仅使用容器运行集群，我们将使用KinD在您的Docker主机上创建一个Kubernetes集群。我们将部署一个多节点集群，您将在未来的章节中用来测试和部署诸如Ingress控制器、认证、RBAC、安全策略等组件。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing Kubernetes components and objects
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Kubernetes组件和对象
- en: Using development clusters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开发集群
- en: Installing KinD
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装KinD
- en: Creating a KinD cluster
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个KinD集群
- en: Reviewing your KinD cluster
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查您的KinD集群
- en: Adding a custom load balancer for Ingress
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为Ingress添加自定义负载均衡器
- en: Let's get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具有以下技术要求：
- en: A Docker host installed using the steps from [*Chapter 1*](B15514_01_Final_ASB_ePub.xhtml#_idTextAnchor018),
    *Docker and Container Essentials*
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[*第1章*](B15514_01_Final_ASB_ePub.xhtml#_idTextAnchor018)中的步骤安装的Docker主机，*Docker和容器基础知识*
- en: Installation scripts from this book's GitHub repository
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书GitHub存储库中的安装脚本
- en: 'You can access the code for this chapter by going to this book''s GitHub repository:
    [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问本书的GitHub存储库来获取本章的代码：[https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide)。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We thought it was important to point out that this chapter will reference multiple
    Kubernetes objects, some without a lot of context. [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*, goes over Kubernetes objects in detail, many with commands
    you can use to understand them, so we thought having a cluster to use while reading
    about this would be useful.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为有必要指出，本章将涉及多个Kubernetes对象，其中一些没有太多的上下文。[*第5章*]（B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150）*，Kubernetes
    Bootcamp*详细介绍了Kubernetes对象，其中许多带有您可以使用的命令来理解它们，因此我们认为在阅读本章时使用集群将会很有用。
- en: Most of the base Kubernetes topics covered in this chapter will be discussed
    in future chapters, so if some topics are a bit foggy after you've read this chapter,
    don't fear! They will be discussed in detail in later chapters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的大多数基本Kubernetes主题将在以后的章节中讨论，因此如果在阅读本章后某些主题有点模糊，不要担心！它们将在以后的章节中详细讨论。
- en: Introducing Kubernetes components and objects
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kubernetes组件和对象
- en: Since this chapter will refer to common Kubernetes objects and components, we
    wanted to provide a short table of terms that you will see and a brief definition
    of each to provide context.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章将涉及常见的Kubernetes对象和组件，我们想提供一个术语表，您将在其中看到每个术语的简要定义，以提供上下文。
- en: 'In [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150), *Kubernetes
    Bootcamp*, we will go over the components of Kubernetes and the base set of objects
    that are included in a cluster. We will also discuss how to interact with a cluster
    using the kubectl executable:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*]（B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150）*，Kubernetes Bootcamp*中，我们将介绍Kubernetes的组件和集群中包含的基本对象集。我们还将讨论如何使用kubectl可执行文件与集群进行交互：
- en: '![Table 4.1 – Kubernetes components and objects'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![表4.1 – Kubernetes组件和对象'
- en: '](image/Table_4.1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Table_4.1.jpg)'
- en: Table 4.1 – Kubernetes components and objects
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 – Kubernetes组件和对象
- en: While these are only a few of the objects that are available in a Kubernetes
    cluster, they are the main objects we will mention in this chapter. Knowing what
    each object is and having basic knowledge of their functionality will help you
    understand this chapter and deploy a KinD cluster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些只是Kubernetes集群中可用的一些对象，但它们是我们将在本章中提到的主要对象。了解每个对象是什么，并具有它们功能的基本知识将有助于您理解本章并部署KinD集群。
- en: Interacting with a cluster
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与集群交互
- en: 'To test our KinD installation, we will interact with the cluster using the
    kubectl executable. We will go over kubectl in [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*, but since we will be using a few commands in this chapter,
    we wanted to provide the commands we will use in a table with an explanation of
    what the options provide:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的KinD安装，我们将使用kubectl可执行文件与集群进行交互。我们将在[*第5章*]（B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150）*，Kubernetes
    Bootcamp*中介绍kubectl，但由于我们将在本章中使用一些命令，我们想提供我们将在表格中使用的命令以及选项提供的解释：
- en: '![Table 4.2 – Basic kubectl commands'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![表4.2 – 基本kubectl命令'
- en: '](image/Table_4.2.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Table_4.2.jpg)'
- en: Table 4.2 – Basic kubectl commands
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2 – 基本kubectl命令
- en: In this chapter, you will use these basic commands to deploy parts of the cluster
    that we will use throughout this book.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您将使用这些基本命令部署我们在本书中将使用的集群的部分。
- en: 'Next, we will introduce the concept of development clusters and then focus
    on one of the most popular tools used to create development clusters: KinD.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍开发集群的概念，然后重点介绍用于创建开发集群的最流行工具之一：KinD。
- en: Using development clusters
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用开发集群
- en: Over the years, various tools have been created to install development Kubernetes
    clusters, allowing admins and developers to perform testing on a local system.
    Many of these tools worked for basic Kubernetes tests, but they often had limitations
    that made them less than ideal for quick, advanced scenarios.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，已经创建了各种工具来安装开发Kubernetes集群，使管理员和开发人员能够在本地系统上进行测试。这些工具中的许多工作都适用于基本的Kubernetes测试，但它们经常存在限制，使它们不太适合快速、高级的场景。
- en: 'Some of the most common solutions available are as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最常见的解决方案如下：
- en: Docker Desktop
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Desktop
- en: minikube
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: minikube
- en: kubeadm
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubeadm
- en: Each solution has benefits, limitations, and use cases. Some solutions limit
    you to a single node that runs both the control plane and worker nodes. Others
    offer multi-node support but require additional resources to create multiple virtual
    machines. Depending on your development or testing requirements, these solutions
    may not fill your needs completely.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每种解决方案都有其优势、局限性和使用情况。有些解决方案限制您只能在单个节点上运行控制平面和工作节点。其他解决方案提供多节点支持，但需要额外的资源来创建多个虚拟机。根据您的开发或测试需求，这些解决方案可能无法完全满足您的需求。
- en: It seems that a new solution is coming out every few weeks, and one of the newest
    options for creating development clusters is a project from a **Kubernetes in
    Docker** (**KinD**) Kubernetes SIG.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎每隔几周就会出现一种新的解决方案，用于创建开发集群的最新选项之一是来自**Kubernetes in Docker**（**KinD**）Kubernetes
    SIG的项目。
- en: Using a single host, KinD allows you to create multiple clusters, and each cluster
    can have multiple control plane and worker nodes. The ability to run multiple
    nodes allows advanced testing that would have required more resources using another
    solution. KinD has been very well received by the community and has an active
    Git community at [https://github.com/kubernetes-sigs/kind](https://github.com/kubernetes-sigs/kind),
    as well as a Slack channel (*#kind*).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个主机，KinD允许您创建多个集群，每个集群可以有多个控制平面和工作节点。运行多个节点的能力允许进行高级测试，而使用其他解决方案可能需要更多的资源。KinD在社区中受到了很好的评价，并且在[https://github.com/kubernetes-sigs/kind](https://github.com/kubernetes-sigs/kind)上有一个活跃的Git社区，以及一个Slack频道（*#kind*）。
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Do not use KinD as a production cluster or expose a KinD cluster to the internet.
    While KinD clusters offer most of the same features you would want in a production
    cluster, it has **not** been designed for production environments.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将KinD用作生产集群或将KinD集群暴露在互联网上。虽然KinD集群提供了大部分您在生产集群中想要的功能，但它**不**是为生产环境而设计的。
- en: Why did we select KinD for this book?
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们选择KinD来写这本书？
- en: When we started this book, we wanted to include theory, as well as hands-on
    experience. KinD allows us to provide scripts to spin up and spin down clusters,
    and while other solutions can do something similar, KinD can create a new multi-node
    cluster in minutes. We wanted to separate the control plane and worker nodes to
    provide a more "realistic" cluster. In order to limit the hardware requirements
    and to make Ingress easier to configure, we will only create a two-node cluster
    for the exercises in this book.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始写这本书时，我们希望包括理论和实践经验。KinD允许我们提供脚本来快速创建和关闭集群，虽然其他解决方案也可以做类似的事情，但KinD可以在几分钟内创建一个新的多节点集群。我们希望将控制平面和工作节点分开，以提供一个更“真实”的集群。为了限制硬件要求并使Ingress更容易配置，我们将在本书的练习中只创建一个两节点集群。
- en: 'A multi-node cluster can be created in a few minutes and once testing has been
    completed, clusters can be torn down in a few seconds. The ability to spin up
    and spin down clusters makes KinD the perfect platform for our exercises. KinD''s
    requirements are simple: you only need a running Docker daemon to create a cluster.
    This means that it is compatible with most operating systems, including the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 多节点集群可以在几分钟内创建，一旦测试完成，集群可以在几秒内被拆除。快速创建和销毁集群的能力使KinD成为我们练习的理想平台。KinD的要求很简单：您只需要运行的Docker守护程序来创建集群。这意味着它与大多数操作系统兼容，包括以下操作系统：
- en: Linux
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux
- en: macOS running Docker Desktop
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Docker桌面的macOS
- en: Windows running Docker Desktop
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行Docker桌面的Windows
- en: Windows running WSL2
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行WSL2的Windows
- en: Important note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At the time of writing, KinD does not offer support for Chrome OS.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，KinD不支持Chrome OS。
- en: While KinD supports most operating systems, we have selected Ubuntu 18.04 as
    our host system. Some of the exercises in this book require files to be in specific
    directories and selecting a single Linux version helps us make sure the exercises
    work as designed. If you do not have access to an Ubuntu server at home, you can
    create a virtual machine in a cloud provider such as GCP. Google offers $300 in
    credit, which is more than enough to run a single Ubuntu server for a few weeks.
    You can view GCP's free options at [https://cloud.google.com/free/](https://cloud.google.com/free/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然KinD支持大多数操作系统，但我们选择了Ubuntu 18.04作为我们的主机系统。本书中的一些练习需要文件放在特定的目录中，选择单个Linux版本可以确保练习按设计工作。如果您在家里没有Ubuntu服务器，可以在GCP等云提供商中创建虚拟机。Google提供300美元的信用额度，足够您运行单个Ubuntu服务器数周。您可以在[https://cloud.google.com/free/](https://cloud.google.com/free/)查看GCP的免费选项。
- en: Now, let's explain how KinD works and what a base KinD Kubernetes cluster looks
    like.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们解释一下KinD的工作原理以及基本的KinD Kubernetes集群是什么样子的。
- en: Working with a base KinD Kubernetes cluster
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基本的KinD Kubernetes集群
- en: At a high level, you can think of a KinD cluster as consisting of a **single**
    Docker container that runs a control plane node and a worker node to create a
    Kubernetes cluster. To make the deployment easy and robust, KinD bundles every
    Kubernetes object into a single image, known as a node image. This node image
    contains all the required Kubernetes components to create a single-node cluster,
    or a multi-node cluster.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，您可以将KinD集群视为由一个**单个**Docker容器组成，该容器运行控制平面节点和工作节点以创建Kubernetes集群。为了使部署简单而稳健，KinD将每个Kubernetes对象捆绑到一个单一的镜像中，称为节点镜像。此节点镜像包含创建单节点集群或多节点集群所需的所有Kubernetes组件。
- en: 'Once a cluster is running, you can use Docker to exec into a control plane
    node container and look at the process list. In the process list, you will see
    the standard Kubernetes components for the control plane nodes running:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群运行起来，您可以使用Docker执行进入控制平面节点容器并查看进程列表。在进程列表中，您将看到运行控制平面节点的标准Kubernetes组件：
- en: '![Figure 4.1 – Host process list showing control plane components'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.1 - 显示控制平面组件的主机进程列表'
- en: '](image/Fig_4.1_B15514.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](图像/Fig_4.1_B15514.jpg)'
- en: Figure 4.1 – Host process list showing control plane components
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 - 显示控制平面组件的主机进程列表
- en: 'If you were to exec into a worker node to check the components, you would see
    all the standard worker node components:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要执行到工作节点以检查组件，您将看到所有标准的工作节点组件：
- en: '![Figure 4.2 – Host process list showing worker components'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 - 显示工作组件的主机进程列表
- en: '](image/Fig_4.2_B15514.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （图像/Fig_4.2_B15514.jpg）
- en: Figure 4.2 – Host process list showing worker components
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 - 显示工作组件的主机进程列表
- en: We will cover the standard Kubernetes components in [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*, including **kube-apiserver**, **kubelets**, **kube-proxy**,
    **kube-scheduler**, and **kube-controller-manager**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5章《Kubernetes Bootcamp》中涵盖标准的Kubernetes组件，包括kube-apiserver，kubelets，kube-proxy，kube-scheduler和kube-controller-manager。
- en: 'In addition to standard Kubernetes components, both KinD nodes have an additional
    component that is not part of most standard installations: Kindnet. Kindnet is
    the included, default CNI when you install a base KinD cluster. While Kindnet
    is the default CNI, you have the option to disable it and use an alternative,
    such as Calico.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准的Kubernetes组件，KinD节点都有一个不是大多数标准安装的组件：Kindnet。当您安装基本的KinD集群时，Kindnet是包含的默认CNI。虽然Kindnet是默认的CNI，但您可以选择禁用它并使用其他选择，比如Calico。
- en: 'Now that you have seen each node and the Kubernetes components, let''s take
    a look at what''s included with a base KinD cluster. To show the complete cluster
    and all the components that are running, we can run the **kubectl get pods --all-namespaces**
    command. This will list all the running components for the cluster, including
    the base components we will discuss in [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150),
    *Kubernetes Bootcamp*. In addition to the base cluster components, you may notice
    a running pod in a namespace called **local-path-storage**, along with a pod named
    **local-path-provisioner**. This pod is running one of the add-ons that KinD includes,
    providing the cluster with the ability to auto-provision **PersistentVolumeClaims**:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了每个节点和Kubernetes组件，让我们来看看基本KinD集群包含的内容。要显示完整的集群和所有正在运行的组件，我们可以运行kubectl
    get pods --all-namespaces命令。这将列出集群的所有运行组件，包括我们将在第5章《Kubernetes Bootcamp》中讨论的基本组件。除了基本集群组件之外，您可能会注意到在一个名为local-path-storage的命名空间中有一个正在运行的pod，以及一个名为local-path-provisioner的pod。这个pod正在运行KinD包含的附加组件之一，为集群提供自动配置PersistentVolumeClaims的能力：
- en: '![Figure 4.3 – kubectl get pods showing local-path-provisioner'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.3 - kubectl get pods显示local-path-provisioner'
- en: '](image/Fig_4.3_B15514.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.3_B15514.jpg)'
- en: Figure 4.3 – kubectl get pods showing local-path-provisioner
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 - kubectl get pods显示local-path-provisioner
- en: Most development cluster offerings provide similar, common functions that people
    need to test deployments on Kubernetes. They all provide a Kubernetes control
    plane and worker nodes, and most include a default CNI for networking. Few offerings
    go beyond this base functionality, and as Kubernetes workloads mature, you may
    find the need for additional plugins such as **local-path-provisioner**. We will
    leverage this component heavily in some of the exercises in this book because
    without it, we will have a tougher time creating some of the procedures.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数开发集群提供类似的常见功能，人们需要在Kubernetes上测试部署。它们都提供一个Kubernetes控制平面和工作节点，大多数包括用于网络的默认CNI。很少有提供超出这些基本功能的功能，随着Kubernetes工作负载的成熟，您可能会发现需要额外的插件，比如local-path-provisioner。我们将在本书的一些练习中大量利用这个组件，因为如果没有它，我们将更难创建一些流程。
- en: Why should you care about persistent volumes in your development cluster? Most
    production clusters running Kubernetes will provide persistent storage to developers.
    Usually, the storage will be backed by storage systems based on block storage,
    S3, or NFS. Aside from NFS, most home labs rarely have the resources to run a
    full-featured storage system. **local-path-provisioner** removes this limitation
    from users by providing all the functions to your KinD cluster that an expensive
    storage solution would provide.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么您应该关心开发集群中的持久卷？大多数运行Kubernetes的生产集群将为开发人员提供持久存储。通常，存储将由基于块存储、S3或NFS的存储系统支持。除了NFS，大多数家庭实验室很少有资源来运行功能齐全的存储系统。本地路径供应程序通过为您的KinD集群提供昂贵存储解决方案提供的所有功能，消除了用户的这一限制。
- en: In [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*, Kubernetes
    Bootcamp*, we will discuss a few API objects that are part of Kubernetes storage.
    We will discuss the **CSIdrivers**, **CSInodes**, and **StorageClass** objects.
    These objects are used by the cluster to provide access to the backend storage
    system. Once installed and configured, pods consume the storage using the **PersistentVolumes**
    and **PersistentVolumeClaims** objects. Storage objects are important to understand,
    but when they were first released, they were difficult for most people to test
    since they aren't included in most Kubernetes development offerings.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在《第5章 Kubernetes Bootcamp》中，我们将讨论一些Kubernetes存储的API对象。我们将讨论CSIdrivers、CSInodes和StorageClass对象。这些对象被集群用来提供对后端存储系统的访问。一旦安装和配置完成，pod将使用PersistentVolumes和PersistentVolumeClaims对象来消耗存储。存储对象很重要，但当它们首次发布时，大多数人很难测试，因为它们没有包含在大多数Kubernetes开发产品中。
- en: KinD recognized this limitation and chose to bundle a project from Rancher called
    **local-path-provisioner**, which is based on the Kubernetes local persistent
    volumes that were introduced in Kubernetes 1.10.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: KinD意识到了这个限制，并选择捆绑来自Rancher的一个名为本地路径供应程序的项目，该项目基于Kubernetes 1.10中引入的Kubernetes本地持久卷。
- en: You may be wondering why anyone would need an add-on since Kubernetes has native
    support for local host persistent volumes. While support may have been added for
    local persistent storage, Kubernetes has not added auto-provisioning capabilities.
    CNCF does offer an auto-provisioner, but it must be installed and configured as
    a separate Kubernetes component. KinD makes it easy to auto-provision since the
    provisioner is included in all base installations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么有人需要一个附加组件，因为Kubernetes本地主机持久卷已经有了原生支持。虽然已经为本地持久存储添加了支持，但Kubernetes还没有添加自动供应的能力。CNCF确实提供了一个自动供应程序，但它必须作为一个单独的Kubernetes组件安装和配置。KinD使自动供应变得容易，因为供应程序包含在所有基本安装中。
- en: 'Rancher''s project provides the following to KinD:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher的项目为KinD提供了以下内容：
- en: Auto-creation of **PersistentVolumes** when a PVC request is created
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当创建PVC请求时自动创建PersistentVolumes
- en: A default **StorageClass** named standard
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为standard的默认StorageClass
- en: When the auto-provisioner sees a **PersistentVolumeClaim** request hit the API
    server, a **PersistentVolume** will be created and the pod's PVC will be bound
    to the newly created PVC.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动供应程序看到一个PersistentVolumeClaim请求命中API服务器时，将创建一个PersistentVolume，并将pod的PVC绑定到新创建的PVC上。
- en: '**local-path-provisioner** adds a feature to KinD clusters that greatly expands
    the potential test scenarios that you can run. Without the ability to auto-provision
    persistent disks, it would be a challenge to test many pre-built deployments that
    require persistent disks.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本地路径供应程序为KinD集群增加了一个功能，极大地扩展了您可以运行的潜在测试场景。如果没有自动供应持久磁盘的能力，测试许多需要持久磁盘的预构建部署将是一项挑战。
- en: With the help of Rancher, KinD provides you with a solution so that you can
    experiment with dynamic volumes, storage classes, and other storage tests that
    would otherwise be impossible to run outside a data center. We will use the provisioner
    in multiple chapters to provide volumes to different deployments. We will point
    these out to reinforce the advantages of using auto-provisioning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 借助 Rancher 的帮助，KinD 为您提供了一个解决方案，以便您可以尝试动态卷、存储类和其他存储测试，否则在数据中心之外是不可能运行的。我们将在多个章节中使用提供程序为不同的部署提供卷。我们将指出这些以强调使用自动配置的优势。
- en: Understanding the node image
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解节点镜像
- en: The node image is what provides KinD the magic to run Kubernetes inside a Docker
    container. This is an impressive accomplishment since Docker relies on a **systemd**
    running system and other components that are not included in most container images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 节点镜像是为 KinD 提供魔力以在 Docker 容器内运行 Kubernetes 的关键。这是一个令人印象深刻的成就，因为 Docker 依赖于运行
    systemd 的系统和大多数容器镜像中不包括的其他组件。
- en: KinD starts off with a base image, which is an image the team has developed
    that contains everything required for Docker, Kubernetes, and **systemd**. Since
    the base image is based on an Ubuntu image, the team removes services that are
    not required and configures **systemd** for Docker. Finally, the node image is
    created using the base image.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: KinD 从一个基础镜像开始，这是团队开发的一个包含运行 Docker、Kubernetes 和 systemd 所需的一切的镜像。由于基础镜像是基于
    Ubuntu 镜像的，团队移除了不需要的服务，并为 Docker 配置了 systemd。最后，使用基础镜像创建节点镜像。
- en: Tip
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to know the details of how the base image is created, you can look
    at the Dockerfile in the KinD team's GitHub repository at [https://github.com/kubernetes-sigs/kind/blob/controlplane/images/base/Dockerfile](https://github.com/kubernetes-sigs/kind/blob/controlplane/images/base/Dockerfile).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解基础镜像是如何创建的细节，您可以查看 KinD 团队在 GitHub 仓库中的 Dockerfile，网址为 [https://github.com/kubernetes-sigs/kind/blob/controlplane/images/base/Dockerfile](https://github.com/kubernetes-sigs/kind/blob/controlplane/images/base/Dockerfile)。
- en: KinD and Docker networking
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KinD 和 Docker 网络
- en: Since KinD uses Docker as the container engine to run the cluster nodes, all
    clusters are limited to the same network constraints that a standard Docker container
    is. In [*Chapter 3*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*, Understanding
    Docker Networking*, we had a refresher on Docker networking and the potential
    limitations of Docker's default networking stack. These limitations do not limit
    testing your KinD Kubernetes cluster from the local host, but they can lead to
    issues when you want to test containers from other machines on your network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 KinD 使用 Docker 作为容器引擎来运行集群节点，所有集群都受到与标准 Docker 容器相同的网络约束限制。在《第三章》《理解 Docker
    网络》中，我们对 Docker 网络和 Docker 默认网络堆栈的潜在限制进行了复习。这些限制不会限制从本地主机测试您的 KinD Kubernetes
    集群，但当您想要从网络上的其他计算机测试容器时，可能会导致问题。
- en: 'Along with the Docker networking considerations, we must consider the Kubernetes
    **Container Network Interface** (**CNI**) as well. Officially, the KinD team has
    limited the networking options to only two CNIs: Kindnet and Calico. Kindnet is
    the only CNI they will support but you do have the option to disable the default
    Kindnet installation, which will create a cluster without a CNI installed. After
    the cluster has been deployed, you can deploy a CNI manifest such as Calico.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Docker 网络考虑因素，我们还必须考虑 Kubernetes 容器网络接口（CNI）。在官方上，KinD 团队将网络选项限制为两种 CNI：Kindnet
    和 Calico。Kindnet 是他们唯一支持的 CNI，但您可以选择禁用默认的 Kindnet 安装，这将创建一个没有安装 CNI 的集群。在集群部署后，您可以部署
    Calico 等 CNI 清单。
- en: Many Kubernetes installations for both small development clusters and enterprise
    clusters use Tigera's Calico for the CNI and as such, we have elected to use Calico
    as our CNI for the exercises in this book.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用于小型开发集群和企业集群的Kubernetes安装都使用Tigera的Calico作为CNI，因此，我们选择在本书的练习中使用Calico作为我们的CNI。
- en: Keeping track of the nesting dolls
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪套娃
- en: Running a solution such as KinD can get confusing due to the container-in-a-container
    deployment. We compare this to Russian nesting dolls, where one doll fits into
    another, then another, and so on. As you start to play with KinD for your own
    cluster, you may lose track of the communication paths between your host, Docker,
    and the Kubernetes nodes. To keep your sanity, you should have a solid understanding
    of where each component is running and how you can interact with each one.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 运行诸如KinD之类的解决方案可能会令人困惑，因为它是一个容器中的容器部署。我们将其比作俄罗斯套娃，一个娃娃套进另一个娃娃，然后再套进另一个，依此类推。当你开始使用KinD来搭建自己的集群时，你可能会迷失在主机、Docker和Kubernetes节点之间的通信路径中。为了保持理智，你应该对每个组件的运行位置有一个清晰的理解，以及如何与每个组件进行交互。
- en: The following diagram shows the three layers that must be running to form a
    KinD cluster. It's important to note that each layer can only interact with the
    layer directly above it. This means that the KinD container in layer 3 can only
    see the Docker image running in layer 2, and the Docker image can see the Linux
    host running in layer 1\. If you wanted to communicate directly from the host
    to a container running in your KinD cluster, you would need to go through the
    Docker layer, and then to the Kubernetes container in layer 3\.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了必须运行的三个层，以形成一个KinD集群。重要的是要注意，每个层只能与直接位于其上方的层进行交互。这意味着第3层中的KinD容器只能看到第2层中运行的Docker镜像，而Docker镜像只能看到第1层中运行的Linux主机。如果你想要直接从主机与运行在你的KinD集群中的容器进行通信，你需要通过Docker层，然后到第3层的Kubernetes容器。
- en: 'This is important to understand so that you can use KinD effectively as a testing
    environment:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一点很重要，这样你才能有效地将KinD作为测试环境使用：
- en: '![Figure 4.4 – Host cannot communicate with KinD directly'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 - 主机无法直接与KinD通信'
- en: '](image/Fig_4.4_B15514.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.4_B15514.jpg)'
- en: Figure 4.4 – Host cannot communicate with KinD directly
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 - 主机无法直接与KinD通信
- en: As an example, consider that you want to deploy a web server to your Kubernetes
    cluster. You deploy an Ingress controller in the KinD cluster and you want to
    test the site using Chrome on your Docker host or a different workstation on the
    network. You attempt to target the host on port 80 and receive a failure in the
    browser. Why would this fail?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设你想要将web服务器部署到你的Kubernetes集群中。你在KinD集群中部署了一个Ingress控制器，并且你想要使用Chrome在你的Docker主机上或者网络上的另一台工作站上测试该网站。你尝试以端口80定位主机，并在浏览器中收到了失败。为什么会失败呢？
- en: 'The pod running the web server is in layer 3 and cannot receive direct traffic
    from the host or machines on the network. In order to access the web server from
    your host, you will need to forward the traffic from the Docker layer to the KinD
    layer. Remember that in [*Chapter 3*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*,
    Understanding Docker Networking*, we explained how to expose a container to the
    network by adding a listening port to the container. In our case, we need port
    80 and port 443\. When a container is started with a port, the Docker daemon will
    forward the incoming traffic from the host to the running Docker container:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 运行web服务器的pod位于第3层，无法直接接收来自主机或网络上的机器的流量。为了从主机访问web服务器，你需要将流量从Docker层转发到KinD层。请记住，在[*第3章*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*，理解Docker网络*中，我们解释了如何通过向容器添加监听端口来将容器暴露给网络。在我们的情况下，我们需要端口80和端口443。当一个容器启动时带有一个端口，Docker守护程序将把来自主机的传入流量转发到正在运行的Docker容器：
- en: '![Figure 4.5 – Host communicates with KinD via an Ingress controller'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 - 主机通过 Ingress 控制器与 KinD 通信'
- en: '](image/Fig_4.5_B15514.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.5_B15514.jpg)'
- en: Figure 4.5 – Host communicates with KinD via an Ingress controller
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 - 主机通过 Ingress 控制器与 KinD 通信
- en: With ports 80 and 443 exposed on the Docker container, the Docker daemon will
    now accept incoming requests for 80 and 443 and the NGINX Ingress controller will
    receive the traffic. This works because we have exposed ports 80 and 443 in two
    places on the Docker layer. We have exposed it in the Kubernetes layer by running
    our NGINX container using host ports 80 and 443\. This installation process will
    be explained later in this chapter, but for now, you just need to understand the
    basic flow.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker 容器上暴露了端口 80 和 443 后，Docker 守护程序现在将接受端口 80 和 443 的传入请求，并且 NGINX Ingress
    控制器将接收流量。这是因为我们在 Docker 层上两个地方都暴露了端口 80 和 443。我们在 Kubernetes 层上通过使用主机端口 80 和 443
    运行我们的 NGINX 容器来暴露它。这个安装过程将在本章后面解释，但现在，您只需要了解基本流程。
- en: 'On the host, you make a request for a web server that has an Ingress rule in
    your Kubernetes cluster:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机上，您对 Kubernetes 集群中具有 Ingress 规则的 Web 服务器发出请求：
- en: The request looks at the IP address that was requested (in this case, the local
    IP address).
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求查看了所请求的 IP 地址（在本例中为本地 IP 地址）。
- en: The Docker container running our Kubernetes node is listening on the IP address
    for ports 80 and 443, so the request is accepted and sent to the running container.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行我们的 Kubernetes 节点的 Docker 容器正在监听端口 80 和 443 的 IP 地址，因此请求被接受并发送到正在运行的容器。
- en: The NGINX pod in your Kubernetes cluster has been configured to use the host
    ports 80 and 443, so the traffic is forwarded to the pod.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的 Kubernetes 集群中的 NGINX pod 已配置为使用主机端口 80 和 443，因此流量被转发到该 pod。
- en: The user receives the requested web page from the web server via the NGINX Ingress
    controller.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户通过 NGINX Ingress 控制器从 Web 服务器接收所请求的网页。
- en: This is a little confusing, but the more you use KinD and interact with it,
    the easier this becomes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点令人困惑，但你使用 KinD 并与其交互的次数越多，这就变得越容易。
- en: To use a KinD cluster for your development requirements, you need to understand
    how KinD works. So far, you have learned about the node image and how the image
    is used to create a cluster. You've also learned how KinD network traffic flows
    between the Docker host and the containers running the cluster. With this base
    knowledge, we will move on to creating a Kubernetes cluster using KinD.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要满足开发需求，您需要了解 KinD 的工作原理。到目前为止，您已经了解了节点镜像以及如何使用该镜像创建集群。您还了解了 KinD 网络流量是如何在 Docker
    主机和运行集群的容器之间流动的。有了这些基础知识，我们将继续使用 KinD 创建一个 Kubernetes 集群。
- en: Installing KinD
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 KinD
- en: The files for this chapter are located in the KinD directory. You can use the
    provided files, or you can create your own files from this chapter's content.
    We will explain each step of the installation process in this section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的文件位于 KinD 目录中。您可以使用提供的文件，也可以根据本章的内容创建自己的文件。我们将在本节中解释安装过程的每个步骤。
- en: Note
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, the current version of KinD is .0.8.1\. Version .0.8.0
    introduced a new feature; that is, maintaining cluster state between reboot and
    Docker restarts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，KinD 的当前版本为 0.8.1。版本 0.8.0 引入了一个新功能；即在重启和 Docker 重新启动之间维护集群状态。
- en: Installing KinD – prerequisites
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 KinD - 先决条件
- en: KinD requires a few prerequisites before you can create a cluster. In this section,
    we will detail each requirement and what how to install each component.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群之前，KinD 需要满足一些先决条件。在本节中，我们将详细介绍每个要求以及如何安装每个组件。
- en: Installing Kubectl
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Kubectl
- en: 'Since KinD is a single executable, it does not install **kubectl**. If you
    do not have **kubectl** installed and you are using an Ubuntu 18.04 system, you
    can install it by running a snap install:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 KinD 是一个单个可执行文件，它不会安装 **kubectl**。如果您尚未安装 **kubectl** 并且正在使用 Ubuntu 18.04
    系统，可以通过运行 snap install 来安装它：
- en: sudo snap install kubectl --classic
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: sudo snap install kubectl --classic
- en: Installing Go
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Go
- en: Before we can create a KinD cluster, you will need to install Go on your host.
    If you already have Go installed and working, you can skip this step. Installing
    Go requires you to download the Go archive, extract the executable, and set a
    project path. The following commands can be used to install Go on your machine.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建 KinD 集群之前，您需要在主机上安装 Go。如果您已经安装并且正常工作，可以跳过此步骤。安装 Go 需要您下载 Go 存档，提取可执行文件，并设置项目路径。以下命令可用于在您的机器上安装
    Go。
- en: 'The script to install Go can be executed from this book''s repository by running
    **/chapter4/install-go.sh**:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过运行 **/chapter4/install-go.sh** 从本书存储库执行安装 Go 的脚本：
- en: wget https://dl.google.com/go/go1.13.3.linux-amd64.tar.gz
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: wget https://dl.google.com/go/go1.13.3.linux-amd64.tar.gz
- en: tar -xzf go1.13.3.linux-amd64.tar.gz
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: tar -xzf go1.13.3.linux-amd64.tar.gz
- en: sudo mv go /usr/local
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: sudo mv go /usr/local
- en: mkdir -p $HOME/Projects/Project1
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: mkdir -p $HOME/Projects/Project1
- en: cat << 'EOF' >> ~/.bash_profile
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: cat << 'EOF' >> ~/.bash_profile
- en: export -p GOROOT=/usr/local/go
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: export -p GOROOT=/usr/local/go
- en: export -p GOPATH=$HOME/Projects/Project1
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: export -p GOPATH=$HOME/Projects/Project1
- en: export -p PATH=$GOPATH/bin:$GOROOT/bin:$PATH
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: export -p PATH=$GOPATH/bin:$GOROOT/bin:$PATH
- en: EOF
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: EOF
- en: source ~/.bash_profile
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: source ~/.bash_profile
- en: 'The commands in the preceding list will do the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前面列表中的命令将执行以下操作：
- en: Download Go to your host, uncompress the archive, and move the files to **/usr/local**.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Go 下载到您的主机，解压缩存档，并将文件移动到 **/usr/local**。
- en: Create a Go project folder in your home directory called **Projects/Project1**.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的主目录中创建一个名为 **Projects/Project1** 的 Go 项目文件夹。
- en: Add Go environment variables to**.bash_profile**, which are required to execute
    Go applications.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Go 环境变量添加到 **.bash_profile**，这些变量是执行 Go 应用程序所需的。
- en: Now that you have the prerequisites in place, we can move on to installing KinD.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经具备了先决条件，我们可以继续安装 KinD。
- en: Installing the KinD binary
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 KinD 二进制文件
- en: 'Installing KinD is an easy process; it can be done with a single command. You
    can install KinD by running the included script in this book''s repository, located
    at **/chapter4/install-kind.sh**. Alternatively, you can execute the following
    command:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 KinD 是一个简单的过程；可以通过一个命令完成。您可以通过运行本书存储库中包含的脚本来安装 KinD，该脚本位于 **/chapter4/install-kind.sh**。或者，您可以执行以下命令：
- en: GO111MODULE="on" go get sigs.k8s.io/kind@v0.7.0
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: GO111MODULE="on" go get sigs.k8s.io/kind@v0.7.0
- en: 'Once installed, you can verify that KinD has been installed correctly by typing
    **kind version** into the prompt:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以通过在提示符中键入 **kind version** 来验证 KinD 是否已正确安装：
- en: kind version
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: kind version
- en: 'This will return the installed version:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回已安装的版本：
- en: kind v0.7.0 go1.13.3 linux/amd64
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: kind v0.7.0 go1.13.3 linux/amd64
- en: 'The KinD executable provides every option you will need to maintain a cluster''s
    life cycle. Of course, the KinD executable can create and delete clusters, but
    it also provides the following capabilites:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: KinD 可执行文件提供了您需要维护集群生命周期的每个选项。当然，KinD 可执行文件可以创建和删除集群，但它还提供了以下功能：
- en: The ability to create custom build base and node images
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自定义构建基础和节点映像的能力
- en: Can export **kubeconfig** or log files
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以导出 **kubeconfig** 或日志文件
- en: Can retrieve clusters, nodes, or **kubeconfig** files
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以检索集群、节点或 **kubeconfig** 文件
- en: Can load images into nodes
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将映像加载到节点中
- en: Now that you have installed the KinD utility, you are almost ready to create
    your KinD cluster. Before we execute a few **create cluster** commands, we will
    explain some of the creation options that KinD provides.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经安装了KinD实用程序，您几乎可以准备好创建您的KinD集群了。在执行一些**create cluster**命令之前，我们将解释KinD提供的一些创建选项。
- en: Creating a KinD cluster
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个KinD集群
- en: Now that you have met all the requirements, you can create your first cluster
    using the KinD executable. The KinD utility can create a single-node cluster,
    as well as a complex cluster that's running multiple nodes for the control plane
    with multiple worker nodes. In this section, we will discuss the KinD executable
    options. By the end of the chapter, you will have a two-node cluster running –
    a single control plane node and a single worker node.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经满足了所有的要求，您可以使用KinD可执行文件创建您的第一个集群。KinD实用程序可以创建单节点集群，也可以创建一个运行多个控制平面节点和多个工作节点的复杂集群。在本节中，我们将讨论KinD可执行文件的选项。在本章结束时，您将拥有一个运行的双节点集群
    - 一个单一的控制平面节点和一个单一的工作节点。
- en: Important note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For the exercises in this book, we will install a multi-node cluster. The simple
    cluster configuration is an example and should not be used for our exercises.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的练习中，我们将安装一个多节点集群。简单的集群配置是一个示例，不应该用于我们的练习。
- en: Creating a simple cluster
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个简单的集群
- en: To create a simple cluster that runs the control plane and a worker node in
    a single container, you only need to execute the KinD executable with the **create
    cluster** option.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个简单的集群，在单个容器中运行控制平面和工作节点，您只需要使用**create cluster**选项执行KinD可执行文件。
- en: 'Let''s create a quick single-node cluster to see how quickly KinD creates a
    fast development cluster. On your host, create a cluster using the following command:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个快速的单节点集群，看看KinD如何快速创建一个快速开发集群。在您的主机上，使用以下命令创建一个集群：
- en: kind create cluster
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: kind create cluster
- en: 'This will quickly create a cluster with all the Kubernetes components in a
    single Docker container by using a cluster name of **kind**. It will also assign
    the Docker container a name of **kind-control-plane**. If you want to assign a
    cluster name, rather than the default name, you need to add the **--name <cluster
    name>** option to the **create cluster** command:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这将快速创建一个集群，其中包含一个单一的Docker容器中的所有Kubernetes组件，使用**kind**作为集群名称。它还将为Docker容器分配一个**kind-control-plane**的名称。如果您想要分配一个集群名称，而不是默认名称，您需要在**create
    cluster**命令中添加**--name <cluster name>**选项：
- en: '**Creating cluster "kind" ...**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建集群"kind" ...**'
- en: '**  Ensuring node image (kindest/node:v1.18.2)**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**确保节点镜像（kindest/node:v1.18.2）**'
- en: '**  Preparing nodes**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**准备节点**'
- en: '**  Writing configuration**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**编写配置**'
- en: '**  Starting control-plane**'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**启动控制平面**'
- en: '**  Installing CNI**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装CNI**'
- en: '**  Installing StorageClass**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装StorageClass**'
- en: '**Set kubectl context to "kind-kind"**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**将kubectl上下文设置为"kind-kind"**'
- en: '**You can now use your cluster with:**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**现在您可以使用您的集群：**'
- en: '**kubectl cluster-info --context kind-kind**'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**kubectl cluster-info --context kind-kind**'
- en: The **create** command will create the cluster and modify the kubectl **config**
    file. KinD will add the new cluster to your current kubectl **config** file, and
    it will set the new cluster as the default context.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**create**命令将创建集群并修改kubectl **config**文件。KinD将把新集群添加到当前的kubectl **config**文件中，并将新集群设置为默认上下文。'
- en: 'We can verify that the cluster was created successfully by listing the nodes
    using the kubectl utility:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用kubectl实用程序列出节点来验证集群是否成功创建：
- en: kubectl get nodes
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl get nodes
- en: 'This will return the running nodes, which, for a basic cluster, are single
    nodes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回正在运行的节点，对于基本集群来说，是单个节点：
- en: NAME               STATUS   ROLES    AGE   VERSION
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 名称               状态     角色     年龄     版本
- en: kind-control-plane Ready    master   130m  v1.18.2
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: kind-control-plane Ready    master   130m  v1.18.2
- en: The main point of deploying this single-node cluster was to show you how quickly
    KinD can create a cluster that you can use for testing. For our exercises, we
    want to split up the control plane and worker node so that we can delete this
    cluster using the steps in the next section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 部署此单节点集群的主要目的是向您展示KinD可以多快地创建一个用于测试的集群。对于我们的练习，我们希望将控制平面和工作节点分开，以便我们可以使用下一节中的步骤删除此集群。
- en: Deleting a cluster
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除集群
- en: 'When you have finished testing, you can delete the cluster using the **delete**
    command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 测试完成后，您可以使用**删除**命令删除集群：
- en: kind delete cluster –name <cluster name>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: kind delete cluster –name <cluster name>
- en: The **delete** command will quickly delete the cluster, including any entries
    in your **kubeconfig** file.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**删除**命令将快速删除集群，包括您的**kubeconfig**文件中的任何条目。'
- en: A quick single-node cluster is useful for many use cases, but you may want to
    create a multi-node cluster for various testing scenarios. Creating a more complex
    cluster requires that you create a config file.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 快速单节点集群对于许多用例都很有用，但您可能希望为各种测试场景创建多节点集群。创建更复杂的集群需要您创建一个配置文件。
- en: Creating a cluster config file
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建集群配置文件
- en: 'When creating a multi-node cluster, such as a two-node cluster with custom
    options, we need to create a cluster config file. The config file is a YAML file
    and the format should look familiar. Setting values in this file allows you to
    customize the KinD cluster, including the number of nodes, API options, and more.
    The config file we''ll use to create the cluster for the book is shown here –
    it is included in this book''s repository at **/chapter4/cluster01-kind.yaml**:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 创建多节点集群（例如具有自定义选项的两节点集群）时，我们需要创建一个集群配置文件。配置文件是一个YAML文件，其格式应该看起来很熟悉。在此文件中设置值允许您自定义KinD集群，包括节点数、API选项等。我们将用于创建本书集群的配置文件如下
    - 它包含在本书的存储库中的**/chapter4/cluster01-kind.yaml**中：
- en: 'kind: Cluster'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：集群
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：kind.x-k8s.io/v1alpha4
- en: 'networking:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 网络：
- en: 'apiServerAddress: "0.0.0.0"'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: apiServerAddress："0.0.0.0"
- en: 'disableDefaultCNI: true'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: disableDefaultCNI：true
- en: 'kubeadmConfigPatches:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: kubeadmConfigPatches：
- en: '- |'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '- |'
- en: 'apiVersion: kubeadm.k8s.io/v1beta2'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：kubeadm.k8s.io/v1beta2
- en: 'kind: ClusterConfiguration'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：集群配置
- en: 'metadata:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: config'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：配置
- en: 'networking:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 网络：
- en: 'serviceSubnet: "10.96.0.1/12"'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 服务子网："10.96.0.1/12"
- en: 'podSubnet: "192.168.0.0/16"'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Pod子网："192.168.0.0/16"
- en: 'nodes:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 节点：
- en: '- role: control-plane'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：控制平面'
- en: '- role: worker'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：工作节点'
- en: 'extraPortMappings:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 额外端口映射：
- en: '- containerPort: 80'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '- containerPort：80'
- en: 'hostPort: 80'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 主机端口：80
- en: '- containerPort: 443'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '- containerPort：443'
- en: 'hostPort: 443'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 主机端口：443
- en: 'extraMounts:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 额外挂载：
- en: '- hostPath: /usr/src'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '- hostPath：/usr/src'
- en: 'containerPath: /usr/src'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 容器路径：/usr/src
- en: 'Details about each of the custom options in the file are provided in the following
    table:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中每个自定义选项的详细信息在下表中提供：
- en: '![Table 4.3 – KinD configuration options'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![表4.3 - KinD配置选项'
- en: '](image/Table_4.3.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Table_4.3.jpg)'
- en: Table 4.3 – KinD configuration options
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.3 - KinD配置选项
- en: If you plan to create a cluster that goes beyond a single-node cluster without
    using advanced options, you will need to create a configuration file. Understanding
    the options available to you will allow you to create a Kubernetes cluster that
    has advanced components such as Ingress controllers or multiple nodes to test
    failure and recovery procedures for deployments.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划创建一个超出单节点集群的集群而不使用高级选项，您将需要创建一个配置文件。了解可用的选项将允许您创建具有高级组件（如Ingress控制器或多个节点）的Kubernetes集群，以测试部署的故障和恢复过程。
- en: Now that you know how to create a simple all-in-one container for running a
    cluster and how to create a multi-node cluster using a config file, let's discuss
    a more complex cluster example.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道如何创建一个简单的一体化容器来运行集群，以及如何使用配置文件创建多节点集群，让我们讨论一个更复杂的集群示例。
- en: Multi-node cluster configuration
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多节点集群配置
- en: 'If you only wanted a multi-node cluster without any extra options, you could
    create a simple configuration file that lists the number and node types you want
    in the cluster. The following **config** file will create a cluster with three
    control plane nodes and three worker nodes:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想要一个没有任何额外选项的多节点集群，您可以创建一个简单的配置文件，列出您在集群中想要的节点数量和类型。以下**配置**文件将创建一个包含三个控制平面节点和三个工作节点的集群：
- en: 'kind: Cluster'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：集群
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: kind.x-k8s.io/v1alpha4'
- en: 'nodes:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 节点：
- en: '- role: control-plane'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：控制平面'
- en: '- role: control-plane'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：控制平面'
- en: '- role: control-plane'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：控制平面'
- en: '- role: worker'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：工作节点'
- en: '- role: worker'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：工作节点'
- en: '- role: worker'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：工作节点'
- en: Using multiple control plane servers introduces additional complexity since
    we can only target a single host or IP in our configuration files. To make this
    configuration usable, we need to deploy a load balancer in front of our cluster.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个控制平面服务器会引入额外的复杂性，因为我们在配置文件中只能针对单个主机或IP。为了使这个配置可用，我们需要在集群前部署一个负载均衡器。
- en: 'KinD has considered this, and if you do deploy multiple control plane nodes,
    the installation will create an additional container running a HAProxy load balancer.
    If we look at the running containers from a multi-node config, we will see six
    node containers running and a HAProxy container:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: KinD已经考虑到了这一点，如果您部署多个控制平面节点，安装将创建一个额外的运行HAProxy负载均衡器的容器。如果我们查看多节点配置的运行容器，我们将看到六个节点容器运行和一个HAProxy容器：
- en: '![Table 4.4 – KinD configuration options'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![表4.4 - KinD配置选项'
- en: '](image/Table_4.4.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Table_4.4.jpg)'
- en: Table 4.4 – KinD configuration options
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.4 - KinD配置选项
- en: Remember that, in [*Chapter 3*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*,
    Understanding Docker Networking*, we explained ports and sockets. Since we have
    a single host, each control plane node and the HAProxy container are running on
    unique ports. Each container needs to be exposed to the host so that they can
    receive incoming requests. In this example, the important one to note is the port
    assigned to HAProxy, since that's the target port for the cluster. If you were
    to look at the Kubernetes config file, you would see that it is targeting [https://127.0.0.1:32791](https://127.0.0.1:32791),
    which is the port that's been assigned to the HAProxy container.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在[*第3章*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*，理解Docker网络*中，我们解释了端口和套接字。由于我们只有一个主机，每个控制平面节点和HAProxy容器都在唯一的端口上运行。每个容器都需要暴露给主机，以便它们可以接收传入的请求。在这个例子中，需要注意的是分配给HAProxy的端口，因为那是集群的目标端口。如果您查看Kubernetes配置文件，您会看到它是针对[https://127.0.0.1:32791](https://127.0.0.1:32791)，这是分配给HAProxy容器的端口。
- en: 'When a command is executed using **kubectl**, it is sent to directly to the
    HAProxy server. Using a configuration file that was created by KinD during the
    cluster''s creation, the HAProxy container knows how to route traffic between
    the three control plane nodes:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用**kubectl**执行命令时，它直接发送到HAProxy服务器。使用KinD在集群创建期间创建的配置文件，HAProxy容器知道如何在三个控制平面节点之间路由流量：
- en: generated by kind
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由kind生成
- en: global
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 全局
- en: log /dev/log local0
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 日志 /dev/log local0
- en: log /dev/log local1 notice
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 日志 /dev/log local1 注意
- en: daemon
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 守护程序
- en: defaults
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值
- en: log global
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 日志 全局
- en: mode tcp
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 模式 tcp
- en: option dontlognull
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 选项 dontlognull
- en: 'TODO: tune these'
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'TODO: 调整这些'
- en: timeout connect 5000
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 连接超时 5000
- en: timeout client 50000
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端超时 50000
- en: timeout server 50000
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器超时 50000
- en: frontend control-plane
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 前端 控制平面
- en: bind *:6443
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 绑定 *:6443
- en: default_backend kube-apiservers
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 默认后端 kube-apiservers
- en: backend kube-apiservers
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 后端 kube-apiservers
- en: option httpchk GET /healthz
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 选项 httpchk GET /healthz
- en: 'TODO: we should be verifying (!)'
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'TODO: 我们应该进行验证(!)'
- en: server config2-control-plane 172.17.0.8:6443 check check-ssl verify none
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器 config2-control-plane 172.17.0.8:6443 检查 检查-ssl 验证 无
- en: server config2-control-plane2 172.17.0.6:6443 check check-ssl verify none
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: server config2-control-plane2 172.17.0.6:6443 check check-ssl verify none
- en: server config2-control-plane3 172.17.0.5:6443 check check-ssl verify none
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: server config2-control-plane3 172.17.0.5:6443 check check-ssl verify none
- en: As shown in the preceding configuration file, there is a backend section called
    **kube-apiservers** that contains the three control plane containers. Each entry
    contains the Docker IP address of a control plane node with a port assignment
    of 6443, targeting the API server running in the container. When you request [https://127.0.0.1:32791](https://127.0.0.1:32791),
    that request will hit the HAProxy container. Using the rules in the HAProxy configuration
    file, the request will be routed to one of the three nodes in the list.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的配置文件所示，有一个名为**kube-apiservers**的后端部分，其中包含三个控制平面容器。每个条目都包含一个控制平面节点的Docker
    IP地址，端口分配为6443，指向容器中运行的API服务器。当您请求[https://127.0.0.1:32791](https://127.0.0.1:32791)时，该请求将命中HAProxy容器。使用HAProxy配置文件中的规则，请求将被路由到列表中的三个节点之一。
- en: Since our cluster is now fronted by a load balancer, we have a highly available
    control plane for testing.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的集群现在由负载均衡器前端，我们有一个高可用的控制平面用于测试。
- en: Note
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The included HAProxy image is not configurable. It is only provided to handle
    the control plane and to load balance the API servers. Due to this limitation,
    if you needed to use a load balancer for the worker nodes, you will need to provide
    your own.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 包含的HAProxy镜像不可配置。它只用于处理控制平面和负载均衡API服务器。由于这个限制，如果您需要为工作节点使用负载均衡器，您将需要提供自己的负载均衡器。
- en: An example use case for this would be if you wanted to use an Ingress controller
    on multiple worker nodes. You would need a load balancer in front of the worker
    nodes to accept incoming 80 and 443 requests that would forward the traffic to
    each node running NGINX. At the end of this chapter, we have provided an example
    configuration that includes a custom HAProxy configuration for load balancing
    traffic to the worker nodes.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用案例是，如果您想要在多个工作节点上使用Ingress控制器。您需要在工作节点前面放置一个负载均衡器，以接受传入的80和443请求，并将流量转发到运行NGINX的每个节点。在本章末尾，我们提供了一个示例配置，其中包括用于将流量负载均衡到工作节点的自定义HAProxy配置。
- en: Customizing the control plane and Kubelet options
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义控制平面和Kubelet选项
- en: 'You may want to go further than this to test features such as OIDC integration
    or Kubernetes feature gates. KinD uses the same configuration that you would use
    for a kubeadm installation. As an example, if you wanted to integrate a cluster
    with an OIDC provider, you could add the required options to the configuration
    patch section:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望进一步测试OIDC集成或Kubernetes功能门。KinD使用与kubeadm安装相同的配置。例如，如果您想要将集群与OIDC提供程序集成，可以将所需的选项添加到配置补丁部分：
- en: 'kind: Cluster'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: Cluster'
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: kind.x-k8s.io/v1alpha4'
- en: 'kubeadmConfigPatches:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 'kubeadmConfigPatches:'
- en: '- |'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '- |'
- en: 'kind: ClusterConfiguration'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: ClusterConfiguration'
- en: 'metadata:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 'metadata:'
- en: 'name: config'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 'name: config'
- en: 'apiServer:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiServer:'
- en: 'extraArgs:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 'extraArgs:'
- en: 'oidc-issuer-url: "https://oidc.testdomain.com/auth/idp/k8sIdp"'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 'oidc-issuer-url: "https://oidc.testdomain.com/auth/idp/k8sIdp"'
- en: 'oidc-client-id: "kubernetes"'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 'oidc-client-id: "kubernetes"'
- en: 'oidc-username-claim: sub'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 'oidc-username-claim: sub'
- en: 'oidc-client-id: kubernetes'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 'oidc-client-id: kubernetes'
- en: 'oidc-ca-file: /etc/oidc/ca.crt'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 'oidc-ca-file: /etc/oidc/ca.crt'
- en: 'nodes:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 'nodes:'
- en: '- role: control-plane'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '- role: control-plane'
- en: '- role: control-plane'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '- role: control-plane'
- en: '- role: control-plane'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '- role: control-plane'
- en: '- role: worker'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '- role: worker'
- en: '- role: worker'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '- role: worker'
- en: '- rol: worker'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '- rol: worker'
- en: For a list of available configuration options, take a look at *Customizing control
    plane configuration with kubeadm* on the Kubernetes site at [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 有关可用配置选项的列表，请查看 Kubernetes 网站上的*使用 kubeadm 自定义控制平面配置* [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/)。
- en: Now that you have created the cluster file, you can create your KinD cluster.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经创建了集群文件，你可以创建你的 KinD 集群。
- en: Creating a custom KinD cluster
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建自定义的 KinD 集群
- en: Finally! Now that you are familiar with KinD, we can move forward and create
    our cluster.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 终于了！现在你已经熟悉了 KinD，我们可以继续并创建我们的集群了。
- en: We need to create a controlled, known environment, so we will give the cluster
    a name and provide the config file that we discussed in the previous section.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个受控的、已知的环境，所以我们将为集群命名，并提供我们在上一节讨论过的配置文件。
- en: Make sure that you are in your cloned repository under the **chapter4** directory.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你在**chapter4**目录下的克隆存储库中。
- en: 'To create a KinD cluster with our required options, we need to run the KinD
    installer with the following options:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用我们需要的选项创建一个 KinD 集群，我们需要使用以下选项运行 KinD 安装程序：
- en: kind create cluster --name cluster01 --config c
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: kind create cluster --name cluster01 --config c
- en: luster01-kind.yamlThe option **--name** will set the name of the cluster to
    cluster01 and the **--config** tells the installer to use the config file **cluster01-kind.yaml**.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: luster01-kind.yaml选项**--name**将集群名称设置为 cluster01，而**--config**告诉安装程序使用配置文件**cluster01-kind.yaml**。
- en: 'When you execute the installer on your host, KinD will start the installation
    and tell you each step that is being performed. The entire cluster creation process
    should take less than 2 minutes:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在主机上执行安装程序时，KinD 将开始安装并告诉你正在执行的每一步。整个集群创建过程应该不到 2 分钟：
- en: '![Figure 4.6 – KinD cluster creation output'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – KinD 集群创建输出'
- en: '](image/Fig_4.6_B15514.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.6_B15514.jpg)'
- en: Figure 4.6 – KinD cluster creation output
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – KinD 集群创建输出
- en: The final step in the deployment creates or edits an existing Kubernetes config
    file. In either case, the installer creates a new context with the name **kind-<cluster
    name>** and sets it as the default context.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的最后一步是创建或编辑现有的 Kubernetes 配置文件。无论哪种情况，安装程序都会创建一个名为**kind-<cluster name>**的新上下文，并将其设置为默认上下文。
- en: While it may appear that the cluster installation procedure has completed its
    tasks, the cluster **is not** ready yet. Some of the tasks take a few minutes
    to fully initialize and since we disabled the default CNI to use Calico, we still
    need to deploy Calico to provide cluster networking.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来集群安装过程已经完成了它的任务，但是集群**还没有**准备好。一些任务需要几分钟才能完全初始化，而且由于我们禁用了默认的 CNI 来使用 Calico，我们仍然需要部署
    Calico 来提供集群网络。
- en: Installing Calico
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Calico
- en: To provide networking to the pods in the cluster, we need to install a Container
    Network Interface, or CNI. We have elected to install Calico as our CNI and since
    KinD only includes the Kindnet CNI, we need to install Calico manually.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为集群中的 pod 提供网络，我们需要安装一个容器网络接口，或者 CNI。我们选择安装 Calico 作为我们的 CNI，而由于 KinD 只包括
    Kindnet CNI，我们需要手动安装 Calico。
- en: 'If you were to pause after the creation step and look at the cluster, you would
    notice that some pods are in a pending state:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在创建步骤之后暂停并查看集群，你会注意到一些 pod 处于等待状态：
- en: coredns-6955765f44-86l77  0/1  Pending  0  10m
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: coredns-6955765f44-86l77 0/1 Pending 0 10m
- en: coredns-6955765f44-bznjl  0/1  Pending  0  10m
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: coredns-6955765f44-bznjl 0/1 Pending 0 10m
- en: local-path-provisioner-7  0/1  Pending  0  11m 745554f7f-jgmxv
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: local-path-provisioner-7 0/1 Pending 0 11m 745554f7f-jgmxv
- en: The pods listed here require a working CNI to start. This puts the pods into
    a pending state, where they are waiting for a network. Since we did not deploy
    the default CNI, our cluster does not have networking support. To get these pods
    from pending to running, we need to install a CNI – and for our cluster, that
    will be Calico.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出的pods需要一个可用的CNI才能启动。这将把pods置于等待网络的挂起状态。由于我们没有部署默认的CNI，我们的集群没有网络支持。为了将这些pods从挂起状态变为运行状态，我们需要安装一个CNI
    - 对于我们的集群来说，就是Calico。
- en: 'To install Calico, we will use the standard Calico deployment, which only requires
    a single manifest. To start deploying Calico, use the following command:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装Calico，我们将使用标准的Calico部署，只需要一个清单。要开始部署Calico，请使用以下命令：
- en: kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml
- en: 'This will pull the manifests from the internet and apply them to the cluster.
    As it deploys, you will see that that a number of Kubernetes objects are created:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从互联网上拉取清单并将其应用到集群。在部署过程中，您会看到创建了许多Kubernetes对象：
- en: '![Figure 4.7 – Calico installation output'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.7 - Calico安装输出'
- en: '](image/Fig_4.7_B15514.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.7_B15514.jpg)'
- en: Figure 4.7 – Calico installation output
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 - Calico安装输出
- en: 'The installation process will take about a minute and you can check on its
    status using **kubectl get pods -n kube-system**. You will see that three Calico
    pods were created. Two are **calico-node** pods, while the other is the **calico-kube-controller**
    pod:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程大约需要一分钟，您可以使用**kubectl get pods -n kube-system**来检查其状态。您会看到创建了三个Calico pods。其中两个是**calico-node**
    pods，另一个是**calico-kube-controller** pod：
- en: NAME                    READY STATUS RESTARTS AGE
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 名称                    准备状态 重启次数 年龄
- en: calico-kube-controllers  1/1  Running    0    64s -5b644bc49c-nm5wn
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: calico-kube-controllers 1/1 Running 0 64s -5b644bc49c-nm5wn
- en: calico-node-4dqnv        1/1  Running    0    64s
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: calico-node-4dqnv 1/1 Running 0 64s
- en: calico-node-vwbpf        1/1  Running    0    64s
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: calico-node-vwbpf 1/1 Running 0 64s
- en: 'If you check the two CoreDNS pods in the **kube-system** namespace again, you
    will notice that they have changed from the pending state, from before we installed
    Calico, to being in a running state:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您再次检查**kube-system**命名空间中的两个CoreDNS pods，您会注意到它们已经从之前安装Calico之前的挂起状态变为运行状态：
- en: coredns-6955765f44-86l77   1/1  Running   0  18m
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: coredns-6955765f44-86l77 1/1 Running 0 18m
- en: coredns-6955765f44-bznjl   1/1  Running   0  18m
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: coredns-6955765f44-bznjl 1/1 Running 0 18m
- en: Now that the cluster has a working CNI installed, any pods that were dependent
    on networking will be in a running state.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在集群安装了一个可用的CNI，任何依赖网络的pods都将处于运行状态。
- en: Installing an Ingress controller
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Ingress控制器
- en: We have a chapter dedicated to Ingress to explain all the technical details.
    Since we are deploying a cluster and we require Ingress for future chapters, we
    need to deploy an Ingress controller to show a complete cluster build. All these
    details will be explained in more detail in [*Chapter 6*](B15514_06_Final_ASB_ePub.xhtml#_idTextAnchor174),
    *Services, Load Balancing, and External DNS*.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个专门的章节来解释Ingress的所有技术细节。由于我们正在部署一个集群，并且未来的章节需要Ingress，我们需要部署一个Ingress控制器来展示一个完整的集群构建。所有这些细节将在[*第6章*](B15514_06_Final_ASB_ePub.xhtml#_idTextAnchor174)中更详细地解释，*服务、负载均衡和外部DNS*。
- en: 'Installing the NGINX Ingress controller requires only two manifests, which
    we will pull from the internet to make the installation easy. To install the controller,
    execute the following two lines:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 安装NGINX Ingress控制器只需要两个清单，我们将从互联网上拉取以使安装更容易。要安装控制器，请执行以下两行：
- en: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.28.0/deploy/static/mandatory.yaml
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.28.0/deploy/static/mandatory.yaml
- en: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.27.0/deploy/static/provider/baremetal/service-nodeport.yaml
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.27.0/deploy/static/provider/baremetal/service-nodeport.yaml
- en: 'The deployment will create a few Kubernetes objects that are required for Ingress
    in a namespace called **ingress-nginx**:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 部署将创建一些Ingress所需的Kubernetes对象，这些对象位于名为**ingress-nginx**的命名空间中：
- en: '![Figure 4.8 – NGINX installation output'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8 – NGINX安装输出'
- en: '](image/Fig_4.8_B15514.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.8_B15514.jpg)'
- en: Figure 4.8 – NGINX installation output
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – NGINX安装输出
- en: 'We have one more step so that we have a fully functioning Ingress controller:
    we need to expose ports 80 and 443 to the running pod. This can be done by patching
    the deployment. Here, we have included the patch to patch the deployment:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一步，以便我们有一个完全运行的Ingress控制器：我们需要将端口80和443暴露给运行的pod。这可以通过对部署进行修补来完成。在这里，我们已经包含了修补部署的修补程序：
- en: kubectl patch deployments -n ingress-nginx nginx-ingress-controller -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx-ingress-controller","ports":[{"containerPort":80,"hostPort":80},{"containerPort":443,"hostPort":443}]}]}}}}'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl patch deployments -n ingress-nginx nginx-ingress-controller -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx-ingress-controller","ports":[{"containerPort":80,"hostPort":80},{"containerPort":443,"hostPort":443}]}]}}}}'
- en: Congratulations! You now have a fully functioning, two-node Kubernetes cluster
    running Calico with an Ingress controller.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您现在拥有一个完全运行的、运行Calico的双节点Kubernetes集群，带有一个Ingress控制器。
- en: Reviewing your KinD cluster
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查您的KinD集群
- en: With a Kubernetes cluster now available, we have the ability to look at Kubernetes
    objects first-hand. This will help you understand the previous chapter, where
    we covered many of the base objects included in a Kubernetes cluster. In particular,
    we will discuss the storage objects that are included with your KinD cluster.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一个Kubernetes集群，我们有能力直接查看Kubernetes对象。这将帮助您理解上一章，我们在其中涵盖了Kubernetes集群中包含的许多基本对象。特别是，我们将讨论包含在您的KinD集群中的存储对象。
- en: KinD storage objects
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KinD存储对象
- en: Remember that KinD includes Rancher's auto-provisioner to provide automated
    persistent disk management for the cluster. In [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150),
    *Kubernetes Bootcamp*, we went over the storage-related objects, and now that
    we have a cluster with a storage system configured, we can explain them in greater
    detail.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，KinD包括Rancher的自动配置程序，为集群提供自动持久磁盘管理。在第5章《Kubernetes Bootcamp》中，我们讨论了与存储相关的对象，现在我们有了一个配置了存储系统的集群，我们可以更详细地解释它们。
- en: 'There is one object that the auto-provisioner does not require since it uses
    a base Kubernetes feature: it does not require a **CSIdriver**. Since the ability
    to use local host paths as PVCs is part of Kubernetes, we will not see any **CSIdriver**
    objects in our KinD cluster.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 由于能够使用本地主机路径作为PVC的功能是Kubernetes的一部分，因此我们在KinD集群中将不会看到任何**CSIdriver**对象。自动配置程序不需要的一个对象是：它不需要**CSIdriver**。
- en: 'The first objects in our KinD cluster we will discuss are our **CSInodes**.
    In the bootcamp, we mentioned that this object was created to decouple any CSI
    objects from the base node object. Any node that can run a workload will have
    a **CSInode** object. On our KinD clusters, both nodes have a **CSInode** object.
    You can verify this by executing **kubectl get csinodes**:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的KinD集群中的第一个对象是我们的**CSInodes**。在bootcamp中，我们提到创建此对象是为了将任何CSI对象与基本节点对象解耦。可以运行工作负载的任何节点都将具有**CSInode**对象。在我们的KinD集群中，两个节点都有**CSInode**对象。您可以通过执行**kubectl
    get csinodes**来验证这一点：
- en: NAME                      CREATED AT
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 名称                      创建于
- en: cluster01-control-plane   2020-03-27T15:18:19Z
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: cluster01-control-plane   2020-03-27T15:18:19Z
- en: cluster01-worker          2020-03-27T15:19:01Z
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: cluster01-worker          2020-03-27T15:19:01Z
- en: 'If we were to describe one of the nodes using **kubectl describe csinodes <node
    name>**, you would see the details of the object:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用**kubectl describe csinodes <node name>**来描述其中一个节点，您将看到对象的详细信息：
- en: '![Figure 4.9 – CSInode describe'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9 - CSInode描述'
- en: '](image/Fig_4.9_B15514.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.9_B15514.jpg)'
- en: Figure 4.9 – CSInode describe
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 - CSInode描述
- en: The main thing to point out is the **Spec** section of the output. This lists
    the details of any drivers that may be installed to support backend storage systems.
    Since we do not have a backend storage system, we do not require an additional
    driver on our cluster.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 要指出的主要事项是输出的**Spec**部分。这列出了可能安装的任何驱动程序的详细信息，以支持后端存储系统。由于我们没有后端存储系统，我们在集群上不需要额外的驱动程序。
- en: 'To show an example of what a node would list, here is the output from a cluster
    that has two drivers installed, supporting two different vendor storage solutions:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示一个节点会列出什么样的示例，这里是一个安装了两个驱动程序，支持两种不同供应商存储解决方案的集群的输出：
- en: '![Figure 4.10 – Multiple driver example'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.10 - 多驱动器示例'
- en: '](image/Fig_4.10_B15514.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.10_B15514.jpg)'
- en: Figure 4.10 – Multiple driver example
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 - 多驱动器示例
- en: If you look at the **spec.drivers** section of this node, you will see two different
    name sections. The first shows that we have a driver installed to support NetApp
    SolidFire, while the second is a driver that supports Reduxio's storage solution.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看这个节点的**spec.drivers**部分，您将看到两个不同的名称部分。第一个显示我们安装了一个驱动程序来支持NetApp SolidFire，而第二个是支持Reduxio的存储解决方案的驱动程序。
- en: Storage drivers
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储驱动程序
- en: As we already mentioned, your KinD cluster does not have any additional storage
    drivers installed. If you execute **kubectl get csidrivers**, the API will not
    list any resources.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，您的KinD集群没有安装任何额外的存储驱动程序。如果您执行**kubectl get csidrivers**，API将不会列出任何资源。
- en: KinD storage classes
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KinD存储类
- en: To attach to any cluster-provided storage, the cluster requires a **StorageClass**
    object. Rancher's provider creates a default storage class called standard. It
    also sets the class as the default **StorageClass**, so you do not need to provide
    a **StorageClass** name in your PVC requests. If a default **StorageClass** is
    not set, every PVC request will require a **StorageClass** name in the request.
    If a default class is not enabled and a PVC request fails to set a **StorageClass**
    name, the PVC allocation will fail since the API server won't be able to link
    the request to a **StorageClass**.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到任何集群提供的存储，集群需要一个**StorageClass**对象。Rancher的提供者创建了一个名为standard的默认存储类。它还将该类设置为默认的**StorageClass**，因此您不需要在PVC请求中提供**StorageClass**名称。如果没有设置默认的**StorageClass**，每个PVC请求都将需要在请求中提供**StorageClass**名称。如果未启用默认类并且PVC请求未能设置**StorageClass**名称，PVC分配将失败，因为API服务器将无法将请求链接到**StorageClass**。
- en: Note
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: On a production cluster, it is considered a good practice to omit assigning
    a default **StorageClass**. Depending on your users, you may have deployments
    that forget to set a class, and the default storage system may not fit the deployment
    needs. This issue may not occur until it becomes a production issue, and that
    may impact business revenue or the company's reputation. If you don't assign a
    default class, the developer will have a failed PVC request, and the issue will
    be discovered before any harm comes to the business.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产集群中，通常认为省略分配默认的**StorageClass**是一个好的做法。根据您的用户，您可能会有一些部署忘记设置一个类，而默认的存储系统可能不适合部署的需求。这个问题可能直到它变成一个生产问题才会出现，并且可能会影响业务收入或公司的声誉。如果您不分配一个默认的类，开发人员将会有一个失败的PVC请求，并且在对业务造成任何伤害之前会发现这个问题。
- en: 'To list the storage classes on the cluster, execute **kubectl get storageclasses**,
    or use the shortened version by using **sc** instead of **storageclasses**:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出集群上的存储类，请执行**kubectl get storageclasses**，或者使用**sc**而不是**storageclasses**的缩写版本：
- en: '![Figure 4.11 – Default storage class'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11 - 默认存储类'
- en: '](image/Fig_4.11_B15514.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.11_B15514.jpg)'
- en: Figure 4.11 – Default storage class
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 - 默认存储类
- en: Next, let's learn how to use the provisioner.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们学习如何使用配置程序。
- en: Using KinD's storage provisioner
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用KinD的存储配置程序
- en: Using the included provisioner is very simple. Since it can auto-provision the
    storage and is set as the default class, any PVC requests that are coming in are
    seen by the provisioning pod, which then creates **PersistentVolume** and **PersistentVolumeClaim**.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含的配置程序非常简单。由于它可以自动配置存储并设置为默认类，任何进来的PVC请求都会被配置pod看到，然后创建**PersistentVolume**和**PersistentVolumeClaim**。
- en: 'To show this process, let''s go through the necessary steps. The following
    is the output of running **get pv** and **get pvc** on a base KinD cluster:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这个过程，让我们通过必要的步骤。以下是在基本KinD集群上运行**get pv**和**get pvc**的输出：
- en: '![Figure 4.12 – PV and PVC example'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.12 - PV和PVC示例'
- en: '](image/Fig_4.12_B15514.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.12_B15514.jpg)'
- en: Figure 4.12 – PV and PVC example
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 - PV和PVC示例
- en: Remember that **PersistentVolume** is not a namespaced object, so we don't need
    to add a namespace option to the command. PVCs are namespaced objects, so I told
    Kubernetes to show me the PVCs that are available in all the namespaces. Since
    this is a new cluster and none of the default workloads require persistent disk,
    there are no PV or PVC objects.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，**PersistentVolume**不是一个命名空间对象，所以我们不需要在命令中添加命名空间选项。PVC是命名空间对象，所以我告诉Kubernetes向我展示所有命名空间中可用的PVC。由于这是一个新的集群，没有默认的工作负载需要持久磁盘，所以没有PV或PVC对象。
- en: 'Without an auto-provisioner, we would need to create a PV before a PVC could
    claim the volume. Since we have the Rancher provisioner running in our cluster,
    we can test the creation process by deploying a pod with a PVC request like the
    one listed here:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有自动配置程序，我们需要在PVC声明卷之前创建PV。由于我们在集群中运行了Rancher配置程序，我们可以通过部署一个带有PVC请求的pod来测试创建过程，就像这里列出的一样：
- en: 'kind: PersistentVolumeClaim'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：PersistentVolumeClaim
- en: 'apiVersion: v1'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：v1
- en: 'metadata:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: test-claim'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：test-claim
- en: 'spec:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 规格：
- en: 'accessModes:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 访问模式：
- en: '- ReadWriteOnce'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '- ReadWriteOnce'
- en: 'resources:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 资源：
- en: 'requests:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 请求：
- en: 'storage: 1Mi'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 存储：1Mi
- en: '---'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '---'
- en: 'kind: Pod'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 种类：Pod
- en: 'apiVersion: v1'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: apiVersion：v1
- en: 'metadata:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: test-pvc-claim'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：test-pvc-claim
- en: 'spec:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 规格：
- en: 'containers:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 容器：
- en: '- name: test-pod'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：测试pod'
- en: 'image: busybox'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图像：busybox
- en: 'command:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 命令：
- en: '- "/bin/sh"'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '- “/bin/sh”'
- en: 'args:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '- "-c"'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '- “-c”'
- en: '- "touch /mnt/test && exit 0 || exit 1"'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '- “touch /mnt/test && exit 0 || exit 1”'
- en: 'volumeMounts:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: volumeMounts：
- en: '- name: test-pvc'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：测试-pvc'
- en: 'mountPath: "/mnt"'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: mountPath：“/mnt”
- en: 'restartPolicy: "Never"'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 重启策略：“永不”
- en: 'volumes:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 卷：
- en: '- name: test-pvc'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '- 名称：测试-pvc'
- en: 'persistentVolumeClaim:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: persistentVolumeClaim：
- en: 'claimName: test-claim'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: claimName：test-claim
- en: This PVC request will be named **test-claim** in the default namespace and it
    is requesting a 1 MB volume. We do need to include the **StorageClass** option
    since KinD has set a default **StorageClass** for the cluster.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这个PVC请求将在默认命名空间中命名为**test-claim**，并且请求一个1MB的卷。由于KinD为集群设置了默认的**StorageClass**，我们确实需要包括**StorageClass**选项。
- en: To create the PVC, we can execute a **create** command using kubectl, such as
    **kubectl create -f pvctest.yaml** – Kubernetes will return, stating that the
    PVC has been created, but it's important to note that this does not mean that
    the PVC is fully working. The PVC object has been created, but if any dependencies
    are missing in the PVC request, it will still create the object, though it will
    fail to fully create the PVC request.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建PVC，我们可以使用kubectl执行**create**命令，例如**kubectl create -f pvctest.yaml** - Kubernetes将返回，指出PVC已创建，但重要的是要注意，这并不意味着PVC完全工作。PVC对象已创建，但如果PVC请求中缺少任何依赖项，它仍将创建对象，尽管无法完全创建PVC请求。
- en: 'After creating a PVC, you can check the real status using one of two options.
    The first is a simple **get** command; that is, **kubectl get pvc**. Since my
    request is in the default namespace, I don''t need to include a namespace value
    in the **get** command (note that we had to shorten the volume''s name so that
    it fits the page):'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 创建PVC后，可以使用两种选项之一检查实际状态。第一个是一个简单的**get**命令；也就是**kubectl get pvc**。由于我的请求在默认命名空间中，所以在**get**命令中不需要包含命名空间值（请注意，我们必须缩短卷的名称以适应页面）：
- en: NAME         STATUS          VOLUME                                     CAPACITY   ACCESS
    MODES   STORAGECLASS   AGE
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 状态 容量 访问模式 存储类 年龄
- en: test-claim   Bound    pvc-9c56cf65-d661-49e3-         1Mi            RWO          standard     2s
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 测试声明 Bound pvc-9c56cf65-d661-49e3- 1Mi RWO 标准 2s
- en: 'We know that we created a PVC request in the manifest, but we did not create
    a PV request. If we look at the PVs now, we will see that a single PV was created
    from our PVC request. Again, we shortened the PV name in order to fit the output
    on a single line:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们在清单中创建了PVC请求，但我们没有创建PV请求。如果我们现在查看PV，我们将看到从我们的PVC请求创建了一个PV。同样，我们缩短了PV名称以适应单行输出：
- en: NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 容量 访问模式 回收策略 状态 声明
- en: pvc-9c56cf65-d661-49e3-   1Mi          RWO           Delete       Bound    default/test-claim
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: pvc-9c56cf65-d661-49e3- 1Mi RWO 删除 已绑定 默认/测试声明
- en: This completes the KinD storage section.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了KinD存储部分。
- en: With so many workloads requiring persistent disks, it is very important to understand
    how Kubernetes workloads integrate with storage systems. In this section, you
    learned how KinD adds the auto-provisioner to the cluster. We will reinforce our
    knowledge of these Kubernetes storage objects in the next chapter, [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多工作负载需要持久磁盘，了解Kubernetes工作负载如何与存储系统集成非常重要。在本节中，您了解了KinD如何向集群添加自动配置程序。我们将在下一章[*第5章*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*，Kubernetes
    Bootcamp*中加强我们对这些Kubernetes存储对象的知识。
- en: Adding a custom load balancer for Ingress
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Ingress添加自定义负载均衡器
- en: Note
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This section is a complex topic that covers adding a custom HAProxy container
    that you can use to load balance worker nodes in a KinD cluster. *You should not
    deploy these steps on the KinD cluster that we will use for the remaining chapters.*
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是一个复杂的主题，涵盖了添加一个自定义的HAProxy容器，您可以使用它来负载均衡KinD集群中的工作节点。*您不应该在我们将用于剩余章节的KinD集群上部署这些步骤。*
- en: We added this section for anybody that may want to know more about how to load
    balance between multiple worker nodes.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为任何想要了解如何在多个工作节点之间进行负载平衡的人添加了这一部分。
- en: KinD does not include a load balancer for worker nodes. The included HAProxy
    container only creates a configuration file for the API server; the team does
    not officially support any modifications to the default image or configuration.
    Since you will interact with load balancers in your everyday work, we wanted to
    add a section on how to configure your own HAProxy container in order to load
    balance between three KinD nodes.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: KinD不包括用于工作节点的负载均衡器。包含的HAProxy容器只为API服务器创建一个配置文件；团队不正式支持对默认镜像或配置的任何修改。由于您在日常工作中将与负载均衡器进行交互，我们希望添加一个关于如何配置自己的HAProxy容器以在三个KinD节点之间进行负载均衡的部分。
- en: First, we will not use this configuration for any of chapters in this book.
    We want to make the exercises available to everyone, so to limit the required
    resources, we will always use the two-node cluster that we created earlier in
    this chapter. If you want to test KinD nodes with a load balancer, we suggest
    using a different Docker host or waiting until you have finished this book and
    deleting your KinD cluster.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们不会在本书的任何章节中使用这个配置。我们希望让练习对每个人都可用，所以为了限制所需的资源，我们将始终使用在本章前面创建的双节点集群。如果您想测试带有负载均衡器的KinD节点，我们建议使用不同的Docker主机，或者等到您完成本书并删除KinD集群后再进行测试。
- en: Installation prerequisites
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装先决条件
- en: 'We assume that you have a KinD cluster based on the following configuration:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您有一个基于以下配置的KinD集群：
- en: Any number of control plane nodes
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意数量的控制平面节点
- en: Three worker nodes
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个工作节点
- en: Cluster name is **cluster01**
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群名称为**cluster01**
- en: A working version of **Kindnet or Calico** (**CNI**)
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可用的**Kindnet或Calico**（**CNI**）
- en: NGINX Ingress controller installed – patched to listen on ports 80 and 443 on
    the host
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装NGINX Ingress控制器 - 补丁以在主机上监听端口80和443
- en: Creating the KinD cluster configuration
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建KinD集群配置
- en: Since you will use an HAProxy container exposed on ports 80 and 443 on your
    Docker host, you do not need to expose any ports in your cluster **config** file.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您将在Docker主机上使用暴露在端口80和443上的HAProxy容器，因此您不需要在集群的**config**文件中暴露任何端口。
- en: 'To make a test deployment easier, you can use the example cluster config shown
    here, which will create a simple six-node cluster with Kindnet disabled:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使测试部署更容易，您可以使用这里显示的示例集群配置，它将创建一个简单的六节点集群，并禁用Kindnet：
- en: 'kind: Cluster'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: Cluster'
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: kind.x-k8s.io/v1alpha4'
- en: 'networking:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 'networking:'
- en: 'apiServerAddress: "0.0.0.0"'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiServerAddress: "0.0.0.0"'
- en: 'disableDefaultCNI: true'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 'disableDefaultCNI: true'
- en: 'kubeadmConfigPatches:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 'kubeadmConfigPatches:'
- en: '- |'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '- |'
- en: 'apiVersion: kubeadm.k8s.io/v1beta2'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 'apiVersion: kubeadm.k8s.io/v1beta2'
- en: 'kind: ClusterConfiguration'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 'kind: ClusterConfiguration'
- en: 'metadata:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据：
- en: 'name: config'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：config
- en: 'networking:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 网络：
- en: 'serviceSubnet: "10.96.0.1/12"'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 'serviceSubnet: "10.96.0.1/12"'
- en: 'podSubnet: "192.168.0.0/16"'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 'podSubnet: "192.168.0.0/16"'
- en: 'nodes:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 'nodes:'
- en: '- role: control-plane'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：控制平面'
- en: '- role: control-plane'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：控制平面'
- en: '- role: control-plane'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：控制平面'
- en: '- role: worker'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：工作节点'
- en: '- role: worker'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：工作节点'
- en: '- role: worker'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '- 角色：工作节点'
- en: You need to install Calico using the same manifest that we used earlier in this
    chapter. After installing Calico, you need to install the NGINX Ingress controller
    using the steps provided earlier in this chapter.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要使用本章前面使用的相同清单安装Calico。安装Calico后，您需要使用本章前面提供的步骤安装NGINX Ingress控制器。
- en: Once you've deployed Calico and NGINX, you should have a working base cluster.
    Now, you can move on to deploying a custom HAProxy container.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您部署了Calico和NGINX，您应该有一个可用的基本集群。现在，您可以继续部署自定义的HAProxy容器。
- en: Deploying a custom HAProxy container
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署自定义的HAProxy容器
- en: HAProxy offers a container on Docker Hub that is easy to deploy, requiring only
    a config file to start the container.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy在Docker Hub上提供了一个容器，很容易部署，只需要一个配置文件就可以启动容器。
- en: To create the configuration file, you will need you to know the IP addresses
    of each worker node in the cluster. In this book's GitHub repository, we have
    included a script file that will find this information for you, create the config
    file, and start the HAProxy container. It is located under the **HAProxy** directory
    and it's called **HAProxy-ingress.sh**.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建配置文件，您需要知道集群中每个工作节点的IP地址。在本书的GitHub存储库中，我们已经包含了一个脚本文件，可以为您找到这些信息，创建配置文件，并启动HAProxy容器。它位于**HAProxy**目录下，名为**HAProxy-ingress.sh**。
- en: 'To help you better understand this script, we will break out sections of the
    script and detail what each section is executing. Firstly, the following code
    block is getting the IP addresses of each worker node in our cluster and saving
    the results in a variable. We will need this information for the backend server
    list:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您更好地理解这个脚本，我们将分解脚本的各个部分，并详细说明每个部分执行的内容。首先，以下代码块正在获取我们集群中每个工作节点的IP地址，并将结果保存在一个变量中。我们将需要这些信息用于后端服务器列表：
- en: '#!/bin/bash'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '#!/bin/bash'
- en: worker1=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker)
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: worker1=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker)
- en: worker2=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker2)
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: worker2=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker2)
- en: worker3=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker3)
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: worker3=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker3)
- en: 'Next, since we will use a bind mount when we start the container, we need to
    have the configuration file in a known location. We elected to store it in the
    current user''s home folder, under a directory called **HAProxy**:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由于我们在启动容器时将使用绑定挂载，我们需要在已知位置拥有配置文件。我们选择将其存储在当前用户的主目录下，一个名为**HAProxy**的目录中：
- en: Create an HAProxy directory in the current users home folder
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在当前用户的主目录下创建一个HAProxy目录
- en: mkdir ~/HAProxy
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: mkdir ~/HAProxy
- en: 'Next, the following part of the script will create the **HAProxy** directory:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，脚本的以下部分将创建**HAProxy**目录：
- en: Create the HAProxy.cfg file for the worker nodes
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为工作节点创建HAProxy.cfg文件
- en: tee ~/HAProxy/HAProxy.cfg <<EOF
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: tee ~/HAProxy/HAProxy.cfg <<EOF
- en: 'The **global** section of the configuration sets process-wide security and
    performance settings:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 配置的**全局**部分设置了全局性的安全和性能设置。
- en: global
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 全局
- en: log /dev/log local0
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: log /dev/log local0
- en: log /dev/log local1 notice
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: log /dev/log local1 notice
- en: daemon
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 守护进程
- en: 'The **defaults** section is used to configure values that will apply to all
    frontend and backend sections in the configuration value:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '**defaults**部分用于配置将应用于配置值中所有前端和后端部分的值：'
- en: defaults
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值
- en: log global
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 全局日志
- en: mode tcp
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: tcp模式
- en: timeout connect 5000
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 连接超时5000
- en: timeout client 50000
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端超时50000
- en: timeout server 50000
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器超时50000
- en: frontend workers_https
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 前端workers_https
- en: bind *:443
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 绑定 *:443
- en: mode tcp
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: tcp模式
- en: use_backend ingress_https
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: use_backend ingress_https
- en: backend ingress_https
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 后端ingress_https
- en: option httpchk GET /healthz
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 选项httpchk GET /healthz
- en: mode tcp
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: tcp模式
- en: server worker $worker1:443 check port 80
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器worker $worker1:443 检查端口80
- en: server worker2 $worker2:443 check port 80
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器worker2 $worker2:443 检查端口80
- en: server worker3 $worker3:443 check port 80
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器worker3 $worker3:443 检查端口80
- en: This tells HAProxy to create a frontend called **workers_https** and the IP
    addresses and ports to bind for incoming requests, to use TCP mode, and to use
    a backend named **ingress_https**.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉HAProxy创建一个名为**workers_https**的前端，以及绑定传入请求的IP地址和端口，使用TCP模式，并使用名为**ingress_https**的后端。
- en: 'The **ingress_https** backend includes the three worker nodes that are using
    port 443 as a destination. The check port is a health check that will test port
    80\. If a server replies on port 80, it will be added as a target for requests.
    While this is an HTTPS port 443 rule, we are only using port 80 to check for a
    network reply from the NGINX pod:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '**ingress_https**后端包括了三个使用端口443作为目的地的工作节点。检查端口是一个健康检查，将测试端口80。如果服务器在端口80上回复，它将被添加为请求的目标。虽然这是一个HTTPS端口443规则，但我们只使用端口80来检查NGINX
    pod的网络回复：'
- en: frontend workers_http
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 前端workers_http
- en: bind *:80
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 绑定*:80
- en: use_backend ingress_http
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: use_backend ingress_http
- en: backend ingress_http
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 后端ingress_http
- en: mode http
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 模式http
- en: option httpchk GET /healthz
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 选项httpchk GET /healthz
- en: server worker $worker1:80 check port 80
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器worker $worker1:80 检查端口80
- en: server worker2 $worker2:80 check port 80
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器worker2 $worker2:80 检查端口80
- en: server worker3 $worker3:80 check port 80
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器worker3 $worker3:80 检查端口80
- en: 'This **frontend** section creates a frontend that accepts incoming HTTP traffic
    on port 80\. It then uses the list of servers in the backend, named **ingress_http**,
    for endpoints. Just like in the HTTPS section, we are using port 80 to check for
    any nodes that are running a service on port 80\. Any endpoint that replies to
    the check will be added as a destination for HTTP traffic, and any nodes that
    do not have NGINX running on them will not reply, which means they won''t be added
    as destinations:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这个**前端**部分创建了一个前端，接受端口80上的传入HTTP流量。然后使用后端**ingress_http**中的服务器列表作为端点。就像在HTTPS部分一样，我们使用端口80来检查是否有任何运行在端口80上的节点。任何回复检查的端点都将被添加为HTTP流量的目的地，而任何没有运行NGINX的节点将不会回复，这意味着它们不会被添加为目的地：
- en: EOF
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: EOF
- en: 'This ends the creation of our file. The final file will be created in the **HAProxy**
    directory:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们文件的创建。最终文件将在**HAProxy**目录中创建：
- en: Start the HAProxy Container for the Worker Nodes
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动工作节点的HAProxy容器
- en: docker run --name HAProxy-workers-lb -d -p 80:80 -p 443:443 -v ~/HAProxy:/usr/local/etc/HAProxy:ro
    HAProxy -f /usr/local/etc/HAProxy/HAProxy.cfg
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: docker run --name HAProxy-workers-lb -d -p 80:80 -p 443:443 -v ~/HAProxy:/usr/local/etc/HAProxy:ro
    HAProxy -f /usr/local/etc/HAProxy/HAProxy.cfg
- en: The final step is to start a Docker container running HAProxy with our created
    configuration file containing the three worker nodes, exposed on the Docker host
    on ports 80 and 443.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是启动一个运行HAProxy的Docker容器，其中包含我们创建的配置文件，其中包含三个工作节点，暴露在Docker主机上的端口80和443上。
- en: Now that you have learned how to install a custom HAProxy load balancer for
    your worker nodes, let's look at how the configuration works.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何为工作节点安装自定义的HAProxy负载均衡器，让我们看看配置是如何工作的。
- en: Understanding HAProxy traffic flow
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解HAProxy流量流向
- en: 'The cluster will have a total of eight containers running. Six of these containers
    will be the standard Kubernetes components; that is, three control plane servers
    and three worker nodes. The other two containers are KinD''s HAProxy server, and
    your own custom HAProxy container:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 集群将总共有八个容器在运行。其中六个容器将是标准的Kubernetes组件，即三个控制平面服务器和三个工作节点。另外两个容器是KinD的HAProxy服务器和您自己的自定义HAProxy容器：
- en: '![Figure 4.13 – Custom HAProxy container running'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.13-运行自定义HAProxy容器'
- en: '](image/Fig_4.13_B15514.jpg)'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.13_B15514.jpg)'
- en: Figure 4.13 – Custom HAProxy container running
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13-运行自定义HAProxy容器
- en: There are a few differences between this cluster output versus our two-node
    cluster for the exercises. Notice that the worker nodes are not exposed on any
    host ports. The worker nodes do not need any mappings since we have our new HAProxy
    server running. If you look at the HAProxy container we created, it is exposed
    on host ports 80 and 443\. This means that any incoming requests to the host on
    port 80 or 443 will be directed to the custom HAProxy container.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 这个集群输出与我们的两节点集群有一些不同。请注意，工作节点没有暴露在任何主机端口上。工作节点不需要任何映射，因为我们有我们的新HAProxy服务器在运行。如果您查看我们创建的HAProxy容器，它是在主机端口80和443上暴露的。这意味着对端口80或443的任何传入请求都将被定向到自定义的HAProxy容器。
- en: 'The default NGINX deployment only has a single replica, which means that the
    Ingress controller is running on a single node. If we look at the logs for the
    HAProxy container, we will see something interesting:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的NGINX部署只有一个副本，这意味着Ingress控制器正在单个节点上运行。如果我们查看HAProxy容器的日志，我们将看到一些有趣的东西：
- en: '[NOTICE] 093/191701 (1) : New worker #1 (6) forked'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '[注意] 093/191701 (1)：新工作人员＃1（6）分叉'
- en: '[WARNING] 093/191701 (6) : Server ingress_https/worker is DOWN, reason: Layer4
    connection problem, info: "SSL handshake failure (Connection refused)", check
    duration: 0ms. 2 active and 0 backup servers left. 0 sessions active, 0 requeued,
    0 remaining in queue.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/191701 (6)：服务器ingress_https/worker已关闭，原因：第4层连接问题，信息：“SSL握手失败（连接被拒绝）”，检查持续时间：0毫秒。剩下2个活动服务器和0个备份服务器。0个会话活动，0个重新排队，0个在队列中剩余。'
- en: '[WARNING] 093/191702 (6) : Server ingress_https/worker3 is DOWN, reason: Layer4
    connection problem, info: "SSL handshake failure (Connection refused)", check
    duration: 0ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued,
    0 remaining in queue.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/191702 (6)：服务器ingress_https/worker3已关闭，原因：第4层连接问题，信息：“SSL握手失败（连接被拒绝）”，检查持续时间：0毫秒。剩下1个活动服务器和0个备份服务器。0个会话活动，0个重新排队，0个在队列中剩余。'
- en: '[WARNING] 093/191702 (6) : Server ingress_http/worker is DOWN, reason: Layer4
    connection problem, info: "Connection refused", check duration: 0ms. 2 active
    and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/191702 (6)：服务器ingress_http/worker已关闭，原因：第4层连接问题，信息：“连接被拒绝”，检查持续时间：0毫秒。剩下2个活动服务器和0个备份服务器。0个会话活动，0个重新排队，0个在队列中剩余。'
- en: '[WARNING] 093/191703 (6) : Server ingress_http/worker3 is DOWN, reason: Layer4
    connection problem, info: "Connection refused", check duration: 0ms. 1 active
    and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/191703 (6)：服务器ingress_http/worker3已关闭，原因：第4层连接问题，信息：“连接被拒绝”，检查持续时间：0毫秒。剩下1个活动服务器和0个备份服务器。0个会话活动，0个重新排队，0个在队列中剩余。'
- en: You may have noticed a few errors in the log, such as SSL handshake failure
    and **Connection refused**. While these do look like errors, they are actually
    failed checked events on the worker nodes. Remember that NGINX is only running
    in a single pod, and since we have all three nodes in our HAProxy backend configuration,
    it will check for the ports on each node. Any nodes that fail to reply will not
    be used to load balance traffic. In our current config, this does load balance,
    since we only have NGINX on one node. It does, however, provide high availability
    to the Ingress controller.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到日志中有一些错误，比如SSL握手失败和**连接被拒绝**。虽然这些看起来像错误，但实际上它们是工作节点上的失败检查事件。请记住，NGINX只在一个pod上运行，而且由于我们在HAProxy后端配置中有所有三个节点，它将检查每个节点上的端口。未能回复的任何节点将不用于负载平衡流量。在我们当前的配置中，这确实进行了负载平衡，因为我们只在一个节点上有NGINX。但是，它确实为Ingress控制器提供了高可用性。
- en: 'If you look carefully at the log output, you will see how many servers are
    active on a defined backend; for example:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细查看日志输出，您将看到在定义的后端上有多少个活动服务器；例如：
- en: 'check duration: 0ms. 1 active and 0 backup servers left.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 检查持续时间：0毫秒。剩下1个活动服务器和0个备份服务器。
- en: Each server pool in the log output shows 1 active endpoint, so we know that
    the HAProxy has successfully found a NGINX controller on both port 80 and 443.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 日志输出中的每个服务器池显示1个活动端点，因此我们知道HAProxy已成功找到端口80和443上的NGINX控制器。
- en: 'To find out what worker the HAProxy server has connected to, we can use the
    failed connections in the log. Each backend will list the failed connections.
    For example, we know that the node that is working is **cluster01-worker2** based
    on the logs that the other two worker nodes show as **DOWN**:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出HAProxy服务器连接到哪个worker，我们可以使用日志中的失败连接。每个后端将列出失败的连接。例如，根据其他两个worker节点显示为**DOWN**的日志，我们知道正在工作的节点是**cluster01-worker2**：
- en: Server ingress_https/worker is DOWN Server ingress_https/worker3 is DOWN
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器ingress_https/worker已宕机 服务器ingress_https/worker3已宕机
- en: Let's simulate a node failure to prove that HAProxy is providing high availability
    to NGINX.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们模拟节点故障，以证明HAProxy为NGINX提供了高可用性。
- en: Simulating a Kubelet failure
  id: totrans-527
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟Kubelet故障
- en: Remember that KinD nodes are ephemeral and that stopping any container may cause
    it to fail on restart. So, how can we simulate a worker node failure since we
    can't simply stop the container?
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，KinD节点是临时的，停止任何容器可能会导致其在重新启动时失败。那么，我们如何模拟worker节点故障，因为我们不能简单地停止容器？
- en: To simulate a failure, we can stop the kubelet service on a node, which will
    alert **kube-apisever** so that it doesn't schedule any additional pods on the
    node. In our example, we want to prove that HAProxy is providing HA support for
    NGINX. We know that the running container is on **worker2**, so that's the node
    we want to "take down."
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 要模拟故障，我们可以停止节点上的kubelet服务，这将提醒**kube-apisever**不在节点上调度任何其他pod。在我们的示例中，我们想证明HAProxy为NGINX提供了HA支持。我们知道正在运行的容器在**worker2**上，因此这是我们想要“关闭”的节点。
- en: 'The easiest way to stop **kubelet** is to send a **docker exec** command to
    the container:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 停止**kubelet**的最简单方法是向容器发送**docker exec**命令：
- en: docker exec cluster01-worker2 systemctl stop kubelet
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: docker exec cluster01-worker2 systemctl stop kubelet
- en: 'You will not see any output from this command, but if you wait a few minutes
    for the cluster to receive the updated node status, you can verify the node is
    down by looking at a list of nodes:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 您不会从此命令中看到任何输出，但是如果您等待几分钟，让集群接收更新的节点状态，您可以通过查看节点列表来验证节点是否已关闭：
- en: kubectl get nodes.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl get nodes。
- en: 'You will receive the following output:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 您将收到以下输出：
- en: '![Figure 4.14 – worker2 is in a NotReady state'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.14 - worker2处于NotReady状态'
- en: '](image/Fig_4.14_B15514.jpg)'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_4.14_B15514.jpg)'
- en: Figure 4.14 – worker2 is in a NotReady state
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 - worker2处于NotReady状态
- en: This verifies that we just simulated a kubelet failure and that **worker2**
    is in a **NotReady** status.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 这验证了我们刚刚模拟了kubelet故障，并且**worker2**处于**NotReady**状态。
- en: Any pods that were running before the kubelet "failure" will continue to run,
    but **kube-scheduler** will not schedule any workloads on the node until the kubelet
    issue is resolved. Since we know the pod will not restart on the node, we can
    delete the pod so that it can be rescheduled on a different node.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 在kubelet“故障”之前运行的任何pod将继续运行，但是**kube-scheduler**在kubelet问题解决之前不会在节点上调度任何工作负载。由于我们知道pod不会在节点上重新启动，我们可以删除pod，以便它可以在不同的节点上重新调度。
- en: 'You need to get the pod name and then delete it to force a restart:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要获取pod名称，然后将其删除以强制重新启动：
- en: kubectl get pods -n ingress-nginx
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl get pods -n ingress-nginx
- en: nginx-ingress-controller-7d6bf88c86-r7ztq
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: nginx-ingress-controller-7d6bf88c86-r7ztq
- en: kubectl delete pod nginx-ingress-controller-7d6bf88c86-r7ztq -n ingress-nginx
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl delete pod nginx-ingress-controller-7d6bf88c86-r7ztq -n ingress-nginx
- en: This will force the scheduler to start the container on another worker node.
    It will also cause the HAProxy container to update the backend list, since the
    NGINX controller has moved to another worker node.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 这将强制调度程序在另一个工作节点上启动容器。它还会导致HAProxy容器更新后端列表，因为NGINX控制器已移动到另一个工作节点。
- en: 'If you look at the HAProxy logs again, you will see that HAProxy has updated
    the backends to include **cluster01-worker3** and that it removed **cluster01-worker2**
    from the active servers list:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您再次查看HAProxy日志，您将看到HAProxy已更新后端以包括**cluster01-worker3**，并且已将**cluster01-worker2**从活动服务器列表中删除：
- en: '[WARNING] 093/194006 (6) : Server ingress_https/worker3 is UP, reason: Layer7
    check passed, code: 200, info: "OK", check duration: 4ms. 2 active and 0 backup
    servers online. 0 sessions requeued, 0 total in queue.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/194006 (6) : 服务器ingress_https/worker3已启动，原因：Layer7检查通过，代码：200，信息：“OK”，检查持续时间：4毫秒。2个活动服务器和0个备用服务器在线。0个会话重新排队，队列中总共0个。'
- en: '[WARNING] 093/194008 (6) : Server ingress_http/worker3 is UP, reason: Layer7
    check passed, code: 200, info: "OK", check duration: 0ms. 2 active and 0 backup
    servers online. 0 sessions requeued, 0 total in queue.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/194008 (6) : 服务器ingress_http/worker3已启动，原因：Layer7检查通过，代码：200，信息：“OK”，检查持续时间：0毫秒。2个活动服务器和0个备用服务器在线。0个会话重新排队，队列中总共0个。'
- en: '[WARNING] 093/195130 (6) : Server ingress_http/worker2 is DOWN, reason: Layer4
    timeout, check duration: 2000ms. 1 active and 0 backup servers left. 0 sessions
    active, 0 requeued, 0 remaining in queue.'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/195130 (6) : 服务器ingress_http/worker2已关闭，原因：Layer4超时，检查持续时间：2000毫秒。1个活动服务器和0个备用服务器剩下。0个会话活动，0个重新排队，0个剩余在队列中。'
- en: '[WARNING] 093/195131 (6) : Server ingress_https/worker2 is DOWN, reason: Layer4
    timeout, check duration: 2001ms. 1 active and 0 backup servers left. 0 sessions
    active, 0 requeued, 0 remaining in queue.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '[警告] 093/195131 (6) : 服务器ingress_https/worker2已关闭，原因：Layer4超时，检查持续时间：2001毫秒。1个活动服务器和0个备用服务器剩下。0个会话活动，0个重新排队，0个剩余在队列中。'
- en: If you plan to use this HA cluster for additional tests, you will want to restart
    the kubelet on **cluster01-worker2**. If you plan to delete the HA cluster, you
    can just run a KinD cluster delete and all the nodes will be deleted.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划将此HA集群用于其他测试，您将需要重新启动**cluster01-worker2**上的kubelet。如果您计划删除HA集群，只需运行KinD集群删除，所有节点将被删除。
- en: Deleting the HAProxy container
  id: totrans-551
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除HAProxy容器
- en: Once you have deleted your KinD cluster, you will need to manually remove the
    HAProxy container we added. Since KinD didn't create our custom load balancer,
    deleting the cluster will not remove the container.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦删除了KinD集群，您将需要手动删除我们添加的HAProxy容器。由于KinD没有创建我们的自定义负载均衡器，删除集群不会删除容器。
- en: 'To delete the custom HAProxy container, run the **docker rm** command to force
    remove the image:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除自定义的HAProxy容器，请运行**docker rm**命令以强制删除镜像：
- en: docker rm HAProxy-workers-lb –force
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: docker rm HAProxy-workers-lb –force
- en: This will stop the container and remove it from Docker's list, allowing you
    to run it again using the same name with a future KinD cluster.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 这将停止容器并将其从Docker的列表中删除，从而允许您在将来的KinD集群中再次使用相同的名称运行它。
- en: Summary
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the Kubernetes SIG project called KinD.We
    went into details on how to install optional components in a KinD cluster, including
    Calico as the CNI and NGINX as the Ingress controller. Finally, we covered the
    details of the Kubernetes storage objects that are included with a KinD cluster.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了名为KinD的Kubernetes SIG项目。我们详细介绍了如何在KinD集群中安装可选组件，包括Calico作为CNI和NGINX作为Ingress控制器。最后，我们介绍了包含在KinD集群中的Kubernetes存储对象的详细信息。
- en: Hopefully, with the help of this chapter, you now understand the power that
    using KinD can bring to you and your organization. It offers an easy to deploy,
    fully configurable Kubernetes cluster. The number of running clusters on a single
    host is theoretically limited only by the host resources.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 希望通过本章的帮助，您现在了解使用KinD可以为您和您的组织带来的力量。它提供了一个易于部署、完全可配置的Kubernetes集群。在单个主机上运行的集群数量在理论上仅受主机资源的限制。
- en: In the next chapter, we will dive into Kubernetes objects. We've called the
    next chapter *Kubernetes Bootcamp* since it will cover the majority of the base
    Kubernetes objects and what each one is used for. The next chapter can be considered
    a "Kubernetes pocket guide." It contains a quick reference to Kubernetes objects
    and what they do, as well as when to use them.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入研究Kubernetes对象。我们将下一章称为*Kubernetes训练营*，因为它将涵盖大多数基本的Kubernetes对象以及它们各自的用途。下一章可以被视为“Kubernetes口袋指南”。它包含了Kubernetes对象的快速参考以及它们的作用，以及何时使用它们。
- en: It's a packed chapter and is designed to be a refresher for those of you who
    have experience with Kubernetes, or as a crash course for those of you who are
    new to Kubernetes. Our intention for this book is to go beyond the base Kubernetes
    objects since there are many books on the market today that cover the basics of
    Kubernetes very well.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个内容丰富的章节，旨在为那些具有Kubernetes经验的人提供复习，或者为那些新手提供速成课程。我们撰写本书的目的是超越基本的Kubernetes对象，因为当今市场上有许多涵盖Kubernetes基础知识的书籍。
- en: Questions
  id: totrans-561
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What object must be created before you can create a **PersistentVolumeClaim**?
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建**持久卷索赔**之前必须创建哪个对象？
- en: A. PVC
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: A. PVC
- en: B. Disk
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: B. 磁盘
- en: C. **PersistentVolume**
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: C. **持久卷**
- en: D. **VirtualDisk**
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: D. **虚拟磁盘**
- en: KinD includes a dynamic disk provisioner. What company created the provisioner?
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KinD包括一个动态磁盘提供程序。是哪家公司创建了这个提供程序？
- en: A. Microsoft
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: A. 微软
- en: B. CNCF
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: B. CNCF
- en: C. VMware
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: C. VMware
- en: D. Rancher
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: D. 牧场主
- en: If you create a KinD cluster with multiple worker nodes, what would you install
    to direct traffic to each node?
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您创建了一个具有多个工作节点的KinD集群，您将安装什么来将流量引导到每个节点？
- en: A. Load balancer
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: A. 负载均衡器
- en: B. Proxy server
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: B. 代理服务器
- en: C. Nothing
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: C. 什么都不用
- en: D. Network load balancer
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: D. 网络负载均衡器
- en: 'True or false: A Kubernetes cluster can only have one CSIdriver installed.'
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确或错误：Kubernetes集群只能安装一个CSIdriver。
- en: A. True
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: A. 正确
- en: B. False
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: B. 错误
