- en: Extending HorizontalPodAutoscaler with Custom Metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义指标扩展HorizontalPodAutoscaler
- en: Computers make excellent and efficient servants, but I have no wish to serve
    under them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机是出色且高效的仆人，但我不愿意在它们的服务下工作。
- en: '- *Spock*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- *斯波克*'
- en: Adoption of **HorizontalPodAutoscaler** (**HPA**) usually goes through three
    phases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**HorizontalPodAutoscaler**（**HPA**）的采用通常经历三个阶段。'
- en: The first phase is *discovery*. The first time we find out what it does, we
    usually end up utterly amazed. "Look at this. It scales our applications automatically.
    I don't need to worry about the number of replicas anymore."
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段是*发现*。第一次我们发现它的功能时，通常会感到非常惊讶。"看这个。它可以自动扩展我们的应用程序。我不再需要担心副本的数量了。"
- en: The second phase is the *usage*. Once we start using HPA, we quickly realize
    that scaling applications based memory and CPU is not enough. Some apps do increase
    their memory and CPU usage with the increase in load, while many others don't.
    Or, to be more precise, not proportionally. HPA, for some applications, works
    well. For many others, it does not work at all, or it is not enough. Sooner or
    later, we'll need to extend HPA thresholds beyond those based on memory and CPU.
    This phase is characterized by *disappointment*. "It seemed like a good idea,
    but we cannot use it with most of our applications. We need to fall back to alerts
    based on metrics and manual changes to the number of replicas."
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段是*使用*。一旦我们开始使用HPA，我们很快意识到基于内存和CPU的应用程序扩展不足够。一些应用程序随着负载的增加而增加其内存和CPU使用率，而许多其他应用程序则没有。或者更准确地说，不成比例。对于一些应用程序，HPA运作良好。对于许多其他应用程序，它根本不起作用，或者不够。迟早，我们需要将HPA的阈值扩展到不仅仅是基于内存和CPU的阈值。这个阶段的特点是*失望*。"这似乎是个好主意，但我们不能用它来处理我们大多数的应用程序。我们需要退回到基于指标和手动更改副本数量的警报。"
- en: The third phase is *re-discovery*. Once we go through the HPA v2 documentation
    (still in beta at the time of this writing) we can see that it allows us to extend
    it to almost any type of metrics and expressions. We can hook HPAs to Prometheus,
    or nearly any other tool, though adapters. Once we master that, there is almost
    no limit to the conditions we can set as triggers of automatic scaling of our
    applications. The only restriction is our ability to transform data into Kubernetes'
    custom metrics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第三阶段是*重新发现*。一旦我们阅读了HPA v2文档（在撰写本文时仍处于测试阶段），我们就会发现它允许我们将其扩展到几乎任何类型的指标和表达式。我们可以通过适配器将HPAs连接到Prometheus，或几乎任何其他工具。一旦我们掌握了这一点，我们几乎没有限制条件可以设置为自动扩展我们的应用程序的触发器。唯一的限制是我们将数据转换为Kubernetes自定义指标的能力。
- en: Our next goal is to extend HorizontalPodAutoscaler definitions to include conditions
    based on data stored in Prometheus.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个目标是扩展HorizontalPodAutoscaler定义，以包括基于Prometheus中存储的数据的条件。
- en: Creating a cluster
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个集群
- en: The `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository will continue to serve as our source of Kubernetes definitions. We'll
    make sure that it is up-to-date by pulling the latest version.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`vfarcic/k8s-specs`（[https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs)）存储库将继续作为我们的Kubernetes定义的来源。我们将确保通过拉取最新版本使其保持最新。'
- en: All the commands from this chapter are available in the `05-hpa-custom-metrics.sh`
    ([https://gist.github.com/vfarcic/cc546f81e060e4f5fc5661e4fa003af7](https://gist.github.com/vfarcic/cc546f81e060e4f5fc5661e4fa003af7))
    Gist.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在`05-hpa-custom-metrics.sh`（[https://gist.github.com/vfarcic/cc546f81e060e4f5fc5661e4fa003af7](https://gist.github.com/vfarcic/cc546f81e060e4f5fc5661e4fa003af7)）Gist中找到。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The requirements are the same as those we had in the previous chapter. The only
    exception is **EKS**. We'll continue using the same Gists as before for all other
    Kuberentes flavors.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要求与上一章相同。唯一的例外是**EKS**。我们将继续为所有其他Kubernetes版本使用与之前相同的Gists。
- en: A note to EKS users Even though three t2.small nodes we used so far have more
    than enough memory and CPU, they might not be able to host all the Pods we'll
    create. EKS (by default) uses AWS networking. A t2.small instance can have a maximum
    of three network interfaces, with four IPv4 address per each. That means that
    we can have up to twelve IPv4 addresses on each t2.small node. Given that each
    Pod needs to have its own address, that means that we can have a maximum of twelve
    Pods per node. In this chapter, we might need more than thirty-six Pods across
    the cluster. Instead of creating a cluster with more than three nodes, we'll add
    Cluster Autoscaler (CA) to the mix, and let the cluster expand if needed. We already
    explored CA in one of the previous chapters and the setup instructions are now
    added to the Gist `eks-hpa-custom.sh` ([https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c)](https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 EKS 用户的注意事项，尽管我们迄今为止使用的三个 t2.small 节点具有足够的内存和 CPU，但它们可能无法承载我们将创建的所有 Pod。EKS（默认情况下）使用
    AWS 网络。t2.small 实例最多可以有三个网络接口，每个接口最多有四个 IPv4 地址。这意味着每个 t2.small 节点上最多可以有十二个 IPv4
    地址。鉴于每个 Pod 需要有自己的地址，这意味着每个节点最多可以有十二个 Pod。在本章中，我们可能需要在整个集群中超过三十六个 Pod。我们将添加 Cluster
    Autoscaler（CA）到集群中，让集群在需要时扩展，而不是创建超过三个节点的集群。我们已经在之前的章节中探讨了 CA，并且设置说明现在已添加到了 Gist
    `eks-hpa-custom.sh` ([https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c)](https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c))。
- en: Please use one of the Gists that follow to create a new cluster. If you already
    have a cluster you'd like to use for the exercises, please use the Gists to validate
    that it fulfills all the requirements.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用以下 Gist 之一创建新的集群。如果您已经有一个要用于练习的集群，请使用 Gist 来验证它是否满足所有要求。
- en: '`gke-instrument.sh`: **GKE** with 3 n1-standard-1 worker nodes, **nginx Ingress**,
    **tiller**, **Prometheus** Chart, and environment variables **LB_IP**, **PROM_ADDR**,
    and **AM_ADDR** ([https://gist.github.com/vfarcic/675f4b3ee2c55ee718cf132e71e04c6e](https://gist.github.com/vfarcic/675f4b3ee2c55ee718cf132e71e04c6e)).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gke-instrument.sh`: **GKE** 配有 3 个 n1-standard-1 工作节点，**nginx Ingress**，**tiller**，**Prometheus**
    图表，以及环境变量 **LB_IP**，**PROM_ADDR** 和 **AM_ADDR** ([https://gist.github.com/vfarcic/675f4b3ee2c55ee718cf132e71e04c6e](https://gist.github.com/vfarcic/675f4b3ee2c55ee718cf132e71e04c6e))。'
- en: '`eks-hpa-custom.sh`: **EKS** with 3 t2.small worker nodes, **nginx Ingress**,
    **tiller**, **Metrics Server**, **Prometheus** Chart, environment variables **LB_IP**,
    **PROM_ADDR**, and **AM_ADDR**, and **Cluster Autoscaler** ([https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c](https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c)).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eks-hpa-custom.sh`: **EKS** 配有 3 个 t2.small 工作节点，**nginx Ingress**，**tiller**，**Metrics
    Server**，**Prometheus** 图表，环境变量 **LB_IP**，**PROM_ADDR** 和 **AM_ADDR**，以及 **Cluster
    Autoscaler** ([https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c](https://gist.github.com/vfarcic/868bf70ac2946458f5485edea1f6fc4c))。'
- en: '`aks-instrument.sh`: **AKS** with 3 Standard_B2s worker nodes, **nginx Ingress**,
    and **tiller**, **Prometheus** Chart, and environment variables **LB_IP**, **PROM_ADDR**,
    and **AM_ADDR** ([https://gist.github.com/vfarcic/65a0d5834c9e20ebf1b99225fba0d339](https://gist.github.com/vfarcic/65a0d5834c9e20ebf1b99225fba0d339)).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aks-instrument.sh`: **AKS** 配有 3 个 Standard_B2s 工作节点，**nginx Ingress**，**tiller**，**Prometheus**
    图表，以及环境变量 **LB_IP**，**PROM_ADDR** 和 **AM_ADDR** ([https://gist.github.com/vfarcic/65a0d5834c9e20ebf1b99225fba0d339](https://gist.github.com/vfarcic/65a0d5834c9e20ebf1b99225fba0d339))。'
- en: '`docker-instrument.sh`: **Docker for Desktop** with **2 CPUs**, **3 GB RAM**,
    **nginx Ingress**, **tiller**, **Metrics Server**, **Prometheus** Chart, and environment
    variables **LB_IP**, **PROM_ADDR**, and **AM_ADDR** ([https://gist.github.com/vfarcic/1dddcae847e97219ab75f936d93451c2](https://gist.github.com/vfarcic/1dddcae847e97219ab75f936d93451c2)).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker-instrument.sh`：**Docker for Desktop**，带有**2个CPU**，**3GB RAM**，**nginx
    Ingress**，**tiller**，**Metrics Server**，**Prometheus**图表，和环境变量**LB_IP**，**PROM_ADDR**，和**AM_ADDR**（[https://gist.github.com/vfarcic/1dddcae847e97219ab75f936d93451c2](https://gist.github.com/vfarcic/1dddcae847e97219ab75f936d93451c2)）。'
- en: '`minikube-instrument.sh`: **Minikube** with **2 CPUs**, **3 GB RAM**, **ingress**,
    **storage-provisioner**, **default-storageclass**, and **metrics-server** addons
    enabled, **tiller**, **Prometheus** Chart, and environment variables **LB_IP**,
    **PROM_ADDR**, and **AM_ADDR** ([https://gist.github.com/vfarcic/779fae2ae374cf91a5929070e47bddc8](https://gist.github.com/vfarcic/779fae2ae374cf91a5929070e47bddc8)).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minikube-instrument.sh`：**Minikube**，带有**2个CPU**，**3GB RAM**，**ingress**，**storage-provisioner**，**default-storageclass**，和**metrics-server**插件已启用，**tiller**，**Prometheus**图表，和环境变量**LB_IP**，**PROM_ADDR**，和**AM_ADDR**（[https://gist.github.com/vfarcic/779fae2ae374cf91a5929070e47bddc8](https://gist.github.com/vfarcic/779fae2ae374cf91a5929070e47bddc8)）。'
- en: Now we're ready to extend our usage of HPA. But, before we do that, let's briefly
    explore (again) how HPA works out of the box.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备扩展我们对HPA的使用。但在我们这样做之前，让我们简要地探索（再次）HPA如何开箱即用。
- en: Using HorizontalPodAutoscaler without metrics adapter
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在没有度量适配器的情况下使用HorizontalPodAutoscaler
- en: If we do not create a metrics adapter, Metrics Aggregator only knows about CPU
    and memory usage related to containers and nodes. To make things more complicated,
    that information is limited only to the last few minutes. Since HPA is just concerned
    about Pods and containers inside them, we are limited to only two metrics. When
    we create an HPA, it will scale or de-scale our Pods if memory or CPU consumption
    of the containers that constitute those Pods is above or below predefined thresholds.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不创建度量适配器，度量聚合器只知道与容器和节点相关的CPU和内存使用情况。更复杂的是，这些信息仅限于最近几分钟。由于HPA只关心Pod和其中的容器，我们只能使用两个指标。当我们创建HPA时，如果构成这些Pod的容器的内存或CPU消耗超过或低于预定义的阈值，它将扩展或缩减我们的Pods。
- en: Metrics Server periodically fetches information (CPU and memory) from Kubelets
    running inside worker nodes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Metrics Server定期从运行在工作节点内的Kubelet获取信息（CPU和内存）。
- en: Those metrics are passed to Metrics Aggregator which, in this scenario, does
    not add any additional value. From there on, HPAs periodically consult the data
    in the Metrics Aggregator (through its API endpoint). When there is a discrepancy
    between target values defined in an HPA and the actual values, an HPA will manipulate
    the number of replicas of a Deployment or a StatefulSet. As we already know, any
    change to those controllers results in rolling updates executed through creation
    and manipulation of ReplicaSets, which create and delete Pods, which are converted
    into containers by a Kubelet running on a node where a Pod is scheduled.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标被传递给度量聚合器，但在这种情况下，度量聚合器并没有增加任何额外的价值。从那里开始，HPA定期查询度量聚合器中的数据（通过其API端点）。当HPA中定义的目标值与实际值存在差异时，HPA将操纵部署或StatefulSet的副本数量。正如我们已经知道的那样，对这些控制器的任何更改都会导致通过创建和操作ReplicaSets执行滚动更新，从而创建和删除Pods，这些Pods由运行在调度了Pod的节点上的Kubelet转换为容器。
- en: '![](assets/d9f95b8d-7de9-4e19-86e5-702c61885999.png)Figure 5-1: HPA with out
    of the box setup (arrows show the flow of data)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d9f95b8d-7de9-4e19-86e5-702c61885999.png)图5-1：开箱即用设置的HPA（箭头显示数据流向）'
- en: Functionally, the flow we just described works well. The only problem is the
    data available in the Metrics Aggregator. It is limited to memory and CPU. More
    often than not, that is not enough. So, there's no need for us to change the process,
    but to extend the data available to HPA. We can do that through a metrics adapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从功能上讲，我们刚刚描述的流程运行良好。唯一的问题是指标聚合器中可用的数据。它仅限于内存和CPU。往往这是不够的。因此，我们不需要改变流程，而是要扩展可用于HPA的数据。我们可以通过指标适配器来实现这一点。
- en: Exploring Prometheus Adapter
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Prometheus适配器
- en: Given that we want to extend the metrics available through the Metrics API,
    and that Kubernetes allows us that through it's *Custom Metrics API* ([https://github.com/kubernetes/metrics/tree/master/pkg/apis/custom_metrics](https://github.com/kubernetes/metrics/tree/master/pkg/apis/custom_metrics)),
    one option to accomplish our goals could be to create our own adapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们希望通过指标API扩展可用的指标，并且Kubernetes允许我们通过其*自定义指标API*（[https://github.com/kubernetes/metrics/tree/master/pkg/apis/custom_metrics](https://github.com/kubernetes/metrics/tree/master/pkg/apis/custom_metrics)）来实现这一点，实现我们目标的一个选项可能是创建我们自己的适配器。
- en: Depending on the application (DB) where we store metrics, that might be a good
    option. But, given that it is pointless to reinvent the wheel, our first step
    should be to search for a solution. If someone already created an adapter that
    suits our needs, it would make sense to adopt it, instead of creating a new one
    by ourselves. Even if we do choose something that provides only part of the features
    we're looking for, it's easier to build on top of it (and contribute back to the
    project), than to start from scratch.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们存储指标的应用程序（DB），这可能是一个不错的选择。但是，鉴于重新发明轮子是毫无意义的，我们的第一步应该是寻找解决方案。如果有人已经创建了一个适合我们需求的适配器，那么采用它而不是自己创建一个新的适配器是有意义的。即使我们选择了只提供部分我们寻找的功能的东西，也比从头开始更容易（并向项目做出贡献），而不是从零开始。
- en: Given that our metrics are stored in Prometheus, we need a metrics adapter that
    will be capable of fetching data from it. Since Prometheus is very popular and
    adopted by the community, there is already a project waiting for us to use it.
    It's called *Kubernetes Custom Metrics Adapter for Prometheus*. It is an implementation
    of the Kubernetes Custom Metrics API that uses Prometheus as the data source.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的指标存储在Prometheus中，我们需要一个指标适配器，它将能够从中获取数据。由于Prometheus非常受欢迎并被社区采用，已经有一个项目在等待我们使用。它被称为*用于Prometheus的Kubernetes自定义指标适配器*。它是使用Prometheus作为数据源的Kubernetes自定义指标API的实现。
- en: Since we adopted Helm for all our installations, we'll use it to install the
    adapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们已经采用Helm进行所有安装，我们将使用它来安装适配器。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We installed `prometheus-adapter` Helm Chart from the `stable` repository. The
    resources were created in the `metrics` Namespace, and the `image.tag` is set
    to `v0.3.0`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`stable`存储库安装了`prometheus-adapter` Helm Chart。资源被创建在`metrics`命名空间中，`image.tag`设置为`v0.3.0`。
- en: We changed `metricsRelistInterval` from the default value of `30s` to `90s`.
    That is the interval the adapter will use to fetch metrics from Prometheus. Since
    our Prometheus setup is fetching metrics from its targets every sixty seconds,
    we had to set the adapter's interval to a value higher than that. Otherwise, the
    adapter's frequency would be higher than pulling frequency of Prometheus, and
    we'd have iterations that would be without new data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`metricsRelistInterval`从默认值`30s`更改为`90s`。这是适配器用来从Prometheus获取指标的间隔。由于我们的Prometheus设置每60秒从其目标获取指标，我们必须将适配器的间隔设置为高于该值。否则，适配器的频率将高于Prometheus的拉取频率，我们将有一些迭代没有新数据。
- en: The last two arguments specified the URL and the port through which the adapter
    can access Prometheus API. In our case, the URL is set to go through Prometheus'
    service.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个参数指定了适配器可以访问Prometheus API的URL和端口。在我们的情况下，URL设置为通过Prometheus的服务。
- en: Please visit *Prometheus Adapter Chart README* ([https://github.com/helm/charts/tree/master/stable/prometheus-adapter](https://github.com/helm/charts/tree/master/stable/prometheus-adapter))
    for more information about all the values you can set to customize the installation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请访问*Prometheus Adapter Chart README*（[https://github.com/helm/charts/tree/master/stable/prometheus-adapter](https://github.com/helm/charts/tree/master/stable/prometheus-adapter)）获取有关所有可以设置以自定义安装的值的更多信息。
- en: Finally, we waited until the `prometheus-adapter` rolls out.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们等待`prometheus-adapter`部署完成。
- en: If everything is working as expected, we should be able to query Kubernetes'
    custom metrics API and retrieve some of the Prometheus data provided through the
    adapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切按预期运行，我们应该能够查询Kubernetes的自定义指标API，并检索通过适配器提供的一些Prometheus数据。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Given the promise that each chapter will feature a different Kubernetes' flavor,
    and that AWS did not have its turn yet, all the outputs are taken from EKS. Depending
    on the platform you're using, your outputs might be slightly different.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于每个章节都将呈现不同的Kubernetes版本的特点，并且AWS还没有轮到，所有输出都来自EKS。根据您使用的平台不同，您的输出可能略有不同。
- en: The first entries of the output from querying Custom Metrics is as follows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 查询自定义指标的输出的前几个条目如下。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The list of the custom metrics available through the adapter is big, and we
    might be compelled to think that it contains all those stored in Prometheus. We'll
    find out whether that's true later. For now, we'll focus on the metrics we might
    need with HPA tied to `go-demo-5` Deployment. After all, providing metrics for
    auto-scaling is adapter's primary, if not the only function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 透过适配器可用的自定义指标列表很长，我们可能会被迫认为它包含了Prometheus中存储的所有指标。我们将在以后发现这是否属实。现在，我们将专注于可能需要的与`go-demo-5`部署绑定的HPA的指标。毕竟，为自动扩展提供指标是适配器的主要功能，如果不是唯一功能的话。
- en: From now on, Metrics Aggregator contains not only the data from the metrics
    server but also those from the Prometheus Adapter which, in turn, is fetching
    metrics from Prometheus Server. We are yet to confirm whether the data we're getting
    through the adapter is enough and whether HPA works with custom metrics.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，Metrics Aggregator不仅包含来自度量服务器的数据，还包括来自Prometheus Adapter的数据，后者又从Prometheus服务器获取度量。我们还需要确认通过适配器获取的数据是否足够，以及HPA是否能够使用自定义指标。
- en: '![](assets/2caa17d4-543c-4aef-bc51-67282a29270b.png)Figure 5-2: Custom metrics
    with Prometheus Adapter (arrows show the flow of data)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2caa17d4-543c-4aef-bc51-67282a29270b.png)图5-2：使用Prometheus Adapter的自定义指标（箭头显示数据流向）'
- en: Before we go deeper into the adapter, we'll define our goals for the `go-demo-5`
    application.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解适配器之前，我们将为`go-demo-5`应用程序定义我们的目标。
- en: We should be able to scale the Pods not only based on memory and CPU usage but
    also by the number of requests entering through Ingress or observed through instrumented
    metrics. There can be many other criteria we could add but, as a learning experience,
    those should be enough. We already know how to configure HPA to scale based on
    CPU and memory, so our mission is to extend it with a request counter. That way,
    we'll be able to set the rules that will increase the number of replicas when
    the application is receiving too many requests, as well as to descale it if traffic
    decreases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该能够根据内存和CPU使用率以及通过Ingress进入或通过仪表化指标观察到的请求数量来扩展Pods。我们可以添加许多其他标准，但作为学习经验，这些应该足够了。我们已经知道如何配置HPA以根据CPU和内存进行扩展，所以我们的任务是通过请求计数器来扩展它。这样，我们将能够设置规则，当应用程序接收到太多请求时增加副本的数量，以及在流量减少时减少副本的数量。
- en: Since we want to extend the HPA connected to `go-demo-5`, our next step is to
    install the application.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要扩展与`go-demo-5`相关的HPA，我们的下一步是安装应用程序。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We defined the address of the application, we installed the Chart, and we waited
    until the Deployment rolled out.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了应用程序的地址，安装了图表，并等待部署完成。
- en: 'A note to EKS users If you received `error: deployment "go-demo-5" exceeded
    its progress deadline` message, the cluster is likely auto-scaling to accommodate
    all the Pods and zones of the PersistentVolumes. That might take more time than
    the `progress deadline`. In that case, wait for a few moments and repeat the `rollout`
    command.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'EKS用户注意：如果收到了`error: deployment "go-demo-5" exceeded its progress deadline`的消息，集群可能正在自动扩展以适应所有Pod和PersistentVolumes的区域。这可能比`progress
    deadline`需要更长的时间。在这种情况下，请等待片刻并重复`rollout`命令。'
- en: Next, we'll generate a bit of traffic by sending a hundred requests to the application
    through its Ingress resource.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过其Ingress资源向应用程序发送一百个请求，以生成一些流量。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we generated some traffic, we can try to find a metric that will help
    us calculate how many requests passed through Ingress. Since we already know (from
    the previous chapters) that `nginx_ingress_controller_requests` provides the number
    of requests that enter through Ingress, we should check whether it is now available
    as a custom metric.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了一些流量，我们可以尝试找到一个指标，帮助我们计算通过Ingress传递了多少请求。由于我们已经知道（从之前的章节中）`nginx_ingress_controller_requests`提供了通过Ingress进入的请求数量，我们应该检查它是否现在作为自定义指标可用。
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We sent a request to `/apis/custom.metrics.k8s.io/v1beta1`. But, as you already
    saw, that alone would return all the metrics, and we are interested in only one.
    That's why we piped the output to `jq` and used its filter to retrieve only the
    entries that contain `nginx_ingress_controller_requests` as the `name`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`/apis/custom.metrics.k8s.io/v1beta1`发送了一个请求。但是，正如你已经看到的，单独这样做会返回所有的指标，而我们只对其中一个感兴趣。这就是为什么我们将输出导入到`jq`并使用它的过滤器来检索只包含`nginx_ingress_controller_requests`作为`name`的条目。
- en: If you received an empty output, please wait for a few moments until the adapter
    pulls the metrics from Prometheus (it does that every ninety seconds) and re-execute
    the command.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你收到了空的输出，请等待片刻，直到适配器从Prometheus中拉取指标（它每九十秒执行一次），然后重新执行命令。
- en: The output is as follows.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We got three results. The name of each consists of the resource type and the
    name of the metric. We'll discard those related to `jobs.batch` and `namespaces`,
    and concentrate on the metric related to `ingresses.extensions` since it provides
    the information we need. We can see that it is `namespaced`, meaning that the
    metrics are, among other things, separated by namespaces of their origin. The
    `kind` and the `verbs` are (almost) always the same, and there's not much value
    going through them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了三个结果。每个的名称由资源类型和指标名称组成。我们将丢弃与`jobs.batch`和`namespaces`相关的内容，并集中在与`ingresses.extensions`相关的指标上，因为它提供了我们需要的信息。我们可以看到它是`namespaced`，这意味着指标在其他方面是由其来源的命名空间分隔的。`kind`和`verbs`（几乎）总是相同的，浏览它们并没有太大的价值。
- en: The major problem with `ingresses.extensions/nginx_ingress_controller_requests`
    is that it provides the number of requests for an Ingress resource. We couldn't
    use it in its current form as an HPA criterion. Instead, we should divide the
    number of requests with the number of replicas. That would give us the average
    number of requests per replica which should be a better HPA threshold. We'll explore
    how to use expressions instead of simple metrics later. Knowing the number of
    requests entering through Ingress is useful, but it might not be enough.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`ingresses.extensions/nginx_ingress_controller_requests`的主要问题是它提供了Ingress资源的请求数量。我们无法将其作为HPA标准的当前形式使用。相反，我们应该将请求数量除以副本数量。这将给我们每个副本的平均请求数量，这应该是一个更好的HPA阈值。我们将探讨如何在后面使用表达式而不是简单的指标。了解通过Ingress进入的请求数量是有用的，但可能还不够。'
- en: Since `go-demo-5` already provides instrumented metrics, it would be helpful
    to see whether we can retrieve `http_server_resp_time_count`. As a reminder, that's
    the same metric we used in the [Chapter 4](31a0e783-db26-40cf-be37-f1a60fc1bc9e.xhtml),
    *Debugging Issues Discovered Through Metrics and Alerts*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`go-demo-5`已经提供了有仪器的指标，看看我们是否可以检索`http_server_resp_time_count`将会很有帮助。提醒一下，这是我们在[第4章](31a0e783-db26-40cf-be37-f1a60fc1bc9e.xhtml)中使用的相同指标，*通过指标和警报发现的故障调试*。
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We used `jq` to filter the result so that only `http_server_resp_time_count`
    is retrieved. Don't be surprised by empty output. That's normal since Prometheus
    Adapter is not configured to process all the metrics from Prometheus, but only
    those that match its own internal rules. So, this might be a good moment to take
    a look at the `prometheus-adapter` ConfigMap that contains its configuration.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`jq`来过滤结果，以便只检索`http_server_resp_time_count`。看到空输出不要感到惊讶。这是正常的，因为Prometheus
    Adapter没有配置为处理来自Prometheus的所有指标，而只处理符合其内部规则的指标。因此，现在可能是时候看一下包含其配置的`prometheus-adapter`
    ConfigMap了。
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output is too big to be presented in a book, so we'll go only through the
    first rule. It is as follows.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出太大，无法在书中呈现，所以我们只会讨论第一个规则。它如下所示。
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first rule retrieves only the metrics with the name that starts with `container`
    (`__name__=~"^container_.*"`), with the label `container_name` being anything
    but `POD`, and with `namespace` and `pod_name` not empty.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个规则仅检索以`container`开头的指标（`__name__=~"^container_.*"`），标签`container_name`不是`POD`，并且`namespace`和`pod_name`不为空。
- en: Each rule has to specify a few resource overrides. In this case, the `namespace`
    label contains the `namespace` resource. Similarly, the `pod` resource is retrieved
    from the label `pod_name`. Further on, we can see the `name` section uses regular
    expressions to name the new metric. Finally, `metricsQuery` tells the adapter
    which Prometheus query it should execute when retrieving data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规则都必须指定一些资源覆盖。在这种情况下，`namespace`标签包含`namespace`资源。类似地，`pod`资源是从标签`pod_name`中检索的。此外，我们可以看到`name`部分使用正则表达式来命名新的指标。最后，`metricsQuery`告诉适配器在检索数据时应执行哪个Prometheus查询。
- en: If that setup looks confusing, you should know that you're not the only one
    bewildered at first sight. Prometheus Adapter, just as Prometheus Server configs
    are complicated to grasp at first. Nevertheless, they are very powerful allowing
    us to define rules for service discovery, instead of specifying individual metrics
    (in the case of the adapter) or targets (in the case of Prometheus Server). Soon
    we'll go into more details with the setup of the adapter rules. For now, the important
    note is that the default configuration tells the adapter to fetch all the metrics
    that match a few rules.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个设置看起来令人困惑，您应该知道您并不是唯一一开始看起来感到困惑的人。就像Prometheus服务器配置一样，Prometheus Adapter一开始很难理解。尽管如此，它们非常强大，可以让我们定义服务发现规则，而不是指定单个指标（对于适配器的情况）或目标（对于Prometheus服务器的情况）。很快我们将更详细地介绍适配器规则的设置。目前，重要的一点是默认配置告诉适配器获取与几个规则匹配的所有指标。
- en: So far, we saw that `nginx_ingress_controller_requests` metric is available
    through the adapter, but that it is of no use since we need to divide the number
    of requests with the number of replicas. We also saw that the `http_server_resp_time_count`
    metric originating in `go-demo-5` Pods is not available. All in all, we do not
    have all the metrics we need, while most of the metrics currently fetched by the
    adapter are of no use. It is wasting time and resources with pointless queries.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到`nginx_ingress_controller_requests`指标可以通过适配器获得，但由于我们需要将请求数除以副本数，因此它没有用处。我们还看到`go-demo-5`
    Pods中的`http_server_resp_time_count`指标不可用。总而言之，我们没有所有需要的指标，而适配器当前获取的大多数指标都没有用。它通过无意义的查询浪费时间和资源。
- en: Our next mission is to reconfigure the adapter so that only the metrics we need
    are retrieved from Prometheus. We'll try to write our own expressions that will
    fetch only the data we need. If we manage to do that, we should be able to create
    HPA as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个任务是重新配置适配器，以便仅从Prometheus中检索我们需要的指标。我们将尝试编写自己的表达式，只获取我们需要的数据。如果我们能做到这一点，我们应该能够创建HPA。
- en: Creating HorizontalPodAutoscaler with custom metrics
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义指标创建HorizontalPodAutoscaler
- en: As you already saw, Prometheus Adapter comes with a set of default rules that
    provide many metrics that we do not need, and not all those that we do. It's wasting
    CPU and memory by doing too much, but not enough. We'll explore how we can customize
    the adapter with our own rules. Our next goal is to make the adapter retrieve
    only the `nginx_ingress_controller_requests` metric since that's the only one
    we need. On top of that, it should provide that metric in two forms. First, it
    should retrieve the rate, grouped by the resource.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您已经看到的，Prometheus Adapter带有一组默认规则，提供了许多我们不需要的指标，而我们需要的并不是所有的指标。它通过做太多事情而浪费CPU和内存，但又不够。我们将探讨如何使用我们自己的规则自定义适配器。我们的下一个目标是使适配器仅检索`nginx_ingress_controller_requests`指标，因为这是我们唯一需要的指标。除此之外，它还应该以两种形式提供该指标。首先，它应该按资源分组检索速率。
- en: The second form should be the same as the first but divided with the number
    of replicas of the Deployment that hosts the Pods where Ingress forwards the resources.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种形式应该与第一种形式相同，但应该除以托管Ingress转发资源的Pod的部署的副本数。
- en: That one should give us an average number of requests per replica and will be
    a good candidate for our first HPA definition based on custom metrics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供每个副本的平均请求数，并将成为基于自定义指标的第一个HPA定义的良好候选。
- en: I already prepared a file with Chart values that might accomplish our current
    objectives, so let's take a look at it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经准备了一个包含可能实现我们当前目标的Chart值的文件，让我们来看一下。
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output is as follows.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first few entries in that definition are the same values as the ones we
    used previously through `--set` arguments. We'll skip those and jump into the
    `rules` section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义中的前几个条目与我们先前通过`--set`参数使用的数值相同。我们将跳过这些条目，直接进入`rules`部分。
- en: Within the `rules` section, we're setting the `default` entry to `false`. That
    will get rid of the default rules we explored previously, and allow us to start
    with a clean slate. Further on, there are two `custom` rules.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在`rules`部分，我们将`default`条目设置为`false`。这将摆脱我们先前探索的默认规则，并使我们能够从干净的状态开始。此外，还有两个`custom`规则。
- en: The first rule is based on the `seriesQuery` with `nginx_ingress_controller_requests`
    as the value. The `overrides` entry inside the `resources` section helps the adapter
    find out which Kubernetes resources are associated with the metric. We're setting
    the value of the `namespace` label to the `namespace` resource. There's a similar
    entry for `ingress`. In other words, we're associating Prometheus labels with
    Kubernetes resources `namespace` and `ingress`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个规则基于`seriesQuery`，其值为`nginx_ingress_controller_requests`。`resources`部分内的`overrides`条目帮助适配器找出与该指标相关联的Kubernetes资源。我们将`namespace`标签的值设置为`namespace`资源。对于`ingress`也有类似的条目。换句话说，我们将Prometheus标签与Kubernetes资源`namespace`和`ingress`相关联。
- en: As you will see soon, the metric itself will be a part of a full query that
    will be treated as a single metric by HPA. Since we are creating something new,
    we need a name. So, we specified the `name` section with a single `as` entry set
    to `http_req_per_second`. That will be the reference in our HPA definitions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 很快你会看到，该指标本身将成为完整查询的一部分，并由HPA视为单一指标。由于我们正在创建新内容，我们需要一个名称。因此，我们在`name`部分指定了一个名为`http_req_per_second`的单一`as`条目。这将成为我们HPA定义中的参考。
- en: You already know that `nginx_ingress_controller_requests` is not very useful
    by itself. When we used it in Prometheus, we had to put it inside a `rate` function,
    we had to `sum` everything, and we had to group the results by a resource. We're
    doing something similar through the `metricsQuery` entry. Think of it as an equivalent
    of expressions we're writing in Prometheus. The only difference is that we are
    using "special" syntax like `<<.Series>>`. That's adapter's templating mechanism.
    Instead of hard-coding the name of the metric, the labels, and the group by statements,
    we have `<<.Series>>`, `<<.LabelMatchers>>`, and `<<.GroupBy>>` clauses that will
    be populated with the correct values depending on what we put in API calls.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道`nginx_ingress_controller_requests`本身并不是很有用。当我们在Prometheus中使用它时，我们必须将其放入`rate`函数中，我们必须`sum`所有内容，并且我们必须按资源对结果进行分组。我们通过`metricsQuery`条目正在做类似的事情。将其视为我们在Prometheus中编写的表达式的等价物。唯一的区别是我们使用了像`<<.Series>>`这样的“特殊”语法。这是适配器的模板机制。我们没有硬编码指标的名称、标签和分组语句，而是使用`<<.Series>>`、`<<.LabelMatchers>>`和`<<.GroupBy>>`子句，这些子句将根据我们在API调用中放入的内容填充正确的值。
- en: The second rule is almost the same as the first. The difference is in the name
    (now it's `http_req_per_second_per_replica`) and in the `metricsQuery`. The latter
    is now dividing the result with the number of replicas of the associated Deployment,
    just as we practiced in the [Chapter 3](13a73b25-73b2-4775-bd32-7945c7a25e46.xhtml),
    *Collecting and Querying Metrics and Sending Alerts*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个规则几乎与第一个相同。不同之处在于名称（现在是`http_req_per_second_per_replica`）和`metricsQuery`。后者现在将结果除以相关部署的副本数，就像我们在[第3章](13a73b25-73b2-4775-bd32-7945c7a25e46.xhtml)中练习的那样，*收集和查询指标并发送警报*。
- en: Next, we'll update the Chart with the new values.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用新数值更新图表。
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that the Deployment rolled out successfully, we can double-check that the
    configuration stored in the ConfigMap is indeed correct.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在部署已成功推出，我们可以再次确认ConfigMap中存储的配置确实是正确的。
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output, limited to the `Data` section, is as follows.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，限于`Data`部分，如下。
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see that the default `rules` we explored earlier are now replaced with
    the two rules we defined in the `rules.custom` section of our Chart values file.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们之前探索的默认`rules`现在被我们在Chart值文件的`rules.custom`部分中定义的两个规则所替换。
- en: The fact that the configuration looks correct does not necessarily mean that
    the adapter now provides data as Kubernetes custom metrics. Let's check that as
    well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 配置看起来正确并不一定意味着适配器现在提供数据作为Kubernetes自定义指标。我们也要检查一下。
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The output is as follows.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see that there are four metrics available, two of `http_req_per_second`
    and two of `http_req_per_second_per_replica`. Each of the two metrics we defined
    is available as both `namespaces` and `ingresses`. Right now, we do not care about
    `namespaces`, and we'll concentrate on `ingresses`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有四个可用的指标，其中两个是`http_req_per_second`，另外两个是`http_req_per_second_per_replica`。我们定义的两个指标都可以作为`namespaces`和`ingresses`使用。现在，我们不关心`namespaces`，我们将集中在`ingresses`上。
- en: I'll assume that at least five minutes (or more) passed since we sent a hundred
    requests. If it didn't, you are a speedy reader, and you'll have to wait for a
    while before we send another hundred requests. We are about to create our first
    HPA based on custom metrics, and I want to make sure that you see how it behaves
    both before and after it is activated.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设至少过去了五分钟（或更长时间），自从我们发送了一百个请求。如果没有，你是一个快速的读者，你将不得不等一会儿，然后我们再发送一百个请求。我们即将创建我们的第一个基于自定义指标的HPA，并且我想确保你在激活之前和之后都能看到它的行为。
- en: Now, let's take a look at an HPA definition.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个HPA定义。
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output is as follows.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The first half of the definition should be familiar since it does not differ
    from what we used before. It will maintain between `3` and `10` replicas of the
    `go-demo-5` Deployment. The new stuff is in the `metrics` section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 定义的前半部分应该是熟悉的，因为它与我们以前使用的内容没有区别。它将维护`go-demo-5`部署的`3`到`10`个副本。新的内容在`metrics`部分。
- en: In the past, we used `spec.metrics.type` set to `Resource`. Through that type,
    we defined CPU and memory targets. This time, however, our type is `Object`. It
    refers to a metric describing a single Kubernetes object which, in our case, happens
    to be a custom metric coming from Prometheus Adapter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，我们使用`spec.metrics.type`设置为`Resource`。通过该类型，我们定义了CPU和内存目标。然而，这一次，我们的类型是`Object`。它指的是描述单个Kubernetes对象的指标，而在我们的情况下，恰好是来自Prometheus
    Adapter的自定义指标。
- en: If we go through the *ObjectMetricSource v2beta1 autoscaling* ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#objectmetricsource-v2beta1-autoscaling](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#objectmetricsource-v2beta1-autoscaling))
    documentation, we can see that the fields of the `Object` type are different than
    those we used before when our type was `Resources`. We set the `metricName` to
    the metric we defined in Prometheus Adapter (`http_req_per_second_per_replica`).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们浏览*ObjectMetricSource v2beta1 autoscaling* ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#objectmetricsource-v2beta1-autoscaling](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#objectmetricsource-v2beta1-autoscaling))
    文档，我们可以看到`Object`类型的字段与我们以前使用的`Resources`类型不同。我们将`metricName`设置为我们在Prometheus
    Adapter中定义的指标（`http_req_per_second_per_replica`）。
- en: Remember that it is not a metric, but that we defined an expression that the
    adapter uses to fetch data from Prometheus and convert it into a custom metric.
    In this case, we are getting the number of requests entering an Ingress resource
    and divided by the number of replicas of a Deployment.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这不是一个指标，而是我们定义的一个表达式，适配器用来从Prometheus获取数据并将其转换为自定义指标。在这种情况下，我们得到了进入Ingress资源的请求数，然后除以部署的副本数。
- en: Finally, the `targetValue` is set to `50m` or 0.05 requests per second. I intentionally
    set it to a very low value so that we can easily reach the target and observe
    what happens.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`targetValue`设置为`50m`或每秒0.05个请求。我故意将其设置为一个非常低的值，以便我们可以轻松达到目标并观察发生了什么。
- en: Let's `apply` the definition.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们`apply`定义。
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we'll describe the newly created HPA, and see whether we can observe anything
    interesting.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述新创建的HPA，并看看是否能观察到一些有趣的东西。
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can see that there is only one entry in the `Metrics` section. The HPA is
    using the custom metric `http_req_per_second_per_replica` based on `Namespace/go-demo-5`.
    At the moment, the current value is `0`, and the `target` is set to `50m` (0.05
    requests per second). If, in your case, the `current` value is `unknown`, please
    wait for a few moments, and re-run the command.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`Metrics`部分只有一个条目。HPA正在使用基于`Namespace/go-demo-5`的自定义指标`http_req_per_second_per_replica`。目前，当前值为`0`，`target`设置为`50m`（每秒0.05个请求）。如果在您的情况下，`current`值为`unknown`，请等待片刻，然后重新运行命令。
- en: Further down, we can see that both the `current` and the `desired` number of
    Deployment Pods is set to `3`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步向下，我们可以看到`Deployment Pods`的`current`和`desired`数量均设置为`3`。
- en: All in all, the target is not reached (there are `0` requests) so there's no
    need for the HPA to do anything. It maintains the minimum number of replicas.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，目标没有达到（有`0`个请求），因此HPA无需做任何事情。它保持最小数量的副本。
- en: Let's spice it up a bit by generating some traffic.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们增加一些流量。
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We sent a hundred requests to the `go-demo-5` Ingress.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`go-demo-5` Ingress发送了一百个请求。
- en: Let's `describe` the HPA again, and see whether there are some changes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次`describe` HPA，并看看是否有一些变化。
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can see that the `current` value of the metric increased. In my case, it
    is `138m` (0.138 requests per second). If your output still shows `0`, you'll
    have to wait until the metrics are pulled by Prometheus, until the adapter fetches
    them, and until the HPA refreshes its status. In other words, wait for a few moments,
    and re-run the previous command.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到指标的`current`值增加了。在我的情况下，它是`138m`（每秒0.138个请求）。如果您的输出仍然显示`0`，您必须等待直到Prometheus拉取指标，直到适配器获取它们，直到HPA刷新其状态。换句话说，请等待片刻，然后重新运行上一个命令。
- en: 'Given that the `current` value is higher than the `target`, in my case, the
    HPA changed the `desired` number of `Deployment pods` to `6` (your number might
    differ depending on the value of the metric). As a result, HPA modified the Deployment
    by changing its number of replicas, and we should see additional Pods running.
    That becomes more evident through the `Events` section. There should be a new
    message stating `New size: 6; reason: Ingress metric http_req_per_second_per_replica
    above target`.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于`current`值高于`target`，在我的情况下，HPA将`desired`的`Deployment pods`数量更改为`6`（根据指标值的不同，您的数字可能会有所不同）。因此，HPA通过更改副本的数量来修改Deployment，并且我们应该看到额外的Pod正在运行。这在`Events`部分中更加明显。应该有一条新消息，说明`New
    size: 6; reason: Ingress metric http_req_per_second_per_replica above target`。'
- en: To be on the safe side, we'll list the Pods in the `go-demo-5` Namespace and
    confirm that the new ones are indeed running.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全起见，我们将列出`go-demo-5` Namespace中的Pods，并确认新的Pod确实正在运行。
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output is as follows.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that there are now six `go-demo-5-*` Pods, with three of them much
    younger than the rest.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到有六个`go-demo-5-*` Pods，其中有三个比其余的年轻得多。
- en: Next, we'll explore what happens when traffic drops below the HPAs `target`.
    We'll accomplish that by not doing anything for a while. Since we are the only
    ones sending requests to the application, all we have to do is to stand still
    for five minutes or, even better, use this time to fetch coffee.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨当流量低于HPA的“目标”时会发生什么。我们将通过一段时间不做任何事情来实现这一点。由于我们是唯一向应用程序发送请求的人，我们所要做的就是静静地站在那里五分钟，或者更好的是利用这段时间去取一杯咖啡。
- en: The reason we need to wait for at least five minutes lies in the frequencies
    HPA uses to scale up and down. By default, an HPA will scale up every three minutes,
    as long as the `current` value is above the `target`. Scaling down requires five
    minutes. An HPA will de-scale only if the `current` value is below the target
    for at least three minutes since the last time it scaled up.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要等待至少五分钟的原因在于HPA用于扩展和缩小的频率。默认情况下，只要“当前”值高于“目标”，HPA就会每三分钟扩展一次。缩小需要五分钟。只有在自上次扩展以来“当前”值低于目标至少三分钟时，HPA才会缩小。
- en: All in all, we need to wait for five minutes, or more, before we see the scaling
    effect in the opposite direction.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们需要等待五分钟或更长时间，然后才能看到相反方向的扩展效果。
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限相关部分，如下所示。
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The most interesting part of the output is the events section. We'll focus on
    the `Age` and `Message` fields. Remember, scale-up events are executed every three
    minutes if the current value is above the target, while scale-down iterations
    are every five minutes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中最有趣的部分是事件部分。我们将专注于“年龄”和“消息”字段。请记住，只要当前值高于目标，扩展事件就会每三分钟执行一次，而缩小迭代则是每五分钟一次。
- en: In my case, the HPA scaled the Deployment again, three minutes later. The number
    of replicas jumped from six to nine. Since the expression used by the adapter
    uses five minutes rate, some of the requests entered the second HPA iteration.
    Scaling up even after we stopped sending requests might not seem like a good idea
    (it isn't) but in the "real world" scenarios that shouldn't occur, since there's
    much more traffic than what we generated and we wouldn't put `50m` (0.2 requests
    per second) as a target.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，HPA在三分钟后再次扩展了部署。副本的数量从六个跳到了九个。由于适配器使用的表达式使用了五分钟的速率，一些请求进入了第二次HPA迭代。即使在我们停止发送请求后仍然扩展可能看起来不是一个好主意（确实不是），但在“现实世界”的场景中不应该发生，因为流量比我们生成的要多得多，我们不会将`50m`（每秒0.2个请求）作为目标。
- en: Five minutes after the last scale-up event, the `current` value was at `0`,
    and the HPA scaled the Deployment down to the minimum number of replicas (`3`).
    There's no traffic anymore, and we're back to where we started.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一次扩展事件后的五分钟内，“当前”值为`0`，HPA将部署缩小到最小副本数（`3`）。再也没有流量了，我们又回到了起点。
- en: We confirmed that Prometheus' metrics, fetched by Prometheus Adapter, and converted
    into Kuberentes' custom metrics, can be used in HPAs. So far, we used metrics
    pulled by Prometheus through exporters (`nginx_ingress_controller_requests`).
    Given that the adapter fetches metrics from Prometheus, it shouldn't matter how
    they got there. Nevertheless, we'll confirm that instrumented metrics can be used
    as well. That will give us an opportunity to cement what we learned so far and,
    at the same time, maybe learn a few new tricks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认了Prometheus的指标，通过Prometheus Adapter获取，并转换为Kuberentes的自定义指标，可以在HPA中使用。到目前为止，我们使用了通过出口商（`nginx_ingress_controller_requests`）从Prometheus获取的指标。鉴于适配器从Prometheus获取指标，它不应该关心它们是如何到达那里的。尽管如此，我们将确认仪表化指标也可以使用。这将为我们提供一个巩固到目前为止学到的知识的机会，同时，也许学到一些新的技巧。
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The output is yet another set of Prometheus Adapter Chart values.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出还是另一组Prometheus Adapter图表值。
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This time, we're combining rules containing different metric series. The first
    rule is based on the `http_server_resp_time_count` instrumented metric that origins
    in `go-demo-5`. We used it in the [Chapter 4](31a0e783-db26-40cf-be37-f1a60fc1bc9e.xhtml),
    *Debugging Issues Discovered Through Metrics and Alerts* and there's nothing truly
    extraordinary in its definition. It follows the same logic as the rules we used
    before. The second rule is the copy of one of the rules we used before.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们将合并包含不同指标系列的规则。第一条规则基于`go-demo-5`中源自`http_server_resp_time_count`的仪表指标。我们在[第4章](31a0e783-db26-40cf-be37-f1a60fc1bc9e.xhtml)中使用过它，*通过指标和警报调试问题*，在其定义中并没有什么特别之处。它遵循与我们之前使用的规则相同的逻辑。第二条规则是我们之前使用过的规则的副本。
- en: What is interesting about those rules is that there are two completely different
    queries producing different results. However, the name is the same (`http_req_per_second_per_replica`)
    in both cases.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是这些规则的是，有两个完全不同的查询产生了不同的结果。然而，在这两种情况下，名称是相同的（`http_req_per_second_per_replica`）。
- en: '"Wait a minute", you might say. The names are not the same. One is called `${1}req_per_second_per_replica`
    while the other is `http_req_per_second_per_replica`. While that is true, the
    final name, excluding resource type, is indeed the same. I wanted to show you
    that you can use regular expressions to form a name. In the first rule, the name
    consists of `matches` and `as` entries. The `(.*)` part of the `matches` entry
    becomes the first variable (there can be others) which is later on used as part
    of the `as` value (`${1}`). Since the metric is `http_server_resp_time_count`,
    it will extract `http_` from `^(.*)server_resp_time_count` which, in the next
    line, is used instead of `${1}`. The final result is `http_req_per_second_per_replica`,
    which is the same as the name of the second rule.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: “等一下”，你可能会说。这两个名称并不相同。一个被称为`${1}req_per_second_per_replica`，而另一个是`http_req_per_second_per_replica`。虽然这是真的，但最终名称，不包括资源类型，确实是相同的。我想向你展示你可以使用正则表达式来形成一个名称。在第一条规则中，名称由`matches`和`as`条目组成。`matches`条目的`(.*)`部分成为第一个变量（还可以有其他变量），稍后用作`as`值（`${1}`）。由于指标是`http_server_resp_time_count`，它将从`^(.*)server_resp_time_count`中提取`http_`，然后在下一行中，用于替换`${1}`。最终结果是`http_req_per_second_per_replica`，这与第二条规则的名称相同。
- en: Now that we established that both rules will provide custom metrics with the
    same name, we might think that will result in a conflict. How will HPA know which
    metric to use, if both are called the same? Will the adapter have to discard one
    and keep the other? The answer lies in the `resources` sections.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了两条规则都将提供相同名称的自定义指标，我们可能会认为这将导致冲突。如果两者都被称为相同的话，HPA将如何知道使用哪个指标？适配器是否必须丢弃一个并保留另一个？答案在“资源”部分。
- en: A true identifier of a metric is a combination of its name and the resource
    it ties with. The first rule produces two custom metrics, one for Services, and
    the other for Namespaces. The second also generates custom metrics for Namespaces,
    but for Ingresses as well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 指标的真正标识符是其名称和其关联的资源的组合。第一条规则生成两个自定义指标，一个用于服务，另一个用于命名空间。第二条规则还为命名空间生成自定义指标，但也为Ingresses生成自定义指标。
- en: How many metrics is that in total? I'll let you think about the answer before
    we check the result. To do that, we'll have to `upgrade` the Chart for the new
    values to take effect.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有多少个指标？在我们检查结果之前，我会让你考虑一下答案。为了做到这一点，我们将不得不“升级”图表，以使新值生效。
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We upgraded the Chart with the new values and waited until the Deployment rolls
    out.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用新值升级了图表，并等待部署完成。
- en: Now we can go back to our pending question "how many custom metrics we've got?"
    Let's see...
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以回到我们未决问题“我们有多少个自定义指标？”让我们看看…
- en: '[PRE33]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now we have three custom metrics, not four. I already explained that the unique
    identifier is the name of the metric combined with the Kubernetes resource it's
    tied to. All the metrics are called `http_req_per_second_per_replica`. But, since
    both rules override two resources, and `namespace` is set in both, one had to
    be discarded. We don't know which one was removed and which stayed. Or, maybe,
    they were merged. It does not matter since we shouldn't override the same resource
    with the metrics with the same name. There was no practical reason for me to include
    `namespace` in adapter's rule, other than to show you that there can be multiple
    overrides, and what happens when they are the same.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有三个自定义度量标准，而不是四个。我已经解释过，唯一的标识符是度量标准的名称与其绑定的 Kubernetes 资源。所有度量标准都被称为 `http_req_per_second_per_replica`。但是，由于两个规则都覆盖了两个资源，并且在两者中都设置了
    `namespace`，因此必须丢弃一个。我们不知道哪一个被移除了，哪一个留下了。或者，它们可能已经合并了。这并不重要，因为我们不应该用相同名称的度量标准覆盖相同的资源。对于我在适配器规则中包含
    `namespace` 的实际原因，除了向您展示可以有多个覆盖以及它们相同时会发生什么之外，没有其他实际原因。
- en: Other than that silly reason, you can mentally ignore the `namespaces/http_req_per_second_per_replica`
    metric.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 除了那个愚蠢的原因，你可以在脑海中忽略 `namespaces/http_req_per_second_per_replica` 度量标准。
- en: We used two different Prometheus' expressions to create two different custom
    metrics, with the same name but related to other resources. One (based on `nginx_ingress_controller_requests`
    expression) comes from Ingress resources, while the other (based on `http_server_resp_time_count`)
    comes from Services. Even though the latter originates in `go-demo-5` Pods, Prometheus
    discovered it through Services (as discussed in the previous chapter).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两个不同的 Prometheus 表达式来创建两个不同的自定义度量标准，它们具有相同的名称，但与其他资源相关。一个（基于 `nginx_ingress_controller_requests`
    表达式）来自 Ingress 资源，而另一个（基于 `http_server_resp_time_count`）来自 Services。尽管后者起源于 `go-demo-5`
    Pods，但 Prometheus 是通过 Services 发现它的（正如在前一章中讨论的那样）。
- en: We can use the `/apis/custom.metrics.k8s.io` endpoint not only to discover which
    custom metrics we have but also to inspect details, including values. For example,
    we can retrieve `services/http_req_per_second_per_replica` metric through the
    command that follows.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以使用 `/apis/custom.metrics.k8s.io` 端点来发现我们拥有哪些自定义度量标准，还可以检查细节，包括数值。例如，我们可以通过以下命令检索
    `services/http_req_per_second_per_replica` 度量标准。
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The output is as follows.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `describedObject` section shows us the details of the items. Right now,
    we have only one Service with that metric.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`describedObject` 部分向我们展示了项目的细节。现在，我们只有一个具有该度量标准的 Service。'
- en: We can see that the Service resides in the `go-demo-5` Namespace, that its name
    is `go-demo-5`, and that it is using `v1` API version.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到该 Service 位于 `go-demo-5` Namespace 中，它的名称是 `go-demo-5`，并且它正在使用 `v1` API
    版本。
- en: Further down, we can see the current value of the metric. In my case, it is
    `1130m`, or slightly above one request per second. Since nobody is sending requests
    to the `go-demo-5` Service, that value is as expected, considering that a health
    check is executed once a second.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在更下面，我们可以看到度量标准的当前值。在我的情况下，它是 `1130m`，或者略高于每秒一个请求。由于没有人向 `go-demo-5` Service
    发送请求，考虑到每秒执行一次健康检查，这个值是预期的。
- en: Next, we'll explore updated HPA definition that will use the Service-based metric.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨更新后的 HPA 定义，将使用基于服务的度量标准。
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The output is as follows.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: When compared with the previous definition, the only change is in the `target`
    and `targetValue` fields. Remember, the full identifier is a combination of the
    `metricName` and `target`. Therefore, this time we changed the `kind` to `Service`.
    We also had to change the `targetValue` since our application is receiving not
    only external requests through Ingress but also internal ones. They could be originating
    in other applications that might communicate with `go-demo-5` or, as in our case,
    in Kubernetes' health checks. Since their frequency is one second, we set the
    `targetValue` to `1500m`, or 1.5 requests per second. That way, scaling will not
    be triggered if we do not send any requests to the application. Normally, you'd
    set a much bigger value. But, for now, we're only trying to observe how it behaves
    before and after scaling.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的定义相比，唯一的变化在于`target`和`targetValue`字段。请记住，完整的标识符是`metricName`和`target`的组合。因此，这次我们将`kind`更改为`Service`。我们还必须更改`targetValue`，因为我们的应用程序不仅接收来自Ingress的外部请求，还接收内部请求。它们可能来自其他可能与`go-demo-5`通信的应用程序，或者像我们的情况一样，来自Kubernetes的健康检查。由于它们的频率是一秒，我们将`targetValue`设置为`1500m`，即每秒1.5个请求。这样，如果我们不向应用程序发送任何请求，就不会触发扩展。通常，您会设置一个更大的值。但是，目前，我们只是尝试观察在扩展之前和之后它的行为。
- en: Next, we'll apply the changes to the HPA, and describe it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用对HPA的更改，并进行描述。
- en: '[PRE39]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The output of the latter command, limited to the relevant parts, is as follows.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 后一条命令的输出，仅限于相关部分，如下所示。
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: For now, there's no reason for the HPA to scale up the Deployment. The current
    value is below the threshold. In my case, it's `1100m`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，没有理由让HPA扩展部署。当前值低于阈值。在我的情况下，它是`1100m`。
- en: Now we can test whether autoscaling based on custom metrics originating from
    instrumentation works as expected. Sending requests through Ingress might be slow,
    especially if our cluster runs in Cloud. The round-trip from out laptop all the
    way to the service might be too slow. So, we'll send requests from inside the
    cluster, by spinning up a Pod and executing a request loop from inside it.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以测试基于来自仪器的自定义指标的自动缩放是否按预期工作。通过Ingress发送请求可能会很慢，特别是如果我们的集群在云中运行。从我们的笔记本到服务的往返可能太慢了。因此，我们将从集群内部发送请求，通过启动一个Pod并从其中执行请求循环。
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Normally, I prefer `alpine` images since they are much small and efficient.
    However, `for` loops do not work from `alpine` (or I don't know how to write them),
    so we switched to `debian` instead. It doesn't have `curl` though, so we'll have
    to install it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我更喜欢`alpine`镜像，因为它们更小更高效。但是，`for`循环在`alpine`中无法工作（或者我不知道如何编写），所以我们改用`debian`。不过`debian`中没有`curl`，所以我们需要安装它。
- en: '[PRE42]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now we can send requests that will generate enough traffic for HPA to trigger
    the scale-up process.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以发送请求，这些请求将产生足够的流量，以便HPA触发扩展过程。
- en: '[PRE43]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We sent five-hundred requests to `/demo/hello` endpoint, and we exited the container.
    Since we used the `--rm` argument when we created the Pod, it will be removed
    automatically from the system, so we do not need to execute any cleanup operation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`/demo/hello`端点发送了五百个请求，然后退出了容器。由于我们在创建Pod时使用了`--rm`参数，它将自动从系统中删除，因此我们不需要执行任何清理操作。
- en: Let's `describe` the HPA and see what happened.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一下HPA并看看发生了什么。
- en: '[PRE44]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果，仅限于相关部分，如下所示。
- en: '[PRE45]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: HPA detected that the `current` value is above the target (in my case it's `1794m`)
    and changed the desired number of replicas from `3` to `4`. We can observe that
    from the last event as well. If, in your case, the `desired` number of replicas
    is still `3`, please wait for a few moments for the next iteration of HPA evaluations,
    and repeat the `describe` command.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: HPA检测到`current`值高于目标值（在我的情况下是`1794m`），并将所需的副本数量从`3`更改为`4`。我们也可以从最后一个事件中观察到这一点。如果在您的情况下，`desired`副本数量仍然是`3`，请等待一段时间进行HPA评估的下一次迭代，并重复`describe`命令。
- en: If we need an additional confirmation that scaling indeed worked as expected,
    we can retrieve the Pods in the `go-demo-5` Namespace.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要额外确认扩展确实按预期工作，我们可以检索`go-demo-5`命名空间中的Pods。
- en: '[PRE46]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The output is as follows.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下。
- en: '[PRE47]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: There's probably no need to confirm that the HPA will soon scale down the `go-demo-5`
    Deployment after we stopped sending requests. Instead, we'll jump into the next
    topic.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，当我们停止发送请求后，HPA很快会缩减`go-demo-5`部署。相反，我们将进入下一个主题。
- en: Combining Metric Server data with custom metrics
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合Metric Server数据和自定义指标
- en: So far, the few HPA examples used a single custom metric to decide whether to
    scale the Deployment. You already know from the [Chapter 1](c24be85c-1380-457c-84cb-7f29cbac1c62.xhtml),
    *Autoscaling Deployments and StatefulSets Based on Resource Usage*, that we can
    combine multiple metrics in an HPA. However, all the examples in that chapter
    used data from the Metrics Server. We learned that in many cases memory and CPU
    metrics from the Metrics Server are not enough, so we introduced Prometheus Adapter
    that feeds custom metrics to the Metrics Aggregator. We successfully configured
    an HPA to use those custom metrics. Still, more often than not, we'll need a combination
    of both types of metrics in our HPA definitions. While memory and CPU metrics
    are not enough by themselves, they are still essential. Can we combine both?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，少数HPA示例使用单个自定义指标来决定是否扩展部署。您已经从[第1章](c24be85c-1380-457c-84cb-7f29cbac1c62.xhtml)中了解到，基于资源使用情况自动扩展部署和StatefulSets，我们可以在HPA中结合多个指标。然而，该章节中的所有示例都使用了来自Metrics
    Server的数据。我们了解到，在许多情况下，来自Metrics Server的内存和CPU指标是不够的，因此我们引入了Prometheus Adapter，它将自定义指标提供给Metrics
    Aggregator。我们成功地配置了HPA来使用这些自定义指标。然而，通常情况下，我们需要在HPA定义中结合这两种类型的指标。虽然内存和CPU指标本身是不够的，但它们仍然是必不可少的。我们能否将两者结合起来呢？
- en: Let's take a look at yet another HPA definition.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看看另一个HPA定义。
- en: '[PRE48]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE49]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This time, HPA has three entries in the `metrics` section. The first two are
    the "standard" `cpu` and `memory` entries based on `Resource` type. The last entry
    is one of the `Object` types we used earlier. With those combined, we're telling
    HPA to scale up if any of the three criteria are met. Similarly, it will scale
    down as well but for that to happen all three criteria need to be below the targets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，HPA在`metrics`部分有三个条目。前两个是基于`Resource`类型的“标准”`cpu`和`memory`条目。最后一个条目是我们之前使用过的`Object`类型之一。通过结合这些，我们告诉HPA如果满足三个标准中的任何一个，就进行扩展。同样，它也会进行缩减，但为了发生这种情况，所有三个标准都需要低于目标值。
- en: Let's `apply` the definition.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们`apply`这个定义。
- en: '[PRE50]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Next, we'll describe the HPA. But, before we do that, we'll have to wait for
    a bit until the updated HPA goes through its next iteration.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述HPA。但在此之前，我们需要等待一段时间，直到更新后的HPA经过下一次迭代。
- en: '[PRE51]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The output, limited to the relevant parts, is as follows.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，仅限于相关部分，如下所示。
- en: '[PRE52]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We can see that the memory-based metric is above the threshold from the start.
    In my case, it is `110%`, while the target is `80%`. As a result, HPA scaled up
    the Deployment. In my case, it set the new size to `5` replicas.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到基于内存的度量从一开始就超过了阈值。在我的情况下，它是`110%`，而目标是`80%`。因此，HPA扩展了部署。在我的情况下，它将新大小设置为`5`个副本。
- en: There's no need to confirm that the new Pods are running. By now, we should
    trust HPA to do the right thing. Instead, we'll comment briefly on the whole flow.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要确认新的Pod是否正在运行。到现在为止，我们应该相信HPA会做正确的事情。相反，我们将简要评论整个流程。
- en: The complete HorizontalPodAutoscaler flow of events
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整的HorizontalPodAutoscaler事件流
- en: Metrics Server is fetching memory and CPU data from Kubelets running on the
    worker nodes. In parallel, Prometheus Adapter is fetching data from Prometheus
    Server which, as you already know, pulls data from different sources. Data from
    both Metrics Server and Prometheus Adapter is combined in Metrics Aggregator.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Metrics Server从运行在工作节点上的Kubelets获取内存和CPU数据。与此同时，Prometheus Adapter从Prometheus
    Server获取数据，而你已经知道，Prometheus Server从不同的来源获取数据。Metrics Server和Prometheus Adapter的数据都合并在Metrics
    Aggregator中。
- en: HPA is periodically evaluating metrics defined as scaling criteria. It's fetching
    data from Metrics Aggregator, and it does not really care whether they're coming
    from Metrics Server, Prometheus Adapter, or any other tool we could have used.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: HPA定期评估定义为缩放标准的度量。它从Metrics Aggregator获取数据，实际上并不在乎它们是来自Metrics Server、Prometheus
    Adapter还是我们可能使用的任何其他工具。
- en: Once scaling criteria is met, HPA manipulates Deployments and StatefulSets by
    changing their number of replicas.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦满足缩放标准，HPA通过改变其副本数量来操作部署和StatefulSets。
- en: As a result, rolling updates are performed by creating and updating ReplicaSets
    which, in turn, create or remove Pods.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过创建和更新ReplicaSets执行滚动更新，ReplicaSets又会创建或删除Pods。
- en: '![](assets/3a9e1aa2-f19e-47a8-ba94-b5c03eccf23a.png)Figure 5-3: HPA using a
    combination of metrics from Metrics Server and those provided by Prometheus Adapter
    (arrows show the flow of data)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3a9e1aa2-f19e-47a8-ba94-b5c03eccf23a.png)图5-3：HPA使用Metrics Server和Prometheus
    Adapter提供的度量指标的组合（箭头显示数据流）'
- en: Reaching nirvana
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 达到涅磐
- en: Now that we know how to add almost any metric to HPAs, they are much more useful
    than what it seemed in the [Chapter 1](c24be85c-1380-457c-84cb-7f29cbac1c62.xhtml),
    *Autoscaling Deployments and StatefulSets Based on Resource Usage*. Initially,
    HPAs weren't very practical since memory and CPU are, in many cases, insufficient
    for making decisions on whether to scale our Pods. We had to learn how to collect
    metrics (we used Prometheus Server for that), and how to instrument our applications
    to gain more detailed visibility. Custom Metrics was the missing piece of the
    puzzle. If we extend the "standard" metrics (CPU and memory) with the additional
    metrics we need (for example, Prometheus Adapter), we gain a potent process that
    will keep the number of replicas of our applications in sync with internal and
    external demands. Assuming that our applications are scalable, we can guarantee
    that they will (almost) always be as performant as needed. There is no need for
    manual interventions any more, at least when scaling is in question. HPA with
    "standard" and custom metrics will guarantee that the number of Pods meets the
    demand, and Cluster Autoscaler (when applicable) will ensure that we have the
    sufficient capacity to run those Pods.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何将几乎任何指标添加到HPA中，它们比在[第1章](c24be85c-1380-457c-84cb-7f29cbac1c62.xhtml)中看起来要有用得多，*基于资源使用情况自动扩展部署和有状态集*。最初，HPA并不是非常实用，因为在许多情况下，内存和CPU是不足以决定是否扩展我们的Pods的。我们必须学会如何收集指标（我们使用Prometheus
    Server进行了这项工作），以及如何为我们的应用程序提供更详细的可见性。自定义指标是这个难题的缺失部分。如果我们用额外的我们需要的指标（例如，Prometheus
    Adapter）扩展了“标准”指标（CPU和内存），我们就获得了一个强大的流程，它将使我们应用程序的副本数量与内部和外部需求保持同步。假设我们的应用程序是可扩展的，我们可以保证它们（几乎）总是能够按需执行。至少在涉及到扩展时，不再需要手动干预。具有“标准”和自定义指标的HPA将保证Pod的数量满足需求，而集群自动扩展器（在适用时）将确保我们有足够的容量来运行这些Pods。
- en: Our system is one step closer to being self-sufficient. It will self-adapt to
    changed conditions, and we (humans) can turn our attention towards more creative
    and less repetitive tasks than those required to maintain the system in the state
    that meets the demand. We are one step closer to nirvana.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统离自给自足又近了一步。它将自适应于变化的条件，而我们（人类）可以把注意力转向比维持系统满足需求状态所需的更有创意和不那么重复的任务。我们离涅槃又近了一步。
- en: What now?
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在呢？
- en: Please note that we used `autoscaling/v2beta1` version of HorizontalPodAutoscaler.
    At the time of this writing (November 2018), only `v1` is stable and production-ready.
    However, `v1` is so limited (it can use only CPU metrics) that it's almost useless.
    Kubernetes community worked on new (`v2`) HPA for a while and, in my experience,
    it works reasonably well. The main problem is not stability but potential changes
    in the API that might not be backward compatible. A short while ago, `autoscaling/v2beta2`
    was released, and it uses a different API. I did not include it in the book because
    (at the time of this writing) most Kubernetes clusters do not yet support it.
    If you're running Kubernetes 1.11+, you might want to switch to `v2beta2`. If
    you do so, remember that you'll need to make a few changes to the HPA definitions
    we explored. The logic is still the same, and it behaves in the same way. The
    only visible difference is in the API.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了`autoscaling/v2beta1`版本的HorizontalPodAutoscaler。在撰写本文时（2018年11月），只有`v1`是稳定且可用于生产环境的。然而，`v1`非常有限（只能使用CPU指标），几乎没有什么用。Kubernetes社区已经在新的（`v2`）HPA上工作了一段时间，并且根据我的经验，它运行得相当不错。主要问题不是稳定性，而是API可能发生的不向后兼容的变化。不久前，`autoscaling/v2beta2`发布了，它使用了不同的API。我没有在书中包含它，因为（在撰写本文时）大多数Kubernetes集群尚不支持它。如果你正在运行Kubernetes
    1.11+，你可能想切换到`v2beta2`。如果这样做，记住你需要对我们探讨的HPA定义进行一些更改。逻辑仍然是一样的，它的行为也是一样的。唯一可见的区别在于API。
- en: Please consult *HorizontalPodAutoscaler v2beta2 autoscaling* ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#horizontalpodautoscaler-v2beta2-autoscaling](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#horizontalpodautoscaler-v2beta2-autoscaling))
    for the changes from `v2beta1` we used to `v2beta2` available in Kubernetes 1.11+.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*HorizontalPodAutoscaler v2beta2 autoscaling* ([https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#horizontalpodautoscaler-v2beta2-autoscaling](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#horizontalpodautoscaler-v2beta2-autoscaling))，了解从`v2beta1`到`v2beta2`的变化，这些变化在Kubernetes
    1.11+中可用。
- en: That's it. Destroy the cluster if its dedicated to this book, or keep it if
    it's not or if you're planning to jump to the next chapter right away. If you're
    keeping it, please delete the `go-demo-5` resources by executing the commands
    that follow.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。如果集群专门用于本书，请销毁它；如果不是，或者您计划立即跳转到下一章节，请保留它。如果您要保留它，请通过执行以下命令删除`go-demo-5`资源。
- en: '[PRE53]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Before you leave, you might want to go over the main points of this chapter.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在您离开之前，您可能希望复习本章的要点。
- en: HPA is periodically evaluating metrics defined as scaling criteria.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPA定期评估定义为扩展标准的指标。
- en: HPA fetching data from Metrics Aggregator, and it does not really care whether
    they're coming from Metrics Server, Prometheus Adapter, or any other tool we could
    have used.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HPA从Metrics Aggregator获取数据，它并不真的在乎这些数据是来自Metrics Server、Prometheus Adapter还是我们可能使用的任何其他工具。
