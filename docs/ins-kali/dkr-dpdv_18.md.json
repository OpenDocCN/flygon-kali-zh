["```\n     $ apt-get update \n    ```", "```\n     $ apt-get install -y \\\n     \t  apt-transport-https \\\n     \t  curl \\\n     \t  software-properties-common \n    ```", "```\n     $ DOCKER_EE_REPO=<paste-in-your-unique-ee-url> \n    ```", "```\n     $ curl -fsSL \"`${``DOCKER_EE_REPO``}`/ubuntu/gpg\" | sudo apt-key add - \n    ```", "```\n     $ add-apt-repository \\\n        \"deb [arch=amd64] $DOCKER_EE_REPO/ubuntu \\\n        $(lsb_release -cs) \\\n        stable-17.06\" \n    ```", "```\n     $ apt-get update \n    ```", "```\n     $ apt-get remove docker docker-engine docker-ce docker.io \n    ```", "```\n     $ apt-get install docker-ee -y \n    ```", "```\n    $ docker --version\n    Docker version `17`.06.2-ee-6, build e75fdb8 \n    ```", "```````` \n\n ```That\u2019s it, you\u2019ve installed the Docker EE engine.\n\nNow you can install Universal Control Plane.\n\n#### Docker Universal Control Plane (UCP)\n\nWe\u2019ll be referring to Docker Universal Control Plane as **UCP** for the rest of the chapter.\n\nUCP is an enterprise-grade container-as-a-service platform with an Operations UI. It takes the Docker Engine, and adds all of the features enterprises love and require. Things like; *RBAC, policies, trust, a highly-available control plane,* and a *simple UI*. Under-the-covers, it\u2019s a containerized microservices app that you download and run as a bunch of containers.\n\nArchitecturally, UCP builds on top of Docker EE in *Swarm mode*. As shown in Figure 16.4, the UCP control plane runs on Swarm managers, and apps are deployed on Swarm workers.\n\n![Figure 16.4 High level UCP architecture](images/figure16-4.png)\n\nFigure 16.4 High level UCP architecture\n\nAt the time of writing, UCP managers have to be Linux. Workers can be a mix of Windows and Linux.\n\n##### Planning a UCP installation\n\nWhen planning a UCP installation, it\u2019s important to size and spec your cluster appropriately. We\u2019ll look at some of things you should consider.\n\nAll nodes in the cluster should have their clocks in sync (e.g. NTP). If they don\u2019t, problems can occur that are a pain to troubleshoot.\n\nAll nodes should have a static IP address and a stable DNS name.\n\nBy default, UCP managers don\u2019t run user workloads. This is a recommended best practice, and you should enforce it for production environments \u2014 it allows managers to focus solely on control plane duties. It also makes troubleshooting easier.\n\nYou should always have an odd number of managers. This helps avoid split-brain conditions where managers fail or become partitioned from the rest of the cluster. The ideal number is 3, 5, or 7, with 3 or 5 usually being the best. Having more than 7 can cause issues with the back-end Raft and cluster reconciliation. If you don\u2019t have enough nodes for 3 managers, 1 is better than 2!\n\nIf you\u2019re implementing a backup schedule (which you should) and taking regular backups, you might want to deploy 5 managers. This is because Swarm and UCP backup operations require stopping Docker and UCP services. Having 5 managers can help maintain cluster resiliency during such operations.\n\nManager nodes should be spread across data center availability zones. The last thing you want, is a single availability zone failing and taking all of the UCP managers with it. However, it\u2019s important to connect your managers via high-speed reliable networks. So if your data center availability zones are not connected by good networks, you might be better-off keeping all managers in a single availability zone. As a general rule, if you\u2019re deploying on public cloud infrastructure, you should deploy your managers in availability zones within a single *region*. Spanning *regions* usually involves less-reliable high-latency networks.\n\nYou can have as many *worker nodes* as you want \u2014 they don\u2019t participate in cluster Raft operations, so won\u2019t impact control plane operations.\n\nPlanning the number and size of worker nodes requires an understanding of the apps you plan on running on the cluster. For example, knowing this will help you determine things like how many Windows vs Linux nodes you require. You will also need to know if any of your apps have special requirements and need specialised worker nodes \u2014 may be PCI workloads.\n\nAlso, although the Docker engine is lightweight and small, the containerized applications you run on your nodes might not be. With this in mind, it\u2019s important to size nodes according to the CPU, RAM, network, and disk I/O requirements of your applications.\n\nMaking server sizing requirements isn\u2019t something I like to do, as it\u2019s entirely dependant on *your* workloads. However, the Docker website is currently suggesting the following **minimum** requirements for Docker UCP 2.2.4 on Linux:\n\n*   UCP Manager nodes running DTR: 8GB of RAM with 3GB of disk space\n*   UCP Worker nodes: 4GB of RAM with 3GB of free disk space\n\n**Recommended** requirements are:\n\n*   UCP Manager nodes running DTR: 8GB RAM, 4 vCPUs, and 100GB disk space\n*   UCP Worker nodes: 4GB RAM 25-100GB of free disk space\n\nTake this with a pinch of salt, and be sure to do your own sizing exercise.\n\nOne thing\u2019s for sure \u2014 Windows images are **a lot bigger** than Linux images. So be sure to factor this into your sizing.\n\nOne final word on sizing requirements. Docker Swarm and Docker UCP make it extremely easy to add and remove managers and workers. New managers are automagically added to the HA control plane, and new workers are immediately available for workload scheduling. Similarly, removing managers and workers is simple. As long as you have multiple managers, you can remove a manager without impacting cluster operations. With worker nodes, you can drain them and remove them from a running cluster. This all makes UCP very forgiving when it comes to changing your managers and workers.\n\nWith these considerations in mind, we\u2019re ready to install UCP.\n\n##### Installing Docker UCP\n\nIn this section, we\u2019ll walk through the process of installing Docker UCP on the first manager node in a new cluster.\n\n1.  Run the following command from a Linux-based Docker EE node that you want to be the first manager in your UCP cluster.\n\n    A few things to note about the command. The example installs UCP using the `docker/ucp:2.2.5` image, you will want to substitute your desired version. The `--host-address` is the address you will use to access the web UI. For example, if you\u2019re installing in AWS and plan on accessing from your corporate network via the internet, you would enter the AWS public IP.\n\n    The installation is interactive, so you\u2019ll be prompted for further input to complete it.\n\n    ```\n     $ docker container run --rm -it --name ucp \\\n       -v /var/run/docker.sock:/var/run/docker.sock \\\n       docker/ucp:2.2.5 install \\\n       --host-address <node-ip-address> \\\n       --interactive \n    ```\n\n`*   Configure credentials.\n\n    You\u2019ll be prompted to create a username and password for the UCP Admin account. This is a local account, and you should follow your corporate guidelines for choosing the username and password. Be sure you don\u2019t forget it :-D\n\n    *   Subject alternative names (SANs).\n\n    The installer gives you the option to enter a list of alternative IP addresses and names that might be used to access UCP. These can be public and private IP addresses and DNS names, and will be added to the certificates.` \n\n `A few things to note about the install.\n\nUCP leverages Docker Swarm. This means UCP managers have to run on Swarm managers. If you install UCP on a node in *single-engine mode*, it will automatically be switched into *Swarm mode*.\n\nThe installer pulls all of the images for the various UCP services, and starts containers from them. The following listing shows some of them being pulled by the installer.\n\n```\nINFO[0008] Pulling required images... (this may take a while)\nINFO[0008] Pulling docker/ucp-auth-store:2.2.5\nINFO[0013] Pulling docker/ucp-hrm:2.2.5\nINFO[0015] Pulling docker/ucp-metrics:2.2.5\nINFO[0020] Pulling docker/ucp-swarm:2.2.5\nINFO[0023] Pulling docker/ucp-auth:2.2.5\nINFO[0026] Pulling docker/ucp-etcd:2.2.5\nINFO[0028] Pulling docker/ucp-agent:2.2.5\nINFO[0030] Pulling docker/ucp-cfssl:2.2.5\nINFO[0032] Pulling docker/ucp-dsinfo:2.2.5\nINFO[0080] Pulling docker/ucp-controller:2.2.5\nINFO[0084] Pulling docker/ucp-proxy:2.2.5 \n```\n\n `Some of the interesting ones include:\n\n*   `ucp-agent` This is the main UCP agent. It gets deployed to all nodes in the cluster and is in charge of making sure the required UCP containers are up and running.\n*   `ucp-etcd` The cluster\u2019s persistent key-value store.\n*   `ucp-auth` Shared authentication service (also used by DTR for single-sign-on).\n*   `ucp-proxy` Controls access to the local Docker socket so that unauthenticated clients cannot make changes to the cluster.\n*   `ucp-swarm` Provides compatibility with the underlying Swarm.\n\nFinally, the installation creates a couple of root CA\u2019s: one for internal cluster communications, and one for external access. They issue self-signed certs, which are fine for labs and testing, but not production.\n\nTo install UCP with certificates from a trusted CA, you will need a certificate bundle with the following three files:\n\n*   `ca.pem` Certificate of the trusted CA (usually one of your internal corporate CA\u2019s).\n*   `cert.pem` UCP\u2019s public certificate. This needs to contain all IP addresses and DNS names that the cluster will be accessed by \u2014 including any load-balancers that are fronting it.\n*   `key.pem` UCP\u2019s private key.\n\nIf you have these files, you need to mount them into a Docker volume called `ucp-controller-server-certs`, and use the `--external-ca` flag to specify the volume. You can also change the certificates from the `Admin Settings` page of the web UI after the installation.\n\nThe last thing the UCP installer outputs is the URL that you can access it from.\n\n```\n<Snip>\nINFO[0049] Login to UCP at https://<IP or DNS>:443 \n```\n\n `Point a web browser to that address and login. If you\u2019re using self-signed certificates you\u2019ll need to accept the browser warnings. You\u2019ll also need to specify your license file, which can be downloaded from the `My Content` section of the Docker Store.\n\nOnce you\u2019re logged in, you\u2019ll be landed at the UCP Dashboard.\n\n![](images/figure16-5.png)\n\nAt this point, you have a single-node UCP cluster.\n\nYou can add more manager and worker nodes from the `Add Nodes` link at the bottom of the Dashboard.\n\nFigure 16.6 shows the Add Nodes screen. You can choose to add `managers` or `workers`, and it gives you the appropriate command to run on the nodes you want to add. The example shows the command to add a Linux worker node. Notice that the command is a `docker swarm` command.\n\n![](images/figure16-6.png)\n\nAdding a node will join it to the Swarm and configure the required UCP services on it. If you\u2019re adding managers, it\u2019s recommended to wait between each new addition. This gives Docker a chance to download and run the required UCP containers, as well as allow the cluster to register the new manager and achieve quorum.\n\nNewly added managers are automatically configured into the highly-available (HA) Raft consensus group and granted access to the cluster store. Also, although external load-balancers aren\u2019t generally considered core parts of UCP HA, they provide a stable DNS hostname that masks what\u2019s going on behind the scenes \u2014 such as node failures.\n\nYou should configure external load-balancers for *TCP pass-through* on port 443, with a custom HTTPS health check for each UCP manager node at `https://<ucp_manager>/_ping`.\n\nNow that you have a working UCP, you should look at the options that can be configured from the `Admin Settings` page.\n\n![Figure 16.7 UCP Admin Settings](images/figure16-7.png)\n\nFigure 16.7 UCP Admin Settings\n\nThe settings on this page make up the bulk of the configuration data that is backed as part of the UCP backup operation.\n\n###### Controlling access to UCP\n\nAll access to UCP is controlled via the identity management sub-system. This means you need to authenticate with a valid UCP username and password before you can perform any actions on the cluster. This includes cluster administration, as well as deploying and managing services.\n\nWe\u2019ve seen this already with UI \u2014 we had to log in with a username and password. But the same applies to the Docker CLI \u2014 you cannot run unauthenticated commands against UCP from the command line! This is because the local Docker socket on UCP cluster nodes is protected by the `ucp-proxy` service that will not accept unauthorized commands.\n\nLet\u2019s see it.\n\n###### Client bundles\n\nAny node running the Docker CLI is capable of deploying and managing workloads on a UCP cluster, **so long as it presents a valid certificate for a UCP user!**\n\nIn this section we\u2019ll create a new UCP user, create and download a certificate bundle for that user, and configure a Docker client to use the certificates. Once we\u2019re done, we\u2019ll explain how it works.\n\n1.  If you aren\u2019t already, login to UCP as `admin`.\n2.  Click `User Management` > `Users` and then create a new user.\n\n    As we haven\u2019t discussed roles and grants yet, make the user a Docker EE Admin.\n\n3.  With the new user still selected, click the `Configure` drop-down box and choose `Client Bundle`.![](images/figure16-8.png)\n\n4.  Click the `New Client Bundle +` link to generate and download a client bundle for the user.\n\n    At this point, it\u2019s important to note that client bundles are user-specific. The certificates downloaded will enable any properly configured Docker client to execute commands on the UCP cluster under the identity of the user that the bundle belongs to.\n\n5.  Copy the bundle to the Docker client that you want to configure to manage UCP.\n6.  Logon to the client node and perform all of the following commands from that node.\n7.  Unzip the contents of the bundle.\n\n    This example uses the Linux `unzip` package to unzip the contents of the bundle to the current directory. Substitute the name of the bundle to match the one in your environment.\n\n    ```\n     $ unzip ucp-bundle-nigelpoulton.zip\n     Archive:  ucp-bundle-nigelpoulton.zip\n     extracting: ca.pem\n     extracting: cert.pem\n     extracting: key.pem\n     extracting: cert.pub\n     extracting: env.sh\n     extracting: env.ps1\n     extracting: env.cmd \n    ```\n\n     `As the output shows, the bundle contains the required `ca.pem`, `cert.pem`, and `key.pem` files. It also includes scripts that will configure the Docker client to use the certificates.` \n`*   Use the appropriate script to configure the Docker client. `env.sh` works on Linux and Mac, `env.ps1` and `env.cmd` work on Windows.\n\n    You\u2019ll probably need administrator/root privileges to run the scripts.\n\n    The example works on Linux and Mac.\n\n    ```\n     $ eval \"$(<env.sh)\" \n    ```\n\n     `At this point, the client node is fully configured.` `*   Test access.\n\n    ```\n     $ docker version\n\n      <Snip>\n\n     Server:\n      Version:      ucp/2.2.5\n      API version:  1.30 (minimum version 1.20)\n      Go version:   go1.8.3\n      Git commit:   42d28d140\n      Built:        Wed Jan 17 04:44:14 UTC 2018\n      OS/Arch:      linux/amd64\n      Experimental: false \n    ```\n\n     `Notice that the server portion of the output shows the version as `ucp/2.2.5`. This proves the Docker client is successfully talking to the daemon on a UCP node!``` \n\n ``Under-the-hood, the script configures three environment variables:\n\n*   `DOCKER_HOST`\n*   `DOCKER_TLS_VERIFY`\n*   `DOCKER_CERT_PATH`\n\nDOCKER_HOST points the client to the remote Docker daemon on the UCP controller. An example might look like this `DOCKER_HOST=tcp://34.242.196.63:443`. As we can see, access via port 443.\n\nDOCKER_TLS_VERIFY is set to 1, telling the client to use TLS verification in *client mode*.\n\nDOCKER_CERT_PATH tells the Docker client where to find the certificate bundle.\n\nThe net result is all `docker` commands from the client will be signed by the user\u2019s certificate and sent across the network to the remote UCP manager. This is shown in Figure 16.9.\n\n![Figure16.9](images/figure16-9.png)\n\nFigure16.9\n\nLet\u2019s switch tack and see how we backup and recover UCP.\n\n###### Backing up UCP\n\nFirst and foremost, high availability (HA) is not the same as a backup!\n\nConsider the following example. You have a highly available UCP cluster with 5 managers nodes. All manager nodes are healthy and the control plane is replicating. A dissatisfied employee corrupts the cluster (or deletes all user accounts). This *corruption* is automatically replicated to all 5 manager nodes, rendering the cluster broken. There is no way that HA can help you in this situation. What you need, is a backup!\n\nA UCP cluster is made from three major components that need backing up separately:\n\n*   Swarm\n*   UCP\n*   Docker Trusted Registry (DTR)\n\nWe\u2019ll walk you through the process of backing up Swarm and UCP, and we\u2019ll show you how to back up DTR later in the chapter.\n\nAlthough UCP sits on top of Swarm, they are separate components. Swarm holds all of the node membership, networks, volumes, and service definitions. UCP sits on top and maintains its own databases and volumes that hold things such as users, groups, grants, bundles, license files, certificates, and more.\n\nLet\u2019s see how to **backup Swarm**.\n\nSwarm configuration and state is stored in `/var/lib/docker/swarm`. This includes Raft log keys, and it\u2019s replicated to every manager node. A Swarm backup is a copy of all the files in this directory.\n\nBecause it\u2019s replicated to every manager, you can perform the backup from any manager.\n\nYou need to stop Docker on the node that you want to perform the backup on. This means it\u2019s probably not a good idea to perform the backup on the leader manager, as a leader election will be instigated. You should also perform the backup at a quiet time for the business \u2014 even though stopping Docker on a single manager node isn\u2019t a problem in a multi-manager Swarm, it can increase the risk of the cluster losing quorum if another manager fails during the backup.\n\nBefore proceeding, you might want to create a couple of Swarm objects so that you can prove the backup and restore operation work. The example Swarm we\u2019ll be backing up in these examples has an overlay network called `vantage-net` and a Swarm service called `vantage-svc`.\n\n1.  Stop Docker on the Swarm manager node you are performing the backup from.\n\n    This will stop all UCP containers on the node. If UCP is configured for HA, the other managers will make sure the control plane remains available.\n\n    ```\n     $ service docker stop \n    ```\n\n`*   Backup the Swarm config.\n\n    The example uses the Linux `tar` utility to perform the file copy. Feel free to use a different tool.\n\n    ```\n     $ tar -czvf swarm.bkp /var/lib/docker/swarm/\n     tar: Removing leading `/' from member names\n     /var/lib/docker/swarm/\n     /var/lib/docker/swarm/docker-state.json\n     /var/lib/docker/swarm/state.json\n     <Snip> \n    ```\n\n    `*   Verify that the backup file exists.\n\n    ```\n     $ ls -l\n     -rw-r--r-- 1 root   root   450727 Jan 29 14:06 swarm.bkp \n    ```\n\n     `You should rotate, and store the backup file off-site according to your corporate backup policies.` `*   Restart Docker.\n\n    ```\n     $ service docker restart \n    `````` \n\n ```Now that Swarm is backed up, it\u2019s time to **backup UCP**.\n\nA few notes on backing up UCP before we start.\n\nThe UCP backup job runs as a container, so Docker needs to be running for the backup to work.\n\nYou can run the backup from any UCP manager node in the cluster, and you only need to run the operation on one node (UCP replicates its configuration to all manager nodes, so backing up from multiple nodes is not required).\n\nBacking up UCP will stop all UCP containers on the manager that you\u2019re executing the operation on. With this in mind, you should be running a highly available UCP cluster, and you should run the operation at a quiet time for the business.\n\nFinally, user workloads running on the manager node will not be stopped. However, it is not recommended to run user workloads on UCP managers.\n\nLet\u2019s backup UCP.\n\nPerform the following command on a UCP manager node. Docker will need to be running on the node.\n\n```\n$ docker container run --log-driver none --rm -i --name ucp `\\`\n  -v /var/run/docker.sock:/var/run/docker.sock `\\`\n  docker/ucp:2.2.5 backup --interactive `\\`\n  --passphrase `\"Password123\"` > ucp.bkp \n```\n\n `It\u2019s a long command, so let\u2019s step through it.\n\nThe first line is a standard `docker container run` command that tells Docker to run a container with no log driver, to remove it when the operation is complete, and to call it `ucp`. The second line mounts the *Docker socket* into the container so that the container has access to the Docker API to stop containers etc. The third line tells Docker to run a `backup --interactive` command inside of a container based on the `docker/ucp:2.2.5` image. The final line creates an encrypted file called `ucp.bkp` and protects it with a password.\n\nA few points worth noting.\n\nIt\u2019s a good idea to be specific about the version (tag) of the UCP image to use. This example specifies `docker/ucp:2.2.5`. One of the reasons for being specific, is that it\u2019s recommended to run backup and restore operations with the same version of the image. If you don\u2019t explicitly state which image to use, Docker will use the one tagged as `latest`, which might be different between the time you run the backup command and the time you run the restore.\n\nYou should always use the `--passphrase` flag to protect the contents of the backup, and you should definitely use a better password than the one in the example :-D\n\nYou should catalogue and make off-site copies of the backup file according to your corporate backup policies. You should also configure a backup schedule and job verification.\n\nNow that Swarm and UCP are backed up, you can safely recover them in the event of disaster. Speaking of which\u2026.\n\n###### Recovering UCP\n\nWe need to be clear about one thing before we get into the weeds of recovering UCP: Restoring from backup is a last resort, and should only be used when the cluster has been corrupted or all manager nodes have been lost!\n\nYou **do not need to recover from a backup if you\u2019ve lost a single manager in an HA cluster**. In that case, you can easily add a new manager and it\u2019ll join the cluster.\n\nWe\u2019ll show how to recover Swarm from a backup, and then UCP.\n\nPerform the following tasks from the Swarm/UCP manager node that you wish to recover.\n\n1.  Stop Docker.\n\n    ```\n     $ service docker stop \n    ```\n\n`*   Delete any existing Swarm configuration.\n\n    ```\n     $ rm -r /var/lib/docker/swarm \n    ```\n\n    `*   Restore the Swarm configuration from backup.\n\n    In this example, we\u2019ll restore from a zipped `tar` file called `swarm.bkp`. Restoring to the root directory is required with this command as it will include the full path to the original files as part of the extract operation. This may be different in your environment.\n\n    ```\n     $ tar -zxvf swarm.bkp -C / \n    ```\n\n    `*   Initialize a new Swarm cluster.\n\n    Remember, you are not recovering a manager and adding it back to a working cluster. This operation is to recover a failed Swarm that has no surviving managers. The `--force-new-cluster` flag tells Docker to create a new cluster using the configuration stored in `/var/lib/docker/swarm` on the current node.\n\n    ```\n     $ docker swarm init --force-new-cluster\n     Swarm initialized: current node (jhsg...3l9h) is now a manager. \n    ```\n\n    `*   Check that the network and service were recovered as part of the operation.\n\n    ```\n     $ docker network ls\n     NETWORK ID        NAME            DRIVER       SCOPE\n     snkqjy0chtd5      vantage-net     overlay      swarm\n\n     $ docker service ls\n     ID              NAME          MODE         REPLICAS    IMAGE\n     w9dimu8jfrze    vantage-svc   replicated   5/5         alpine:latest \n    ```\n\n     `Congratulations. The Swarm is recovered.` `*   Add new manager and worker nodes to the Swarm, and take a fresh backup.`````\n\n ```With Swarm recovered, you can now **recover UCP.**\n\nIn this example, UCP was backed up to a file called `ucp.bkp` in the current directory. Despite the name of the backup file, it is a Linux tarball.\n\nRun the following commands from the node that you want to recover UCP on. This can be the node that you just recovered Swarm on.\n\n1.  Remove any existing, and potentially corrupted, UCP installations.\n\n    ```\n     $ docker container run --rm -it --name ucp \\\n       -v /var/run/docker.sock:/var/run/docker.sock \\\n       docker/ucp:2.2.5 uninstall-ucp --interactive\n\n     INFO[0000] Your engine version 17.06.2-ee-6, build e75fdb8 is compatible\n     INFO[0000] We're about to uninstall from this swarm cluster.\n     Do you want to proceed with the uninstall? (y/n): y\n     INFO[0000] Uninstalling UCP on each node...\n     INFO[0009] UCP has been removed from this cluster successfully.\n     INFO[0011] Removing UCP Services \n    ```\n\n`*   Restore UCP from the backup.\n\n    ```\n     $ docker container run --rm -i --name ucp \\\n       -v /var/run/docker.sock:/var/run/docker.sock  \\\n       docker/ucp:2.2.5 restore --passphrase \"Password123\" < ucp.bkp\n\n     INFO[0000] Your engine version 17.06.2-ee-6, build e75fdb8 is compatible\n     <Snip>\n     time=\"2018-01-30T10:16:29Z\" level=info msg=\"Parsing backup file\"\n     time=\"2018-01-30T10:16:38Z\" level=info msg=\"Deploying UCP Agent Service\"\n     time=\"2018-01-30T10:17:18Z\" level=info msg=\"Cluster successfully restored. \n    ```\n\n    `*   Log on to the UCP web UI and ensure that the user created earlier is still present (or any other UCP objects that previously existed in your environment).``\n\n ``Congrats. You now know how to backup and recover Docker Swarm and Docker UCP.\n\nLet\u2019s shift our attention to Docker Trusted Registry.\n\n#### Docker Trusted Registry (DTR)\n\nDocker Trusted Registry, which we\u2019re going to refer to as DTR, is a secure, highly available on-premises Docker registry. If you know Docker Hub, think of DTR as a private Docker Hub that you can install on-premises and manage yourself.\n\nIn this section, we\u2019ll show how to install it in an HA configuration, and how to back it up and perform recovery operations. We\u2019ll show how DTR implements advanced features in the next chapter.\n\nLet\u2019s mention a few important things before getting your hands dirty with the installation.\n\nIf possible, you should run your DTR instances on dedicated nodes. You definitely shouldn\u2019t run user workloads on your production DTR nodes.\n\nAs with UCP, you should run an odd number of DTR instances. 3 or 5 is best for fault tolerance. A recommended configuration for a production environment might be:\n\n*   3 dedicated UCP managers\n*   3 dedicated DTR instances\n*   However many worker nodes your application requirements demand\n\nLet\u2019s install and configure a single DTR instance.\n\n##### Install DTR\n\nThe next few steps will walk through the process of configuring the first DTR instance in a UCP cluster.\n\nTo follow along, you\u2019ll need a UCP node that you will install DTR on, and a load balancer configured to listen on port 443 in TCP passthrough mode with a health check configured for `/health` on port 443\\. Figure 16.10 shows a high-level diagram of what we\u2019ll build.\n\nConfiguring a load balancer is beyond the scope of this book, but the diagram shows the important DTR-related configuration requirements.\n\n![Figure 16.10 High level single-instance DTR config.](images/figure16-10.png)\n\nFigure 16.10 High level single-instance DTR config.\n\n1.  Log on to the UCP web UI and click `Admin` > `Admin Settings` > `Docker Trusted Registry`.\n2.  Fill out the DTR configuration form.\n    *   `DTR EXTERNAL URL:` Set this to the URL of your external load balancer.\n    *   `UCP NODE:` Select the name of the node you wish to install DTR on.\n    *   `Disable TLS Verification For UCP:` Check this box if you\u2019re using self-signed certificates.\n3.  Copy the long command at the bottom of the form.\n4.  Paste the command into any UCP manager node.\n\n    The command includes the `--ucp-node` flag telling UCP which node to perform the install on.\n\n    The following is an example DTR install command that matches the configuration in Figure 16.10\\. It assumes that you already have a load balancer configured at `dtr.mydns.com`\n\n    ```\n     $ docker run -it --rm docker/dtr install \\\n       --dtr-external-url dtr.mydns.com \\\n       --ucp-node dtr1  \\\n       --ucp-url https://34.252.195.122 \\\n       --ucp-username admin --ucp-insecure-tls \n    ```\n\n     `You will need to provide the UCP admin password to complete the installation.` \n`*   Once the installation is complete, point your web browser to your load balancer. You will be automatically logged in to DTR.![Figure 16.11 DTR home page](images/figure16-11.png)\n\n    Figure 16.11 DTR home page` \n\n `DTR is ready to use. But it\u2019s not configured for HA.\n\n##### Configure DTR for high availability\n\nConfiguring DTR with multiple replicas for HA requires a shared storage backend. This can be NFS or object storage, and can be on-premises or in the public cloud. We\u2019ll walk through the process of configuring DTR for HA using an Amazon S3 bucket as the shared backend.\n\n1.  Log on to the DTR console and navigate to `Settings`.\n2.  Select the `Storage` tab and configure the shared storage backend.\n\n    Figure 16.12 shows DTR configured to use an AWS S3 bucket called `deep-dive-dtr` in the `eu-west-1` AWS availability zone. You will not be able to use this example.\n\n    ![Figure 16.12 DTR Shared Storage configuration for AWS](images/figure16-12.png)\n\n    Figure 16.12 DTR Shared Storage configuration for AWS\n\nDTR is now configured with a shared storage backend and ready to have additional replicas.\n\n1.  Run the following command from a manager node in the UCP cluster.\n\n    ```\n     $ docker run -it --rm \\\n       docker/dtr:2.4.1 join \\\n       --ucp-node dtr2 \\\n       --existing-replica-id 47f20fb864cf \\\n       --ucp-insecure-tls \n    ```\n\n     `The `--ucp-node` flag tells the command which node to add the new DTR replica on. The `--insecure-tls` flag is required if you\u2019re using self-signed certificates.\n\n    You will need to substitute the version of the image and the replica ID. The replica ID was displayed as part of the output when you installed the initial replica.` \n`*   Enter the UCP URL and port, as well as admin credentials when prompted.`\n\n `When the join is complete, you will see some messages like the following.\n\n```\nINFO[0166] Join is complete\nINFO[0166] Replica ID is set to: a6a628053157\nINFO[0166] There are currently 2 replicas in your DTR cluster\nINFO[0166] You have an even number of replicas which can impact availability\nINFO[0166] It is recommended that you have 3, 5 or 7 replicas in your cluster \n```\n\n `Be sure to follow the advice and install additional replicas so that you operate an odd number.\n\nYou may need to update your load balancer configuration so that it balances traffic across the new replicas.\n\nDTR is now configured for HA. This means you can lose a replica without impacting the availability of the service. Figure 16.13 shows an HA DTR configuration.\n\n![Figure 16.13 DTR HA](images/figure16-13.png)\n\nFigure 16.13 DTR HA\n\nNotice that the external load balancer is sending traffic to all three DTR replicas, as well as performing health checks on all three. All three DTR replicas are also sharing the same external shared storage backend.\n\nIn the diagram, the load balancer and the shared storage backend are 3rd party products and depicted as singletons (not HA). In order to keep the entire environment as highly available as possible, you should ensure they have native HA, and that you back up their contents and configurations as well (e.g. make sure the load balancer and storage systems are natively HA, and perform backups of them).\n\n##### Backup DTR\n\nAs with UCP, DTR has a native `backup` command that is part of the Docker image that was used to install the DTR. This native backup command will backup the DTR configuration that is stored in a set of named volumes, and includes:\n\n*   DTR configuration\n*   Repository metadata\n*   Notary data\n*   Certificates\n\n**Images are not backed up as part of a native DTR backup**. It is expected that images are stored in a highly available storage backend that has its own independent backup schedule using non-Docker tools.\n\nRun the following command from a UCP manager node to perform a DTR backup.\n\n```\n$ `read` -sp `'ucp password: '` UCP_PASSWORD`;` `\\`\n    docker run --log-driver none -i --rm `\\`\n    --env `UCP_PASSWORD``=``$UCP_PASSWORD` `\\`\n    docker/dtr:2.4.1 backup `\\`\n    --ucp-insecure-tls `\\`\n    --ucp-username admin `\\`\n    > ucp.bkp \n```\n\n `Let\u2019s explain what the command is doing.\n\nThe `read` command will prompt you to enter the password for the UCP admin account, and will store it in a variable called `UCP_PASSWORD`. The second line tells Docker to start a new temporary container for the operation. The third line makes the UCP password available inside the container as an environment variable. The fourth line issues the backup command. The fifth line makes it work with self-signed certificates. The sixth line sets the UCP username to \u201cadmin\u201d. The last line directs the backup to a file in the current directory called `ucp.bkp`.\n\nYou will be prompted to enter the UCP URL as well as a replica ID. You can specify these as part of the backup command, I just didn\u2019t want to explain a single command that was 9 lines long!\n\nWhen the backup is finished, you will have a file called `ucp.bkp` in your working directory. This should be picked up by your corporate backup tool and managed in-line with your existing corporate backup policies.\n\n##### Recover DTR from backups\n\nRestoring DTR from backups should be a last resort, and only attempted when the majority of replicas are down and the cluster cannot be recovered any other way. If you have lost a single replica and the majority are still up, you should add a new replica using the `dtr join` command.\n\nIf you are sure you have to restore from backup, the workflow is like this:\n\n1.  Stop and delete DTR on the node (might already be stopped)\n2.  Restore images to the shared storage backend (might not be required)\n3.  Restore DTR\n\nRun the following commands from the node that you want to restore DTR to. This node will obviously need to be a member of the same UCP cluster that the DTR is a member of. You should also use the same version of the `docker/dtr` image that was used to create the backup.\n\n1.  Stop and delete DTR.\n\n    ```\n     $ docker run -it --rm \\\n       docker/dtr:2.4.1 destroy \\\n       --ucp-insecure-tls\n\n     INFO[0000] Beginning Docker Trusted Registry replica destroy\n     ucp-url (The UCP URL including domain and port): https://34.252.195.122:443\n     ucp-username (The UCP administrator username): admin\n     ucp-password:\n     INFO[0020] Validating UCP cert\n     INFO[0020] Connecting to UCP\n     INFO[0021] Searching containers in UCP for DTR replicas\n     INFO[0023] This cluster contains the replicas: 47f20fb864cf a6a628053157\n     Choose a replica to destroy [47f20fb864cf]:\n     INFO[0030] Force removing replica\n     INFO[0030] Stopping containers\n     INFO[0035] Removing containers\n     INFO[0045] Removing volumes\n     INFO[0047] Replica removed. \n    ```\n\n     `You\u2019ll be prompted to enter the UCP URL, admin credentials, and replica ID that you want to delete.\n\n    If you have multiple replicas, you can run the command multiple times to remove them all.` \n`*   If the images were lost from the shared backend, you will need to recover them. This step is beyond the scope of the book as it can be specific to your shared storage backend.*   Restore DTR with the following command.\n\n    You will need to substitute the values on lines 5 and 6 with the values from your environment. Unfortunately the `restore` command cannot be ran interactively, so you cannot be prompted for values once the `restore` has started.\n\n    ```\n     `$` `read` `-sp` `'ucp password: '` `UCP_PASSWORD``;` `\\`\n     `docker` `run` `-i` `--rm` `\\`\n     `--env` `UCP_PASSWORD``=$``UCP_PASSWORD` `\\`\n     `docker``/``dtr``:``2``.``4``.``1` `restore` `\\`\n     `--ucp-url` `<``ENTER_YOUR_ucp-url``>` `\\`\n     `--ucp-node` `<``ENTER_DTR_NODE_hostname``>` `\\`\n     `--ucp-insecure-tls` `\\`\n     `--ucp-username` `admin` `\\`\n     `<` `ucp``.``bkp` \n    ```` \n\n ``DTR is now recovered.\n\nCongratulations. You now know how to backup and recover; Swarm, UCP, and DTR.\n\nTime for one final thing before wrapping up the chapter \u2014 network ports!\n\nUCP managers, workers, and DTR nodes need to be able to communicate over the network. Figure 16.14 summarizes the port requirements.\n\n![Figure 16.14 UCP cluster network port requirements](images/figure16-14.png)\n\nFigure 16.14 UCP cluster network port requirements\n\n### Chapter Summary\n\nDocker Enterprise Edition (EE) is a suite of products that form an \u201c*enterprise friendly*\u201d container-as-a-service platform. It comprises a hardened Docker Engine, an Operations UI, and a secure registry. All of which can be deployed on-premises and managed by the customer. It\u2019s even bundled with a support contract.\n\nDocker Universal Control Plane (UCP) provides a simple-to-use web UI focussed at traditional enterprise Ops teams. It supports native high availability (HA) and has tools to perform backup and restore operations. Once up and running, it provides a whole suite of enterprise-grade features that we\u2019ll discuss in the next chapter.\n\nDocker Trusted Registry (DTR) sits on top of UCP and provides a highly available secure registry. Like UCP, this can be deployed on-premises within the safety of the corporate *\u201cfirewall\u201d*, and provides native tools for backup and recovery.````````", "```````````````"]